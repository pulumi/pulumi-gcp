// Code generated by pulumi-language-go DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package dataproc

import (
	"context"
	"reflect"

	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Dataproc Serverless Batches lets you run Spark workloads without requiring you to
// provision and manage your own Dataproc cluster.
//
// To get more information about Batch, see:
//
// * [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches)
// * How-to Guides
//   - [Dataproc Serverless Batches Intro](https://cloud.google.com/dataproc-serverless/docs/overview)
//
// ## Example Usage
//
// ### Dataproc Batch Spark
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/dataproc"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := dataproc.NewBatch(ctx, "example_batch_spark", &dataproc.BatchArgs{
//				BatchId:  pulumi.String("tf-test-batch_88722"),
//				Location: pulumi.String("us-central1"),
//				Labels: pulumi.StringMap{
//					"batch_test": pulumi.String("terraform"),
//				},
//				RuntimeConfig: &dataproc.BatchRuntimeConfigArgs{
//					Properties: pulumi.StringMap{
//						"spark.dynamicAllocation.enabled": pulumi.String("false"),
//						"spark.executor.instances":        pulumi.String("2"),
//					},
//				},
//				EnvironmentConfig: &dataproc.BatchEnvironmentConfigArgs{
//					ExecutionConfig: &dataproc.BatchEnvironmentConfigExecutionConfigArgs{
//						SubnetworkUri: pulumi.String("default"),
//						Ttl:           pulumi.String("3600s"),
//						NetworkTags: pulumi.StringArray{
//							pulumi.String("tag1"),
//						},
//					},
//				},
//				SparkBatch: &dataproc.BatchSparkBatchArgs{
//					MainClass: pulumi.String("org.apache.spark.examples.SparkPi"),
//					Args: pulumi.StringArray{
//						pulumi.String("10"),
//					},
//					JarFileUris: pulumi.StringArray{
//						pulumi.String("file:///usr/lib/spark/examples/jars/spark-examples.jar"),
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ### Dataproc Batch Spark Full
//
// ```go
// package main
//
// import (
//
//	"fmt"
//
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/dataproc"
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/kms"
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/organizations"
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/storage"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			project, err := organizations.LookupProject(ctx, &organizations.LookupProjectArgs{}, nil)
//			if err != nil {
//				return err
//			}
//			_, err = storage.GetProjectServiceAccount(ctx, &storage.GetProjectServiceAccountArgs{}, nil)
//			if err != nil {
//				return err
//			}
//			bucket, err := storage.NewBucket(ctx, "bucket", &storage.BucketArgs{
//				UniformBucketLevelAccess: pulumi.Bool(true),
//				Name:                     pulumi.String("dataproc-bucket"),
//				Location:                 pulumi.String("US"),
//				ForceDestroy:             pulumi.Bool(true),
//			})
//			if err != nil {
//				return err
//			}
//			cryptoKeyMember1, err := kms.NewCryptoKeyIAMMember(ctx, "crypto_key_member_1", &kms.CryptoKeyIAMMemberArgs{
//				CryptoKeyId: pulumi.String("example-key"),
//				Role:        pulumi.String("roles/cloudkms.cryptoKeyEncrypterDecrypter"),
//				Member:      pulumi.Sprintf("serviceAccount:service-%v@dataproc-accounts.iam.gserviceaccount.com", project.Number),
//			})
//			if err != nil {
//				return err
//			}
//			ms, err := dataproc.NewMetastoreService(ctx, "ms", &dataproc.MetastoreServiceArgs{
//				ServiceId: pulumi.String("dataproc-batch"),
//				Location:  pulumi.String("us-central1"),
//				Port:      pulumi.Int(9080),
//				Tier:      pulumi.String("DEVELOPER"),
//				MaintenanceWindow: &dataproc.MetastoreServiceMaintenanceWindowArgs{
//					HourOfDay: pulumi.Int(2),
//					DayOfWeek: pulumi.String("SUNDAY"),
//				},
//				HiveMetastoreConfig: &dataproc.MetastoreServiceHiveMetastoreConfigArgs{
//					Version: pulumi.String("3.1.2"),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			basic, err := dataproc.NewCluster(ctx, "basic", &dataproc.ClusterArgs{
//				Name:   pulumi.String("dataproc-batch"),
//				Region: pulumi.String("us-central1"),
//				ClusterConfig: &dataproc.ClusterClusterConfigArgs{
//					SoftwareConfig: &dataproc.ClusterClusterConfigSoftwareConfigArgs{
//						OverrideProperties: pulumi.StringMap{
//							"dataproc:dataproc.allow.zero.workers": pulumi.String("true"),
//							"spark:spark.history.fs.logDirectory": bucket.Name.ApplyT(func(name string) (string, error) {
//								return fmt.Sprintf("gs://%v/*/spark-job-history", name), nil
//							}).(pulumi.StringOutput),
//						},
//					},
//					EndpointConfig: &dataproc.ClusterClusterConfigEndpointConfigArgs{
//						EnableHttpPortAccess: pulumi.Bool(true),
//					},
//					MasterConfig: &dataproc.ClusterClusterConfigMasterConfigArgs{
//						NumInstances: pulumi.Int(1),
//						MachineType:  pulumi.String("e2-standard-2"),
//						DiskConfig: &dataproc.ClusterClusterConfigMasterConfigDiskConfigArgs{
//							BootDiskSizeGb: pulumi.Int(35),
//						},
//					},
//					MetastoreConfig: &dataproc.ClusterClusterConfigMetastoreConfigArgs{
//						DataprocMetastoreService: ms.Name,
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			_, err = dataproc.NewBatch(ctx, "example_batch_spark", &dataproc.BatchArgs{
//				BatchId:  pulumi.String("dataproc-batch"),
//				Location: pulumi.String("us-central1"),
//				Labels: pulumi.StringMap{
//					"batch_test": pulumi.String("terraform"),
//				},
//				RuntimeConfig: &dataproc.BatchRuntimeConfigArgs{
//					Properties: pulumi.StringMap{
//						"spark.dynamicAllocation.enabled": pulumi.String("false"),
//						"spark.executor.instances":        pulumi.String("2"),
//					},
//					Version: pulumi.String("2.2"),
//				},
//				EnvironmentConfig: &dataproc.BatchEnvironmentConfigArgs{
//					ExecutionConfig: &dataproc.BatchEnvironmentConfigExecutionConfigArgs{
//						Ttl: pulumi.String("3600s"),
//						NetworkTags: pulumi.StringArray{
//							pulumi.String("tag1"),
//						},
//						KmsKey:         pulumi.String("example-key"),
//						NetworkUri:     pulumi.String("default"),
//						ServiceAccount: pulumi.Sprintf("%v-compute@developer.gserviceaccount.com", project.Number),
//						StagingBucket:  bucket.Name,
//					},
//					PeripheralsConfig: &dataproc.BatchEnvironmentConfigPeripheralsConfigArgs{
//						MetastoreService: ms.Name,
//						SparkHistoryServerConfig: &dataproc.BatchEnvironmentConfigPeripheralsConfigSparkHistoryServerConfigArgs{
//							DataprocCluster: basic.ID(),
//						},
//					},
//				},
//				SparkBatch: &dataproc.BatchSparkBatchArgs{
//					MainClass: pulumi.String("org.apache.spark.examples.SparkPi"),
//					Args: pulumi.StringArray{
//						pulumi.String("10"),
//					},
//					JarFileUris: pulumi.StringArray{
//						pulumi.String("file:///usr/lib/spark/examples/jars/spark-examples.jar"),
//					},
//				},
//			}, pulumi.DependsOn([]pulumi.Resource{
//				cryptoKeyMember1,
//			}))
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ### Dataproc Batch Sparksql
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/dataproc"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := dataproc.NewBatch(ctx, "example_batch_sparsql", &dataproc.BatchArgs{
//				BatchId:  pulumi.String("tf-test-batch_39249"),
//				Location: pulumi.String("us-central1"),
//				RuntimeConfig: &dataproc.BatchRuntimeConfigArgs{
//					Properties: pulumi.StringMap{
//						"spark.dynamicAllocation.enabled": pulumi.String("false"),
//						"spark.executor.instances":        pulumi.String("2"),
//					},
//				},
//				EnvironmentConfig: &dataproc.BatchEnvironmentConfigArgs{
//					ExecutionConfig: &dataproc.BatchEnvironmentConfigExecutionConfigArgs{
//						SubnetworkUri: pulumi.String("default"),
//					},
//				},
//				SparkSqlBatch: &dataproc.BatchSparkSqlBatchArgs{
//					QueryFileUri: pulumi.String("gs://dataproc-examples/spark-sql/natality/cigarette_correlations.sql"),
//					JarFileUris: pulumi.StringArray{
//						pulumi.String("file:///usr/lib/spark/examples/jars/spark-examples.jar"),
//					},
//					QueryVariables: pulumi.StringMap{
//						"name": pulumi.String("value"),
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ### Dataproc Batch Pyspark
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/dataproc"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := dataproc.NewBatch(ctx, "example_batch_pyspark", &dataproc.BatchArgs{
//				BatchId:  pulumi.String("tf-test-batch_74391"),
//				Location: pulumi.String("us-central1"),
//				RuntimeConfig: &dataproc.BatchRuntimeConfigArgs{
//					Properties: pulumi.StringMap{
//						"spark.dynamicAllocation.enabled": pulumi.String("false"),
//						"spark.executor.instances":        pulumi.String("2"),
//					},
//				},
//				EnvironmentConfig: &dataproc.BatchEnvironmentConfigArgs{
//					ExecutionConfig: &dataproc.BatchEnvironmentConfigExecutionConfigArgs{
//						SubnetworkUri: pulumi.String("default"),
//					},
//				},
//				PysparkBatch: &dataproc.BatchPysparkBatchArgs{
//					MainPythonFileUri: pulumi.String("https://storage.googleapis.com/terraform-batches/test_util.py"),
//					Args: pulumi.StringArray{
//						pulumi.String("10"),
//					},
//					JarFileUris: pulumi.StringArray{
//						pulumi.String("file:///usr/lib/spark/examples/jars/spark-examples.jar"),
//					},
//					PythonFileUris: pulumi.StringArray{
//						pulumi.String("gs://dataproc-examples/pyspark/hello-world/hello-world.py"),
//					},
//					ArchiveUris: pulumi.StringArray{
//						pulumi.String("https://storage.googleapis.com/terraform-batches/animals.txt.tar.gz#unpacked"),
//						pulumi.String("https://storage.googleapis.com/terraform-batches/animals.txt.jar"),
//						pulumi.String("https://storage.googleapis.com/terraform-batches/animals.txt"),
//					},
//					FileUris: pulumi.StringArray{
//						pulumi.String("https://storage.googleapis.com/terraform-batches/people.txt"),
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ### Dataproc Batch Sparkr
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/dataproc"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := dataproc.NewBatch(ctx, "example_batch_sparkr", &dataproc.BatchArgs{
//				BatchId:  pulumi.String("tf-test-batch_16511"),
//				Location: pulumi.String("us-central1"),
//				Labels: pulumi.StringMap{
//					"batch_test": pulumi.String("terraform"),
//				},
//				RuntimeConfig: &dataproc.BatchRuntimeConfigArgs{
//					Properties: pulumi.StringMap{
//						"spark.dynamicAllocation.enabled": pulumi.String("false"),
//						"spark.executor.instances":        pulumi.String("2"),
//					},
//				},
//				EnvironmentConfig: &dataproc.BatchEnvironmentConfigArgs{
//					ExecutionConfig: &dataproc.BatchEnvironmentConfigExecutionConfigArgs{
//						SubnetworkUri: pulumi.String("default"),
//						Ttl:           pulumi.String("3600s"),
//						NetworkTags: pulumi.StringArray{
//							pulumi.String("tag1"),
//						},
//					},
//				},
//				SparkRBatch: &dataproc.BatchSparkRBatchArgs{
//					MainRFileUri: pulumi.String("https://storage.googleapis.com/terraform-batches/spark-r-flights.r"),
//					Args: pulumi.StringArray{
//						pulumi.String("https://storage.googleapis.com/terraform-batches/flights.csv"),
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ### Dataproc Batch Autotuning
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-gcp/sdk/v8/go/gcp/dataproc"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := dataproc.NewBatch(ctx, "example_batch_autotuning", &dataproc.BatchArgs{
//				BatchId:  pulumi.String("tf-test-batch_8493"),
//				Location: pulumi.String("us-central1"),
//				Labels: pulumi.StringMap{
//					"batch_test": pulumi.String("terraform"),
//				},
//				RuntimeConfig: &dataproc.BatchRuntimeConfigArgs{
//					Version: pulumi.String("2.2"),
//					Properties: pulumi.StringMap{
//						"spark.dynamicAllocation.enabled": pulumi.String("false"),
//						"spark.executor.instances":        pulumi.String("2"),
//					},
//					Cohort: pulumi.String("tf-dataproc-batch-example"),
//					AutotuningConfig: &dataproc.BatchRuntimeConfigAutotuningConfigArgs{
//						Scenarios: pulumi.StringArray{
//							pulumi.String("SCALING"),
//							pulumi.String("MEMORY"),
//						},
//					},
//				},
//				EnvironmentConfig: &dataproc.BatchEnvironmentConfigArgs{
//					ExecutionConfig: &dataproc.BatchEnvironmentConfigExecutionConfigArgs{
//						SubnetworkUri: pulumi.String("default"),
//						Ttl:           pulumi.String("3600s"),
//					},
//				},
//				SparkBatch: &dataproc.BatchSparkBatchArgs{
//					MainClass: pulumi.String("org.apache.spark.examples.SparkPi"),
//					Args: pulumi.StringArray{
//						pulumi.String("10"),
//					},
//					JarFileUris: pulumi.StringArray{
//						pulumi.String("file:///usr/lib/spark/examples/jars/spark-examples.jar"),
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Import
//
// Batch can be imported using any of these accepted formats:
//
// * `projects/{{project}}/locations/{{location}}/batches/{{batch_id}}`
//
// * `{{project}}/{{location}}/{{batch_id}}`
//
// * `{{location}}/{{batch_id}}`
//
// When using the `pulumi import` command, Batch can be imported using one of the formats above. For example:
//
// ```sh
// $ pulumi import gcp:dataproc/batch:Batch default projects/{{project}}/locations/{{location}}/batches/{{batch_id}}
// ```
//
// ```sh
// $ pulumi import gcp:dataproc/batch:Batch default {{project}}/{{location}}/{{batch_id}}
// ```
//
// ```sh
// $ pulumi import gcp:dataproc/batch:Batch default {{location}}/{{batch_id}}
// ```
type Batch struct {
	pulumi.CustomResourceState

	// The ID to use for the batch, which will become the final component of the batch's resource name.
	// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
	BatchId pulumi.StringPtrOutput `pulumi:"batchId"`
	// The time when the batch was created.
	CreateTime pulumi.StringOutput `pulumi:"createTime"`
	// The email address of the user who created the batch.
	Creator pulumi.StringOutput `pulumi:"creator"`
	// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
	EffectiveLabels pulumi.StringMapOutput `pulumi:"effectiveLabels"`
	// Environment configuration for the batch execution.
	// Structure is documented below.
	EnvironmentConfig BatchEnvironmentConfigPtrOutput `pulumi:"environmentConfig"`
	// The labels to associate with this batch.
	//
	// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field `effectiveLabels` for all of the labels present on the resource.
	Labels pulumi.StringMapOutput `pulumi:"labels"`
	// The location in which the batch will be created in.
	Location pulumi.StringPtrOutput `pulumi:"location"`
	// The resource name of the batch.
	Name pulumi.StringOutput `pulumi:"name"`
	// The resource name of the operation associated with this batch.
	Operation pulumi.StringOutput `pulumi:"operation"`
	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project pulumi.StringOutput `pulumi:"project"`
	// The combination of labels configured directly on the resource
	// and default labels configured on the provider.
	PulumiLabels pulumi.StringMapOutput `pulumi:"pulumiLabels"`
	// PySpark batch config.
	// Structure is documented below.
	PysparkBatch BatchPysparkBatchPtrOutput `pulumi:"pysparkBatch"`
	// Runtime configuration for the batch execution.
	// Structure is documented below.
	RuntimeConfig BatchRuntimeConfigPtrOutput `pulumi:"runtimeConfig"`
	// Runtime information about batch execution.
	// Structure is documented below.
	RuntimeInfos BatchRuntimeInfoArrayOutput `pulumi:"runtimeInfos"`
	// Spark batch config.
	// Structure is documented below.
	SparkBatch BatchSparkBatchPtrOutput `pulumi:"sparkBatch"`
	// SparkR batch config.
	// Structure is documented below.
	SparkRBatch BatchSparkRBatchPtrOutput `pulumi:"sparkRBatch"`
	// Spark SQL batch config.
	// Structure is documented below.
	SparkSqlBatch BatchSparkSqlBatchPtrOutput `pulumi:"sparkSqlBatch"`
	// (Output)
	// The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
	State pulumi.StringOutput `pulumi:"state"`
	// Historical state information for the batch.
	// Structure is documented below.
	StateHistories BatchStateHistoryArrayOutput `pulumi:"stateHistories"`
	// (Output)
	// Details about the state at this point in history.
	StateMessage pulumi.StringOutput `pulumi:"stateMessage"`
	// Batch state details, such as a failure description if the state is FAILED.
	StateTime pulumi.StringOutput `pulumi:"stateTime"`
	// A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
	Uuid pulumi.StringOutput `pulumi:"uuid"`
}

// NewBatch registers a new resource with the given unique name, arguments, and options.
func NewBatch(ctx *pulumi.Context,
	name string, args *BatchArgs, opts ...pulumi.ResourceOption) (*Batch, error) {
	if args == nil {
		args = &BatchArgs{}
	}

	secrets := pulumi.AdditionalSecretOutputs([]string{
		"effectiveLabels",
		"pulumiLabels",
	})
	opts = append(opts, secrets)
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource Batch
	err := ctx.RegisterResource("gcp:dataproc/batch:Batch", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetBatch gets an existing Batch resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetBatch(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *BatchState, opts ...pulumi.ResourceOption) (*Batch, error) {
	var resource Batch
	err := ctx.ReadResource("gcp:dataproc/batch:Batch", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering Batch resources.
type batchState struct {
	// The ID to use for the batch, which will become the final component of the batch's resource name.
	// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
	BatchId *string `pulumi:"batchId"`
	// The time when the batch was created.
	CreateTime *string `pulumi:"createTime"`
	// The email address of the user who created the batch.
	Creator *string `pulumi:"creator"`
	// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
	EffectiveLabels map[string]string `pulumi:"effectiveLabels"`
	// Environment configuration for the batch execution.
	// Structure is documented below.
	EnvironmentConfig *BatchEnvironmentConfig `pulumi:"environmentConfig"`
	// The labels to associate with this batch.
	//
	// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field `effectiveLabels` for all of the labels present on the resource.
	Labels map[string]string `pulumi:"labels"`
	// The location in which the batch will be created in.
	Location *string `pulumi:"location"`
	// The resource name of the batch.
	Name *string `pulumi:"name"`
	// The resource name of the operation associated with this batch.
	Operation *string `pulumi:"operation"`
	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project *string `pulumi:"project"`
	// The combination of labels configured directly on the resource
	// and default labels configured on the provider.
	PulumiLabels map[string]string `pulumi:"pulumiLabels"`
	// PySpark batch config.
	// Structure is documented below.
	PysparkBatch *BatchPysparkBatch `pulumi:"pysparkBatch"`
	// Runtime configuration for the batch execution.
	// Structure is documented below.
	RuntimeConfig *BatchRuntimeConfig `pulumi:"runtimeConfig"`
	// Runtime information about batch execution.
	// Structure is documented below.
	RuntimeInfos []BatchRuntimeInfo `pulumi:"runtimeInfos"`
	// Spark batch config.
	// Structure is documented below.
	SparkBatch *BatchSparkBatch `pulumi:"sparkBatch"`
	// SparkR batch config.
	// Structure is documented below.
	SparkRBatch *BatchSparkRBatch `pulumi:"sparkRBatch"`
	// Spark SQL batch config.
	// Structure is documented below.
	SparkSqlBatch *BatchSparkSqlBatch `pulumi:"sparkSqlBatch"`
	// (Output)
	// The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
	State *string `pulumi:"state"`
	// Historical state information for the batch.
	// Structure is documented below.
	StateHistories []BatchStateHistory `pulumi:"stateHistories"`
	// (Output)
	// Details about the state at this point in history.
	StateMessage *string `pulumi:"stateMessage"`
	// Batch state details, such as a failure description if the state is FAILED.
	StateTime *string `pulumi:"stateTime"`
	// A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
	Uuid *string `pulumi:"uuid"`
}

type BatchState struct {
	// The ID to use for the batch, which will become the final component of the batch's resource name.
	// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
	BatchId pulumi.StringPtrInput
	// The time when the batch was created.
	CreateTime pulumi.StringPtrInput
	// The email address of the user who created the batch.
	Creator pulumi.StringPtrInput
	// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
	EffectiveLabels pulumi.StringMapInput
	// Environment configuration for the batch execution.
	// Structure is documented below.
	EnvironmentConfig BatchEnvironmentConfigPtrInput
	// The labels to associate with this batch.
	//
	// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field `effectiveLabels` for all of the labels present on the resource.
	Labels pulumi.StringMapInput
	// The location in which the batch will be created in.
	Location pulumi.StringPtrInput
	// The resource name of the batch.
	Name pulumi.StringPtrInput
	// The resource name of the operation associated with this batch.
	Operation pulumi.StringPtrInput
	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project pulumi.StringPtrInput
	// The combination of labels configured directly on the resource
	// and default labels configured on the provider.
	PulumiLabels pulumi.StringMapInput
	// PySpark batch config.
	// Structure is documented below.
	PysparkBatch BatchPysparkBatchPtrInput
	// Runtime configuration for the batch execution.
	// Structure is documented below.
	RuntimeConfig BatchRuntimeConfigPtrInput
	// Runtime information about batch execution.
	// Structure is documented below.
	RuntimeInfos BatchRuntimeInfoArrayInput
	// Spark batch config.
	// Structure is documented below.
	SparkBatch BatchSparkBatchPtrInput
	// SparkR batch config.
	// Structure is documented below.
	SparkRBatch BatchSparkRBatchPtrInput
	// Spark SQL batch config.
	// Structure is documented below.
	SparkSqlBatch BatchSparkSqlBatchPtrInput
	// (Output)
	// The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
	State pulumi.StringPtrInput
	// Historical state information for the batch.
	// Structure is documented below.
	StateHistories BatchStateHistoryArrayInput
	// (Output)
	// Details about the state at this point in history.
	StateMessage pulumi.StringPtrInput
	// Batch state details, such as a failure description if the state is FAILED.
	StateTime pulumi.StringPtrInput
	// A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
	Uuid pulumi.StringPtrInput
}

func (BatchState) ElementType() reflect.Type {
	return reflect.TypeOf((*batchState)(nil)).Elem()
}

type batchArgs struct {
	// The ID to use for the batch, which will become the final component of the batch's resource name.
	// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
	BatchId *string `pulumi:"batchId"`
	// Environment configuration for the batch execution.
	// Structure is documented below.
	EnvironmentConfig *BatchEnvironmentConfig `pulumi:"environmentConfig"`
	// The labels to associate with this batch.
	//
	// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field `effectiveLabels` for all of the labels present on the resource.
	Labels map[string]string `pulumi:"labels"`
	// The location in which the batch will be created in.
	Location *string `pulumi:"location"`
	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project *string `pulumi:"project"`
	// PySpark batch config.
	// Structure is documented below.
	PysparkBatch *BatchPysparkBatch `pulumi:"pysparkBatch"`
	// Runtime configuration for the batch execution.
	// Structure is documented below.
	RuntimeConfig *BatchRuntimeConfig `pulumi:"runtimeConfig"`
	// Spark batch config.
	// Structure is documented below.
	SparkBatch *BatchSparkBatch `pulumi:"sparkBatch"`
	// SparkR batch config.
	// Structure is documented below.
	SparkRBatch *BatchSparkRBatch `pulumi:"sparkRBatch"`
	// Spark SQL batch config.
	// Structure is documented below.
	SparkSqlBatch *BatchSparkSqlBatch `pulumi:"sparkSqlBatch"`
}

// The set of arguments for constructing a Batch resource.
type BatchArgs struct {
	// The ID to use for the batch, which will become the final component of the batch's resource name.
	// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
	BatchId pulumi.StringPtrInput
	// Environment configuration for the batch execution.
	// Structure is documented below.
	EnvironmentConfig BatchEnvironmentConfigPtrInput
	// The labels to associate with this batch.
	//
	// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field `effectiveLabels` for all of the labels present on the resource.
	Labels pulumi.StringMapInput
	// The location in which the batch will be created in.
	Location pulumi.StringPtrInput
	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project pulumi.StringPtrInput
	// PySpark batch config.
	// Structure is documented below.
	PysparkBatch BatchPysparkBatchPtrInput
	// Runtime configuration for the batch execution.
	// Structure is documented below.
	RuntimeConfig BatchRuntimeConfigPtrInput
	// Spark batch config.
	// Structure is documented below.
	SparkBatch BatchSparkBatchPtrInput
	// SparkR batch config.
	// Structure is documented below.
	SparkRBatch BatchSparkRBatchPtrInput
	// Spark SQL batch config.
	// Structure is documented below.
	SparkSqlBatch BatchSparkSqlBatchPtrInput
}

func (BatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*batchArgs)(nil)).Elem()
}

type BatchInput interface {
	pulumi.Input

	ToBatchOutput() BatchOutput
	ToBatchOutputWithContext(ctx context.Context) BatchOutput
}

func (*Batch) ElementType() reflect.Type {
	return reflect.TypeOf((**Batch)(nil)).Elem()
}

func (i *Batch) ToBatchOutput() BatchOutput {
	return i.ToBatchOutputWithContext(context.Background())
}

func (i *Batch) ToBatchOutputWithContext(ctx context.Context) BatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BatchOutput)
}

// BatchArrayInput is an input type that accepts BatchArray and BatchArrayOutput values.
// You can construct a concrete instance of `BatchArrayInput` via:
//
//	BatchArray{ BatchArgs{...} }
type BatchArrayInput interface {
	pulumi.Input

	ToBatchArrayOutput() BatchArrayOutput
	ToBatchArrayOutputWithContext(context.Context) BatchArrayOutput
}

type BatchArray []BatchInput

func (BatchArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*Batch)(nil)).Elem()
}

func (i BatchArray) ToBatchArrayOutput() BatchArrayOutput {
	return i.ToBatchArrayOutputWithContext(context.Background())
}

func (i BatchArray) ToBatchArrayOutputWithContext(ctx context.Context) BatchArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BatchArrayOutput)
}

// BatchMapInput is an input type that accepts BatchMap and BatchMapOutput values.
// You can construct a concrete instance of `BatchMapInput` via:
//
//	BatchMap{ "key": BatchArgs{...} }
type BatchMapInput interface {
	pulumi.Input

	ToBatchMapOutput() BatchMapOutput
	ToBatchMapOutputWithContext(context.Context) BatchMapOutput
}

type BatchMap map[string]BatchInput

func (BatchMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*Batch)(nil)).Elem()
}

func (i BatchMap) ToBatchMapOutput() BatchMapOutput {
	return i.ToBatchMapOutputWithContext(context.Background())
}

func (i BatchMap) ToBatchMapOutputWithContext(ctx context.Context) BatchMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BatchMapOutput)
}

type BatchOutput struct{ *pulumi.OutputState }

func (BatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**Batch)(nil)).Elem()
}

func (o BatchOutput) ToBatchOutput() BatchOutput {
	return o
}

func (o BatchOutput) ToBatchOutputWithContext(ctx context.Context) BatchOutput {
	return o
}

// The ID to use for the batch, which will become the final component of the batch's resource name.
// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
func (o BatchOutput) BatchId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringPtrOutput { return v.BatchId }).(pulumi.StringPtrOutput)
}

// The time when the batch was created.
func (o BatchOutput) CreateTime() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.CreateTime }).(pulumi.StringOutput)
}

// The email address of the user who created the batch.
func (o BatchOutput) Creator() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.Creator }).(pulumi.StringOutput)
}

// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
func (o BatchOutput) EffectiveLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringMapOutput { return v.EffectiveLabels }).(pulumi.StringMapOutput)
}

// Environment configuration for the batch execution.
// Structure is documented below.
func (o BatchOutput) EnvironmentConfig() BatchEnvironmentConfigPtrOutput {
	return o.ApplyT(func(v *Batch) BatchEnvironmentConfigPtrOutput { return v.EnvironmentConfig }).(BatchEnvironmentConfigPtrOutput)
}

// The labels to associate with this batch.
//
// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
// Please refer to the field `effectiveLabels` for all of the labels present on the resource.
func (o BatchOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringMapOutput { return v.Labels }).(pulumi.StringMapOutput)
}

// The location in which the batch will be created in.
func (o BatchOutput) Location() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringPtrOutput { return v.Location }).(pulumi.StringPtrOutput)
}

// The resource name of the batch.
func (o BatchOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

// The resource name of the operation associated with this batch.
func (o BatchOutput) Operation() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.Operation }).(pulumi.StringOutput)
}

// The ID of the project in which the resource belongs.
// If it is not provided, the provider project is used.
func (o BatchOutput) Project() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.Project }).(pulumi.StringOutput)
}

// The combination of labels configured directly on the resource
// and default labels configured on the provider.
func (o BatchOutput) PulumiLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringMapOutput { return v.PulumiLabels }).(pulumi.StringMapOutput)
}

// PySpark batch config.
// Structure is documented below.
func (o BatchOutput) PysparkBatch() BatchPysparkBatchPtrOutput {
	return o.ApplyT(func(v *Batch) BatchPysparkBatchPtrOutput { return v.PysparkBatch }).(BatchPysparkBatchPtrOutput)
}

// Runtime configuration for the batch execution.
// Structure is documented below.
func (o BatchOutput) RuntimeConfig() BatchRuntimeConfigPtrOutput {
	return o.ApplyT(func(v *Batch) BatchRuntimeConfigPtrOutput { return v.RuntimeConfig }).(BatchRuntimeConfigPtrOutput)
}

// Runtime information about batch execution.
// Structure is documented below.
func (o BatchOutput) RuntimeInfos() BatchRuntimeInfoArrayOutput {
	return o.ApplyT(func(v *Batch) BatchRuntimeInfoArrayOutput { return v.RuntimeInfos }).(BatchRuntimeInfoArrayOutput)
}

// Spark batch config.
// Structure is documented below.
func (o BatchOutput) SparkBatch() BatchSparkBatchPtrOutput {
	return o.ApplyT(func(v *Batch) BatchSparkBatchPtrOutput { return v.SparkBatch }).(BatchSparkBatchPtrOutput)
}

// SparkR batch config.
// Structure is documented below.
func (o BatchOutput) SparkRBatch() BatchSparkRBatchPtrOutput {
	return o.ApplyT(func(v *Batch) BatchSparkRBatchPtrOutput { return v.SparkRBatch }).(BatchSparkRBatchPtrOutput)
}

// Spark SQL batch config.
// Structure is documented below.
func (o BatchOutput) SparkSqlBatch() BatchSparkSqlBatchPtrOutput {
	return o.ApplyT(func(v *Batch) BatchSparkSqlBatchPtrOutput { return v.SparkSqlBatch }).(BatchSparkSqlBatchPtrOutput)
}

// (Output)
// The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
func (o BatchOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.State }).(pulumi.StringOutput)
}

// Historical state information for the batch.
// Structure is documented below.
func (o BatchOutput) StateHistories() BatchStateHistoryArrayOutput {
	return o.ApplyT(func(v *Batch) BatchStateHistoryArrayOutput { return v.StateHistories }).(BatchStateHistoryArrayOutput)
}

// (Output)
// Details about the state at this point in history.
func (o BatchOutput) StateMessage() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.StateMessage }).(pulumi.StringOutput)
}

// Batch state details, such as a failure description if the state is FAILED.
func (o BatchOutput) StateTime() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.StateTime }).(pulumi.StringOutput)
}

// A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
func (o BatchOutput) Uuid() pulumi.StringOutput {
	return o.ApplyT(func(v *Batch) pulumi.StringOutput { return v.Uuid }).(pulumi.StringOutput)
}

type BatchArrayOutput struct{ *pulumi.OutputState }

func (BatchArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*Batch)(nil)).Elem()
}

func (o BatchArrayOutput) ToBatchArrayOutput() BatchArrayOutput {
	return o
}

func (o BatchArrayOutput) ToBatchArrayOutputWithContext(ctx context.Context) BatchArrayOutput {
	return o
}

func (o BatchArrayOutput) Index(i pulumi.IntInput) BatchOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *Batch {
		return vs[0].([]*Batch)[vs[1].(int)]
	}).(BatchOutput)
}

type BatchMapOutput struct{ *pulumi.OutputState }

func (BatchMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*Batch)(nil)).Elem()
}

func (o BatchMapOutput) ToBatchMapOutput() BatchMapOutput {
	return o
}

func (o BatchMapOutput) ToBatchMapOutputWithContext(ctx context.Context) BatchMapOutput {
	return o
}

func (o BatchMapOutput) MapIndex(k pulumi.StringInput) BatchOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *Batch {
		return vs[0].(map[string]*Batch)[vs[1].(string)]
	}).(BatchOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*BatchInput)(nil)).Elem(), &Batch{})
	pulumi.RegisterInputType(reflect.TypeOf((*BatchArrayInput)(nil)).Elem(), BatchArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*BatchMapInput)(nil)).Elem(), BatchMap{})
	pulumi.RegisterOutputType(BatchOutput{})
	pulumi.RegisterOutputType(BatchArrayOutput{})
	pulumi.RegisterOutputType(BatchMapOutput{})
}

// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package container

import (
	"context"
	"reflect"

	"github.com/pkg/errors"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// ## Import
//
// Node pools can be imported using the `project`, `location`, `cluster` and `name`. If the project is omitted, the project value in the provider configuration will be used. Examples
//
// ```sh
//
//	$ pulumi import gcp:container/nodePool:NodePool mainpool my-gcp-project/us-east1-a/my-cluster/main-pool
//
// ```
//
// ```sh
//
//	$ pulumi import gcp:container/nodePool:NodePool mainpool us-east1/my-cluster/main-pool
//
// ```
type NodePool struct {
	pulumi.CustomResourceState

	// Configuration required by cluster autoscaler to adjust
	// the size of the node pool to the current cluster usage. Structure is documented below.
	Autoscaling NodePoolAutoscalingPtrOutput `pulumi:"autoscaling"`
	// The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
	Cluster pulumi.StringOutput `pulumi:"cluster"`
	// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
	// Changing this will force recreation of the resource.
	InitialNodeCount pulumi.IntOutput `pulumi:"initialNodeCount"`
	// The resource URLs of the managed instance groups associated with this node pool.
	InstanceGroupUrls pulumi.StringArrayOutput `pulumi:"instanceGroupUrls"`
	// The location (region or zone) of the cluster.
	Location pulumi.StringOutput `pulumi:"location"`
	// List of instance group URLs which have been assigned to this node pool.
	ManagedInstanceGroupUrls pulumi.StringArrayOutput `pulumi:"managedInstanceGroupUrls"`
	// Node management configuration, wherein auto-repair and
	// auto-upgrade is configured. Structure is documented below.
	Management NodePoolManagementOutput `pulumi:"management"`
	// The maximum number of pods per node in this node pool.
	// Note that this does not work on node pools which are "route-based" - that is, node
	// pools belonging to clusters that do not have IP Aliasing enabled.
	// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
	// for more information.
	MaxPodsPerNode pulumi.IntOutput `pulumi:"maxPodsPerNode"`
	// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
	Name pulumi.StringOutput `pulumi:"name"`
	// Creates a unique name for the node pool beginning
	// with the specified prefix. Conflicts with `name`.
	NamePrefix pulumi.StringOutput `pulumi:"namePrefix"`
	// The network configuration of the pool. Such as
	// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
	// documented below
	NetworkConfig NodePoolNetworkConfigOutput `pulumi:"networkConfig"`
	// Parameters used in creating the node pool. See
	// container.Cluster for schema.
	NodeConfig NodePoolNodeConfigOutput `pulumi:"nodeConfig"`
	// The number of nodes per instance group. This field can be used to
	// update the number of nodes per instance group but should not be used alongside `autoscaling`.
	NodeCount pulumi.IntOutput `pulumi:"nodeCount"`
	// The list of zones in which the node pool's nodes should be located. Nodes must
	// be in the region of their regional cluster or in the same region as their
	// cluster's zone for zonal clusters. If unspecified, the cluster-level
	// `nodeLocations` will be used.
	NodeLocations pulumi.StringArrayOutput `pulumi:"nodeLocations"`
	Operation     pulumi.StringOutput      `pulumi:"operation"`
	// Specifies a custom placement policy for the
	// nodes.
	PlacementPolicy NodePoolPlacementPolicyPtrOutput `pulumi:"placementPolicy"`
	// The ID of the project in which to create the node pool. If blank,
	// the provider-configured project will be used.
	Project pulumi.StringOutput `pulumi:"project"`
	// Specify node upgrade settings to change how GKE upgrades nodes.
	// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
	UpgradeSettings NodePoolUpgradeSettingsOutput `pulumi:"upgradeSettings"`
	// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
	// will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
	// can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
	// versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
	// versions in a Terraform-compatible way.
	Version pulumi.StringOutput `pulumi:"version"`
}

// NewNodePool registers a new resource with the given unique name, arguments, and options.
func NewNodePool(ctx *pulumi.Context,
	name string, args *NodePoolArgs, opts ...pulumi.ResourceOption) (*NodePool, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.Cluster == nil {
		return nil, errors.New("invalid value for required argument 'Cluster'")
	}
	var resource NodePool
	err := ctx.RegisterResource("gcp:container/nodePool:NodePool", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetNodePool gets an existing NodePool resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetNodePool(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *NodePoolState, opts ...pulumi.ResourceOption) (*NodePool, error) {
	var resource NodePool
	err := ctx.ReadResource("gcp:container/nodePool:NodePool", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering NodePool resources.
type nodePoolState struct {
	// Configuration required by cluster autoscaler to adjust
	// the size of the node pool to the current cluster usage. Structure is documented below.
	Autoscaling *NodePoolAutoscaling `pulumi:"autoscaling"`
	// The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
	Cluster *string `pulumi:"cluster"`
	// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
	// Changing this will force recreation of the resource.
	InitialNodeCount *int `pulumi:"initialNodeCount"`
	// The resource URLs of the managed instance groups associated with this node pool.
	InstanceGroupUrls []string `pulumi:"instanceGroupUrls"`
	// The location (region or zone) of the cluster.
	Location *string `pulumi:"location"`
	// List of instance group URLs which have been assigned to this node pool.
	ManagedInstanceGroupUrls []string `pulumi:"managedInstanceGroupUrls"`
	// Node management configuration, wherein auto-repair and
	// auto-upgrade is configured. Structure is documented below.
	Management *NodePoolManagement `pulumi:"management"`
	// The maximum number of pods per node in this node pool.
	// Note that this does not work on node pools which are "route-based" - that is, node
	// pools belonging to clusters that do not have IP Aliasing enabled.
	// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
	// for more information.
	MaxPodsPerNode *int `pulumi:"maxPodsPerNode"`
	// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
	Name *string `pulumi:"name"`
	// Creates a unique name for the node pool beginning
	// with the specified prefix. Conflicts with `name`.
	NamePrefix *string `pulumi:"namePrefix"`
	// The network configuration of the pool. Such as
	// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
	// documented below
	NetworkConfig *NodePoolNetworkConfig `pulumi:"networkConfig"`
	// Parameters used in creating the node pool. See
	// container.Cluster for schema.
	NodeConfig *NodePoolNodeConfig `pulumi:"nodeConfig"`
	// The number of nodes per instance group. This field can be used to
	// update the number of nodes per instance group but should not be used alongside `autoscaling`.
	NodeCount *int `pulumi:"nodeCount"`
	// The list of zones in which the node pool's nodes should be located. Nodes must
	// be in the region of their regional cluster or in the same region as their
	// cluster's zone for zonal clusters. If unspecified, the cluster-level
	// `nodeLocations` will be used.
	NodeLocations []string `pulumi:"nodeLocations"`
	Operation     *string  `pulumi:"operation"`
	// Specifies a custom placement policy for the
	// nodes.
	PlacementPolicy *NodePoolPlacementPolicy `pulumi:"placementPolicy"`
	// The ID of the project in which to create the node pool. If blank,
	// the provider-configured project will be used.
	Project *string `pulumi:"project"`
	// Specify node upgrade settings to change how GKE upgrades nodes.
	// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
	UpgradeSettings *NodePoolUpgradeSettings `pulumi:"upgradeSettings"`
	// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
	// will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
	// can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
	// versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
	// versions in a Terraform-compatible way.
	Version *string `pulumi:"version"`
}

type NodePoolState struct {
	// Configuration required by cluster autoscaler to adjust
	// the size of the node pool to the current cluster usage. Structure is documented below.
	Autoscaling NodePoolAutoscalingPtrInput
	// The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
	Cluster pulumi.StringPtrInput
	// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
	// Changing this will force recreation of the resource.
	InitialNodeCount pulumi.IntPtrInput
	// The resource URLs of the managed instance groups associated with this node pool.
	InstanceGroupUrls pulumi.StringArrayInput
	// The location (region or zone) of the cluster.
	Location pulumi.StringPtrInput
	// List of instance group URLs which have been assigned to this node pool.
	ManagedInstanceGroupUrls pulumi.StringArrayInput
	// Node management configuration, wherein auto-repair and
	// auto-upgrade is configured. Structure is documented below.
	Management NodePoolManagementPtrInput
	// The maximum number of pods per node in this node pool.
	// Note that this does not work on node pools which are "route-based" - that is, node
	// pools belonging to clusters that do not have IP Aliasing enabled.
	// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
	// for more information.
	MaxPodsPerNode pulumi.IntPtrInput
	// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
	Name pulumi.StringPtrInput
	// Creates a unique name for the node pool beginning
	// with the specified prefix. Conflicts with `name`.
	NamePrefix pulumi.StringPtrInput
	// The network configuration of the pool. Such as
	// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
	// documented below
	NetworkConfig NodePoolNetworkConfigPtrInput
	// Parameters used in creating the node pool. See
	// container.Cluster for schema.
	NodeConfig NodePoolNodeConfigPtrInput
	// The number of nodes per instance group. This field can be used to
	// update the number of nodes per instance group but should not be used alongside `autoscaling`.
	NodeCount pulumi.IntPtrInput
	// The list of zones in which the node pool's nodes should be located. Nodes must
	// be in the region of their regional cluster or in the same region as their
	// cluster's zone for zonal clusters. If unspecified, the cluster-level
	// `nodeLocations` will be used.
	NodeLocations pulumi.StringArrayInput
	Operation     pulumi.StringPtrInput
	// Specifies a custom placement policy for the
	// nodes.
	PlacementPolicy NodePoolPlacementPolicyPtrInput
	// The ID of the project in which to create the node pool. If blank,
	// the provider-configured project will be used.
	Project pulumi.StringPtrInput
	// Specify node upgrade settings to change how GKE upgrades nodes.
	// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
	UpgradeSettings NodePoolUpgradeSettingsPtrInput
	// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
	// will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
	// can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
	// versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
	// versions in a Terraform-compatible way.
	Version pulumi.StringPtrInput
}

func (NodePoolState) ElementType() reflect.Type {
	return reflect.TypeOf((*nodePoolState)(nil)).Elem()
}

type nodePoolArgs struct {
	// Configuration required by cluster autoscaler to adjust
	// the size of the node pool to the current cluster usage. Structure is documented below.
	Autoscaling *NodePoolAutoscaling `pulumi:"autoscaling"`
	// The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
	Cluster string `pulumi:"cluster"`
	// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
	// Changing this will force recreation of the resource.
	InitialNodeCount *int `pulumi:"initialNodeCount"`
	// The location (region or zone) of the cluster.
	Location *string `pulumi:"location"`
	// Node management configuration, wherein auto-repair and
	// auto-upgrade is configured. Structure is documented below.
	Management *NodePoolManagement `pulumi:"management"`
	// The maximum number of pods per node in this node pool.
	// Note that this does not work on node pools which are "route-based" - that is, node
	// pools belonging to clusters that do not have IP Aliasing enabled.
	// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
	// for more information.
	MaxPodsPerNode *int `pulumi:"maxPodsPerNode"`
	// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
	Name *string `pulumi:"name"`
	// Creates a unique name for the node pool beginning
	// with the specified prefix. Conflicts with `name`.
	NamePrefix *string `pulumi:"namePrefix"`
	// The network configuration of the pool. Such as
	// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
	// documented below
	NetworkConfig *NodePoolNetworkConfig `pulumi:"networkConfig"`
	// Parameters used in creating the node pool. See
	// container.Cluster for schema.
	NodeConfig *NodePoolNodeConfig `pulumi:"nodeConfig"`
	// The number of nodes per instance group. This field can be used to
	// update the number of nodes per instance group but should not be used alongside `autoscaling`.
	NodeCount *int `pulumi:"nodeCount"`
	// The list of zones in which the node pool's nodes should be located. Nodes must
	// be in the region of their regional cluster or in the same region as their
	// cluster's zone for zonal clusters. If unspecified, the cluster-level
	// `nodeLocations` will be used.
	NodeLocations []string `pulumi:"nodeLocations"`
	// Specifies a custom placement policy for the
	// nodes.
	PlacementPolicy *NodePoolPlacementPolicy `pulumi:"placementPolicy"`
	// The ID of the project in which to create the node pool. If blank,
	// the provider-configured project will be used.
	Project *string `pulumi:"project"`
	// Specify node upgrade settings to change how GKE upgrades nodes.
	// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
	UpgradeSettings *NodePoolUpgradeSettings `pulumi:"upgradeSettings"`
	// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
	// will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
	// can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
	// versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
	// versions in a Terraform-compatible way.
	Version *string `pulumi:"version"`
}

// The set of arguments for constructing a NodePool resource.
type NodePoolArgs struct {
	// Configuration required by cluster autoscaler to adjust
	// the size of the node pool to the current cluster usage. Structure is documented below.
	Autoscaling NodePoolAutoscalingPtrInput
	// The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
	Cluster pulumi.StringInput
	// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
	// Changing this will force recreation of the resource.
	InitialNodeCount pulumi.IntPtrInput
	// The location (region or zone) of the cluster.
	Location pulumi.StringPtrInput
	// Node management configuration, wherein auto-repair and
	// auto-upgrade is configured. Structure is documented below.
	Management NodePoolManagementPtrInput
	// The maximum number of pods per node in this node pool.
	// Note that this does not work on node pools which are "route-based" - that is, node
	// pools belonging to clusters that do not have IP Aliasing enabled.
	// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
	// for more information.
	MaxPodsPerNode pulumi.IntPtrInput
	// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
	Name pulumi.StringPtrInput
	// Creates a unique name for the node pool beginning
	// with the specified prefix. Conflicts with `name`.
	NamePrefix pulumi.StringPtrInput
	// The network configuration of the pool. Such as
	// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
	// documented below
	NetworkConfig NodePoolNetworkConfigPtrInput
	// Parameters used in creating the node pool. See
	// container.Cluster for schema.
	NodeConfig NodePoolNodeConfigPtrInput
	// The number of nodes per instance group. This field can be used to
	// update the number of nodes per instance group but should not be used alongside `autoscaling`.
	NodeCount pulumi.IntPtrInput
	// The list of zones in which the node pool's nodes should be located. Nodes must
	// be in the region of their regional cluster or in the same region as their
	// cluster's zone for zonal clusters. If unspecified, the cluster-level
	// `nodeLocations` will be used.
	NodeLocations pulumi.StringArrayInput
	// Specifies a custom placement policy for the
	// nodes.
	PlacementPolicy NodePoolPlacementPolicyPtrInput
	// The ID of the project in which to create the node pool. If blank,
	// the provider-configured project will be used.
	Project pulumi.StringPtrInput
	// Specify node upgrade settings to change how GKE upgrades nodes.
	// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
	UpgradeSettings NodePoolUpgradeSettingsPtrInput
	// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
	// will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
	// can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
	// versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
	// versions in a Terraform-compatible way.
	Version pulumi.StringPtrInput
}

func (NodePoolArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*nodePoolArgs)(nil)).Elem()
}

type NodePoolInput interface {
	pulumi.Input

	ToNodePoolOutput() NodePoolOutput
	ToNodePoolOutputWithContext(ctx context.Context) NodePoolOutput
}

func (*NodePool) ElementType() reflect.Type {
	return reflect.TypeOf((**NodePool)(nil)).Elem()
}

func (i *NodePool) ToNodePoolOutput() NodePoolOutput {
	return i.ToNodePoolOutputWithContext(context.Background())
}

func (i *NodePool) ToNodePoolOutputWithContext(ctx context.Context) NodePoolOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodePoolOutput)
}

// NodePoolArrayInput is an input type that accepts NodePoolArray and NodePoolArrayOutput values.
// You can construct a concrete instance of `NodePoolArrayInput` via:
//
//	NodePoolArray{ NodePoolArgs{...} }
type NodePoolArrayInput interface {
	pulumi.Input

	ToNodePoolArrayOutput() NodePoolArrayOutput
	ToNodePoolArrayOutputWithContext(context.Context) NodePoolArrayOutput
}

type NodePoolArray []NodePoolInput

func (NodePoolArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*NodePool)(nil)).Elem()
}

func (i NodePoolArray) ToNodePoolArrayOutput() NodePoolArrayOutput {
	return i.ToNodePoolArrayOutputWithContext(context.Background())
}

func (i NodePoolArray) ToNodePoolArrayOutputWithContext(ctx context.Context) NodePoolArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodePoolArrayOutput)
}

// NodePoolMapInput is an input type that accepts NodePoolMap and NodePoolMapOutput values.
// You can construct a concrete instance of `NodePoolMapInput` via:
//
//	NodePoolMap{ "key": NodePoolArgs{...} }
type NodePoolMapInput interface {
	pulumi.Input

	ToNodePoolMapOutput() NodePoolMapOutput
	ToNodePoolMapOutputWithContext(context.Context) NodePoolMapOutput
}

type NodePoolMap map[string]NodePoolInput

func (NodePoolMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*NodePool)(nil)).Elem()
}

func (i NodePoolMap) ToNodePoolMapOutput() NodePoolMapOutput {
	return i.ToNodePoolMapOutputWithContext(context.Background())
}

func (i NodePoolMap) ToNodePoolMapOutputWithContext(ctx context.Context) NodePoolMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodePoolMapOutput)
}

type NodePoolOutput struct{ *pulumi.OutputState }

func (NodePoolOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**NodePool)(nil)).Elem()
}

func (o NodePoolOutput) ToNodePoolOutput() NodePoolOutput {
	return o
}

func (o NodePoolOutput) ToNodePoolOutputWithContext(ctx context.Context) NodePoolOutput {
	return o
}

// Configuration required by cluster autoscaler to adjust
// the size of the node pool to the current cluster usage. Structure is documented below.
func (o NodePoolOutput) Autoscaling() NodePoolAutoscalingPtrOutput {
	return o.ApplyT(func(v *NodePool) NodePoolAutoscalingPtrOutput { return v.Autoscaling }).(NodePoolAutoscalingPtrOutput)
}

// The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
func (o NodePoolOutput) Cluster() pulumi.StringOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringOutput { return v.Cluster }).(pulumi.StringOutput)
}

// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
// Changing this will force recreation of the resource.
func (o NodePoolOutput) InitialNodeCount() pulumi.IntOutput {
	return o.ApplyT(func(v *NodePool) pulumi.IntOutput { return v.InitialNodeCount }).(pulumi.IntOutput)
}

// The resource URLs of the managed instance groups associated with this node pool.
func (o NodePoolOutput) InstanceGroupUrls() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringArrayOutput { return v.InstanceGroupUrls }).(pulumi.StringArrayOutput)
}

// The location (region or zone) of the cluster.
func (o NodePoolOutput) Location() pulumi.StringOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringOutput { return v.Location }).(pulumi.StringOutput)
}

// List of instance group URLs which have been assigned to this node pool.
func (o NodePoolOutput) ManagedInstanceGroupUrls() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringArrayOutput { return v.ManagedInstanceGroupUrls }).(pulumi.StringArrayOutput)
}

// Node management configuration, wherein auto-repair and
// auto-upgrade is configured. Structure is documented below.
func (o NodePoolOutput) Management() NodePoolManagementOutput {
	return o.ApplyT(func(v *NodePool) NodePoolManagementOutput { return v.Management }).(NodePoolManagementOutput)
}

// The maximum number of pods per node in this node pool.
// Note that this does not work on node pools which are "route-based" - that is, node
// pools belonging to clusters that do not have IP Aliasing enabled.
// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
// for more information.
func (o NodePoolOutput) MaxPodsPerNode() pulumi.IntOutput {
	return o.ApplyT(func(v *NodePool) pulumi.IntOutput { return v.MaxPodsPerNode }).(pulumi.IntOutput)
}

// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
func (o NodePoolOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

// Creates a unique name for the node pool beginning
// with the specified prefix. Conflicts with `name`.
func (o NodePoolOutput) NamePrefix() pulumi.StringOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringOutput { return v.NamePrefix }).(pulumi.StringOutput)
}

// The network configuration of the pool. Such as
// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
// documented below
func (o NodePoolOutput) NetworkConfig() NodePoolNetworkConfigOutput {
	return o.ApplyT(func(v *NodePool) NodePoolNetworkConfigOutput { return v.NetworkConfig }).(NodePoolNetworkConfigOutput)
}

// Parameters used in creating the node pool. See
// container.Cluster for schema.
func (o NodePoolOutput) NodeConfig() NodePoolNodeConfigOutput {
	return o.ApplyT(func(v *NodePool) NodePoolNodeConfigOutput { return v.NodeConfig }).(NodePoolNodeConfigOutput)
}

// The number of nodes per instance group. This field can be used to
// update the number of nodes per instance group but should not be used alongside `autoscaling`.
func (o NodePoolOutput) NodeCount() pulumi.IntOutput {
	return o.ApplyT(func(v *NodePool) pulumi.IntOutput { return v.NodeCount }).(pulumi.IntOutput)
}

// The list of zones in which the node pool's nodes should be located. Nodes must
// be in the region of their regional cluster or in the same region as their
// cluster's zone for zonal clusters. If unspecified, the cluster-level
// `nodeLocations` will be used.
func (o NodePoolOutput) NodeLocations() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringArrayOutput { return v.NodeLocations }).(pulumi.StringArrayOutput)
}

func (o NodePoolOutput) Operation() pulumi.StringOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringOutput { return v.Operation }).(pulumi.StringOutput)
}

// Specifies a custom placement policy for the
// nodes.
func (o NodePoolOutput) PlacementPolicy() NodePoolPlacementPolicyPtrOutput {
	return o.ApplyT(func(v *NodePool) NodePoolPlacementPolicyPtrOutput { return v.PlacementPolicy }).(NodePoolPlacementPolicyPtrOutput)
}

// The ID of the project in which to create the node pool. If blank,
// the provider-configured project will be used.
func (o NodePoolOutput) Project() pulumi.StringOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringOutput { return v.Project }).(pulumi.StringOutput)
}

// Specify node upgrade settings to change how GKE upgrades nodes.
// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
func (o NodePoolOutput) UpgradeSettings() NodePoolUpgradeSettingsOutput {
	return o.ApplyT(func(v *NodePool) NodePoolUpgradeSettingsOutput { return v.UpgradeSettings }).(NodePoolUpgradeSettingsOutput)
}

// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
// will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
// can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
// versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
// versions in a Terraform-compatible way.
func (o NodePoolOutput) Version() pulumi.StringOutput {
	return o.ApplyT(func(v *NodePool) pulumi.StringOutput { return v.Version }).(pulumi.StringOutput)
}

type NodePoolArrayOutput struct{ *pulumi.OutputState }

func (NodePoolArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*NodePool)(nil)).Elem()
}

func (o NodePoolArrayOutput) ToNodePoolArrayOutput() NodePoolArrayOutput {
	return o
}

func (o NodePoolArrayOutput) ToNodePoolArrayOutputWithContext(ctx context.Context) NodePoolArrayOutput {
	return o
}

func (o NodePoolArrayOutput) Index(i pulumi.IntInput) NodePoolOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *NodePool {
		return vs[0].([]*NodePool)[vs[1].(int)]
	}).(NodePoolOutput)
}

type NodePoolMapOutput struct{ *pulumi.OutputState }

func (NodePoolMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*NodePool)(nil)).Elem()
}

func (o NodePoolMapOutput) ToNodePoolMapOutput() NodePoolMapOutput {
	return o
}

func (o NodePoolMapOutput) ToNodePoolMapOutputWithContext(ctx context.Context) NodePoolMapOutput {
	return o
}

func (o NodePoolMapOutput) MapIndex(k pulumi.StringInput) NodePoolOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *NodePool {
		return vs[0].(map[string]*NodePool)[vs[1].(string)]
	}).(NodePoolOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*NodePoolInput)(nil)).Elem(), &NodePool{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodePoolArrayInput)(nil)).Elem(), NodePoolArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodePoolMapInput)(nil)).Elem(), NodePoolMap{})
	pulumi.RegisterOutputType(NodePoolOutput{})
	pulumi.RegisterOutputType(NodePoolArrayOutput{})
	pulumi.RegisterOutputType(NodePoolMapOutput{})
}

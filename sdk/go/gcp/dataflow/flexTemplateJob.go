// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package dataflow

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-gcp/sdk/v7/go/gcp/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumix"
)

// ## Example Usage
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-gcp/sdk/v7/go/gcp/dataflow"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := dataflow.NewFlexTemplateJob(ctx, "bigDataJob", &dataflow.FlexTemplateJobArgs{
//				ContainerSpecGcsPath: pulumi.String("gs://my-bucket/templates/template.json"),
//				Parameters: pulumi.Map{
//					"inputSubscription": pulumi.Any("messages"),
//				},
//			}, pulumi.Provider(google_beta))
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
// ## Note on "destroy" / "apply"
//
// There are many types of Dataflow jobs.  Some Dataflow jobs run constantly,
// getting new data from (e.g.) a GCS bucket, and outputting data continuously.
// Some jobs process a set amount of data then terminate. All jobs can fail while
// running due to programming errors or other issues. In this way, Dataflow jobs
// are different from most other provider / Google resources.
//
// The Dataflow resource is considered 'existing' while it is in a nonterminal
// state.  If it reaches a terminal state (e.g. 'FAILED', 'COMPLETE',
// 'CANCELLED'), it will be recreated on the next 'apply'.  This is as expected for
// jobs which run continuously, but may surprise users who use this resource for
// other kinds of Dataflow jobs.
//
// A Dataflow job which is 'destroyed' may be "cancelled" or "drained".  If
// "cancelled", the job terminates - any data written remains where it is, but no
// new data will be processed.  If "drained", no new data will enter the pipeline,
// but any data currently in the pipeline will finish being processed.  The default
// is "cancelled", but if a user sets `onDelete` to `"drain"` in the
// configuration, you may experience a long wait for your `pulumi destroy` to
// complete.
//
// You can potentially short-circuit the wait by setting `skipWaitOnJobTermination`
// to `true`, but beware that unless you take active steps to ensure that the job
// `name` parameter changes between instances, the name will conflict and the launch
// of the new job will fail. One way to do this is with a
// randomId
// resource, for example:
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-gcp/sdk/v7/go/gcp/dataflow"
//	"github.com/pulumi/pulumi-random/sdk/v4/go/random"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			cfg := config.New(ctx, "")
//			bigDataJobSubscriptionId := "projects/myproject/subscriptions/messages"
//			if param := cfg.Get("bigDataJobSubscriptionId"); param != "" {
//				bigDataJobSubscriptionId = param
//			}
//			_, err := random.NewRandomId(ctx, "bigDataJobNameSuffix", &random.RandomIdArgs{
//				ByteLength: pulumi.Int(4),
//				Keepers: pulumi.Map{
//					"region":          pulumi.Any(_var.Region),
//					"subscription_id": pulumi.String(bigDataJobSubscriptionId),
//				},
//			})
//			if err != nil {
//				return err
//			}
//			_, err = dataflow.NewFlexTemplateJob(ctx, "bigDataJob", &dataflow.FlexTemplateJobArgs{
//				Region:                   pulumi.Any(_var.Region),
//				ContainerSpecGcsPath:     pulumi.String("gs://my-bucket/templates/template.json"),
//				SkipWaitOnJobTermination: pulumi.Bool(true),
//				Parameters: pulumi.Map{
//					"inputSubscription": pulumi.String(bigDataJobSubscriptionId),
//				},
//			}, pulumi.Provider(google_beta))
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Import
//
// This resource does not support import.
type FlexTemplateJob struct {
	pulumi.CustomResourceState

	// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
	AdditionalExperiments pulumi.StringArrayOutput `pulumi:"additionalExperiments"`
	// The algorithm to use for autoscaling
	AutoscalingAlgorithm pulumi.StringOutput `pulumi:"autoscalingAlgorithm"`
	// The GCS path to the Dataflow job Flex
	// Template.
	//
	// ***
	ContainerSpecGcsPath pulumi.StringOutput `pulumi:"containerSpecGcsPath"`
	// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other
	// clients and services.
	EffectiveLabels pulumi.StringMapOutput `pulumi:"effectiveLabels"`
	// Indicates if the job should use the streaming engine feature.
	EnableStreamingEngine pulumi.BoolPtrOutput `pulumi:"enableStreamingEngine"`
	// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
	IpConfiguration pulumi.StringPtrOutput `pulumi:"ipConfiguration"`
	// The unique ID of this job.
	JobId pulumi.StringOutput `pulumi:"jobId"`
	// The name for the Cloud KMS key for the job. Key format is:
	// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
	KmsKeyName pulumi.StringOutput `pulumi:"kmsKeyName"`
	// User labels to be specified for the job. Keys and values
	// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
	// page. **Note**: This field is marked as deprecated as the API does not currently
	// support adding labels.
	// **NOTE**: Google-provided Dataflow templates often provide default labels
	// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
	// labels will be ignored to prevent diffs on re-apply.
	Labels pulumi.MapOutput `pulumi:"labels"`
	// The machine type to use for launching the job. The default is n1-standard-1.
	LauncherMachineType pulumi.StringOutput `pulumi:"launcherMachineType"`
	// The machine type to use for the job.
	MachineType pulumi.StringOutput `pulumi:"machineType"`
	// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
	// 1000.
	MaxWorkers pulumi.IntOutput `pulumi:"maxWorkers"`
	// A unique name for the resource, required by Dataflow.
	Name pulumi.StringOutput `pulumi:"name"`
	// The network to which VMs will be assigned. If it is not provided, "default" will be used.
	Network pulumi.StringOutput `pulumi:"network"`
	// The initial number of Google Compute Engine instances for the job.
	NumWorkers pulumi.IntOutput `pulumi:"numWorkers"`
	// One of "drain" or "cancel". Specifies behavior of
	// deletion during `pulumi destroy`.  See above note.
	OnDelete pulumi.StringPtrOutput `pulumi:"onDelete"`
	// Key/Value pairs to be passed to the Dataflow job (as
	// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
	// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
	Parameters pulumi.MapOutput `pulumi:"parameters"`
	// The project in which the resource belongs. If it is not
	// provided, the provider project is used.
	Project pulumi.StringOutput `pulumi:"project"`
	// The combination of labels configured directly on the resource and default labels configured on the provider.
	PulumiLabels pulumi.StringMapOutput `pulumi:"pulumiLabels"`
	// The region in which the created job should run.
	Region pulumi.StringOutput `pulumi:"region"`
	// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
	// the SDK. Note this field is only valid for portable pipelines.
	SdkContainerImage pulumi.StringOutput `pulumi:"sdkContainerImage"`
	// The Service Account email used to create the job.
	ServiceAccountEmail pulumi.StringOutput `pulumi:"serviceAccountEmail"`
	// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
	// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
	// different, e.g. by embedding a release ID or by using a random_id.
	SkipWaitOnJobTermination pulumi.BoolPtrOutput `pulumi:"skipWaitOnJobTermination"`
	// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
	StagingLocation pulumi.StringOutput `pulumi:"stagingLocation"`
	// The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
	State pulumi.StringOutput `pulumi:"state"`
	// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
	Subnetwork pulumi.StringOutput `pulumi:"subnetwork"`
	// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
	TempLocation pulumi.StringOutput `pulumi:"tempLocation"`
	// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
	// corresponding name prefixes of the new job.
	TransformNameMapping pulumi.MapOutput `pulumi:"transformNameMapping"`
	// The type of this job, selected from the JobType enum.
	Type pulumi.StringOutput `pulumi:"type"`
}

// NewFlexTemplateJob registers a new resource with the given unique name, arguments, and options.
func NewFlexTemplateJob(ctx *pulumi.Context,
	name string, args *FlexTemplateJobArgs, opts ...pulumi.ResourceOption) (*FlexTemplateJob, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.ContainerSpecGcsPath == nil {
		return nil, errors.New("invalid value for required argument 'ContainerSpecGcsPath'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource FlexTemplateJob
	err := ctx.RegisterResource("gcp:dataflow/flexTemplateJob:FlexTemplateJob", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetFlexTemplateJob gets an existing FlexTemplateJob resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetFlexTemplateJob(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *FlexTemplateJobState, opts ...pulumi.ResourceOption) (*FlexTemplateJob, error) {
	var resource FlexTemplateJob
	err := ctx.ReadResource("gcp:dataflow/flexTemplateJob:FlexTemplateJob", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering FlexTemplateJob resources.
type flexTemplateJobState struct {
	// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
	AdditionalExperiments []string `pulumi:"additionalExperiments"`
	// The algorithm to use for autoscaling
	AutoscalingAlgorithm *string `pulumi:"autoscalingAlgorithm"`
	// The GCS path to the Dataflow job Flex
	// Template.
	//
	// ***
	ContainerSpecGcsPath *string `pulumi:"containerSpecGcsPath"`
	// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other
	// clients and services.
	EffectiveLabels map[string]string `pulumi:"effectiveLabels"`
	// Indicates if the job should use the streaming engine feature.
	EnableStreamingEngine *bool `pulumi:"enableStreamingEngine"`
	// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
	IpConfiguration *string `pulumi:"ipConfiguration"`
	// The unique ID of this job.
	JobId *string `pulumi:"jobId"`
	// The name for the Cloud KMS key for the job. Key format is:
	// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
	KmsKeyName *string `pulumi:"kmsKeyName"`
	// User labels to be specified for the job. Keys and values
	// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
	// page. **Note**: This field is marked as deprecated as the API does not currently
	// support adding labels.
	// **NOTE**: Google-provided Dataflow templates often provide default labels
	// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
	// labels will be ignored to prevent diffs on re-apply.
	Labels map[string]interface{} `pulumi:"labels"`
	// The machine type to use for launching the job. The default is n1-standard-1.
	LauncherMachineType *string `pulumi:"launcherMachineType"`
	// The machine type to use for the job.
	MachineType *string `pulumi:"machineType"`
	// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
	// 1000.
	MaxWorkers *int `pulumi:"maxWorkers"`
	// A unique name for the resource, required by Dataflow.
	Name *string `pulumi:"name"`
	// The network to which VMs will be assigned. If it is not provided, "default" will be used.
	Network *string `pulumi:"network"`
	// The initial number of Google Compute Engine instances for the job.
	NumWorkers *int `pulumi:"numWorkers"`
	// One of "drain" or "cancel". Specifies behavior of
	// deletion during `pulumi destroy`.  See above note.
	OnDelete *string `pulumi:"onDelete"`
	// Key/Value pairs to be passed to the Dataflow job (as
	// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
	// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
	Parameters map[string]interface{} `pulumi:"parameters"`
	// The project in which the resource belongs. If it is not
	// provided, the provider project is used.
	Project *string `pulumi:"project"`
	// The combination of labels configured directly on the resource and default labels configured on the provider.
	PulumiLabels map[string]string `pulumi:"pulumiLabels"`
	// The region in which the created job should run.
	Region *string `pulumi:"region"`
	// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
	// the SDK. Note this field is only valid for portable pipelines.
	SdkContainerImage *string `pulumi:"sdkContainerImage"`
	// The Service Account email used to create the job.
	ServiceAccountEmail *string `pulumi:"serviceAccountEmail"`
	// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
	// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
	// different, e.g. by embedding a release ID or by using a random_id.
	SkipWaitOnJobTermination *bool `pulumi:"skipWaitOnJobTermination"`
	// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
	StagingLocation *string `pulumi:"stagingLocation"`
	// The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
	State *string `pulumi:"state"`
	// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
	Subnetwork *string `pulumi:"subnetwork"`
	// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
	TempLocation *string `pulumi:"tempLocation"`
	// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
	// corresponding name prefixes of the new job.
	TransformNameMapping map[string]interface{} `pulumi:"transformNameMapping"`
	// The type of this job, selected from the JobType enum.
	Type *string `pulumi:"type"`
}

type FlexTemplateJobState struct {
	// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
	AdditionalExperiments pulumi.StringArrayInput
	// The algorithm to use for autoscaling
	AutoscalingAlgorithm pulumi.StringPtrInput
	// The GCS path to the Dataflow job Flex
	// Template.
	//
	// ***
	ContainerSpecGcsPath pulumi.StringPtrInput
	// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other
	// clients and services.
	EffectiveLabels pulumi.StringMapInput
	// Indicates if the job should use the streaming engine feature.
	EnableStreamingEngine pulumi.BoolPtrInput
	// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
	IpConfiguration pulumi.StringPtrInput
	// The unique ID of this job.
	JobId pulumi.StringPtrInput
	// The name for the Cloud KMS key for the job. Key format is:
	// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
	KmsKeyName pulumi.StringPtrInput
	// User labels to be specified for the job. Keys and values
	// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
	// page. **Note**: This field is marked as deprecated as the API does not currently
	// support adding labels.
	// **NOTE**: Google-provided Dataflow templates often provide default labels
	// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
	// labels will be ignored to prevent diffs on re-apply.
	Labels pulumi.MapInput
	// The machine type to use for launching the job. The default is n1-standard-1.
	LauncherMachineType pulumi.StringPtrInput
	// The machine type to use for the job.
	MachineType pulumi.StringPtrInput
	// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
	// 1000.
	MaxWorkers pulumi.IntPtrInput
	// A unique name for the resource, required by Dataflow.
	Name pulumi.StringPtrInput
	// The network to which VMs will be assigned. If it is not provided, "default" will be used.
	Network pulumi.StringPtrInput
	// The initial number of Google Compute Engine instances for the job.
	NumWorkers pulumi.IntPtrInput
	// One of "drain" or "cancel". Specifies behavior of
	// deletion during `pulumi destroy`.  See above note.
	OnDelete pulumi.StringPtrInput
	// Key/Value pairs to be passed to the Dataflow job (as
	// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
	// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
	Parameters pulumi.MapInput
	// The project in which the resource belongs. If it is not
	// provided, the provider project is used.
	Project pulumi.StringPtrInput
	// The combination of labels configured directly on the resource and default labels configured on the provider.
	PulumiLabels pulumi.StringMapInput
	// The region in which the created job should run.
	Region pulumi.StringPtrInput
	// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
	// the SDK. Note this field is only valid for portable pipelines.
	SdkContainerImage pulumi.StringPtrInput
	// The Service Account email used to create the job.
	ServiceAccountEmail pulumi.StringPtrInput
	// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
	// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
	// different, e.g. by embedding a release ID or by using a random_id.
	SkipWaitOnJobTermination pulumi.BoolPtrInput
	// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
	StagingLocation pulumi.StringPtrInput
	// The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
	State pulumi.StringPtrInput
	// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
	Subnetwork pulumi.StringPtrInput
	// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
	TempLocation pulumi.StringPtrInput
	// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
	// corresponding name prefixes of the new job.
	TransformNameMapping pulumi.MapInput
	// The type of this job, selected from the JobType enum.
	Type pulumi.StringPtrInput
}

func (FlexTemplateJobState) ElementType() reflect.Type {
	return reflect.TypeOf((*flexTemplateJobState)(nil)).Elem()
}

type flexTemplateJobArgs struct {
	// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
	AdditionalExperiments []string `pulumi:"additionalExperiments"`
	// The algorithm to use for autoscaling
	AutoscalingAlgorithm *string `pulumi:"autoscalingAlgorithm"`
	// The GCS path to the Dataflow job Flex
	// Template.
	//
	// ***
	ContainerSpecGcsPath string `pulumi:"containerSpecGcsPath"`
	// Indicates if the job should use the streaming engine feature.
	EnableStreamingEngine *bool `pulumi:"enableStreamingEngine"`
	// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
	IpConfiguration *string `pulumi:"ipConfiguration"`
	// The name for the Cloud KMS key for the job. Key format is:
	// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
	KmsKeyName *string `pulumi:"kmsKeyName"`
	// User labels to be specified for the job. Keys and values
	// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
	// page. **Note**: This field is marked as deprecated as the API does not currently
	// support adding labels.
	// **NOTE**: Google-provided Dataflow templates often provide default labels
	// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
	// labels will be ignored to prevent diffs on re-apply.
	Labels map[string]interface{} `pulumi:"labels"`
	// The machine type to use for launching the job. The default is n1-standard-1.
	LauncherMachineType *string `pulumi:"launcherMachineType"`
	// The machine type to use for the job.
	MachineType *string `pulumi:"machineType"`
	// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
	// 1000.
	MaxWorkers *int `pulumi:"maxWorkers"`
	// A unique name for the resource, required by Dataflow.
	Name *string `pulumi:"name"`
	// The network to which VMs will be assigned. If it is not provided, "default" will be used.
	Network *string `pulumi:"network"`
	// The initial number of Google Compute Engine instances for the job.
	NumWorkers *int `pulumi:"numWorkers"`
	// One of "drain" or "cancel". Specifies behavior of
	// deletion during `pulumi destroy`.  See above note.
	OnDelete *string `pulumi:"onDelete"`
	// Key/Value pairs to be passed to the Dataflow job (as
	// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
	// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
	Parameters map[string]interface{} `pulumi:"parameters"`
	// The project in which the resource belongs. If it is not
	// provided, the provider project is used.
	Project *string `pulumi:"project"`
	// The region in which the created job should run.
	Region *string `pulumi:"region"`
	// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
	// the SDK. Note this field is only valid for portable pipelines.
	SdkContainerImage *string `pulumi:"sdkContainerImage"`
	// The Service Account email used to create the job.
	ServiceAccountEmail *string `pulumi:"serviceAccountEmail"`
	// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
	// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
	// different, e.g. by embedding a release ID or by using a random_id.
	SkipWaitOnJobTermination *bool `pulumi:"skipWaitOnJobTermination"`
	// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
	StagingLocation *string `pulumi:"stagingLocation"`
	// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
	Subnetwork *string `pulumi:"subnetwork"`
	// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
	TempLocation *string `pulumi:"tempLocation"`
	// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
	// corresponding name prefixes of the new job.
	TransformNameMapping map[string]interface{} `pulumi:"transformNameMapping"`
}

// The set of arguments for constructing a FlexTemplateJob resource.
type FlexTemplateJobArgs struct {
	// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
	AdditionalExperiments pulumi.StringArrayInput
	// The algorithm to use for autoscaling
	AutoscalingAlgorithm pulumi.StringPtrInput
	// The GCS path to the Dataflow job Flex
	// Template.
	//
	// ***
	ContainerSpecGcsPath pulumi.StringInput
	// Indicates if the job should use the streaming engine feature.
	EnableStreamingEngine pulumi.BoolPtrInput
	// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
	IpConfiguration pulumi.StringPtrInput
	// The name for the Cloud KMS key for the job. Key format is:
	// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
	KmsKeyName pulumi.StringPtrInput
	// User labels to be specified for the job. Keys and values
	// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
	// page. **Note**: This field is marked as deprecated as the API does not currently
	// support adding labels.
	// **NOTE**: Google-provided Dataflow templates often provide default labels
	// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
	// labels will be ignored to prevent diffs on re-apply.
	Labels pulumi.MapInput
	// The machine type to use for launching the job. The default is n1-standard-1.
	LauncherMachineType pulumi.StringPtrInput
	// The machine type to use for the job.
	MachineType pulumi.StringPtrInput
	// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
	// 1000.
	MaxWorkers pulumi.IntPtrInput
	// A unique name for the resource, required by Dataflow.
	Name pulumi.StringPtrInput
	// The network to which VMs will be assigned. If it is not provided, "default" will be used.
	Network pulumi.StringPtrInput
	// The initial number of Google Compute Engine instances for the job.
	NumWorkers pulumi.IntPtrInput
	// One of "drain" or "cancel". Specifies behavior of
	// deletion during `pulumi destroy`.  See above note.
	OnDelete pulumi.StringPtrInput
	// Key/Value pairs to be passed to the Dataflow job (as
	// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
	// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
	Parameters pulumi.MapInput
	// The project in which the resource belongs. If it is not
	// provided, the provider project is used.
	Project pulumi.StringPtrInput
	// The region in which the created job should run.
	Region pulumi.StringPtrInput
	// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
	// the SDK. Note this field is only valid for portable pipelines.
	SdkContainerImage pulumi.StringPtrInput
	// The Service Account email used to create the job.
	ServiceAccountEmail pulumi.StringPtrInput
	// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
	// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
	// different, e.g. by embedding a release ID or by using a random_id.
	SkipWaitOnJobTermination pulumi.BoolPtrInput
	// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
	StagingLocation pulumi.StringPtrInput
	// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
	Subnetwork pulumi.StringPtrInput
	// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
	TempLocation pulumi.StringPtrInput
	// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
	// corresponding name prefixes of the new job.
	TransformNameMapping pulumi.MapInput
}

func (FlexTemplateJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*flexTemplateJobArgs)(nil)).Elem()
}

type FlexTemplateJobInput interface {
	pulumi.Input

	ToFlexTemplateJobOutput() FlexTemplateJobOutput
	ToFlexTemplateJobOutputWithContext(ctx context.Context) FlexTemplateJobOutput
}

func (*FlexTemplateJob) ElementType() reflect.Type {
	return reflect.TypeOf((**FlexTemplateJob)(nil)).Elem()
}

func (i *FlexTemplateJob) ToFlexTemplateJobOutput() FlexTemplateJobOutput {
	return i.ToFlexTemplateJobOutputWithContext(context.Background())
}

func (i *FlexTemplateJob) ToFlexTemplateJobOutputWithContext(ctx context.Context) FlexTemplateJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(FlexTemplateJobOutput)
}

func (i *FlexTemplateJob) ToOutput(ctx context.Context) pulumix.Output[*FlexTemplateJob] {
	return pulumix.Output[*FlexTemplateJob]{
		OutputState: i.ToFlexTemplateJobOutputWithContext(ctx).OutputState,
	}
}

// FlexTemplateJobArrayInput is an input type that accepts FlexTemplateJobArray and FlexTemplateJobArrayOutput values.
// You can construct a concrete instance of `FlexTemplateJobArrayInput` via:
//
//	FlexTemplateJobArray{ FlexTemplateJobArgs{...} }
type FlexTemplateJobArrayInput interface {
	pulumi.Input

	ToFlexTemplateJobArrayOutput() FlexTemplateJobArrayOutput
	ToFlexTemplateJobArrayOutputWithContext(context.Context) FlexTemplateJobArrayOutput
}

type FlexTemplateJobArray []FlexTemplateJobInput

func (FlexTemplateJobArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*FlexTemplateJob)(nil)).Elem()
}

func (i FlexTemplateJobArray) ToFlexTemplateJobArrayOutput() FlexTemplateJobArrayOutput {
	return i.ToFlexTemplateJobArrayOutputWithContext(context.Background())
}

func (i FlexTemplateJobArray) ToFlexTemplateJobArrayOutputWithContext(ctx context.Context) FlexTemplateJobArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(FlexTemplateJobArrayOutput)
}

func (i FlexTemplateJobArray) ToOutput(ctx context.Context) pulumix.Output[[]*FlexTemplateJob] {
	return pulumix.Output[[]*FlexTemplateJob]{
		OutputState: i.ToFlexTemplateJobArrayOutputWithContext(ctx).OutputState,
	}
}

// FlexTemplateJobMapInput is an input type that accepts FlexTemplateJobMap and FlexTemplateJobMapOutput values.
// You can construct a concrete instance of `FlexTemplateJobMapInput` via:
//
//	FlexTemplateJobMap{ "key": FlexTemplateJobArgs{...} }
type FlexTemplateJobMapInput interface {
	pulumi.Input

	ToFlexTemplateJobMapOutput() FlexTemplateJobMapOutput
	ToFlexTemplateJobMapOutputWithContext(context.Context) FlexTemplateJobMapOutput
}

type FlexTemplateJobMap map[string]FlexTemplateJobInput

func (FlexTemplateJobMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*FlexTemplateJob)(nil)).Elem()
}

func (i FlexTemplateJobMap) ToFlexTemplateJobMapOutput() FlexTemplateJobMapOutput {
	return i.ToFlexTemplateJobMapOutputWithContext(context.Background())
}

func (i FlexTemplateJobMap) ToFlexTemplateJobMapOutputWithContext(ctx context.Context) FlexTemplateJobMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(FlexTemplateJobMapOutput)
}

func (i FlexTemplateJobMap) ToOutput(ctx context.Context) pulumix.Output[map[string]*FlexTemplateJob] {
	return pulumix.Output[map[string]*FlexTemplateJob]{
		OutputState: i.ToFlexTemplateJobMapOutputWithContext(ctx).OutputState,
	}
}

type FlexTemplateJobOutput struct{ *pulumi.OutputState }

func (FlexTemplateJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**FlexTemplateJob)(nil)).Elem()
}

func (o FlexTemplateJobOutput) ToFlexTemplateJobOutput() FlexTemplateJobOutput {
	return o
}

func (o FlexTemplateJobOutput) ToFlexTemplateJobOutputWithContext(ctx context.Context) FlexTemplateJobOutput {
	return o
}

func (o FlexTemplateJobOutput) ToOutput(ctx context.Context) pulumix.Output[*FlexTemplateJob] {
	return pulumix.Output[*FlexTemplateJob]{
		OutputState: o.OutputState,
	}
}

// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
func (o FlexTemplateJobOutput) AdditionalExperiments() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringArrayOutput { return v.AdditionalExperiments }).(pulumi.StringArrayOutput)
}

// The algorithm to use for autoscaling
func (o FlexTemplateJobOutput) AutoscalingAlgorithm() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.AutoscalingAlgorithm }).(pulumi.StringOutput)
}

// The GCS path to the Dataflow job Flex
// Template.
//
// ***
func (o FlexTemplateJobOutput) ContainerSpecGcsPath() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.ContainerSpecGcsPath }).(pulumi.StringOutput)
}

// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other
// clients and services.
func (o FlexTemplateJobOutput) EffectiveLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringMapOutput { return v.EffectiveLabels }).(pulumi.StringMapOutput)
}

// Indicates if the job should use the streaming engine feature.
func (o FlexTemplateJobOutput) EnableStreamingEngine() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.BoolPtrOutput { return v.EnableStreamingEngine }).(pulumi.BoolPtrOutput)
}

// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
func (o FlexTemplateJobOutput) IpConfiguration() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringPtrOutput { return v.IpConfiguration }).(pulumi.StringPtrOutput)
}

// The unique ID of this job.
func (o FlexTemplateJobOutput) JobId() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.JobId }).(pulumi.StringOutput)
}

// The name for the Cloud KMS key for the job. Key format is:
// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
func (o FlexTemplateJobOutput) KmsKeyName() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.KmsKeyName }).(pulumi.StringOutput)
}

// User labels to be specified for the job. Keys and values
// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
// page. **Note**: This field is marked as deprecated as the API does not currently
// support adding labels.
// **NOTE**: Google-provided Dataflow templates often provide default labels
// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
// labels will be ignored to prevent diffs on re-apply.
func (o FlexTemplateJobOutput) Labels() pulumi.MapOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.MapOutput { return v.Labels }).(pulumi.MapOutput)
}

// The machine type to use for launching the job. The default is n1-standard-1.
func (o FlexTemplateJobOutput) LauncherMachineType() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.LauncherMachineType }).(pulumi.StringOutput)
}

// The machine type to use for the job.
func (o FlexTemplateJobOutput) MachineType() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.MachineType }).(pulumi.StringOutput)
}

// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
// 1000.
func (o FlexTemplateJobOutput) MaxWorkers() pulumi.IntOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.IntOutput { return v.MaxWorkers }).(pulumi.IntOutput)
}

// A unique name for the resource, required by Dataflow.
func (o FlexTemplateJobOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

// The network to which VMs will be assigned. If it is not provided, "default" will be used.
func (o FlexTemplateJobOutput) Network() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.Network }).(pulumi.StringOutput)
}

// The initial number of Google Compute Engine instances for the job.
func (o FlexTemplateJobOutput) NumWorkers() pulumi.IntOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.IntOutput { return v.NumWorkers }).(pulumi.IntOutput)
}

// One of "drain" or "cancel". Specifies behavior of
// deletion during `pulumi destroy`.  See above note.
func (o FlexTemplateJobOutput) OnDelete() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringPtrOutput { return v.OnDelete }).(pulumi.StringPtrOutput)
}

// Key/Value pairs to be passed to the Dataflow job (as
// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
func (o FlexTemplateJobOutput) Parameters() pulumi.MapOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.MapOutput { return v.Parameters }).(pulumi.MapOutput)
}

// The project in which the resource belongs. If it is not
// provided, the provider project is used.
func (o FlexTemplateJobOutput) Project() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.Project }).(pulumi.StringOutput)
}

// The combination of labels configured directly on the resource and default labels configured on the provider.
func (o FlexTemplateJobOutput) PulumiLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringMapOutput { return v.PulumiLabels }).(pulumi.StringMapOutput)
}

// The region in which the created job should run.
func (o FlexTemplateJobOutput) Region() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.Region }).(pulumi.StringOutput)
}

// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
// the SDK. Note this field is only valid for portable pipelines.
func (o FlexTemplateJobOutput) SdkContainerImage() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.SdkContainerImage }).(pulumi.StringOutput)
}

// The Service Account email used to create the job.
func (o FlexTemplateJobOutput) ServiceAccountEmail() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.ServiceAccountEmail }).(pulumi.StringOutput)
}

// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
// different, e.g. by embedding a release ID or by using a random_id.
func (o FlexTemplateJobOutput) SkipWaitOnJobTermination() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.BoolPtrOutput { return v.SkipWaitOnJobTermination }).(pulumi.BoolPtrOutput)
}

// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
func (o FlexTemplateJobOutput) StagingLocation() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.StagingLocation }).(pulumi.StringOutput)
}

// The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
func (o FlexTemplateJobOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.State }).(pulumi.StringOutput)
}

// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
func (o FlexTemplateJobOutput) Subnetwork() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.Subnetwork }).(pulumi.StringOutput)
}

// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
func (o FlexTemplateJobOutput) TempLocation() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.TempLocation }).(pulumi.StringOutput)
}

// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
// corresponding name prefixes of the new job.
func (o FlexTemplateJobOutput) TransformNameMapping() pulumi.MapOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.MapOutput { return v.TransformNameMapping }).(pulumi.MapOutput)
}

// The type of this job, selected from the JobType enum.
func (o FlexTemplateJobOutput) Type() pulumi.StringOutput {
	return o.ApplyT(func(v *FlexTemplateJob) pulumi.StringOutput { return v.Type }).(pulumi.StringOutput)
}

type FlexTemplateJobArrayOutput struct{ *pulumi.OutputState }

func (FlexTemplateJobArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*FlexTemplateJob)(nil)).Elem()
}

func (o FlexTemplateJobArrayOutput) ToFlexTemplateJobArrayOutput() FlexTemplateJobArrayOutput {
	return o
}

func (o FlexTemplateJobArrayOutput) ToFlexTemplateJobArrayOutputWithContext(ctx context.Context) FlexTemplateJobArrayOutput {
	return o
}

func (o FlexTemplateJobArrayOutput) ToOutput(ctx context.Context) pulumix.Output[[]*FlexTemplateJob] {
	return pulumix.Output[[]*FlexTemplateJob]{
		OutputState: o.OutputState,
	}
}

func (o FlexTemplateJobArrayOutput) Index(i pulumi.IntInput) FlexTemplateJobOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *FlexTemplateJob {
		return vs[0].([]*FlexTemplateJob)[vs[1].(int)]
	}).(FlexTemplateJobOutput)
}

type FlexTemplateJobMapOutput struct{ *pulumi.OutputState }

func (FlexTemplateJobMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*FlexTemplateJob)(nil)).Elem()
}

func (o FlexTemplateJobMapOutput) ToFlexTemplateJobMapOutput() FlexTemplateJobMapOutput {
	return o
}

func (o FlexTemplateJobMapOutput) ToFlexTemplateJobMapOutputWithContext(ctx context.Context) FlexTemplateJobMapOutput {
	return o
}

func (o FlexTemplateJobMapOutput) ToOutput(ctx context.Context) pulumix.Output[map[string]*FlexTemplateJob] {
	return pulumix.Output[map[string]*FlexTemplateJob]{
		OutputState: o.OutputState,
	}
}

func (o FlexTemplateJobMapOutput) MapIndex(k pulumi.StringInput) FlexTemplateJobOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *FlexTemplateJob {
		return vs[0].(map[string]*FlexTemplateJob)[vs[1].(string)]
	}).(FlexTemplateJobOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*FlexTemplateJobInput)(nil)).Elem(), &FlexTemplateJob{})
	pulumi.RegisterInputType(reflect.TypeOf((*FlexTemplateJobArrayInput)(nil)).Elem(), FlexTemplateJobArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*FlexTemplateJobMapInput)(nil)).Elem(), FlexTemplateJobMap{})
	pulumi.RegisterOutputType(FlexTemplateJobOutput{})
	pulumi.RegisterOutputType(FlexTemplateJobArrayOutput{})
	pulumi.RegisterOutputType(FlexTemplateJobMapOutput{})
}

// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.gcp.diagflow.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import java.lang.Double;
import java.lang.Integer;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class GeneratorInferenceParameterArgs extends com.pulumi.resources.ResourceArgs {

    public static final GeneratorInferenceParameterArgs Empty = new GeneratorInferenceParameterArgs();

    /**
     * Optional. Maximum number of the output tokens for the generator.
     * 
     */
    @Import(name="maxOutputTokens")
    private @Nullable Output<Integer> maxOutputTokens;

    /**
     * @return Optional. Maximum number of the output tokens for the generator.
     * 
     */
    public Optional<Output<Integer>> maxOutputTokens() {
        return Optional.ofNullable(this.maxOutputTokens);
    }

    /**
     * Optional. Controls the randomness of LLM predictions. Low temperature = less random. High temperature = more random. If unset (or 0), uses a default value of 0.
     * 
     */
    @Import(name="temperature")
    private @Nullable Output<Double> temperature;

    /**
     * @return Optional. Controls the randomness of LLM predictions. Low temperature = less random. High temperature = more random. If unset (or 0), uses a default value of 0.
     * 
     */
    public Optional<Output<Double>> temperature() {
        return Optional.ofNullable(this.temperature);
    }

    /**
     * Optional. Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model&#39;s vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [1, 40], default to 40.
     * 
     */
    @Import(name="topK")
    private @Nullable Output<Integer> topK;

    /**
     * @return Optional. Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model&#39;s vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [1, 40], default to 40.
     * 
     */
    public Optional<Output<Integer>> topK() {
        return Optional.ofNullable(this.topK);
    }

    /**
     * Optional. Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn&#39;t consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [0.0, 1.0], default to 0.95.
     * 
     */
    @Import(name="topP")
    private @Nullable Output<Double> topP;

    /**
     * @return Optional. Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn&#39;t consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [0.0, 1.0], default to 0.95.
     * 
     */
    public Optional<Output<Double>> topP() {
        return Optional.ofNullable(this.topP);
    }

    private GeneratorInferenceParameterArgs() {}

    private GeneratorInferenceParameterArgs(GeneratorInferenceParameterArgs $) {
        this.maxOutputTokens = $.maxOutputTokens;
        this.temperature = $.temperature;
        this.topK = $.topK;
        this.topP = $.topP;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(GeneratorInferenceParameterArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private GeneratorInferenceParameterArgs $;

        public Builder() {
            $ = new GeneratorInferenceParameterArgs();
        }

        public Builder(GeneratorInferenceParameterArgs defaults) {
            $ = new GeneratorInferenceParameterArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param maxOutputTokens Optional. Maximum number of the output tokens for the generator.
         * 
         * @return builder
         * 
         */
        public Builder maxOutputTokens(@Nullable Output<Integer> maxOutputTokens) {
            $.maxOutputTokens = maxOutputTokens;
            return this;
        }

        /**
         * @param maxOutputTokens Optional. Maximum number of the output tokens for the generator.
         * 
         * @return builder
         * 
         */
        public Builder maxOutputTokens(Integer maxOutputTokens) {
            return maxOutputTokens(Output.of(maxOutputTokens));
        }

        /**
         * @param temperature Optional. Controls the randomness of LLM predictions. Low temperature = less random. High temperature = more random. If unset (or 0), uses a default value of 0.
         * 
         * @return builder
         * 
         */
        public Builder temperature(@Nullable Output<Double> temperature) {
            $.temperature = temperature;
            return this;
        }

        /**
         * @param temperature Optional. Controls the randomness of LLM predictions. Low temperature = less random. High temperature = more random. If unset (or 0), uses a default value of 0.
         * 
         * @return builder
         * 
         */
        public Builder temperature(Double temperature) {
            return temperature(Output.of(temperature));
        }

        /**
         * @param topK Optional. Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model&#39;s vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [1, 40], default to 40.
         * 
         * @return builder
         * 
         */
        public Builder topK(@Nullable Output<Integer> topK) {
            $.topK = topK;
            return this;
        }

        /**
         * @param topK Optional. Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model&#39;s vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [1, 40], default to 40.
         * 
         * @return builder
         * 
         */
        public Builder topK(Integer topK) {
            return topK(Output.of(topK));
        }

        /**
         * @param topP Optional. Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn&#39;t consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [0.0, 1.0], default to 0.95.
         * 
         * @return builder
         * 
         */
        public Builder topP(@Nullable Output<Double> topP) {
            $.topP = topP;
            return this;
        }

        /**
         * @param topP Optional. Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn&#39;t consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses. Acceptable value is [0.0, 1.0], default to 0.95.
         * 
         * @return builder
         * 
         */
        public Builder topP(Double topP) {
            return topP(Output.of(topP));
        }

        public GeneratorInferenceParameterArgs build() {
            return $;
        }
    }

}

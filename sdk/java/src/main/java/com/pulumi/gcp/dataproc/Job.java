// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.gcp.dataproc;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.gcp.Utilities;
import com.pulumi.gcp.dataproc.JobArgs;
import com.pulumi.gcp.dataproc.inputs.JobState;
import com.pulumi.gcp.dataproc.outputs.JobHadoopConfig;
import com.pulumi.gcp.dataproc.outputs.JobHiveConfig;
import com.pulumi.gcp.dataproc.outputs.JobPigConfig;
import com.pulumi.gcp.dataproc.outputs.JobPlacement;
import com.pulumi.gcp.dataproc.outputs.JobPrestoConfig;
import com.pulumi.gcp.dataproc.outputs.JobPysparkConfig;
import com.pulumi.gcp.dataproc.outputs.JobReference;
import com.pulumi.gcp.dataproc.outputs.JobScheduling;
import com.pulumi.gcp.dataproc.outputs.JobSparkConfig;
import com.pulumi.gcp.dataproc.outputs.JobSparksqlConfig;
import com.pulumi.gcp.dataproc.outputs.JobStatus;
import java.lang.Boolean;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * Manages a job resource within a Dataproc cluster within GCE. For more information see
 * [the official dataproc documentation](https://cloud.google.com/dataproc/).
 * 
 * !&gt; **Note:** This resource does not support &#39;update&#39; and changing any attributes will cause the resource to be recreated.
 * 
 * ## Example Usage
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.dataproc.Cluster;
 * import com.pulumi.gcp.dataproc.ClusterArgs;
 * import com.pulumi.gcp.dataproc.Job;
 * import com.pulumi.gcp.dataproc.JobArgs;
 * import com.pulumi.gcp.dataproc.inputs.JobPlacementArgs;
 * import com.pulumi.gcp.dataproc.inputs.JobSparkConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.JobSparkConfigLoggingConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.JobPysparkConfigArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var mycluster = new Cluster(&#34;mycluster&#34;, ClusterArgs.builder()        
 *             .region(&#34;us-central1&#34;)
 *             .build());
 * 
 *         var spark = new Job(&#34;spark&#34;, JobArgs.builder()        
 *             .region(mycluster.region())
 *             .forceDelete(true)
 *             .placement(JobPlacementArgs.builder()
 *                 .clusterName(mycluster.name())
 *                 .build())
 *             .sparkConfig(JobSparkConfigArgs.builder()
 *                 .mainClass(&#34;org.apache.spark.examples.SparkPi&#34;)
 *                 .jarFileUris(&#34;file:///usr/lib/spark/examples/jars/spark-examples.jar&#34;)
 *                 .args(&#34;1000&#34;)
 *                 .properties(Map.of(&#34;spark.logConf&#34;, &#34;true&#34;))
 *                 .loggingConfig(JobSparkConfigLoggingConfigArgs.builder()
 *                     .driverLogLevels(Map.of(&#34;root&#34;, &#34;INFO&#34;))
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *         var pyspark = new Job(&#34;pyspark&#34;, JobArgs.builder()        
 *             .region(mycluster.region())
 *             .forceDelete(true)
 *             .placement(JobPlacementArgs.builder()
 *                 .clusterName(mycluster.name())
 *                 .build())
 *             .pysparkConfig(JobPysparkConfigArgs.builder()
 *                 .mainPythonFileUri(&#34;gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py&#34;)
 *                 .properties(Map.of(&#34;spark.logConf&#34;, &#34;true&#34;))
 *                 .build())
 *             .build());
 * 
 *         ctx.export(&#34;sparkStatus&#34;, spark.statuses().applyValue(statuses -&gt; statuses[0].state()));
 *         ctx.export(&#34;pysparkStatus&#34;, pyspark.statuses().applyValue(statuses -&gt; statuses[0].state()));
 *     }
 * }
 * ```
 * 
 * ## Import
 * 
 * This resource does not support import.
 * 
 */
@ResourceType(type="gcp:dataproc/job:Job")
public class Job extends com.pulumi.resources.CustomResource {
    /**
     * If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
     * 
     */
    @Export(name="driverControlsFilesUri", type=String.class, parameters={})
    private Output<String> driverControlsFilesUri;

    /**
     * @return If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
     * 
     */
    public Output<String> driverControlsFilesUri() {
        return this.driverControlsFilesUri;
    }
    /**
     * A URI pointing to the location of the stdout of the job&#39;s driver program.
     * 
     */
    @Export(name="driverOutputResourceUri", type=String.class, parameters={})
    private Output<String> driverOutputResourceUri;

    /**
     * @return A URI pointing to the location of the stdout of the job&#39;s driver program.
     * 
     */
    public Output<String> driverOutputResourceUri() {
        return this.driverOutputResourceUri;
    }
    /**
     * By default, you can only delete inactive jobs within
     * Dataproc. Setting this to true, and calling destroy, will ensure that the
     * job is first cancelled before issuing the delete.
     * 
     */
    @Export(name="forceDelete", type=Boolean.class, parameters={})
    private Output</* @Nullable */ Boolean> forceDelete;

    /**
     * @return By default, you can only delete inactive jobs within
     * Dataproc. Setting this to true, and calling destroy, will ensure that the
     * job is first cancelled before issuing the delete.
     * 
     */
    public Output<Optional<Boolean>> forceDelete() {
        return Codegen.optional(this.forceDelete);
    }
    /**
     * The config of Hadoop job
     * 
     */
    @Export(name="hadoopConfig", type=JobHadoopConfig.class, parameters={})
    private Output</* @Nullable */ JobHadoopConfig> hadoopConfig;

    /**
     * @return The config of Hadoop job
     * 
     */
    public Output<Optional<JobHadoopConfig>> hadoopConfig() {
        return Codegen.optional(this.hadoopConfig);
    }
    /**
     * The config of hive job
     * 
     */
    @Export(name="hiveConfig", type=JobHiveConfig.class, parameters={})
    private Output</* @Nullable */ JobHiveConfig> hiveConfig;

    /**
     * @return The config of hive job
     * 
     */
    public Output<Optional<JobHiveConfig>> hiveConfig() {
        return Codegen.optional(this.hiveConfig);
    }
    /**
     * The list of labels (key/value pairs) to add to the job.
     * 
     * * `scheduling.max_failures_per_hour` - (Required) Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     * * `scheduling.max_failures_total` - (Required) Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     */
    @Export(name="labels", type=Map.class, parameters={String.class, String.class})
    private Output</* @Nullable */ Map<String,String>> labels;

    /**
     * @return The list of labels (key/value pairs) to add to the job.
     * 
     * * `scheduling.max_failures_per_hour` - (Required) Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     * * `scheduling.max_failures_total` - (Required) Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     */
    public Output<Optional<Map<String,String>>> labels() {
        return Codegen.optional(this.labels);
    }
    /**
     * The config of pag job.
     * 
     */
    @Export(name="pigConfig", type=JobPigConfig.class, parameters={})
    private Output</* @Nullable */ JobPigConfig> pigConfig;

    /**
     * @return The config of pag job.
     * 
     */
    public Output<Optional<JobPigConfig>> pigConfig() {
        return Codegen.optional(this.pigConfig);
    }
    /**
     * The config of job placement.
     * 
     */
    @Export(name="placement", type=JobPlacement.class, parameters={})
    private Output<JobPlacement> placement;

    /**
     * @return The config of job placement.
     * 
     */
    public Output<JobPlacement> placement() {
        return this.placement;
    }
    /**
     * The config of presto job
     * 
     */
    @Export(name="prestoConfig", type=JobPrestoConfig.class, parameters={})
    private Output</* @Nullable */ JobPrestoConfig> prestoConfig;

    /**
     * @return The config of presto job
     * 
     */
    public Output<Optional<JobPrestoConfig>> prestoConfig() {
        return Codegen.optional(this.prestoConfig);
    }
    /**
     * The project in which the `cluster` can be found and jobs
     * subsequently run against. If it is not provided, the provider project is used.
     * 
     */
    @Export(name="project", type=String.class, parameters={})
    private Output<String> project;

    /**
     * @return The project in which the `cluster` can be found and jobs
     * subsequently run against. If it is not provided, the provider project is used.
     * 
     */
    public Output<String> project() {
        return this.project;
    }
    /**
     * The config of pySpark job.
     * 
     */
    @Export(name="pysparkConfig", type=JobPysparkConfig.class, parameters={})
    private Output</* @Nullable */ JobPysparkConfig> pysparkConfig;

    /**
     * @return The config of pySpark job.
     * 
     */
    public Output<Optional<JobPysparkConfig>> pysparkConfig() {
        return Codegen.optional(this.pysparkConfig);
    }
    /**
     * The reference of the job
     * 
     */
    @Export(name="reference", type=JobReference.class, parameters={})
    private Output<JobReference> reference;

    /**
     * @return The reference of the job
     * 
     */
    public Output<JobReference> reference() {
        return this.reference;
    }
    /**
     * The Cloud Dataproc region. This essentially determines which clusters are available
     * for this job to be submitted to. If not specified, defaults to `global`.
     * 
     */
    @Export(name="region", type=String.class, parameters={})
    private Output</* @Nullable */ String> region;

    /**
     * @return The Cloud Dataproc region. This essentially determines which clusters are available
     * for this job to be submitted to. If not specified, defaults to `global`.
     * 
     */
    public Output<Optional<String>> region() {
        return Codegen.optional(this.region);
    }
    /**
     * Optional. Job scheduling configuration.
     * 
     */
    @Export(name="scheduling", type=JobScheduling.class, parameters={})
    private Output</* @Nullable */ JobScheduling> scheduling;

    /**
     * @return Optional. Job scheduling configuration.
     * 
     */
    public Output<Optional<JobScheduling>> scheduling() {
        return Codegen.optional(this.scheduling);
    }
    /**
     * The config of the Spark job.
     * 
     */
    @Export(name="sparkConfig", type=JobSparkConfig.class, parameters={})
    private Output</* @Nullable */ JobSparkConfig> sparkConfig;

    /**
     * @return The config of the Spark job.
     * 
     */
    public Output<Optional<JobSparkConfig>> sparkConfig() {
        return Codegen.optional(this.sparkConfig);
    }
    /**
     * The config of SparkSql job
     * 
     */
    @Export(name="sparksqlConfig", type=JobSparksqlConfig.class, parameters={})
    private Output</* @Nullable */ JobSparksqlConfig> sparksqlConfig;

    /**
     * @return The config of SparkSql job
     * 
     */
    public Output<Optional<JobSparksqlConfig>> sparksqlConfig() {
        return Codegen.optional(this.sparksqlConfig);
    }
    /**
     * The status of the job.
     * 
     */
    @Export(name="statuses", type=List.class, parameters={JobStatus.class})
    private Output<List<JobStatus>> statuses;

    /**
     * @return The status of the job.
     * 
     */
    public Output<List<JobStatus>> statuses() {
        return this.statuses;
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Job(String name) {
        this(name, JobArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Job(String name, JobArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Job(String name, JobArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/job:Job", name, args == null ? JobArgs.Empty : args, makeResourceOptions(options, Codegen.empty()));
    }

    private Job(String name, Output<String> id, @Nullable JobState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/job:Job", name, state, makeResourceOptions(options, id));
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Job get(String name, Output<String> id, @Nullable JobState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Job(name, id, state, options);
    }
}

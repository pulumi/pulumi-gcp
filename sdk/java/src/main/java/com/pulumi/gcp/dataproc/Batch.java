// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.gcp.dataproc;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.gcp.Utilities;
import com.pulumi.gcp.dataproc.BatchArgs;
import com.pulumi.gcp.dataproc.inputs.BatchState;
import com.pulumi.gcp.dataproc.outputs.BatchEnvironmentConfig;
import com.pulumi.gcp.dataproc.outputs.BatchPysparkBatch;
import com.pulumi.gcp.dataproc.outputs.BatchRuntimeConfig;
import com.pulumi.gcp.dataproc.outputs.BatchRuntimeInfo;
import com.pulumi.gcp.dataproc.outputs.BatchSparkBatch;
import com.pulumi.gcp.dataproc.outputs.BatchSparkRBatch;
import com.pulumi.gcp.dataproc.outputs.BatchSparkSqlBatch;
import com.pulumi.gcp.dataproc.outputs.BatchStateHistory;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * Dataproc Serverless Batches lets you run Spark workloads without requiring you to
 * provision and manage your own Dataproc cluster.
 * 
 * To get more information about Batch, see:
 * 
 * * [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches)
 * * How-to Guides
 *     * [Dataproc Serverless Batches Intro](https://cloud.google.com/dataproc-serverless/docs/overview)
 * 
 * ## Example Usage
 * 
 * ### Dataproc Batch Spark
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.dataproc.Batch;
 * import com.pulumi.gcp.dataproc.BatchArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchRuntimeConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigExecutionConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchSparkBatchArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var exampleBatchSpark = new Batch("exampleBatchSpark", BatchArgs.builder()
 *             .batchId("tf-test-batch_22375")
 *             .location("us-central1")
 *             .labels(Map.of("batch_test", "terraform"))
 *             .runtimeConfig(BatchRuntimeConfigArgs.builder()
 *                 .properties(Map.ofEntries(
 *                     Map.entry("spark.dynamicAllocation.enabled", "false"),
 *                     Map.entry("spark.executor.instances", "2")
 *                 ))
 *                 .build())
 *             .environmentConfig(BatchEnvironmentConfigArgs.builder()
 *                 .executionConfig(BatchEnvironmentConfigExecutionConfigArgs.builder()
 *                     .subnetworkUri("default")
 *                     .ttl("3600s")
 *                     .networkTags("tag1")
 *                     .build())
 *                 .build())
 *             .sparkBatch(BatchSparkBatchArgs.builder()
 *                 .mainClass("org.apache.spark.examples.SparkPi")
 *                 .args("10")
 *                 .jarFileUris("file:///usr/lib/spark/examples/jars/spark-examples.jar")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * ### Dataproc Batch Spark Full
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.organizations.OrganizationsFunctions;
 * import com.pulumi.gcp.organizations.inputs.GetProjectArgs;
 * import com.pulumi.gcp.storage.StorageFunctions;
 * import com.pulumi.gcp.storage.inputs.GetProjectServiceAccountArgs;
 * import com.pulumi.gcp.storage.Bucket;
 * import com.pulumi.gcp.storage.BucketArgs;
 * import com.pulumi.gcp.kms.CryptoKeyIAMMember;
 * import com.pulumi.gcp.kms.CryptoKeyIAMMemberArgs;
 * import com.pulumi.gcp.dataproc.MetastoreService;
 * import com.pulumi.gcp.dataproc.MetastoreServiceArgs;
 * import com.pulumi.gcp.dataproc.inputs.MetastoreServiceMaintenanceWindowArgs;
 * import com.pulumi.gcp.dataproc.inputs.MetastoreServiceHiveMetastoreConfigArgs;
 * import com.pulumi.gcp.dataproc.Cluster;
 * import com.pulumi.gcp.dataproc.ClusterArgs;
 * import com.pulumi.gcp.dataproc.inputs.ClusterClusterConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.ClusterClusterConfigSoftwareConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.ClusterClusterConfigEndpointConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.ClusterClusterConfigMasterConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.ClusterClusterConfigMasterConfigDiskConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.ClusterClusterConfigMetastoreConfigArgs;
 * import com.pulumi.gcp.dataproc.Batch;
 * import com.pulumi.gcp.dataproc.BatchArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchRuntimeConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigExecutionConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigExecutionConfigAuthenticationConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigPeripheralsConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigPeripheralsConfigSparkHistoryServerConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchSparkBatchArgs;
 * import com.pulumi.resources.CustomResourceOptions;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App }{{@code
 *     public static void main(String[] args) }{{@code
 *         Pulumi.run(App::stack);
 *     }}{@code
 * 
 *     public static void stack(Context ctx) }{{@code
 *         final var project = OrganizationsFunctions.getProject(GetProjectArgs.builder()
 *             .build());
 * 
 *         final var gcsAccount = StorageFunctions.getProjectServiceAccount(GetProjectServiceAccountArgs.builder()
 *             .build());
 * 
 *         var bucket = new Bucket("bucket", BucketArgs.builder()
 *             .uniformBucketLevelAccess(true)
 *             .name("dataproc-bucket")
 *             .location("US")
 *             .forceDestroy(true)
 *             .build());
 * 
 *         var cryptoKeyMember1 = new CryptoKeyIAMMember("cryptoKeyMember1", CryptoKeyIAMMemberArgs.builder()
 *             .cryptoKeyId("example-key")
 *             .role("roles/cloudkms.cryptoKeyEncrypterDecrypter")
 *             .member(String.format("serviceAccount:service-%s}{@literal @}{@code dataproc-accounts.iam.gserviceaccount.com", project.number()))
 *             .build());
 * 
 *         var ms = new MetastoreService("ms", MetastoreServiceArgs.builder()
 *             .serviceId("dataproc-batch")
 *             .location("us-central1")
 *             .port(9080)
 *             .tier("DEVELOPER")
 *             .maintenanceWindow(MetastoreServiceMaintenanceWindowArgs.builder()
 *                 .hourOfDay(2)
 *                 .dayOfWeek("SUNDAY")
 *                 .build())
 *             .hiveMetastoreConfig(MetastoreServiceHiveMetastoreConfigArgs.builder()
 *                 .version("3.1.2")
 *                 .build())
 *             .build());
 * 
 *         var basic = new Cluster("basic", ClusterArgs.builder()
 *             .name("dataproc-batch")
 *             .region("us-central1")
 *             .clusterConfig(ClusterClusterConfigArgs.builder()
 *                 .softwareConfig(ClusterClusterConfigSoftwareConfigArgs.builder()
 *                     .overrideProperties(Map.ofEntries(
 *                         Map.entry("dataproc:dataproc.allow.zero.workers", "true"),
 *                         Map.entry("spark:spark.history.fs.logDirectory", bucket.name().applyValue(_name -> String.format("gs://%s/*}&#47;{@code spark-job-history", _name)))
 *                     ))
 *                     .build())
 *                 .endpointConfig(ClusterClusterConfigEndpointConfigArgs.builder()
 *                     .enableHttpPortAccess(true)
 *                     .build())
 *                 .masterConfig(ClusterClusterConfigMasterConfigArgs.builder()
 *                     .numInstances(1)
 *                     .machineType("e2-standard-2")
 *                     .diskConfig(ClusterClusterConfigMasterConfigDiskConfigArgs.builder()
 *                         .bootDiskSizeGb(35)
 *                         .build())
 *                     .build())
 *                 .metastoreConfig(ClusterClusterConfigMetastoreConfigArgs.builder()
 *                     .dataprocMetastoreService(ms.name())
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *         var exampleBatchSpark = new Batch("exampleBatchSpark", BatchArgs.builder()
 *             .batchId("dataproc-batch")
 *             .location("us-central1")
 *             .labels(Map.of("batch_test", "terraform"))
 *             .runtimeConfig(BatchRuntimeConfigArgs.builder()
 *                 .properties(Map.ofEntries(
 *                     Map.entry("spark.dynamicAllocation.enabled", "false"),
 *                     Map.entry("spark.executor.instances", "2")
 *                 ))
 *                 .version("2.2")
 *                 .build())
 *             .environmentConfig(BatchEnvironmentConfigArgs.builder()
 *                 .executionConfig(BatchEnvironmentConfigExecutionConfigArgs.builder()
 *                     .ttl("3600s")
 *                     .networkTags("tag1")
 *                     .kmsKey("example-key")
 *                     .networkUri("default")
 *                     .serviceAccount(String.format("%s-compute}{@literal @}{@code developer.gserviceaccount.com", project.number()))
 *                     .stagingBucket(bucket.name())
 *                     .authenticationConfig(BatchEnvironmentConfigExecutionConfigAuthenticationConfigArgs.builder()
 *                         .userWorkloadAuthenticationType("SERVICE_ACCOUNT")
 *                         .build())
 *                     .build())
 *                 .peripheralsConfig(BatchEnvironmentConfigPeripheralsConfigArgs.builder()
 *                     .metastoreService(ms.name())
 *                     .sparkHistoryServerConfig(BatchEnvironmentConfigPeripheralsConfigSparkHistoryServerConfigArgs.builder()
 *                         .dataprocCluster(basic.id())
 *                         .build())
 *                     .build())
 *                 .build())
 *             .sparkBatch(BatchSparkBatchArgs.builder()
 *                 .mainClass("org.apache.spark.examples.SparkPi")
 *                 .args("10")
 *                 .jarFileUris("file:///usr/lib/spark/examples/jars/spark-examples.jar")
 *                 .build())
 *             .build(), CustomResourceOptions.builder()
 *                 .dependsOn(cryptoKeyMember1)
 *                 .build());
 * 
 *     }}{@code
 * }}{@code
 * }
 * </pre>
 * ### Dataproc Batch Sparksql
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.dataproc.Batch;
 * import com.pulumi.gcp.dataproc.BatchArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchRuntimeConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigExecutionConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchSparkSqlBatchArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var exampleBatchSparsql = new Batch("exampleBatchSparsql", BatchArgs.builder()
 *             .batchId("tf-test-batch_29439")
 *             .location("us-central1")
 *             .runtimeConfig(BatchRuntimeConfigArgs.builder()
 *                 .properties(Map.ofEntries(
 *                     Map.entry("spark.dynamicAllocation.enabled", "false"),
 *                     Map.entry("spark.executor.instances", "2")
 *                 ))
 *                 .build())
 *             .environmentConfig(BatchEnvironmentConfigArgs.builder()
 *                 .executionConfig(BatchEnvironmentConfigExecutionConfigArgs.builder()
 *                     .subnetworkUri("default")
 *                     .build())
 *                 .build())
 *             .sparkSqlBatch(BatchSparkSqlBatchArgs.builder()
 *                 .queryFileUri("gs://dataproc-examples/spark-sql/natality/cigarette_correlations.sql")
 *                 .jarFileUris("file:///usr/lib/spark/examples/jars/spark-examples.jar")
 *                 .queryVariables(Map.of("name", "value"))
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * ### Dataproc Batch Pyspark
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.dataproc.Batch;
 * import com.pulumi.gcp.dataproc.BatchArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchRuntimeConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigExecutionConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchPysparkBatchArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var exampleBatchPyspark = new Batch("exampleBatchPyspark", BatchArgs.builder()
 *             .batchId("tf-test-batch_87786")
 *             .location("us-central1")
 *             .runtimeConfig(BatchRuntimeConfigArgs.builder()
 *                 .properties(Map.ofEntries(
 *                     Map.entry("spark.dynamicAllocation.enabled", "false"),
 *                     Map.entry("spark.executor.instances", "2")
 *                 ))
 *                 .build())
 *             .environmentConfig(BatchEnvironmentConfigArgs.builder()
 *                 .executionConfig(BatchEnvironmentConfigExecutionConfigArgs.builder()
 *                     .subnetworkUri("default")
 *                     .build())
 *                 .build())
 *             .pysparkBatch(BatchPysparkBatchArgs.builder()
 *                 .mainPythonFileUri("https://storage.googleapis.com/terraform-batches/test_util.py")
 *                 .args("10")
 *                 .jarFileUris("file:///usr/lib/spark/examples/jars/spark-examples.jar")
 *                 .pythonFileUris("gs://dataproc-examples/pyspark/hello-world/hello-world.py")
 *                 .archiveUris(                
 *                     "https://storage.googleapis.com/terraform-batches/animals.txt.tar.gz#unpacked",
 *                     "https://storage.googleapis.com/terraform-batches/animals.txt.jar",
 *                     "https://storage.googleapis.com/terraform-batches/animals.txt")
 *                 .fileUris("https://storage.googleapis.com/terraform-batches/people.txt")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * ### Dataproc Batch Sparkr
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.dataproc.Batch;
 * import com.pulumi.gcp.dataproc.BatchArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchRuntimeConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigExecutionConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchSparkRBatchArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var exampleBatchSparkr = new Batch("exampleBatchSparkr", BatchArgs.builder()
 *             .batchId("tf-test-batch_2067")
 *             .location("us-central1")
 *             .labels(Map.of("batch_test", "terraform"))
 *             .runtimeConfig(BatchRuntimeConfigArgs.builder()
 *                 .properties(Map.ofEntries(
 *                     Map.entry("spark.dynamicAllocation.enabled", "false"),
 *                     Map.entry("spark.executor.instances", "2")
 *                 ))
 *                 .build())
 *             .environmentConfig(BatchEnvironmentConfigArgs.builder()
 *                 .executionConfig(BatchEnvironmentConfigExecutionConfigArgs.builder()
 *                     .subnetworkUri("default")
 *                     .ttl("3600s")
 *                     .networkTags("tag1")
 *                     .build())
 *                 .build())
 *             .sparkRBatch(BatchSparkRBatchArgs.builder()
 *                 .mainRFileUri("https://storage.googleapis.com/terraform-batches/spark-r-flights.r")
 *                 .args("https://storage.googleapis.com/terraform-batches/flights.csv")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * ### Dataproc Batch Autotuning
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.dataproc.Batch;
 * import com.pulumi.gcp.dataproc.BatchArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchRuntimeConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchRuntimeConfigAutotuningConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchEnvironmentConfigExecutionConfigArgs;
 * import com.pulumi.gcp.dataproc.inputs.BatchSparkBatchArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var exampleBatchAutotuning = new Batch("exampleBatchAutotuning", BatchArgs.builder()
 *             .batchId("tf-test-batch_40785")
 *             .location("us-central1")
 *             .labels(Map.of("batch_test", "terraform"))
 *             .runtimeConfig(BatchRuntimeConfigArgs.builder()
 *                 .version("2.2")
 *                 .properties(Map.ofEntries(
 *                     Map.entry("spark.dynamicAllocation.enabled", "false"),
 *                     Map.entry("spark.executor.instances", "2")
 *                 ))
 *                 .cohort("tf-dataproc-batch-example")
 *                 .autotuningConfig(BatchRuntimeConfigAutotuningConfigArgs.builder()
 *                     .scenarios(                    
 *                         "SCALING",
 *                         "MEMORY")
 *                     .build())
 *                 .build())
 *             .environmentConfig(BatchEnvironmentConfigArgs.builder()
 *                 .executionConfig(BatchEnvironmentConfigExecutionConfigArgs.builder()
 *                     .subnetworkUri("default")
 *                     .ttl("3600s")
 *                     .build())
 *                 .build())
 *             .sparkBatch(BatchSparkBatchArgs.builder()
 *                 .mainClass("org.apache.spark.examples.SparkPi")
 *                 .args("10")
 *                 .jarFileUris("file:///usr/lib/spark/examples/jars/spark-examples.jar")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Import
 * 
 * Batch can be imported using any of these accepted formats:
 * 
 * * `projects/{{project}}/locations/{{location}}/batches/{{batch_id}}`
 * 
 * * `{{project}}/{{location}}/{{batch_id}}`
 * 
 * * `{{location}}/{{batch_id}}`
 * 
 * When using the `pulumi import` command, Batch can be imported using one of the formats above. For example:
 * 
 * ```sh
 * $ pulumi import gcp:dataproc/batch:Batch default projects/{{project}}/locations/{{location}}/batches/{{batch_id}}
 * ```
 * 
 * ```sh
 * $ pulumi import gcp:dataproc/batch:Batch default {{project}}/{{location}}/{{batch_id}}
 * ```
 * 
 * ```sh
 * $ pulumi import gcp:dataproc/batch:Batch default {{location}}/{{batch_id}}
 * ```
 * 
 */
@ResourceType(type="gcp:dataproc/batch:Batch")
public class Batch extends com.pulumi.resources.CustomResource {
    /**
     * The ID to use for the batch, which will become the final component of the batch&#39;s resource name.
     * This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
     * 
     */
    @Export(name="batchId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> batchId;

    /**
     * @return The ID to use for the batch, which will become the final component of the batch&#39;s resource name.
     * This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
     * 
     */
    public Output<Optional<String>> batchId() {
        return Codegen.optional(this.batchId);
    }
    /**
     * The time when the batch was created.
     * 
     */
    @Export(name="createTime", refs={String.class}, tree="[0]")
    private Output<String> createTime;

    /**
     * @return The time when the batch was created.
     * 
     */
    public Output<String> createTime() {
        return this.createTime;
    }
    /**
     * The email address of the user who created the batch.
     * 
     */
    @Export(name="creator", refs={String.class}, tree="[0]")
    private Output<String> creator;

    /**
     * @return The email address of the user who created the batch.
     * 
     */
    public Output<String> creator() {
        return this.creator;
    }
    /**
     * All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
     * 
     */
    @Export(name="effectiveLabels", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output<Map<String,String>> effectiveLabels;

    /**
     * @return All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
     * 
     */
    public Output<Map<String,String>> effectiveLabels() {
        return this.effectiveLabels;
    }
    /**
     * Environment configuration for the batch execution.
     * Structure is documented below.
     * 
     */
    @Export(name="environmentConfig", refs={BatchEnvironmentConfig.class}, tree="[0]")
    private Output</* @Nullable */ BatchEnvironmentConfig> environmentConfig;

    /**
     * @return Environment configuration for the batch execution.
     * Structure is documented below.
     * 
     */
    public Output<Optional<BatchEnvironmentConfig>> environmentConfig() {
        return Codegen.optional(this.environmentConfig);
    }
    /**
     * The labels to associate with this batch.
     * 
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field `effectiveLabels` for all of the labels present on the resource.
     * 
     */
    @Export(name="labels", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> labels;

    /**
     * @return The labels to associate with this batch.
     * 
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field `effectiveLabels` for all of the labels present on the resource.
     * 
     */
    public Output<Optional<Map<String,String>>> labels() {
        return Codegen.optional(this.labels);
    }
    /**
     * The location in which the batch will be created in.
     * 
     */
    @Export(name="location", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> location;

    /**
     * @return The location in which the batch will be created in.
     * 
     */
    public Output<Optional<String>> location() {
        return Codegen.optional(this.location);
    }
    /**
     * The resource name of the batch.
     * 
     */
    @Export(name="name", refs={String.class}, tree="[0]")
    private Output<String> name;

    /**
     * @return The resource name of the batch.
     * 
     */
    public Output<String> name() {
        return this.name;
    }
    /**
     * The resource name of the operation associated with this batch.
     * 
     */
    @Export(name="operation", refs={String.class}, tree="[0]")
    private Output<String> operation;

    /**
     * @return The resource name of the operation associated with this batch.
     * 
     */
    public Output<String> operation() {
        return this.operation;
    }
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     * 
     */
    @Export(name="project", refs={String.class}, tree="[0]")
    private Output<String> project;

    /**
     * @return The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     * 
     */
    public Output<String> project() {
        return this.project;
    }
    /**
     * The combination of labels configured directly on the resource
     * and default labels configured on the provider.
     * 
     */
    @Export(name="pulumiLabels", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output<Map<String,String>> pulumiLabels;

    /**
     * @return The combination of labels configured directly on the resource
     * and default labels configured on the provider.
     * 
     */
    public Output<Map<String,String>> pulumiLabels() {
        return this.pulumiLabels;
    }
    /**
     * PySpark batch config.
     * Structure is documented below.
     * 
     */
    @Export(name="pysparkBatch", refs={BatchPysparkBatch.class}, tree="[0]")
    private Output</* @Nullable */ BatchPysparkBatch> pysparkBatch;

    /**
     * @return PySpark batch config.
     * Structure is documented below.
     * 
     */
    public Output<Optional<BatchPysparkBatch>> pysparkBatch() {
        return Codegen.optional(this.pysparkBatch);
    }
    /**
     * Runtime configuration for the batch execution.
     * Structure is documented below.
     * 
     */
    @Export(name="runtimeConfig", refs={BatchRuntimeConfig.class}, tree="[0]")
    private Output</* @Nullable */ BatchRuntimeConfig> runtimeConfig;

    /**
     * @return Runtime configuration for the batch execution.
     * Structure is documented below.
     * 
     */
    public Output<Optional<BatchRuntimeConfig>> runtimeConfig() {
        return Codegen.optional(this.runtimeConfig);
    }
    /**
     * Runtime information about batch execution.
     * Structure is documented below.
     * 
     */
    @Export(name="runtimeInfos", refs={List.class,BatchRuntimeInfo.class}, tree="[0,1]")
    private Output<List<BatchRuntimeInfo>> runtimeInfos;

    /**
     * @return Runtime information about batch execution.
     * Structure is documented below.
     * 
     */
    public Output<List<BatchRuntimeInfo>> runtimeInfos() {
        return this.runtimeInfos;
    }
    /**
     * Spark batch config.
     * Structure is documented below.
     * 
     */
    @Export(name="sparkBatch", refs={BatchSparkBatch.class}, tree="[0]")
    private Output</* @Nullable */ BatchSparkBatch> sparkBatch;

    /**
     * @return Spark batch config.
     * Structure is documented below.
     * 
     */
    public Output<Optional<BatchSparkBatch>> sparkBatch() {
        return Codegen.optional(this.sparkBatch);
    }
    /**
     * SparkR batch config.
     * Structure is documented below.
     * 
     */
    @Export(name="sparkRBatch", refs={BatchSparkRBatch.class}, tree="[0]")
    private Output</* @Nullable */ BatchSparkRBatch> sparkRBatch;

    /**
     * @return SparkR batch config.
     * Structure is documented below.
     * 
     */
    public Output<Optional<BatchSparkRBatch>> sparkRBatch() {
        return Codegen.optional(this.sparkRBatch);
    }
    /**
     * Spark SQL batch config.
     * Structure is documented below.
     * 
     */
    @Export(name="sparkSqlBatch", refs={BatchSparkSqlBatch.class}, tree="[0]")
    private Output</* @Nullable */ BatchSparkSqlBatch> sparkSqlBatch;

    /**
     * @return Spark SQL batch config.
     * Structure is documented below.
     * 
     */
    public Output<Optional<BatchSparkSqlBatch>> sparkSqlBatch() {
        return Codegen.optional(this.sparkSqlBatch);
    }
    /**
     * (Output)
     * The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
     * 
     */
    @Export(name="state", refs={String.class}, tree="[0]")
    private Output<String> state;

    /**
     * @return (Output)
     * The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
     * 
     */
    public Output<String> state() {
        return this.state;
    }
    /**
     * Historical state information for the batch.
     * Structure is documented below.
     * 
     */
    @Export(name="stateHistories", refs={List.class,BatchStateHistory.class}, tree="[0,1]")
    private Output<List<BatchStateHistory>> stateHistories;

    /**
     * @return Historical state information for the batch.
     * Structure is documented below.
     * 
     */
    public Output<List<BatchStateHistory>> stateHistories() {
        return this.stateHistories;
    }
    /**
     * (Output)
     * Details about the state at this point in history.
     * 
     */
    @Export(name="stateMessage", refs={String.class}, tree="[0]")
    private Output<String> stateMessage;

    /**
     * @return (Output)
     * Details about the state at this point in history.
     * 
     */
    public Output<String> stateMessage() {
        return this.stateMessage;
    }
    /**
     * Batch state details, such as a failure description if the state is FAILED.
     * 
     */
    @Export(name="stateTime", refs={String.class}, tree="[0]")
    private Output<String> stateTime;

    /**
     * @return Batch state details, such as a failure description if the state is FAILED.
     * 
     */
    public Output<String> stateTime() {
        return this.stateTime;
    }
    /**
     * A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
     * 
     */
    @Export(name="uuid", refs={String.class}, tree="[0]")
    private Output<String> uuid;

    /**
     * @return A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
     * 
     */
    public Output<String> uuid() {
        return this.uuid;
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Batch(java.lang.String name) {
        this(name, BatchArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Batch(java.lang.String name, @Nullable BatchArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Batch(java.lang.String name, @Nullable BatchArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/batch:Batch", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private Batch(java.lang.String name, Output<java.lang.String> id, @Nullable BatchState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/batch:Batch", name, state, makeResourceOptions(options, id), false);
    }

    private static BatchArgs makeArgs(@Nullable BatchArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? BatchArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .additionalSecretOutputs(List.of(
                "effectiveLabels",
                "pulumiLabels"
            ))
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Batch get(java.lang.String name, Output<java.lang.String> id, @Nullable BatchState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Batch(name, id, state, options);
    }
}

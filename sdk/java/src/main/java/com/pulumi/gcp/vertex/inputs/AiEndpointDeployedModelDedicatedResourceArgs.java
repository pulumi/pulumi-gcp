// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.gcp.vertex.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.gcp.vertex.inputs.AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs;
import com.pulumi.gcp.vertex.inputs.AiEndpointDeployedModelDedicatedResourceMachineSpecArgs;
import java.lang.Integer;
import java.util.List;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class AiEndpointDeployedModelDedicatedResourceArgs extends com.pulumi.resources.ResourceArgs {

    public static final AiEndpointDeployedModelDedicatedResourceArgs Empty = new AiEndpointDeployedModelDedicatedResourceArgs();

    /**
     * (Output)
     * The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator&#39;s duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator&#39;s duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
     * Structure is documented below.
     * 
     */
    @Import(name="autoscalingMetricSpecs")
    private @Nullable Output<List<AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs>> autoscalingMetricSpecs;

    /**
     * @return (Output)
     * The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator&#39;s duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator&#39;s duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
     * Structure is documented below.
     * 
     */
    public Optional<Output<List<AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs>>> autoscalingMetricSpecs() {
        return Optional.ofNullable(this.autoscalingMetricSpecs);
    }

    /**
     * (Output)
     * The specification of a single machine used by the prediction.
     * Structure is documented below.
     * 
     */
    @Import(name="machineSpecs")
    private @Nullable Output<List<AiEndpointDeployedModelDedicatedResourceMachineSpecArgs>> machineSpecs;

    /**
     * @return (Output)
     * The specification of a single machine used by the prediction.
     * Structure is documented below.
     * 
     */
    public Optional<Output<List<AiEndpointDeployedModelDedicatedResourceMachineSpecArgs>>> machineSpecs() {
        return Optional.ofNullable(this.machineSpecs);
    }

    /**
     * (Output)
     * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
     * 
     */
    @Import(name="maxReplicaCount")
    private @Nullable Output<Integer> maxReplicaCount;

    /**
     * @return (Output)
     * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
     * 
     */
    public Optional<Output<Integer>> maxReplicaCount() {
        return Optional.ofNullable(this.maxReplicaCount);
    }

    /**
     * (Output)
     * The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
     * 
     */
    @Import(name="minReplicaCount")
    private @Nullable Output<Integer> minReplicaCount;

    /**
     * @return (Output)
     * The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
     * 
     */
    public Optional<Output<Integer>> minReplicaCount() {
        return Optional.ofNullable(this.minReplicaCount);
    }

    private AiEndpointDeployedModelDedicatedResourceArgs() {}

    private AiEndpointDeployedModelDedicatedResourceArgs(AiEndpointDeployedModelDedicatedResourceArgs $) {
        this.autoscalingMetricSpecs = $.autoscalingMetricSpecs;
        this.machineSpecs = $.machineSpecs;
        this.maxReplicaCount = $.maxReplicaCount;
        this.minReplicaCount = $.minReplicaCount;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(AiEndpointDeployedModelDedicatedResourceArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private AiEndpointDeployedModelDedicatedResourceArgs $;

        public Builder() {
            $ = new AiEndpointDeployedModelDedicatedResourceArgs();
        }

        public Builder(AiEndpointDeployedModelDedicatedResourceArgs defaults) {
            $ = new AiEndpointDeployedModelDedicatedResourceArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param autoscalingMetricSpecs (Output)
         * The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator&#39;s duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator&#39;s duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder autoscalingMetricSpecs(@Nullable Output<List<AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs>> autoscalingMetricSpecs) {
            $.autoscalingMetricSpecs = autoscalingMetricSpecs;
            return this;
        }

        /**
         * @param autoscalingMetricSpecs (Output)
         * The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator&#39;s duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator&#39;s duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder autoscalingMetricSpecs(List<AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs> autoscalingMetricSpecs) {
            return autoscalingMetricSpecs(Output.of(autoscalingMetricSpecs));
        }

        /**
         * @param autoscalingMetricSpecs (Output)
         * The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator&#39;s duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator&#39;s duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder autoscalingMetricSpecs(AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs... autoscalingMetricSpecs) {
            return autoscalingMetricSpecs(List.of(autoscalingMetricSpecs));
        }

        /**
         * @param machineSpecs (Output)
         * The specification of a single machine used by the prediction.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder machineSpecs(@Nullable Output<List<AiEndpointDeployedModelDedicatedResourceMachineSpecArgs>> machineSpecs) {
            $.machineSpecs = machineSpecs;
            return this;
        }

        /**
         * @param machineSpecs (Output)
         * The specification of a single machine used by the prediction.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder machineSpecs(List<AiEndpointDeployedModelDedicatedResourceMachineSpecArgs> machineSpecs) {
            return machineSpecs(Output.of(machineSpecs));
        }

        /**
         * @param machineSpecs (Output)
         * The specification of a single machine used by the prediction.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder machineSpecs(AiEndpointDeployedModelDedicatedResourceMachineSpecArgs... machineSpecs) {
            return machineSpecs(List.of(machineSpecs));
        }

        /**
         * @param maxReplicaCount (Output)
         * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
         * 
         * @return builder
         * 
         */
        public Builder maxReplicaCount(@Nullable Output<Integer> maxReplicaCount) {
            $.maxReplicaCount = maxReplicaCount;
            return this;
        }

        /**
         * @param maxReplicaCount (Output)
         * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
         * 
         * @return builder
         * 
         */
        public Builder maxReplicaCount(Integer maxReplicaCount) {
            return maxReplicaCount(Output.of(maxReplicaCount));
        }

        /**
         * @param minReplicaCount (Output)
         * The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
         * 
         * @return builder
         * 
         */
        public Builder minReplicaCount(@Nullable Output<Integer> minReplicaCount) {
            $.minReplicaCount = minReplicaCount;
            return this;
        }

        /**
         * @param minReplicaCount (Output)
         * The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
         * 
         * @return builder
         * 
         */
        public Builder minReplicaCount(Integer minReplicaCount) {
            return minReplicaCount(Output.of(minReplicaCount));
        }

        public AiEndpointDeployedModelDedicatedResourceArgs build() {
            return $;
        }
    }

}

// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.gcp.vertex.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.gcp.vertex.inputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityArgs;
import java.lang.Integer;
import java.lang.String;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs extends com.pulumi.resources.ResourceArgs {

    public static final AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs Empty = new AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs();

    /**
     * The number of accelerators to attach to the machine.
     * 
     */
    @Import(name="acceleratorCount")
    private @Nullable Output<Integer> acceleratorCount;

    /**
     * @return The number of accelerators to attach to the machine.
     * 
     */
    public Optional<Output<Integer>> acceleratorCount() {
        return Optional.ofNullable(this.acceleratorCount);
    }

    /**
     * Possible values:
     * ACCELERATOR_TYPE_UNSPECIFIED
     * NVIDIA_TESLA_K80
     * NVIDIA_TESLA_P100
     * NVIDIA_TESLA_V100
     * NVIDIA_TESLA_P4
     * NVIDIA_TESLA_T4
     * NVIDIA_TESLA_A100
     * NVIDIA_A100_80GB
     * NVIDIA_L4
     * NVIDIA_H100_80GB
     * NVIDIA_H100_MEGA_80GB
     * NVIDIA_H200_141GB
     * NVIDIA_B200
     * TPU_V2
     * TPU_V3
     * TPU_V4_POD
     * TPU_V5_LITEPOD
     * 
     */
    @Import(name="acceleratorType")
    private @Nullable Output<String> acceleratorType;

    /**
     * @return Possible values:
     * ACCELERATOR_TYPE_UNSPECIFIED
     * NVIDIA_TESLA_K80
     * NVIDIA_TESLA_P100
     * NVIDIA_TESLA_V100
     * NVIDIA_TESLA_P4
     * NVIDIA_TESLA_T4
     * NVIDIA_TESLA_A100
     * NVIDIA_A100_80GB
     * NVIDIA_L4
     * NVIDIA_H100_80GB
     * NVIDIA_H100_MEGA_80GB
     * NVIDIA_H200_141GB
     * NVIDIA_B200
     * TPU_V2
     * TPU_V3
     * TPU_V4_POD
     * TPU_V5_LITEPOD
     * 
     */
    public Optional<Output<String>> acceleratorType() {
        return Optional.ofNullable(this.acceleratorType);
    }

    /**
     * The type of the machine.
     * See the [list of machine types supported for
     * prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
     * See the [list of machine types supported for custom
     * training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
     * For DeployedModel this field is optional, and the default
     * value is `n1-standard-2`. For BatchPredictionJob or as part of
     * WorkerPoolSpec this field is required.
     * 
     */
    @Import(name="machineType")
    private @Nullable Output<String> machineType;

    /**
     * @return The type of the machine.
     * See the [list of machine types supported for
     * prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
     * See the [list of machine types supported for custom
     * training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
     * For DeployedModel this field is optional, and the default
     * value is `n1-standard-2`. For BatchPredictionJob or as part of
     * WorkerPoolSpec this field is required.
     * 
     */
    public Optional<Output<String>> machineType() {
        return Optional.ofNullable(this.machineType);
    }

    /**
     * The number of nodes per replica for multihost GPU deployments.
     * 
     */
    @Import(name="multihostGpuNodeCount")
    private @Nullable Output<Integer> multihostGpuNodeCount;

    /**
     * @return The number of nodes per replica for multihost GPU deployments.
     * 
     */
    public Optional<Output<Integer>> multihostGpuNodeCount() {
        return Optional.ofNullable(this.multihostGpuNodeCount);
    }

    /**
     * A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
     * DeployedModel) to draw its Compute Engine resources from a Shared
     * Reservation, or exclusively from on-demand capacity.
     * Structure is documented below.
     * 
     */
    @Import(name="reservationAffinity")
    private @Nullable Output<AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityArgs> reservationAffinity;

    /**
     * @return A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
     * DeployedModel) to draw its Compute Engine resources from a Shared
     * Reservation, or exclusively from on-demand capacity.
     * Structure is documented below.
     * 
     */
    public Optional<Output<AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityArgs>> reservationAffinity() {
        return Optional.ofNullable(this.reservationAffinity);
    }

    /**
     * The topology of the TPUs. Corresponds to the TPU topologies available from
     * GKE. (Example: tpu_topology: &#34;2x2x1&#34;).
     * 
     */
    @Import(name="tpuTopology")
    private @Nullable Output<String> tpuTopology;

    /**
     * @return The topology of the TPUs. Corresponds to the TPU topologies available from
     * GKE. (Example: tpu_topology: &#34;2x2x1&#34;).
     * 
     */
    public Optional<Output<String>> tpuTopology() {
        return Optional.ofNullable(this.tpuTopology);
    }

    private AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs() {}

    private AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs(AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs $) {
        this.acceleratorCount = $.acceleratorCount;
        this.acceleratorType = $.acceleratorType;
        this.machineType = $.machineType;
        this.multihostGpuNodeCount = $.multihostGpuNodeCount;
        this.reservationAffinity = $.reservationAffinity;
        this.tpuTopology = $.tpuTopology;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs $;

        public Builder() {
            $ = new AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs();
        }

        public Builder(AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs defaults) {
            $ = new AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param acceleratorCount The number of accelerators to attach to the machine.
         * 
         * @return builder
         * 
         */
        public Builder acceleratorCount(@Nullable Output<Integer> acceleratorCount) {
            $.acceleratorCount = acceleratorCount;
            return this;
        }

        /**
         * @param acceleratorCount The number of accelerators to attach to the machine.
         * 
         * @return builder
         * 
         */
        public Builder acceleratorCount(Integer acceleratorCount) {
            return acceleratorCount(Output.of(acceleratorCount));
        }

        /**
         * @param acceleratorType Possible values:
         * ACCELERATOR_TYPE_UNSPECIFIED
         * NVIDIA_TESLA_K80
         * NVIDIA_TESLA_P100
         * NVIDIA_TESLA_V100
         * NVIDIA_TESLA_P4
         * NVIDIA_TESLA_T4
         * NVIDIA_TESLA_A100
         * NVIDIA_A100_80GB
         * NVIDIA_L4
         * NVIDIA_H100_80GB
         * NVIDIA_H100_MEGA_80GB
         * NVIDIA_H200_141GB
         * NVIDIA_B200
         * TPU_V2
         * TPU_V3
         * TPU_V4_POD
         * TPU_V5_LITEPOD
         * 
         * @return builder
         * 
         */
        public Builder acceleratorType(@Nullable Output<String> acceleratorType) {
            $.acceleratorType = acceleratorType;
            return this;
        }

        /**
         * @param acceleratorType Possible values:
         * ACCELERATOR_TYPE_UNSPECIFIED
         * NVIDIA_TESLA_K80
         * NVIDIA_TESLA_P100
         * NVIDIA_TESLA_V100
         * NVIDIA_TESLA_P4
         * NVIDIA_TESLA_T4
         * NVIDIA_TESLA_A100
         * NVIDIA_A100_80GB
         * NVIDIA_L4
         * NVIDIA_H100_80GB
         * NVIDIA_H100_MEGA_80GB
         * NVIDIA_H200_141GB
         * NVIDIA_B200
         * TPU_V2
         * TPU_V3
         * TPU_V4_POD
         * TPU_V5_LITEPOD
         * 
         * @return builder
         * 
         */
        public Builder acceleratorType(String acceleratorType) {
            return acceleratorType(Output.of(acceleratorType));
        }

        /**
         * @param machineType The type of the machine.
         * See the [list of machine types supported for
         * prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
         * See the [list of machine types supported for custom
         * training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
         * For DeployedModel this field is optional, and the default
         * value is `n1-standard-2`. For BatchPredictionJob or as part of
         * WorkerPoolSpec this field is required.
         * 
         * @return builder
         * 
         */
        public Builder machineType(@Nullable Output<String> machineType) {
            $.machineType = machineType;
            return this;
        }

        /**
         * @param machineType The type of the machine.
         * See the [list of machine types supported for
         * prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
         * See the [list of machine types supported for custom
         * training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
         * For DeployedModel this field is optional, and the default
         * value is `n1-standard-2`. For BatchPredictionJob or as part of
         * WorkerPoolSpec this field is required.
         * 
         * @return builder
         * 
         */
        public Builder machineType(String machineType) {
            return machineType(Output.of(machineType));
        }

        /**
         * @param multihostGpuNodeCount The number of nodes per replica for multihost GPU deployments.
         * 
         * @return builder
         * 
         */
        public Builder multihostGpuNodeCount(@Nullable Output<Integer> multihostGpuNodeCount) {
            $.multihostGpuNodeCount = multihostGpuNodeCount;
            return this;
        }

        /**
         * @param multihostGpuNodeCount The number of nodes per replica for multihost GPU deployments.
         * 
         * @return builder
         * 
         */
        public Builder multihostGpuNodeCount(Integer multihostGpuNodeCount) {
            return multihostGpuNodeCount(Output.of(multihostGpuNodeCount));
        }

        /**
         * @param reservationAffinity A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
         * DeployedModel) to draw its Compute Engine resources from a Shared
         * Reservation, or exclusively from on-demand capacity.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder reservationAffinity(@Nullable Output<AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityArgs> reservationAffinity) {
            $.reservationAffinity = reservationAffinity;
            return this;
        }

        /**
         * @param reservationAffinity A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
         * DeployedModel) to draw its Compute Engine resources from a Shared
         * Reservation, or exclusively from on-demand capacity.
         * Structure is documented below.
         * 
         * @return builder
         * 
         */
        public Builder reservationAffinity(AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityArgs reservationAffinity) {
            return reservationAffinity(Output.of(reservationAffinity));
        }

        /**
         * @param tpuTopology The topology of the TPUs. Corresponds to the TPU topologies available from
         * GKE. (Example: tpu_topology: &#34;2x2x1&#34;).
         * 
         * @return builder
         * 
         */
        public Builder tpuTopology(@Nullable Output<String> tpuTopology) {
            $.tpuTopology = tpuTopology;
            return this;
        }

        /**
         * @param tpuTopology The topology of the TPUs. Corresponds to the TPU topologies available from
         * GKE. (Example: tpu_topology: &#34;2x2x1&#34;).
         * 
         * @return builder
         * 
         */
        public Builder tpuTopology(String tpuTopology) {
            return tpuTopology(Output.of(tpuTopology));
        }

        public AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs build() {
            return $;
        }
    }

}

// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.gcp.dataproc.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.gcp.dataproc.inputs.JobHadoopConfigArgs;
import com.pulumi.gcp.dataproc.inputs.JobHiveConfigArgs;
import com.pulumi.gcp.dataproc.inputs.JobPigConfigArgs;
import com.pulumi.gcp.dataproc.inputs.JobPlacementArgs;
import com.pulumi.gcp.dataproc.inputs.JobPrestoConfigArgs;
import com.pulumi.gcp.dataproc.inputs.JobPysparkConfigArgs;
import com.pulumi.gcp.dataproc.inputs.JobReferenceArgs;
import com.pulumi.gcp.dataproc.inputs.JobSchedulingArgs;
import com.pulumi.gcp.dataproc.inputs.JobSparkConfigArgs;
import com.pulumi.gcp.dataproc.inputs.JobSparksqlConfigArgs;
import com.pulumi.gcp.dataproc.inputs.JobStatusArgs;
import java.lang.Boolean;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class JobState extends com.pulumi.resources.ResourceArgs {

    public static final JobState Empty = new JobState();

    /**
     * If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
     * 
     */
    @Import(name="driverControlsFilesUri")
    private @Nullable Output<String> driverControlsFilesUri;

    /**
     * @return If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
     * 
     */
    public Optional<Output<String>> driverControlsFilesUri() {
        return Optional.ofNullable(this.driverControlsFilesUri);
    }

    /**
     * A URI pointing to the location of the stdout of the job&#39;s driver program.
     * 
     */
    @Import(name="driverOutputResourceUri")
    private @Nullable Output<String> driverOutputResourceUri;

    /**
     * @return A URI pointing to the location of the stdout of the job&#39;s driver program.
     * 
     */
    public Optional<Output<String>> driverOutputResourceUri() {
        return Optional.ofNullable(this.driverOutputResourceUri);
    }

    /**
     * All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
     * 
     * * `scheduling.max_failures_per_hour` - (Required) Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     * * `scheduling.max_failures_total` - (Required) Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     */
    @Import(name="effectiveLabels")
    private @Nullable Output<Map<String,String>> effectiveLabels;

    /**
     * @return All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
     * 
     * * `scheduling.max_failures_per_hour` - (Required) Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     * * `scheduling.max_failures_total` - (Required) Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     * 
     */
    public Optional<Output<Map<String,String>>> effectiveLabels() {
        return Optional.ofNullable(this.effectiveLabels);
    }

    /**
     * By default, you can only delete inactive jobs within
     * Dataproc. Setting this to true, and calling destroy, will ensure that the
     * job is first cancelled before issuing the delete.
     * 
     */
    @Import(name="forceDelete")
    private @Nullable Output<Boolean> forceDelete;

    /**
     * @return By default, you can only delete inactive jobs within
     * Dataproc. Setting this to true, and calling destroy, will ensure that the
     * job is first cancelled before issuing the delete.
     * 
     */
    public Optional<Output<Boolean>> forceDelete() {
        return Optional.ofNullable(this.forceDelete);
    }

    /**
     * The config of Hadoop job
     * 
     */
    @Import(name="hadoopConfig")
    private @Nullable Output<JobHadoopConfigArgs> hadoopConfig;

    /**
     * @return The config of Hadoop job
     * 
     */
    public Optional<Output<JobHadoopConfigArgs>> hadoopConfig() {
        return Optional.ofNullable(this.hadoopConfig);
    }

    /**
     * The config of hive job
     * 
     */
    @Import(name="hiveConfig")
    private @Nullable Output<JobHiveConfigArgs> hiveConfig;

    /**
     * @return The config of hive job
     * 
     */
    public Optional<Output<JobHiveConfigArgs>> hiveConfig() {
        return Optional.ofNullable(this.hiveConfig);
    }

    /**
     * The list of labels (key/value pairs) to add to the job.
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field &#39;effective_labels&#39; for all of the labels present on the resource.
     * 
     */
    @Import(name="labels")
    private @Nullable Output<Map<String,String>> labels;

    /**
     * @return The list of labels (key/value pairs) to add to the job.
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field &#39;effective_labels&#39; for all of the labels present on the resource.
     * 
     */
    public Optional<Output<Map<String,String>>> labels() {
        return Optional.ofNullable(this.labels);
    }

    /**
     * The config of pag job.
     * 
     */
    @Import(name="pigConfig")
    private @Nullable Output<JobPigConfigArgs> pigConfig;

    /**
     * @return The config of pag job.
     * 
     */
    public Optional<Output<JobPigConfigArgs>> pigConfig() {
        return Optional.ofNullable(this.pigConfig);
    }

    /**
     * The config of job placement.
     * 
     */
    @Import(name="placement")
    private @Nullable Output<JobPlacementArgs> placement;

    /**
     * @return The config of job placement.
     * 
     */
    public Optional<Output<JobPlacementArgs>> placement() {
        return Optional.ofNullable(this.placement);
    }

    /**
     * The config of presto job
     * 
     */
    @Import(name="prestoConfig")
    private @Nullable Output<JobPrestoConfigArgs> prestoConfig;

    /**
     * @return The config of presto job
     * 
     */
    public Optional<Output<JobPrestoConfigArgs>> prestoConfig() {
        return Optional.ofNullable(this.prestoConfig);
    }

    /**
     * The project in which the `cluster` can be found and jobs
     * subsequently run against. If it is not provided, the provider project is used.
     * 
     */
    @Import(name="project")
    private @Nullable Output<String> project;

    /**
     * @return The project in which the `cluster` can be found and jobs
     * subsequently run against. If it is not provided, the provider project is used.
     * 
     */
    public Optional<Output<String>> project() {
        return Optional.ofNullable(this.project);
    }

    /**
     * The combination of labels configured directly on the resource and default labels configured on the provider.
     * 
     */
    @Import(name="pulumiLabels")
    private @Nullable Output<Map<String,String>> pulumiLabels;

    /**
     * @return The combination of labels configured directly on the resource and default labels configured on the provider.
     * 
     */
    public Optional<Output<Map<String,String>>> pulumiLabels() {
        return Optional.ofNullable(this.pulumiLabels);
    }

    /**
     * The config of pySpark job.
     * 
     */
    @Import(name="pysparkConfig")
    private @Nullable Output<JobPysparkConfigArgs> pysparkConfig;

    /**
     * @return The config of pySpark job.
     * 
     */
    public Optional<Output<JobPysparkConfigArgs>> pysparkConfig() {
        return Optional.ofNullable(this.pysparkConfig);
    }

    /**
     * The reference of the job
     * 
     */
    @Import(name="reference")
    private @Nullable Output<JobReferenceArgs> reference;

    /**
     * @return The reference of the job
     * 
     */
    public Optional<Output<JobReferenceArgs>> reference() {
        return Optional.ofNullable(this.reference);
    }

    /**
     * The Cloud Dataproc region. This essentially determines which clusters are available
     * for this job to be submitted to. If not specified, defaults to `global`.
     * 
     */
    @Import(name="region")
    private @Nullable Output<String> region;

    /**
     * @return The Cloud Dataproc region. This essentially determines which clusters are available
     * for this job to be submitted to. If not specified, defaults to `global`.
     * 
     */
    public Optional<Output<String>> region() {
        return Optional.ofNullable(this.region);
    }

    /**
     * Optional. Job scheduling configuration.
     * 
     */
    @Import(name="scheduling")
    private @Nullable Output<JobSchedulingArgs> scheduling;

    /**
     * @return Optional. Job scheduling configuration.
     * 
     */
    public Optional<Output<JobSchedulingArgs>> scheduling() {
        return Optional.ofNullable(this.scheduling);
    }

    /**
     * The config of the Spark job.
     * 
     */
    @Import(name="sparkConfig")
    private @Nullable Output<JobSparkConfigArgs> sparkConfig;

    /**
     * @return The config of the Spark job.
     * 
     */
    public Optional<Output<JobSparkConfigArgs>> sparkConfig() {
        return Optional.ofNullable(this.sparkConfig);
    }

    /**
     * The config of SparkSql job
     * 
     */
    @Import(name="sparksqlConfig")
    private @Nullable Output<JobSparksqlConfigArgs> sparksqlConfig;

    /**
     * @return The config of SparkSql job
     * 
     */
    public Optional<Output<JobSparksqlConfigArgs>> sparksqlConfig() {
        return Optional.ofNullable(this.sparksqlConfig);
    }

    /**
     * The status of the job.
     * 
     */
    @Import(name="statuses")
    private @Nullable Output<List<JobStatusArgs>> statuses;

    /**
     * @return The status of the job.
     * 
     */
    public Optional<Output<List<JobStatusArgs>>> statuses() {
        return Optional.ofNullable(this.statuses);
    }

    /**
     * If set to true, Terraform will wait for the job to reach a terminal state (`DONE`, `ERROR`, `CANCELLED`, `ATTEMPT_FAILURE`). Otherwise, Terraform will consider the job &#39;created&#39; once it is in the `RUNNING` state.
     * 
     */
    @Import(name="waitForCompletion")
    private @Nullable Output<Boolean> waitForCompletion;

    /**
     * @return If set to true, Terraform will wait for the job to reach a terminal state (`DONE`, `ERROR`, `CANCELLED`, `ATTEMPT_FAILURE`). Otherwise, Terraform will consider the job &#39;created&#39; once it is in the `RUNNING` state.
     * 
     */
    public Optional<Output<Boolean>> waitForCompletion() {
        return Optional.ofNullable(this.waitForCompletion);
    }

    private JobState() {}

    private JobState(JobState $) {
        this.driverControlsFilesUri = $.driverControlsFilesUri;
        this.driverOutputResourceUri = $.driverOutputResourceUri;
        this.effectiveLabels = $.effectiveLabels;
        this.forceDelete = $.forceDelete;
        this.hadoopConfig = $.hadoopConfig;
        this.hiveConfig = $.hiveConfig;
        this.labels = $.labels;
        this.pigConfig = $.pigConfig;
        this.placement = $.placement;
        this.prestoConfig = $.prestoConfig;
        this.project = $.project;
        this.pulumiLabels = $.pulumiLabels;
        this.pysparkConfig = $.pysparkConfig;
        this.reference = $.reference;
        this.region = $.region;
        this.scheduling = $.scheduling;
        this.sparkConfig = $.sparkConfig;
        this.sparksqlConfig = $.sparksqlConfig;
        this.statuses = $.statuses;
        this.waitForCompletion = $.waitForCompletion;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(JobState defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private JobState $;

        public Builder() {
            $ = new JobState();
        }

        public Builder(JobState defaults) {
            $ = new JobState(Objects.requireNonNull(defaults));
        }

        /**
         * @param driverControlsFilesUri If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
         * 
         * @return builder
         * 
         */
        public Builder driverControlsFilesUri(@Nullable Output<String> driverControlsFilesUri) {
            $.driverControlsFilesUri = driverControlsFilesUri;
            return this;
        }

        /**
         * @param driverControlsFilesUri If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
         * 
         * @return builder
         * 
         */
        public Builder driverControlsFilesUri(String driverControlsFilesUri) {
            return driverControlsFilesUri(Output.of(driverControlsFilesUri));
        }

        /**
         * @param driverOutputResourceUri A URI pointing to the location of the stdout of the job&#39;s driver program.
         * 
         * @return builder
         * 
         */
        public Builder driverOutputResourceUri(@Nullable Output<String> driverOutputResourceUri) {
            $.driverOutputResourceUri = driverOutputResourceUri;
            return this;
        }

        /**
         * @param driverOutputResourceUri A URI pointing to the location of the stdout of the job&#39;s driver program.
         * 
         * @return builder
         * 
         */
        public Builder driverOutputResourceUri(String driverOutputResourceUri) {
            return driverOutputResourceUri(Output.of(driverOutputResourceUri));
        }

        /**
         * @param effectiveLabels All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
         * 
         * * `scheduling.max_failures_per_hour` - (Required) Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
         * 
         * * `scheduling.max_failures_total` - (Required) Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
         * 
         * @return builder
         * 
         */
        public Builder effectiveLabels(@Nullable Output<Map<String,String>> effectiveLabels) {
            $.effectiveLabels = effectiveLabels;
            return this;
        }

        /**
         * @param effectiveLabels All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
         * 
         * * `scheduling.max_failures_per_hour` - (Required) Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
         * 
         * * `scheduling.max_failures_total` - (Required) Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
         * 
         * @return builder
         * 
         */
        public Builder effectiveLabels(Map<String,String> effectiveLabels) {
            return effectiveLabels(Output.of(effectiveLabels));
        }

        /**
         * @param forceDelete By default, you can only delete inactive jobs within
         * Dataproc. Setting this to true, and calling destroy, will ensure that the
         * job is first cancelled before issuing the delete.
         * 
         * @return builder
         * 
         */
        public Builder forceDelete(@Nullable Output<Boolean> forceDelete) {
            $.forceDelete = forceDelete;
            return this;
        }

        /**
         * @param forceDelete By default, you can only delete inactive jobs within
         * Dataproc. Setting this to true, and calling destroy, will ensure that the
         * job is first cancelled before issuing the delete.
         * 
         * @return builder
         * 
         */
        public Builder forceDelete(Boolean forceDelete) {
            return forceDelete(Output.of(forceDelete));
        }

        /**
         * @param hadoopConfig The config of Hadoop job
         * 
         * @return builder
         * 
         */
        public Builder hadoopConfig(@Nullable Output<JobHadoopConfigArgs> hadoopConfig) {
            $.hadoopConfig = hadoopConfig;
            return this;
        }

        /**
         * @param hadoopConfig The config of Hadoop job
         * 
         * @return builder
         * 
         */
        public Builder hadoopConfig(JobHadoopConfigArgs hadoopConfig) {
            return hadoopConfig(Output.of(hadoopConfig));
        }

        /**
         * @param hiveConfig The config of hive job
         * 
         * @return builder
         * 
         */
        public Builder hiveConfig(@Nullable Output<JobHiveConfigArgs> hiveConfig) {
            $.hiveConfig = hiveConfig;
            return this;
        }

        /**
         * @param hiveConfig The config of hive job
         * 
         * @return builder
         * 
         */
        public Builder hiveConfig(JobHiveConfigArgs hiveConfig) {
            return hiveConfig(Output.of(hiveConfig));
        }

        /**
         * @param labels The list of labels (key/value pairs) to add to the job.
         * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
         * Please refer to the field &#39;effective_labels&#39; for all of the labels present on the resource.
         * 
         * @return builder
         * 
         */
        public Builder labels(@Nullable Output<Map<String,String>> labels) {
            $.labels = labels;
            return this;
        }

        /**
         * @param labels The list of labels (key/value pairs) to add to the job.
         * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
         * Please refer to the field &#39;effective_labels&#39; for all of the labels present on the resource.
         * 
         * @return builder
         * 
         */
        public Builder labels(Map<String,String> labels) {
            return labels(Output.of(labels));
        }

        /**
         * @param pigConfig The config of pag job.
         * 
         * @return builder
         * 
         */
        public Builder pigConfig(@Nullable Output<JobPigConfigArgs> pigConfig) {
            $.pigConfig = pigConfig;
            return this;
        }

        /**
         * @param pigConfig The config of pag job.
         * 
         * @return builder
         * 
         */
        public Builder pigConfig(JobPigConfigArgs pigConfig) {
            return pigConfig(Output.of(pigConfig));
        }

        /**
         * @param placement The config of job placement.
         * 
         * @return builder
         * 
         */
        public Builder placement(@Nullable Output<JobPlacementArgs> placement) {
            $.placement = placement;
            return this;
        }

        /**
         * @param placement The config of job placement.
         * 
         * @return builder
         * 
         */
        public Builder placement(JobPlacementArgs placement) {
            return placement(Output.of(placement));
        }

        /**
         * @param prestoConfig The config of presto job
         * 
         * @return builder
         * 
         */
        public Builder prestoConfig(@Nullable Output<JobPrestoConfigArgs> prestoConfig) {
            $.prestoConfig = prestoConfig;
            return this;
        }

        /**
         * @param prestoConfig The config of presto job
         * 
         * @return builder
         * 
         */
        public Builder prestoConfig(JobPrestoConfigArgs prestoConfig) {
            return prestoConfig(Output.of(prestoConfig));
        }

        /**
         * @param project The project in which the `cluster` can be found and jobs
         * subsequently run against. If it is not provided, the provider project is used.
         * 
         * @return builder
         * 
         */
        public Builder project(@Nullable Output<String> project) {
            $.project = project;
            return this;
        }

        /**
         * @param project The project in which the `cluster` can be found and jobs
         * subsequently run against. If it is not provided, the provider project is used.
         * 
         * @return builder
         * 
         */
        public Builder project(String project) {
            return project(Output.of(project));
        }

        /**
         * @param pulumiLabels The combination of labels configured directly on the resource and default labels configured on the provider.
         * 
         * @return builder
         * 
         */
        public Builder pulumiLabels(@Nullable Output<Map<String,String>> pulumiLabels) {
            $.pulumiLabels = pulumiLabels;
            return this;
        }

        /**
         * @param pulumiLabels The combination of labels configured directly on the resource and default labels configured on the provider.
         * 
         * @return builder
         * 
         */
        public Builder pulumiLabels(Map<String,String> pulumiLabels) {
            return pulumiLabels(Output.of(pulumiLabels));
        }

        /**
         * @param pysparkConfig The config of pySpark job.
         * 
         * @return builder
         * 
         */
        public Builder pysparkConfig(@Nullable Output<JobPysparkConfigArgs> pysparkConfig) {
            $.pysparkConfig = pysparkConfig;
            return this;
        }

        /**
         * @param pysparkConfig The config of pySpark job.
         * 
         * @return builder
         * 
         */
        public Builder pysparkConfig(JobPysparkConfigArgs pysparkConfig) {
            return pysparkConfig(Output.of(pysparkConfig));
        }

        /**
         * @param reference The reference of the job
         * 
         * @return builder
         * 
         */
        public Builder reference(@Nullable Output<JobReferenceArgs> reference) {
            $.reference = reference;
            return this;
        }

        /**
         * @param reference The reference of the job
         * 
         * @return builder
         * 
         */
        public Builder reference(JobReferenceArgs reference) {
            return reference(Output.of(reference));
        }

        /**
         * @param region The Cloud Dataproc region. This essentially determines which clusters are available
         * for this job to be submitted to. If not specified, defaults to `global`.
         * 
         * @return builder
         * 
         */
        public Builder region(@Nullable Output<String> region) {
            $.region = region;
            return this;
        }

        /**
         * @param region The Cloud Dataproc region. This essentially determines which clusters are available
         * for this job to be submitted to. If not specified, defaults to `global`.
         * 
         * @return builder
         * 
         */
        public Builder region(String region) {
            return region(Output.of(region));
        }

        /**
         * @param scheduling Optional. Job scheduling configuration.
         * 
         * @return builder
         * 
         */
        public Builder scheduling(@Nullable Output<JobSchedulingArgs> scheduling) {
            $.scheduling = scheduling;
            return this;
        }

        /**
         * @param scheduling Optional. Job scheduling configuration.
         * 
         * @return builder
         * 
         */
        public Builder scheduling(JobSchedulingArgs scheduling) {
            return scheduling(Output.of(scheduling));
        }

        /**
         * @param sparkConfig The config of the Spark job.
         * 
         * @return builder
         * 
         */
        public Builder sparkConfig(@Nullable Output<JobSparkConfigArgs> sparkConfig) {
            $.sparkConfig = sparkConfig;
            return this;
        }

        /**
         * @param sparkConfig The config of the Spark job.
         * 
         * @return builder
         * 
         */
        public Builder sparkConfig(JobSparkConfigArgs sparkConfig) {
            return sparkConfig(Output.of(sparkConfig));
        }

        /**
         * @param sparksqlConfig The config of SparkSql job
         * 
         * @return builder
         * 
         */
        public Builder sparksqlConfig(@Nullable Output<JobSparksqlConfigArgs> sparksqlConfig) {
            $.sparksqlConfig = sparksqlConfig;
            return this;
        }

        /**
         * @param sparksqlConfig The config of SparkSql job
         * 
         * @return builder
         * 
         */
        public Builder sparksqlConfig(JobSparksqlConfigArgs sparksqlConfig) {
            return sparksqlConfig(Output.of(sparksqlConfig));
        }

        /**
         * @param statuses The status of the job.
         * 
         * @return builder
         * 
         */
        public Builder statuses(@Nullable Output<List<JobStatusArgs>> statuses) {
            $.statuses = statuses;
            return this;
        }

        /**
         * @param statuses The status of the job.
         * 
         * @return builder
         * 
         */
        public Builder statuses(List<JobStatusArgs> statuses) {
            return statuses(Output.of(statuses));
        }

        /**
         * @param statuses The status of the job.
         * 
         * @return builder
         * 
         */
        public Builder statuses(JobStatusArgs... statuses) {
            return statuses(List.of(statuses));
        }

        /**
         * @param waitForCompletion If set to true, Terraform will wait for the job to reach a terminal state (`DONE`, `ERROR`, `CANCELLED`, `ATTEMPT_FAILURE`). Otherwise, Terraform will consider the job &#39;created&#39; once it is in the `RUNNING` state.
         * 
         * @return builder
         * 
         */
        public Builder waitForCompletion(@Nullable Output<Boolean> waitForCompletion) {
            $.waitForCompletion = waitForCompletion;
            return this;
        }

        /**
         * @param waitForCompletion If set to true, Terraform will wait for the job to reach a terminal state (`DONE`, `ERROR`, `CANCELLED`, `ATTEMPT_FAILURE`). Otherwise, Terraform will consider the job &#39;created&#39; once it is in the `RUNNING` state.
         * 
         * @return builder
         * 
         */
        public Builder waitForCompletion(Boolean waitForCompletion) {
            return waitForCompletion(Output.of(waitForCompletion));
        }

        public JobState build() {
            return $;
        }
    }

}

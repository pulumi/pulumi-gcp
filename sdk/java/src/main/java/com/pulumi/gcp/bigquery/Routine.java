// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.gcp.bigquery;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.gcp.Utilities;
import com.pulumi.gcp.bigquery.RoutineArgs;
import com.pulumi.gcp.bigquery.inputs.RoutineState;
import com.pulumi.gcp.bigquery.outputs.RoutineArgument;
import com.pulumi.gcp.bigquery.outputs.RoutineRemoteFunctionOptions;
import com.pulumi.gcp.bigquery.outputs.RoutineSparkOptions;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * A user-defined function or a stored procedure that belongs to a Dataset
 * 
 * To get more information about Routine, see:
 * 
 * * [API documentation](https://cloud.google.com/bigquery/docs/reference/rest/v2/routines)
 * * How-to Guides
 *     * [Routines Intro](https://cloud.google.com/bigquery/docs/reference/rest/v2/routines)
 * 
 * ## Example Usage
 * ### Bigquery Routine Basic
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.bigquery.Dataset;
 * import com.pulumi.gcp.bigquery.DatasetArgs;
 * import com.pulumi.gcp.bigquery.Routine;
 * import com.pulumi.gcp.bigquery.RoutineArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var test = new Dataset(&#34;test&#34;, DatasetArgs.builder()        
 *             .datasetId(&#34;dataset_id&#34;)
 *             .build());
 * 
 *         var sproc = new Routine(&#34;sproc&#34;, RoutineArgs.builder()        
 *             .datasetId(test.datasetId())
 *             .routineId(&#34;routine_id&#34;)
 *             .routineType(&#34;PROCEDURE&#34;)
 *             .language(&#34;SQL&#34;)
 *             .definitionBody(&#34;CREATE FUNCTION Add(x FLOAT64, y FLOAT64) RETURNS FLOAT64 AS (x + y);&#34;)
 *             .build());
 * 
 *     }
 * }
 * ```
 * ### Bigquery Routine Json
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.bigquery.Dataset;
 * import com.pulumi.gcp.bigquery.DatasetArgs;
 * import com.pulumi.gcp.bigquery.Routine;
 * import com.pulumi.gcp.bigquery.RoutineArgs;
 * import com.pulumi.gcp.bigquery.inputs.RoutineArgumentArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var test = new Dataset(&#34;test&#34;, DatasetArgs.builder()        
 *             .datasetId(&#34;dataset_id&#34;)
 *             .build());
 * 
 *         var sproc = new Routine(&#34;sproc&#34;, RoutineArgs.builder()        
 *             .datasetId(test.datasetId())
 *             .routineId(&#34;routine_id&#34;)
 *             .routineType(&#34;SCALAR_FUNCTION&#34;)
 *             .language(&#34;JAVASCRIPT&#34;)
 *             .definitionBody(&#34;CREATE FUNCTION multiplyInputs return x*y;&#34;)
 *             .arguments(            
 *                 RoutineArgumentArgs.builder()
 *                     .name(&#34;x&#34;)
 *                     .dataType(&#34;{\&#34;typeKind\&#34; :  \&#34;FLOAT64\&#34;}&#34;)
 *                     .build(),
 *                 RoutineArgumentArgs.builder()
 *                     .name(&#34;y&#34;)
 *                     .dataType(&#34;{\&#34;typeKind\&#34; :  \&#34;FLOAT64\&#34;}&#34;)
 *                     .build())
 *             .returnType(&#34;{\&#34;typeKind\&#34; :  \&#34;FLOAT64\&#34;}&#34;)
 *             .build());
 * 
 *     }
 * }
 * ```
 * ### Bigquery Routine Tvf
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.bigquery.Dataset;
 * import com.pulumi.gcp.bigquery.DatasetArgs;
 * import com.pulumi.gcp.bigquery.Routine;
 * import com.pulumi.gcp.bigquery.RoutineArgs;
 * import com.pulumi.gcp.bigquery.inputs.RoutineArgumentArgs;
 * import static com.pulumi.codegen.internal.Serialization.*;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var test = new Dataset(&#34;test&#34;, DatasetArgs.builder()        
 *             .datasetId(&#34;dataset_id&#34;)
 *             .build());
 * 
 *         var sproc = new Routine(&#34;sproc&#34;, RoutineArgs.builder()        
 *             .datasetId(test.datasetId())
 *             .routineId(&#34;routine_id&#34;)
 *             .routineType(&#34;TABLE_VALUED_FUNCTION&#34;)
 *             .language(&#34;SQL&#34;)
 *             .definitionBody(&#34;&#34;&#34;
 * SELECT 1 + value AS value
 *             &#34;&#34;&#34;)
 *             .arguments(RoutineArgumentArgs.builder()
 *                 .name(&#34;value&#34;)
 *                 .argumentKind(&#34;FIXED_TYPE&#34;)
 *                 .dataType(serializeJson(
 *                     jsonObject(
 *                         jsonProperty(&#34;typeKind&#34;, &#34;INT64&#34;)
 *                     )))
 *                 .build())
 *             .returnTableType(serializeJson(
 *                 jsonObject(
 *                     jsonProperty(&#34;columns&#34;, jsonArray(jsonObject(
 *                         jsonProperty(&#34;name&#34;, &#34;value&#34;),
 *                         jsonProperty(&#34;type&#34;, jsonObject(
 *                             jsonProperty(&#34;typeKind&#34;, &#34;INT64&#34;)
 *                         ))
 *                     )))
 *                 )))
 *             .build());
 * 
 *     }
 * }
 * ```
 * ### Bigquery Routine Pyspark
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.bigquery.Dataset;
 * import com.pulumi.gcp.bigquery.DatasetArgs;
 * import com.pulumi.gcp.bigquery.Connection;
 * import com.pulumi.gcp.bigquery.ConnectionArgs;
 * import com.pulumi.gcp.bigquery.inputs.ConnectionSparkArgs;
 * import com.pulumi.gcp.bigquery.Routine;
 * import com.pulumi.gcp.bigquery.RoutineArgs;
 * import com.pulumi.gcp.bigquery.inputs.RoutineSparkOptionsArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var test = new Dataset(&#34;test&#34;, DatasetArgs.builder()        
 *             .datasetId(&#34;dataset_id&#34;)
 *             .build());
 * 
 *         var testConnection = new Connection(&#34;testConnection&#34;, ConnectionArgs.builder()        
 *             .connectionId(&#34;connection_id&#34;)
 *             .location(&#34;US&#34;)
 *             .spark()
 *             .build());
 * 
 *         var pyspark = new Routine(&#34;pyspark&#34;, RoutineArgs.builder()        
 *             .datasetId(test.datasetId())
 *             .routineId(&#34;routine_id&#34;)
 *             .routineType(&#34;PROCEDURE&#34;)
 *             .language(&#34;PYTHON&#34;)
 *             .definitionBody(&#34;&#34;&#34;
 * from pyspark.sql import SparkSession
 * 
 * spark = SparkSession.builder.appName(&#34;spark-bigquery-demo&#34;).getOrCreate()
 *     
 * # Load data from BigQuery.
 * words = spark.read.format(&#34;bigquery&#34;) \
 *   .option(&#34;table&#34;, &#34;bigquery-public-data:samples.shakespeare&#34;) \
 *   .load()
 * words.createOrReplaceTempView(&#34;words&#34;)
 *     
 * # Perform word count.
 * word_count = words.select(&#39;word&#39;, &#39;word_count&#39;).groupBy(&#39;word&#39;).sum(&#39;word_count&#39;).withColumnRenamed(&#34;sum(word_count)&#34;, &#34;sum_word_count&#34;)
 * word_count.show()
 * word_count.printSchema()
 *     
 * # Saving the data to BigQuery
 * word_count.write.format(&#34;bigquery&#34;) \
 *   .option(&#34;writeMethod&#34;, &#34;direct&#34;) \
 *   .save(&#34;wordcount_dataset.wordcount_output&#34;)
 *             &#34;&#34;&#34;)
 *             .sparkOptions(RoutineSparkOptionsArgs.builder()
 *                 .connection(testConnection.name())
 *                 .runtimeVersion(&#34;2.1&#34;)
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * ```
 * ### Bigquery Routine Pyspark Mainfile
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.bigquery.Dataset;
 * import com.pulumi.gcp.bigquery.DatasetArgs;
 * import com.pulumi.gcp.bigquery.Connection;
 * import com.pulumi.gcp.bigquery.ConnectionArgs;
 * import com.pulumi.gcp.bigquery.inputs.ConnectionSparkArgs;
 * import com.pulumi.gcp.bigquery.Routine;
 * import com.pulumi.gcp.bigquery.RoutineArgs;
 * import com.pulumi.gcp.bigquery.inputs.RoutineSparkOptionsArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var test = new Dataset(&#34;test&#34;, DatasetArgs.builder()        
 *             .datasetId(&#34;dataset_id&#34;)
 *             .build());
 * 
 *         var testConnection = new Connection(&#34;testConnection&#34;, ConnectionArgs.builder()        
 *             .connectionId(&#34;connection_id&#34;)
 *             .location(&#34;US&#34;)
 *             .spark()
 *             .build());
 * 
 *         var pysparkMainfile = new Routine(&#34;pysparkMainfile&#34;, RoutineArgs.builder()        
 *             .datasetId(test.datasetId())
 *             .routineId(&#34;routine_id&#34;)
 *             .routineType(&#34;PROCEDURE&#34;)
 *             .language(&#34;PYTHON&#34;)
 *             .definitionBody(&#34;&#34;)
 *             .sparkOptions(RoutineSparkOptionsArgs.builder()
 *                 .connection(testConnection.name())
 *                 .runtimeVersion(&#34;2.1&#34;)
 *                 .mainFileUri(&#34;gs://test-bucket/main.py&#34;)
 *                 .pyFileUris(&#34;gs://test-bucket/lib.py&#34;)
 *                 .fileUris(&#34;gs://test-bucket/distribute_in_executor.json&#34;)
 *                 .archiveUris(&#34;gs://test-bucket/distribute_in_executor.tar.gz&#34;)
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * ```
 * ### Bigquery Routine Spark Jar
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.bigquery.Dataset;
 * import com.pulumi.gcp.bigquery.DatasetArgs;
 * import com.pulumi.gcp.bigquery.Connection;
 * import com.pulumi.gcp.bigquery.ConnectionArgs;
 * import com.pulumi.gcp.bigquery.inputs.ConnectionSparkArgs;
 * import com.pulumi.gcp.bigquery.Routine;
 * import com.pulumi.gcp.bigquery.RoutineArgs;
 * import com.pulumi.gcp.bigquery.inputs.RoutineSparkOptionsArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var test = new Dataset(&#34;test&#34;, DatasetArgs.builder()        
 *             .datasetId(&#34;dataset_id&#34;)
 *             .build());
 * 
 *         var testConnection = new Connection(&#34;testConnection&#34;, ConnectionArgs.builder()        
 *             .connectionId(&#34;connection_id&#34;)
 *             .location(&#34;US&#34;)
 *             .spark()
 *             .build());
 * 
 *         var sparkJar = new Routine(&#34;sparkJar&#34;, RoutineArgs.builder()        
 *             .datasetId(test.datasetId())
 *             .routineId(&#34;routine_id&#34;)
 *             .routineType(&#34;PROCEDURE&#34;)
 *             .language(&#34;SCALA&#34;)
 *             .definitionBody(&#34;&#34;)
 *             .sparkOptions(RoutineSparkOptionsArgs.builder()
 *                 .connection(testConnection.name())
 *                 .runtimeVersion(&#34;2.1&#34;)
 *                 .containerImage(&#34;gcr.io/my-project-id/my-spark-image:latest&#34;)
 *                 .mainClass(&#34;com.google.test.jar.MainClass&#34;)
 *                 .jarUris(&#34;gs://test-bucket/uberjar_spark_spark3.jar&#34;)
 *                 .properties(Map.ofEntries(
 *                     Map.entry(&#34;spark.dataproc.scaling.version&#34;, &#34;2&#34;),
 *                     Map.entry(&#34;spark.reducer.fetchMigratedShuffle.enabled&#34;, &#34;true&#34;)
 *                 ))
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * ```
 * ### Bigquery Routine Remote Function
 * ```java
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.gcp.bigquery.Dataset;
 * import com.pulumi.gcp.bigquery.DatasetArgs;
 * import com.pulumi.gcp.bigquery.Connection;
 * import com.pulumi.gcp.bigquery.ConnectionArgs;
 * import com.pulumi.gcp.bigquery.inputs.ConnectionCloudResourceArgs;
 * import com.pulumi.gcp.bigquery.Routine;
 * import com.pulumi.gcp.bigquery.RoutineArgs;
 * import com.pulumi.gcp.bigquery.inputs.RoutineRemoteFunctionOptionsArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var test = new Dataset(&#34;test&#34;, DatasetArgs.builder()        
 *             .datasetId(&#34;dataset_id&#34;)
 *             .build());
 * 
 *         var testConnection = new Connection(&#34;testConnection&#34;, ConnectionArgs.builder()        
 *             .connectionId(&#34;connection_id&#34;)
 *             .location(&#34;US&#34;)
 *             .cloudResource()
 *             .build());
 * 
 *         var remoteFunction = new Routine(&#34;remoteFunction&#34;, RoutineArgs.builder()        
 *             .datasetId(test.datasetId())
 *             .routineId(&#34;routine_id&#34;)
 *             .routineType(&#34;SCALAR_FUNCTION&#34;)
 *             .definitionBody(&#34;&#34;)
 *             .returnType(&#34;{\&#34;typeKind\&#34; :  \&#34;STRING\&#34;}&#34;)
 *             .remoteFunctionOptions(RoutineRemoteFunctionOptionsArgs.builder()
 *                 .endpoint(&#34;https://us-east1-my_gcf_project.cloudfunctions.net/remote_add&#34;)
 *                 .connection(testConnection.name())
 *                 .maxBatchingRows(&#34;10&#34;)
 *                 .userDefinedContext(Map.of(&#34;z&#34;, &#34;1.5&#34;))
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * ```
 * 
 * ## Import
 * 
 * Routine can be imported using any of these accepted formats:
 * 
 *  * `projects/{{project}}/datasets/{{dataset_id}}/routines/{{routine_id}}`
 * 
 *  * `{{project}}/{{dataset_id}}/{{routine_id}}`
 * 
 *  * `{{dataset_id}}/{{routine_id}}`
 * 
 *  When using the `pulumi import` command, Routine can be imported using one of the formats above. For example:
 * 
 * ```sh
 * $ pulumi import gcp:bigquery/routine:Routine default projects/{{project}}/datasets/{{dataset_id}}/routines/{{routine_id}}
 * ```
 * 
 * ```sh
 * $ pulumi import gcp:bigquery/routine:Routine default {{project}}/{{dataset_id}}/{{routine_id}}
 * ```
 * 
 * ```sh
 * $ pulumi import gcp:bigquery/routine:Routine default {{dataset_id}}/{{routine_id}}
 * ```
 * 
 */
@ResourceType(type="gcp:bigquery/routine:Routine")
public class Routine extends com.pulumi.resources.CustomResource {
    /**
     * Input/output argument of a function or a stored procedure.
     * Structure is documented below.
     * 
     */
    @Export(name="arguments", refs={List.class,RoutineArgument.class}, tree="[0,1]")
    private Output</* @Nullable */ List<RoutineArgument>> arguments;

    /**
     * @return Input/output argument of a function or a stored procedure.
     * Structure is documented below.
     * 
     */
    public Output<Optional<List<RoutineArgument>>> arguments() {
        return Codegen.optional(this.arguments);
    }
    /**
     * The time when this routine was created, in milliseconds since the
     * epoch.
     * 
     */
    @Export(name="creationTime", refs={Integer.class}, tree="[0]")
    private Output<Integer> creationTime;

    /**
     * @return The time when this routine was created, in milliseconds since the
     * epoch.
     * 
     */
    public Output<Integer> creationTime() {
        return this.creationTime;
    }
    /**
     * The ID of the dataset containing this routine
     * 
     */
    @Export(name="datasetId", refs={String.class}, tree="[0]")
    private Output<String> datasetId;

    /**
     * @return The ID of the dataset containing this routine
     * 
     */
    public Output<String> datasetId() {
        return this.datasetId;
    }
    /**
     * The body of the routine. For functions, this is the expression in the AS clause.
     * If language=SQL, it is the substring inside (but excluding) the parentheses.
     * 
     * ***
     * 
     */
    @Export(name="definitionBody", refs={String.class}, tree="[0]")
    private Output<String> definitionBody;

    /**
     * @return The body of the routine. For functions, this is the expression in the AS clause.
     * If language=SQL, it is the substring inside (but excluding) the parentheses.
     * 
     * ***
     * 
     */
    public Output<String> definitionBody() {
        return this.definitionBody;
    }
    /**
     * The description of the routine if defined.
     * 
     */
    @Export(name="description", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> description;

    /**
     * @return The description of the routine if defined.
     * 
     */
    public Output<Optional<String>> description() {
        return Codegen.optional(this.description);
    }
    /**
     * The determinism level of the JavaScript UDF if defined.
     * Possible values are: `DETERMINISM_LEVEL_UNSPECIFIED`, `DETERMINISTIC`, `NOT_DETERMINISTIC`.
     * 
     */
    @Export(name="determinismLevel", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> determinismLevel;

    /**
     * @return The determinism level of the JavaScript UDF if defined.
     * Possible values are: `DETERMINISM_LEVEL_UNSPECIFIED`, `DETERMINISTIC`, `NOT_DETERMINISTIC`.
     * 
     */
    public Output<Optional<String>> determinismLevel() {
        return Codegen.optional(this.determinismLevel);
    }
    /**
     * Optional. If language = &#34;JAVASCRIPT&#34;, this field stores the path of the
     * imported JAVASCRIPT libraries.
     * 
     */
    @Export(name="importedLibraries", refs={List.class,String.class}, tree="[0,1]")
    private Output</* @Nullable */ List<String>> importedLibraries;

    /**
     * @return Optional. If language = &#34;JAVASCRIPT&#34;, this field stores the path of the
     * imported JAVASCRIPT libraries.
     * 
     */
    public Output<Optional<List<String>>> importedLibraries() {
        return Codegen.optional(this.importedLibraries);
    }
    /**
     * The language of the routine.
     * Possible values are: `SQL`, `JAVASCRIPT`, `PYTHON`, `JAVA`, `SCALA`.
     * 
     */
    @Export(name="language", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> language;

    /**
     * @return The language of the routine.
     * Possible values are: `SQL`, `JAVASCRIPT`, `PYTHON`, `JAVA`, `SCALA`.
     * 
     */
    public Output<Optional<String>> language() {
        return Codegen.optional(this.language);
    }
    /**
     * The time when this routine was modified, in milliseconds since the
     * epoch.
     * 
     */
    @Export(name="lastModifiedTime", refs={Integer.class}, tree="[0]")
    private Output<Integer> lastModifiedTime;

    /**
     * @return The time when this routine was modified, in milliseconds since the
     * epoch.
     * 
     */
    public Output<Integer> lastModifiedTime() {
        return this.lastModifiedTime;
    }
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     * 
     */
    @Export(name="project", refs={String.class}, tree="[0]")
    private Output<String> project;

    /**
     * @return The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     * 
     */
    public Output<String> project() {
        return this.project;
    }
    /**
     * Remote function specific options.
     * Structure is documented below.
     * 
     */
    @Export(name="remoteFunctionOptions", refs={RoutineRemoteFunctionOptions.class}, tree="[0]")
    private Output</* @Nullable */ RoutineRemoteFunctionOptions> remoteFunctionOptions;

    /**
     * @return Remote function specific options.
     * Structure is documented below.
     * 
     */
    public Output<Optional<RoutineRemoteFunctionOptions>> remoteFunctionOptions() {
        return Codegen.optional(this.remoteFunctionOptions);
    }
    /**
     * Optional. Can be set only if routineType = &#34;TABLE_VALUED_FUNCTION&#34;.
     * If absent, the return table type is inferred from definitionBody at query time in each query
     * that references this routine. If present, then the columns in the evaluated table result will
     * be cast to match the column types specificed in return table type, at query time.
     * 
     */
    @Export(name="returnTableType", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> returnTableType;

    /**
     * @return Optional. Can be set only if routineType = &#34;TABLE_VALUED_FUNCTION&#34;.
     * If absent, the return table type is inferred from definitionBody at query time in each query
     * that references this routine. If present, then the columns in the evaluated table result will
     * be cast to match the column types specificed in return table type, at query time.
     * 
     */
    public Output<Optional<String>> returnTableType() {
        return Codegen.optional(this.returnTableType);
    }
    /**
     * A JSON schema for the return type. Optional if language = &#34;SQL&#34;; required otherwise.
     * If absent, the return type is inferred from definitionBody at query time in each query
     * that references this routine. If present, then the evaluated result will be cast to
     * the specified returned type at query time. ~&gt;**NOTE**: Because this field expects a JSON
     * string, any changes to the string will create a diff, even if the JSON itself hasn&#39;t
     * changed. If the API returns a different value for the same schema, e.g. it switche
     * d the order of values or replaced STRUCT field type with RECORD field type, we currently
     * cannot suppress the recurring diff this causes. As a workaround, we recommend using
     * the schema as returned by the API.
     * 
     */
    @Export(name="returnType", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> returnType;

    /**
     * @return A JSON schema for the return type. Optional if language = &#34;SQL&#34;; required otherwise.
     * If absent, the return type is inferred from definitionBody at query time in each query
     * that references this routine. If present, then the evaluated result will be cast to
     * the specified returned type at query time. ~&gt;**NOTE**: Because this field expects a JSON
     * string, any changes to the string will create a diff, even if the JSON itself hasn&#39;t
     * changed. If the API returns a different value for the same schema, e.g. it switche
     * d the order of values or replaced STRUCT field type with RECORD field type, we currently
     * cannot suppress the recurring diff this causes. As a workaround, we recommend using
     * the schema as returned by the API.
     * 
     */
    public Output<Optional<String>> returnType() {
        return Codegen.optional(this.returnType);
    }
    /**
     * The ID of the the routine. The ID must contain only letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum length is 256 characters.
     * 
     */
    @Export(name="routineId", refs={String.class}, tree="[0]")
    private Output<String> routineId;

    /**
     * @return The ID of the the routine. The ID must contain only letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum length is 256 characters.
     * 
     */
    public Output<String> routineId() {
        return this.routineId;
    }
    /**
     * The type of routine.
     * Possible values are: `SCALAR_FUNCTION`, `PROCEDURE`, `TABLE_VALUED_FUNCTION`.
     * 
     */
    @Export(name="routineType", refs={String.class}, tree="[0]")
    private Output<String> routineType;

    /**
     * @return The type of routine.
     * Possible values are: `SCALAR_FUNCTION`, `PROCEDURE`, `TABLE_VALUED_FUNCTION`.
     * 
     */
    public Output<String> routineType() {
        return this.routineType;
    }
    /**
     * Optional. If language is one of &#34;PYTHON&#34;, &#34;JAVA&#34;, &#34;SCALA&#34;, this field stores the options for spark stored procedure.
     * Structure is documented below.
     * 
     */
    @Export(name="sparkOptions", refs={RoutineSparkOptions.class}, tree="[0]")
    private Output</* @Nullable */ RoutineSparkOptions> sparkOptions;

    /**
     * @return Optional. If language is one of &#34;PYTHON&#34;, &#34;JAVA&#34;, &#34;SCALA&#34;, this field stores the options for spark stored procedure.
     * Structure is documented below.
     * 
     */
    public Output<Optional<RoutineSparkOptions>> sparkOptions() {
        return Codegen.optional(this.sparkOptions);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Routine(String name) {
        this(name, RoutineArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Routine(String name, RoutineArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Routine(String name, RoutineArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("gcp:bigquery/routine:Routine", name, args == null ? RoutineArgs.Empty : args, makeResourceOptions(options, Codegen.empty()));
    }

    private Routine(String name, Output<String> id, @Nullable RoutineState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("gcp:bigquery/routine:Routine", name, state, makeResourceOptions(options, id));
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Routine get(String name, Output<String> id, @Nullable RoutineState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Routine(name, id, state, options);
    }
}

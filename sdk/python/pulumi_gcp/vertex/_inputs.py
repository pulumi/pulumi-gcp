# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from .. import _utilities

__all__ = [
    'AiDatasetEncryptionSpecArgs',
    'AiDatasetEncryptionSpecArgsDict',
    'AiDeploymentResourcePoolDedicatedResourcesArgs',
    'AiDeploymentResourcePoolDedicatedResourcesArgsDict',
    'AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgs',
    'AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgsDict',
    'AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgs',
    'AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgsDict',
    'AiEndpointDeployedModelArgs',
    'AiEndpointDeployedModelArgsDict',
    'AiEndpointDeployedModelAutomaticResourceArgs',
    'AiEndpointDeployedModelAutomaticResourceArgsDict',
    'AiEndpointDeployedModelDedicatedResourceArgs',
    'AiEndpointDeployedModelDedicatedResourceArgsDict',
    'AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs',
    'AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgsDict',
    'AiEndpointDeployedModelDedicatedResourceMachineSpecArgs',
    'AiEndpointDeployedModelDedicatedResourceMachineSpecArgsDict',
    'AiEndpointDeployedModelPrivateEndpointArgs',
    'AiEndpointDeployedModelPrivateEndpointArgsDict',
    'AiEndpointEncryptionSpecArgs',
    'AiEndpointEncryptionSpecArgsDict',
    'AiEndpointIamBindingConditionArgs',
    'AiEndpointIamBindingConditionArgsDict',
    'AiEndpointIamMemberConditionArgs',
    'AiEndpointIamMemberConditionArgsDict',
    'AiFeatureGroupBigQueryArgs',
    'AiFeatureGroupBigQueryArgsDict',
    'AiFeatureGroupBigQueryBigQuerySourceArgs',
    'AiFeatureGroupBigQueryBigQuerySourceArgsDict',
    'AiFeatureOnlineStoreBigtableArgs',
    'AiFeatureOnlineStoreBigtableArgsDict',
    'AiFeatureOnlineStoreBigtableAutoScalingArgs',
    'AiFeatureOnlineStoreBigtableAutoScalingArgsDict',
    'AiFeatureOnlineStoreDedicatedServingEndpointArgs',
    'AiFeatureOnlineStoreDedicatedServingEndpointArgsDict',
    'AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgs',
    'AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgsDict',
    'AiFeatureOnlineStoreEmbeddingManagementArgs',
    'AiFeatureOnlineStoreEmbeddingManagementArgsDict',
    'AiFeatureOnlineStoreFeatureviewBigQuerySourceArgs',
    'AiFeatureOnlineStoreFeatureviewBigQuerySourceArgsDict',
    'AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceArgs',
    'AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceArgsDict',
    'AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgs',
    'AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgsDict',
    'AiFeatureOnlineStoreFeatureviewSyncConfigArgs',
    'AiFeatureOnlineStoreFeatureviewSyncConfigArgsDict',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigArgs',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigArgsDict',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgs',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgsDict',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgs',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgsDict',
    'AiFeatureOnlineStoreOptimizedArgs',
    'AiFeatureOnlineStoreOptimizedArgsDict',
    'AiFeatureStoreEncryptionSpecArgs',
    'AiFeatureStoreEncryptionSpecArgsDict',
    'AiFeatureStoreEntityTypeIamBindingConditionArgs',
    'AiFeatureStoreEntityTypeIamBindingConditionArgsDict',
    'AiFeatureStoreEntityTypeIamMemberConditionArgs',
    'AiFeatureStoreEntityTypeIamMemberConditionArgsDict',
    'AiFeatureStoreEntityTypeMonitoringConfigArgs',
    'AiFeatureStoreEntityTypeMonitoringConfigArgsDict',
    'AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgs',
    'AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgsDict',
    'AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgs',
    'AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgsDict',
    'AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgs',
    'AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgsDict',
    'AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgs',
    'AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgsDict',
    'AiFeatureStoreIamBindingConditionArgs',
    'AiFeatureStoreIamBindingConditionArgsDict',
    'AiFeatureStoreIamMemberConditionArgs',
    'AiFeatureStoreIamMemberConditionArgsDict',
    'AiFeatureStoreOnlineServingConfigArgs',
    'AiFeatureStoreOnlineServingConfigArgsDict',
    'AiFeatureStoreOnlineServingConfigScalingArgs',
    'AiFeatureStoreOnlineServingConfigScalingArgsDict',
    'AiIndexDeployedIndexArgs',
    'AiIndexDeployedIndexArgsDict',
    'AiIndexEndpointPrivateServiceConnectConfigArgs',
    'AiIndexEndpointPrivateServiceConnectConfigArgsDict',
    'AiIndexIndexStatArgs',
    'AiIndexIndexStatArgsDict',
    'AiIndexMetadataArgs',
    'AiIndexMetadataArgsDict',
    'AiIndexMetadataConfigArgs',
    'AiIndexMetadataConfigArgsDict',
    'AiIndexMetadataConfigAlgorithmConfigArgs',
    'AiIndexMetadataConfigAlgorithmConfigArgsDict',
    'AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs',
    'AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgsDict',
    'AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs',
    'AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgsDict',
    'AiMetadataStoreEncryptionSpecArgs',
    'AiMetadataStoreEncryptionSpecArgsDict',
    'AiMetadataStoreStateArgs',
    'AiMetadataStoreStateArgsDict',
    'AiTensorboardEncryptionSpecArgs',
    'AiTensorboardEncryptionSpecArgsDict',
]

MYPY = False

if not MYPY:
    class AiDatasetEncryptionSpecArgsDict(TypedDict):
        kms_key_name: NotRequired[pulumi.Input[str]]
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
elif False:
    AiDatasetEncryptionSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiDatasetEncryptionSpecArgs:
    def __init__(__self__, *,
                 kms_key_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
               Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        if kms_key_name is not None:
            pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> Optional[pulumi.Input[str]]:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kms_key_name", value)


if not MYPY:
    class AiDeploymentResourcePoolDedicatedResourcesArgsDict(TypedDict):
        machine_spec: pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgsDict']
        """
        The specification of a single machine used by the prediction
        Structure is documented below.
        """
        min_replica_count: pulumi.Input[int]
        """
        The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        """
        autoscaling_metric_specs: NotRequired[pulumi.Input[Sequence[pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgsDict']]]]
        """
        A list of the metric specifications that overrides a resource utilization metric.
        Structure is documented below.
        """
        max_replica_count: NotRequired[pulumi.Input[int]]
        """
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
elif False:
    AiDeploymentResourcePoolDedicatedResourcesArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiDeploymentResourcePoolDedicatedResourcesArgs:
    def __init__(__self__, *,
                 machine_spec: pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgs'],
                 min_replica_count: pulumi.Input[int],
                 autoscaling_metric_specs: Optional[pulumi.Input[Sequence[pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgs']]]] = None,
                 max_replica_count: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgs'] machine_spec: The specification of a single machine used by the prediction
               Structure is documented below.
        :param pulumi.Input[int] min_replica_count: The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        :param pulumi.Input[Sequence[pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgs']]] autoscaling_metric_specs: A list of the metric specifications that overrides a resource utilization metric.
               Structure is documented below.
        :param pulumi.Input[int] max_replica_count: The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "min_replica_count", min_replica_count)
        if autoscaling_metric_specs is not None:
            pulumi.set(__self__, "autoscaling_metric_specs", autoscaling_metric_specs)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)

    @property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgs']:
        """
        The specification of a single machine used by the prediction
        Structure is documented below.
        """
        return pulumi.get(self, "machine_spec")

    @machine_spec.setter
    def machine_spec(self, value: pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgs']):
        pulumi.set(self, "machine_spec", value)

    @property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> pulumi.Input[int]:
        """
        The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        """
        return pulumi.get(self, "min_replica_count")

    @min_replica_count.setter
    def min_replica_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "min_replica_count", value)

    @property
    @pulumi.getter(name="autoscalingMetricSpecs")
    def autoscaling_metric_specs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgs']]]]:
        """
        A list of the metric specifications that overrides a resource utilization metric.
        Structure is documented below.
        """
        return pulumi.get(self, "autoscaling_metric_specs")

    @autoscaling_metric_specs.setter
    def autoscaling_metric_specs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgs']]]]):
        pulumi.set(self, "autoscaling_metric_specs", value)

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
        return pulumi.get(self, "max_replica_count")

    @max_replica_count.setter
    def max_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_replica_count", value)


if not MYPY:
    class AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgsDict(TypedDict):
        metric_name: pulumi.Input[str]
        """
        The resource metric name. Supported metrics: For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        target: NotRequired[pulumi.Input[int]]
        """
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
elif False:
    AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgs:
    def __init__(__self__, *,
                 metric_name: pulumi.Input[str],
                 target: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[str] metric_name: The resource metric name. Supported metrics: For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        :param pulumi.Input[int] target: The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        pulumi.set(__self__, "metric_name", metric_name)
        if target is not None:
            pulumi.set(__self__, "target", target)

    @property
    @pulumi.getter(name="metricName")
    def metric_name(self) -> pulumi.Input[str]:
        """
        The resource metric name. Supported metrics: For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        return pulumi.get(self, "metric_name")

    @metric_name.setter
    def metric_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "metric_name", value)

    @property
    @pulumi.getter
    def target(self) -> Optional[pulumi.Input[int]]:
        """
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        return pulumi.get(self, "target")

    @target.setter
    def target(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "target", value)


if not MYPY:
    class AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgsDict(TypedDict):
        accelerator_count: NotRequired[pulumi.Input[int]]
        """
        The number of accelerators to attach to the machine.
        """
        accelerator_type: NotRequired[pulumi.Input[str]]
        """
        The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
        """
elif False:
    AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgs:
    def __init__(__self__, *,
                 accelerator_count: Optional[pulumi.Input[int]] = None,
                 accelerator_type: Optional[pulumi.Input[str]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] accelerator_count: The number of accelerators to attach to the machine.
        :param pulumi.Input[str] accelerator_type: The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        :param pulumi.Input[str] machine_type: The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of accelerators to attach to the machine.
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[pulumi.Input[str]]:
        """
        The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "accelerator_type", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)


if not MYPY:
    class AiEndpointDeployedModelArgsDict(TypedDict):
        automatic_resources: NotRequired[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelAutomaticResourceArgsDict']]]]
        """
        (Output)
        A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
        Structure is documented below.
        """
        create_time: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Output only. Timestamp when the DeployedModel was created.
        """
        dedicated_resources: NotRequired[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceArgsDict']]]]
        """
        (Output)
        A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
        Structure is documented below.
        """
        display_name: NotRequired[pulumi.Input[str]]
        """
        Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        enable_access_logging: NotRequired[pulumi.Input[bool]]
        """
        (Output)
        These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        """
        enable_container_logging: NotRequired[pulumi.Input[bool]]
        """
        (Output)
        If true, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
        """
        id: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
        """
        model: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
        """
        model_version_id: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Output only. The version ID of the model that is deployed.
        """
        private_endpoints: NotRequired[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelPrivateEndpointArgsDict']]]]
        """
        (Output)
        Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
        Structure is documented below.
        """
        service_account: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        """
        shared_resources: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
        """
elif False:
    AiEndpointDeployedModelArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointDeployedModelArgs:
    def __init__(__self__, *,
                 automatic_resources: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelAutomaticResourceArgs']]]] = None,
                 create_time: Optional[pulumi.Input[str]] = None,
                 dedicated_resources: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceArgs']]]] = None,
                 display_name: Optional[pulumi.Input[str]] = None,
                 enable_access_logging: Optional[pulumi.Input[bool]] = None,
                 enable_container_logging: Optional[pulumi.Input[bool]] = None,
                 id: Optional[pulumi.Input[str]] = None,
                 model: Optional[pulumi.Input[str]] = None,
                 model_version_id: Optional[pulumi.Input[str]] = None,
                 private_endpoints: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelPrivateEndpointArgs']]]] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 shared_resources: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelAutomaticResourceArgs']]] automatic_resources: (Output)
               A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
               Structure is documented below.
        :param pulumi.Input[str] create_time: (Output)
               Output only. Timestamp when the DeployedModel was created.
        :param pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceArgs']]] dedicated_resources: (Output)
               A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
               Structure is documented below.
        :param pulumi.Input[str] display_name: Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param pulumi.Input[bool] enable_access_logging: (Output)
               These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        :param pulumi.Input[bool] enable_container_logging: (Output)
               If true, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
        :param pulumi.Input[str] id: (Output)
               The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
        :param pulumi.Input[str] model: (Output)
               The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
        :param pulumi.Input[str] model_version_id: (Output)
               Output only. The version ID of the model that is deployed.
        :param pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelPrivateEndpointArgs']]] private_endpoints: (Output)
               Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
               Structure is documented below.
        :param pulumi.Input[str] service_account: (Output)
               The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        :param pulumi.Input[str] shared_resources: (Output)
               The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
        """
        if automatic_resources is not None:
            pulumi.set(__self__, "automatic_resources", automatic_resources)
        if create_time is not None:
            pulumi.set(__self__, "create_time", create_time)
        if dedicated_resources is not None:
            pulumi.set(__self__, "dedicated_resources", dedicated_resources)
        if display_name is not None:
            pulumi.set(__self__, "display_name", display_name)
        if enable_access_logging is not None:
            pulumi.set(__self__, "enable_access_logging", enable_access_logging)
        if enable_container_logging is not None:
            pulumi.set(__self__, "enable_container_logging", enable_container_logging)
        if id is not None:
            pulumi.set(__self__, "id", id)
        if model is not None:
            pulumi.set(__self__, "model", model)
        if model_version_id is not None:
            pulumi.set(__self__, "model_version_id", model_version_id)
        if private_endpoints is not None:
            pulumi.set(__self__, "private_endpoints", private_endpoints)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if shared_resources is not None:
            pulumi.set(__self__, "shared_resources", shared_resources)

    @property
    @pulumi.getter(name="automaticResources")
    def automatic_resources(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelAutomaticResourceArgs']]]]:
        """
        (Output)
        A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
        Structure is documented below.
        """
        return pulumi.get(self, "automatic_resources")

    @automatic_resources.setter
    def automatic_resources(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelAutomaticResourceArgs']]]]):
        pulumi.set(self, "automatic_resources", value)

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Output only. Timestamp when the DeployedModel was created.
        """
        return pulumi.get(self, "create_time")

    @create_time.setter
    def create_time(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "create_time", value)

    @property
    @pulumi.getter(name="dedicatedResources")
    def dedicated_resources(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceArgs']]]]:
        """
        (Output)
        A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
        Structure is documented below.
        """
        return pulumi.get(self, "dedicated_resources")

    @dedicated_resources.setter
    def dedicated_resources(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceArgs']]]]):
        pulumi.set(self, "dedicated_resources", value)

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> Optional[pulumi.Input[str]]:
        """
        Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @display_name.setter
    def display_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "display_name", value)

    @property
    @pulumi.getter(name="enableAccessLogging")
    def enable_access_logging(self) -> Optional[pulumi.Input[bool]]:
        """
        (Output)
        These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        """
        return pulumi.get(self, "enable_access_logging")

    @enable_access_logging.setter
    def enable_access_logging(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_access_logging", value)

    @property
    @pulumi.getter(name="enableContainerLogging")
    def enable_container_logging(self) -> Optional[pulumi.Input[bool]]:
        """
        (Output)
        If true, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
        """
        return pulumi.get(self, "enable_container_logging")

    @enable_container_logging.setter
    def enable_container_logging(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_container_logging", value)

    @property
    @pulumi.getter
    def id(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
        """
        return pulumi.get(self, "id")

    @id.setter
    def id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "id", value)

    @property
    @pulumi.getter
    def model(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
        """
        return pulumi.get(self, "model")

    @model.setter
    def model(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "model", value)

    @property
    @pulumi.getter(name="modelVersionId")
    def model_version_id(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Output only. The version ID of the model that is deployed.
        """
        return pulumi.get(self, "model_version_id")

    @model_version_id.setter
    def model_version_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "model_version_id", value)

    @property
    @pulumi.getter(name="privateEndpoints")
    def private_endpoints(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelPrivateEndpointArgs']]]]:
        """
        (Output)
        Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
        Structure is documented below.
        """
        return pulumi.get(self, "private_endpoints")

    @private_endpoints.setter
    def private_endpoints(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelPrivateEndpointArgs']]]]):
        pulumi.set(self, "private_endpoints", value)

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        """
        return pulumi.get(self, "service_account")

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_account", value)

    @property
    @pulumi.getter(name="sharedResources")
    def shared_resources(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
        """
        return pulumi.get(self, "shared_resources")

    @shared_resources.setter
    def shared_resources(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "shared_resources", value)


if not MYPY:
    class AiEndpointDeployedModelAutomaticResourceArgsDict(TypedDict):
        max_replica_count: NotRequired[pulumi.Input[int]]
        """
        (Output)
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        min_replica_count: NotRequired[pulumi.Input[int]]
        """
        (Output)
        The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
elif False:
    AiEndpointDeployedModelAutomaticResourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointDeployedModelAutomaticResourceArgs:
    def __init__(__self__, *,
                 max_replica_count: Optional[pulumi.Input[int]] = None,
                 min_replica_count: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] max_replica_count: (Output)
               The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        :param pulumi.Input[int] min_replica_count: (Output)
               The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)
        if min_replica_count is not None:
            pulumi.set(__self__, "min_replica_count", min_replica_count)

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        (Output)
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        return pulumi.get(self, "max_replica_count")

    @max_replica_count.setter
    def max_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_replica_count", value)

    @property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        (Output)
        The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        return pulumi.get(self, "min_replica_count")

    @min_replica_count.setter
    def min_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_replica_count", value)


if not MYPY:
    class AiEndpointDeployedModelDedicatedResourceArgsDict(TypedDict):
        autoscaling_metric_specs: NotRequired[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgsDict']]]]
        """
        (Output)
        The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
        Structure is documented below.
        """
        machine_specs: NotRequired[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceMachineSpecArgsDict']]]]
        """
        (Output)
        The specification of a single machine used by the prediction.
        Structure is documented below.
        """
        max_replica_count: NotRequired[pulumi.Input[int]]
        """
        (Output)
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        min_replica_count: NotRequired[pulumi.Input[int]]
        """
        (Output)
        The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
elif False:
    AiEndpointDeployedModelDedicatedResourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointDeployedModelDedicatedResourceArgs:
    def __init__(__self__, *,
                 autoscaling_metric_specs: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs']]]] = None,
                 machine_specs: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceMachineSpecArgs']]]] = None,
                 max_replica_count: Optional[pulumi.Input[int]] = None,
                 min_replica_count: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs']]] autoscaling_metric_specs: (Output)
               The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
               Structure is documented below.
        :param pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceMachineSpecArgs']]] machine_specs: (Output)
               The specification of a single machine used by the prediction.
               Structure is documented below.
        :param pulumi.Input[int] max_replica_count: (Output)
               The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        :param pulumi.Input[int] min_replica_count: (Output)
               The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        if autoscaling_metric_specs is not None:
            pulumi.set(__self__, "autoscaling_metric_specs", autoscaling_metric_specs)
        if machine_specs is not None:
            pulumi.set(__self__, "machine_specs", machine_specs)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)
        if min_replica_count is not None:
            pulumi.set(__self__, "min_replica_count", min_replica_count)

    @property
    @pulumi.getter(name="autoscalingMetricSpecs")
    def autoscaling_metric_specs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs']]]]:
        """
        (Output)
        The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
        Structure is documented below.
        """
        return pulumi.get(self, "autoscaling_metric_specs")

    @autoscaling_metric_specs.setter
    def autoscaling_metric_specs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs']]]]):
        pulumi.set(self, "autoscaling_metric_specs", value)

    @property
    @pulumi.getter(name="machineSpecs")
    def machine_specs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceMachineSpecArgs']]]]:
        """
        (Output)
        The specification of a single machine used by the prediction.
        Structure is documented below.
        """
        return pulumi.get(self, "machine_specs")

    @machine_specs.setter
    def machine_specs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['AiEndpointDeployedModelDedicatedResourceMachineSpecArgs']]]]):
        pulumi.set(self, "machine_specs", value)

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        (Output)
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        return pulumi.get(self, "max_replica_count")

    @max_replica_count.setter
    def max_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_replica_count", value)

    @property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        (Output)
        The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        return pulumi.get(self, "min_replica_count")

    @min_replica_count.setter
    def min_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_replica_count", value)


if not MYPY:
    class AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgsDict(TypedDict):
        metric_name: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        target: NotRequired[pulumi.Input[int]]
        """
        (Output)
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
elif False:
    AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs:
    def __init__(__self__, *,
                 metric_name: Optional[pulumi.Input[str]] = None,
                 target: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[str] metric_name: (Output)
               The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        :param pulumi.Input[int] target: (Output)
               The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        if metric_name is not None:
            pulumi.set(__self__, "metric_name", metric_name)
        if target is not None:
            pulumi.set(__self__, "target", target)

    @property
    @pulumi.getter(name="metricName")
    def metric_name(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        return pulumi.get(self, "metric_name")

    @metric_name.setter
    def metric_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "metric_name", value)

    @property
    @pulumi.getter
    def target(self) -> Optional[pulumi.Input[int]]:
        """
        (Output)
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        return pulumi.get(self, "target")

    @target.setter
    def target(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "target", value)


if not MYPY:
    class AiEndpointDeployedModelDedicatedResourceMachineSpecArgsDict(TypedDict):
        accelerator_count: NotRequired[pulumi.Input[int]]
        """
        (Output)
        The number of accelerators to attach to the machine.
        """
        accelerator_type: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO(rsurowka): Try to better unify the required vs optional.
        """
elif False:
    AiEndpointDeployedModelDedicatedResourceMachineSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointDeployedModelDedicatedResourceMachineSpecArgs:
    def __init__(__self__, *,
                 accelerator_count: Optional[pulumi.Input[int]] = None,
                 accelerator_type: Optional[pulumi.Input[str]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] accelerator_count: (Output)
               The number of accelerators to attach to the machine.
        :param pulumi.Input[str] accelerator_type: (Output)
               The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        :param pulumi.Input[str] machine_type: (Output)
               The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO(rsurowka): Try to better unify the required vs optional.
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[pulumi.Input[int]]:
        """
        (Output)
        The number of accelerators to attach to the machine.
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "accelerator_type", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO(rsurowka): Try to better unify the required vs optional.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)


if not MYPY:
    class AiEndpointDeployedModelPrivateEndpointArgsDict(TypedDict):
        explain_http_uri: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Output only. Http(s) path to send explain requests.
        """
        health_http_uri: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Output only. Http(s) path to send health check requests.
        """
        predict_http_uri: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Output only. Http(s) path to send prediction requests.
        """
        service_attachment: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Output only. The name of the service attachment resource. Populated if private service connect is enabled.
        """
elif False:
    AiEndpointDeployedModelPrivateEndpointArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointDeployedModelPrivateEndpointArgs:
    def __init__(__self__, *,
                 explain_http_uri: Optional[pulumi.Input[str]] = None,
                 health_http_uri: Optional[pulumi.Input[str]] = None,
                 predict_http_uri: Optional[pulumi.Input[str]] = None,
                 service_attachment: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] explain_http_uri: (Output)
               Output only. Http(s) path to send explain requests.
        :param pulumi.Input[str] health_http_uri: (Output)
               Output only. Http(s) path to send health check requests.
        :param pulumi.Input[str] predict_http_uri: (Output)
               Output only. Http(s) path to send prediction requests.
        :param pulumi.Input[str] service_attachment: (Output)
               Output only. The name of the service attachment resource. Populated if private service connect is enabled.
        """
        if explain_http_uri is not None:
            pulumi.set(__self__, "explain_http_uri", explain_http_uri)
        if health_http_uri is not None:
            pulumi.set(__self__, "health_http_uri", health_http_uri)
        if predict_http_uri is not None:
            pulumi.set(__self__, "predict_http_uri", predict_http_uri)
        if service_attachment is not None:
            pulumi.set(__self__, "service_attachment", service_attachment)

    @property
    @pulumi.getter(name="explainHttpUri")
    def explain_http_uri(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Output only. Http(s) path to send explain requests.
        """
        return pulumi.get(self, "explain_http_uri")

    @explain_http_uri.setter
    def explain_http_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "explain_http_uri", value)

    @property
    @pulumi.getter(name="healthHttpUri")
    def health_http_uri(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Output only. Http(s) path to send health check requests.
        """
        return pulumi.get(self, "health_http_uri")

    @health_http_uri.setter
    def health_http_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "health_http_uri", value)

    @property
    @pulumi.getter(name="predictHttpUri")
    def predict_http_uri(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Output only. Http(s) path to send prediction requests.
        """
        return pulumi.get(self, "predict_http_uri")

    @predict_http_uri.setter
    def predict_http_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "predict_http_uri", value)

    @property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Output only. The name of the service attachment resource. Populated if private service connect is enabled.
        """
        return pulumi.get(self, "service_attachment")

    @service_attachment.setter
    def service_attachment(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_attachment", value)


if not MYPY:
    class AiEndpointEncryptionSpecArgsDict(TypedDict):
        kms_key_name: pulumi.Input[str]
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
elif False:
    AiEndpointEncryptionSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointEncryptionSpecArgs:
    def __init__(__self__, *,
                 kms_key_name: pulumi.Input[str]):
        """
        :param pulumi.Input[str] kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> pulumi.Input[str]:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "kms_key_name", value)


if not MYPY:
    class AiEndpointIamBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AiEndpointIamBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointIamBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AiEndpointIamMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AiEndpointIamMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiEndpointIamMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AiFeatureGroupBigQueryArgsDict(TypedDict):
        big_query_source: pulumi.Input['AiFeatureGroupBigQueryBigQuerySourceArgsDict']
        """
        The BigQuery source URI that points to either a BigQuery Table or View.
        Structure is documented below.
        """
        entity_id_columns: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Columns to construct entityId / row keys. Currently only supports 1 entity_id_column. If not provided defaults to entityId.
        """
elif False:
    AiFeatureGroupBigQueryArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureGroupBigQueryArgs:
    def __init__(__self__, *,
                 big_query_source: pulumi.Input['AiFeatureGroupBigQueryBigQuerySourceArgs'],
                 entity_id_columns: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input['AiFeatureGroupBigQueryBigQuerySourceArgs'] big_query_source: The BigQuery source URI that points to either a BigQuery Table or View.
               Structure is documented below.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] entity_id_columns: Columns to construct entityId / row keys. Currently only supports 1 entity_id_column. If not provided defaults to entityId.
        """
        pulumi.set(__self__, "big_query_source", big_query_source)
        if entity_id_columns is not None:
            pulumi.set(__self__, "entity_id_columns", entity_id_columns)

    @property
    @pulumi.getter(name="bigQuerySource")
    def big_query_source(self) -> pulumi.Input['AiFeatureGroupBigQueryBigQuerySourceArgs']:
        """
        The BigQuery source URI that points to either a BigQuery Table or View.
        Structure is documented below.
        """
        return pulumi.get(self, "big_query_source")

    @big_query_source.setter
    def big_query_source(self, value: pulumi.Input['AiFeatureGroupBigQueryBigQuerySourceArgs']):
        pulumi.set(self, "big_query_source", value)

    @property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Columns to construct entityId / row keys. Currently only supports 1 entity_id_column. If not provided defaults to entityId.
        """
        return pulumi.get(self, "entity_id_columns")

    @entity_id_columns.setter
    def entity_id_columns(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "entity_id_columns", value)


if not MYPY:
    class AiFeatureGroupBigQueryBigQuerySourceArgsDict(TypedDict):
        input_uri: pulumi.Input[str]
        """
        BigQuery URI to a table, up to 2000 characters long. For example: `bq://projectId.bqDatasetId.bqTableId.`
        """
elif False:
    AiFeatureGroupBigQueryBigQuerySourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureGroupBigQueryBigQuerySourceArgs:
    def __init__(__self__, *,
                 input_uri: pulumi.Input[str]):
        """
        :param pulumi.Input[str] input_uri: BigQuery URI to a table, up to 2000 characters long. For example: `bq://projectId.bqDatasetId.bqTableId.`
        """
        pulumi.set(__self__, "input_uri", input_uri)

    @property
    @pulumi.getter(name="inputUri")
    def input_uri(self) -> pulumi.Input[str]:
        """
        BigQuery URI to a table, up to 2000 characters long. For example: `bq://projectId.bqDatasetId.bqTableId.`
        """
        return pulumi.get(self, "input_uri")

    @input_uri.setter
    def input_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "input_uri", value)


if not MYPY:
    class AiFeatureOnlineStoreBigtableArgsDict(TypedDict):
        auto_scaling: pulumi.Input['AiFeatureOnlineStoreBigtableAutoScalingArgsDict']
        """
        Autoscaling config applied to Bigtable Instance.
        Structure is documented below.
        """
elif False:
    AiFeatureOnlineStoreBigtableArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreBigtableArgs:
    def __init__(__self__, *,
                 auto_scaling: pulumi.Input['AiFeatureOnlineStoreBigtableAutoScalingArgs']):
        """
        :param pulumi.Input['AiFeatureOnlineStoreBigtableAutoScalingArgs'] auto_scaling: Autoscaling config applied to Bigtable Instance.
               Structure is documented below.
        """
        pulumi.set(__self__, "auto_scaling", auto_scaling)

    @property
    @pulumi.getter(name="autoScaling")
    def auto_scaling(self) -> pulumi.Input['AiFeatureOnlineStoreBigtableAutoScalingArgs']:
        """
        Autoscaling config applied to Bigtable Instance.
        Structure is documented below.
        """
        return pulumi.get(self, "auto_scaling")

    @auto_scaling.setter
    def auto_scaling(self, value: pulumi.Input['AiFeatureOnlineStoreBigtableAutoScalingArgs']):
        pulumi.set(self, "auto_scaling", value)


if not MYPY:
    class AiFeatureOnlineStoreBigtableAutoScalingArgsDict(TypedDict):
        max_node_count: pulumi.Input[int]
        """
        The maximum number of nodes to scale up to. Must be greater than or equal to minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        """
        min_node_count: pulumi.Input[int]
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        cpu_utilization_target: NotRequired[pulumi.Input[int]]
        """
        A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
elif False:
    AiFeatureOnlineStoreBigtableAutoScalingArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreBigtableAutoScalingArgs:
    def __init__(__self__, *,
                 max_node_count: pulumi.Input[int],
                 min_node_count: pulumi.Input[int],
                 cpu_utilization_target: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] max_node_count: The maximum number of nodes to scale up to. Must be greater than or equal to minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        :param pulumi.Input[int] min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        :param pulumi.Input[int] cpu_utilization_target: A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)
        if cpu_utilization_target is not None:
            pulumi.set(__self__, "cpu_utilization_target", cpu_utilization_target)

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> pulumi.Input[int]:
        """
        The maximum number of nodes to scale up to. Must be greater than or equal to minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        """
        return pulumi.get(self, "max_node_count")

    @max_node_count.setter
    def max_node_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_node_count", value)

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> pulumi.Input[int]:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")

    @min_node_count.setter
    def min_node_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "min_node_count", value)

    @property
    @pulumi.getter(name="cpuUtilizationTarget")
    def cpu_utilization_target(self) -> Optional[pulumi.Input[int]]:
        """
        A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
        return pulumi.get(self, "cpu_utilization_target")

    @cpu_utilization_target.setter
    def cpu_utilization_target(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "cpu_utilization_target", value)


if not MYPY:
    class AiFeatureOnlineStoreDedicatedServingEndpointArgsDict(TypedDict):
        private_service_connect_config: NotRequired[pulumi.Input['AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgsDict']]
        """
        Private service connect config.
        Structure is documented below.
        """
        public_endpoint_domain_name: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Domain name to use for this FeatureOnlineStore
        """
        service_attachment: NotRequired[pulumi.Input[str]]
        """
        (Output)
        Name of the service attachment resource. Applicable only if private service connect is enabled and after FeatureViewSync is created.
        """
elif False:
    AiFeatureOnlineStoreDedicatedServingEndpointArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreDedicatedServingEndpointArgs:
    def __init__(__self__, *,
                 private_service_connect_config: Optional[pulumi.Input['AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgs']] = None,
                 public_endpoint_domain_name: Optional[pulumi.Input[str]] = None,
                 service_attachment: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input['AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgs'] private_service_connect_config: Private service connect config.
               Structure is documented below.
        :param pulumi.Input[str] public_endpoint_domain_name: (Output)
               Domain name to use for this FeatureOnlineStore
        :param pulumi.Input[str] service_attachment: (Output)
               Name of the service attachment resource. Applicable only if private service connect is enabled and after FeatureViewSync is created.
        """
        if private_service_connect_config is not None:
            pulumi.set(__self__, "private_service_connect_config", private_service_connect_config)
        if public_endpoint_domain_name is not None:
            pulumi.set(__self__, "public_endpoint_domain_name", public_endpoint_domain_name)
        if service_attachment is not None:
            pulumi.set(__self__, "service_attachment", service_attachment)

    @property
    @pulumi.getter(name="privateServiceConnectConfig")
    def private_service_connect_config(self) -> Optional[pulumi.Input['AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgs']]:
        """
        Private service connect config.
        Structure is documented below.
        """
        return pulumi.get(self, "private_service_connect_config")

    @private_service_connect_config.setter
    def private_service_connect_config(self, value: Optional[pulumi.Input['AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgs']]):
        pulumi.set(self, "private_service_connect_config", value)

    @property
    @pulumi.getter(name="publicEndpointDomainName")
    def public_endpoint_domain_name(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Domain name to use for this FeatureOnlineStore
        """
        return pulumi.get(self, "public_endpoint_domain_name")

    @public_endpoint_domain_name.setter
    def public_endpoint_domain_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "public_endpoint_domain_name", value)

    @property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        Name of the service attachment resource. Applicable only if private service connect is enabled and after FeatureViewSync is created.
        """
        return pulumi.get(self, "service_attachment")

    @service_attachment.setter
    def service_attachment(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_attachment", value)


if not MYPY:
    class AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgsDict(TypedDict):
        enable_private_service_connect: pulumi.Input[bool]
        """
        If set to true, customers will use private service connection to send request. Otherwise, the connection will set to public endpoint.
        """
        project_allowlists: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
elif False:
    AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgs:
    def __init__(__self__, *,
                 enable_private_service_connect: pulumi.Input[bool],
                 project_allowlists: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[bool] enable_private_service_connect: If set to true, customers will use private service connection to send request. Otherwise, the connection will set to public endpoint.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] project_allowlists: A list of Projects from which the forwarding rule will target the service attachment.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        if project_allowlists is not None:
            pulumi.set(__self__, "project_allowlists", project_allowlists)

    @property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> pulumi.Input[bool]:
        """
        If set to true, customers will use private service connection to send request. Otherwise, the connection will set to public endpoint.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @enable_private_service_connect.setter
    def enable_private_service_connect(self, value: pulumi.Input[bool]):
        pulumi.set(self, "enable_private_service_connect", value)

    @property
    @pulumi.getter(name="projectAllowlists")
    def project_allowlists(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlists")

    @project_allowlists.setter
    def project_allowlists(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "project_allowlists", value)


if not MYPY:
    class AiFeatureOnlineStoreEmbeddingManagementArgsDict(TypedDict):
        enabled: NotRequired[pulumi.Input[bool]]
        """
        Enable embedding management.
        """
elif False:
    AiFeatureOnlineStoreEmbeddingManagementArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreEmbeddingManagementArgs:
    def __init__(__self__, *,
                 enabled: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[bool] enabled: Enable embedding management.
        """
        if enabled is not None:
            pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Enable embedding management.
        """
        return pulumi.get(self, "enabled")

    @enabled.setter
    def enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enabled", value)


if not MYPY:
    class AiFeatureOnlineStoreFeatureviewBigQuerySourceArgsDict(TypedDict):
        entity_id_columns: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Columns to construct entityId / row keys. Start by supporting 1 only.
        """
        uri: pulumi.Input[str]
        """
        The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
elif False:
    AiFeatureOnlineStoreFeatureviewBigQuerySourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreFeatureviewBigQuerySourceArgs:
    def __init__(__self__, *,
                 entity_id_columns: pulumi.Input[Sequence[pulumi.Input[str]]],
                 uri: pulumi.Input[str]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] entity_id_columns: Columns to construct entityId / row keys. Start by supporting 1 only.
        :param pulumi.Input[str] uri: The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        pulumi.set(__self__, "entity_id_columns", entity_id_columns)
        pulumi.set(__self__, "uri", uri)

    @property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Columns to construct entityId / row keys. Start by supporting 1 only.
        """
        return pulumi.get(self, "entity_id_columns")

    @entity_id_columns.setter
    def entity_id_columns(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "entity_id_columns", value)

    @property
    @pulumi.getter
    def uri(self) -> pulumi.Input[str]:
        """
        The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        return pulumi.get(self, "uri")

    @uri.setter
    def uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "uri", value)


if not MYPY:
    class AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceArgsDict(TypedDict):
        feature_groups: pulumi.Input[Sequence[pulumi.Input['AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgsDict']]]
        """
        List of features that need to be synced to Online Store.
        Structure is documented below.
        """
elif False:
    AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceArgs:
    def __init__(__self__, *,
                 feature_groups: pulumi.Input[Sequence[pulumi.Input['AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgs']]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input['AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgs']]] feature_groups: List of features that need to be synced to Online Store.
               Structure is documented below.
        """
        pulumi.set(__self__, "feature_groups", feature_groups)

    @property
    @pulumi.getter(name="featureGroups")
    def feature_groups(self) -> pulumi.Input[Sequence[pulumi.Input['AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgs']]]:
        """
        List of features that need to be synced to Online Store.
        Structure is documented below.
        """
        return pulumi.get(self, "feature_groups")

    @feature_groups.setter
    def feature_groups(self, value: pulumi.Input[Sequence[pulumi.Input['AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgs']]]):
        pulumi.set(self, "feature_groups", value)


if not MYPY:
    class AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgsDict(TypedDict):
        feature_group_id: pulumi.Input[str]
        """
        Identifier of the feature group.
        """
        feature_ids: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Identifiers of features under the feature group.
        """
elif False:
    AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgs:
    def __init__(__self__, *,
                 feature_group_id: pulumi.Input[str],
                 feature_ids: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        :param pulumi.Input[str] feature_group_id: Identifier of the feature group.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] feature_ids: Identifiers of features under the feature group.
        """
        pulumi.set(__self__, "feature_group_id", feature_group_id)
        pulumi.set(__self__, "feature_ids", feature_ids)

    @property
    @pulumi.getter(name="featureGroupId")
    def feature_group_id(self) -> pulumi.Input[str]:
        """
        Identifier of the feature group.
        """
        return pulumi.get(self, "feature_group_id")

    @feature_group_id.setter
    def feature_group_id(self, value: pulumi.Input[str]):
        pulumi.set(self, "feature_group_id", value)

    @property
    @pulumi.getter(name="featureIds")
    def feature_ids(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Identifiers of features under the feature group.
        """
        return pulumi.get(self, "feature_ids")

    @feature_ids.setter
    def feature_ids(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "feature_ids", value)


if not MYPY:
    class AiFeatureOnlineStoreFeatureviewSyncConfigArgsDict(TypedDict):
        cron: NotRequired[pulumi.Input[str]]
        """
        Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs.
        To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
        """
elif False:
    AiFeatureOnlineStoreFeatureviewSyncConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreFeatureviewSyncConfigArgs:
    def __init__(__self__, *,
                 cron: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] cron: Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs.
               To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
        """
        if cron is not None:
            pulumi.set(__self__, "cron", cron)

    @property
    @pulumi.getter
    def cron(self) -> Optional[pulumi.Input[str]]:
        """
        Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs.
        To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
        """
        return pulumi.get(self, "cron")

    @cron.setter
    def cron(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cron", value)


if not MYPY:
    class AiFeatureOnlineStoreFeatureviewVectorSearchConfigArgsDict(TypedDict):
        embedding_column: pulumi.Input[str]
        """
        Column of embedding. This column contains the source data to create index for vector search.
        """
        brute_force_config: NotRequired[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgsDict']]
        """
        Configuration options for using brute force search, which simply implements the standard linear search in the database for each query. It is primarily meant for benchmarking and to generate the ground truth for approximate search.
        """
        crowding_column: NotRequired[pulumi.Input[str]]
        """
        Column of crowding. This column contains crowding attribute which is a constraint on a neighbor list produced by nearest neighbor search requiring that no more than some value k' of the k neighbors returned have the same value of crowdingAttribute.
        """
        distance_measure_type: NotRequired[pulumi.Input[str]]
        """
        The distance measure used in nearest neighbor search.
        For details on allowed values, see the [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.featureOnlineStores.featureViews#DistanceMeasureType).
        Possible values are: `SQUARED_L2_DISTANCE`, `COSINE_DISTANCE`, `DOT_PRODUCT_DISTANCE`.
        """
        embedding_dimension: NotRequired[pulumi.Input[int]]
        """
        The number of dimensions of the input embedding.
        """
        filter_columns: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Columns of features that are used to filter vector search results.
        """
        tree_ah_config: NotRequired[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgsDict']]
        """
        Configuration options for the tree-AH algorithm (Shallow tree + Asymmetric Hashing). Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        Structure is documented below.
        """
elif False:
    AiFeatureOnlineStoreFeatureviewVectorSearchConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreFeatureviewVectorSearchConfigArgs:
    def __init__(__self__, *,
                 embedding_column: pulumi.Input[str],
                 brute_force_config: Optional[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgs']] = None,
                 crowding_column: Optional[pulumi.Input[str]] = None,
                 distance_measure_type: Optional[pulumi.Input[str]] = None,
                 embedding_dimension: Optional[pulumi.Input[int]] = None,
                 filter_columns: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 tree_ah_config: Optional[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgs']] = None):
        """
        :param pulumi.Input[str] embedding_column: Column of embedding. This column contains the source data to create index for vector search.
        :param pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgs'] brute_force_config: Configuration options for using brute force search, which simply implements the standard linear search in the database for each query. It is primarily meant for benchmarking and to generate the ground truth for approximate search.
        :param pulumi.Input[str] crowding_column: Column of crowding. This column contains crowding attribute which is a constraint on a neighbor list produced by nearest neighbor search requiring that no more than some value k' of the k neighbors returned have the same value of crowdingAttribute.
        :param pulumi.Input[str] distance_measure_type: The distance measure used in nearest neighbor search.
               For details on allowed values, see the [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.featureOnlineStores.featureViews#DistanceMeasureType).
               Possible values are: `SQUARED_L2_DISTANCE`, `COSINE_DISTANCE`, `DOT_PRODUCT_DISTANCE`.
        :param pulumi.Input[int] embedding_dimension: The number of dimensions of the input embedding.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] filter_columns: Columns of features that are used to filter vector search results.
        :param pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgs'] tree_ah_config: Configuration options for the tree-AH algorithm (Shallow tree + Asymmetric Hashing). Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
               Structure is documented below.
        """
        pulumi.set(__self__, "embedding_column", embedding_column)
        if brute_force_config is not None:
            pulumi.set(__self__, "brute_force_config", brute_force_config)
        if crowding_column is not None:
            pulumi.set(__self__, "crowding_column", crowding_column)
        if distance_measure_type is not None:
            pulumi.set(__self__, "distance_measure_type", distance_measure_type)
        if embedding_dimension is not None:
            pulumi.set(__self__, "embedding_dimension", embedding_dimension)
        if filter_columns is not None:
            pulumi.set(__self__, "filter_columns", filter_columns)
        if tree_ah_config is not None:
            pulumi.set(__self__, "tree_ah_config", tree_ah_config)

    @property
    @pulumi.getter(name="embeddingColumn")
    def embedding_column(self) -> pulumi.Input[str]:
        """
        Column of embedding. This column contains the source data to create index for vector search.
        """
        return pulumi.get(self, "embedding_column")

    @embedding_column.setter
    def embedding_column(self, value: pulumi.Input[str]):
        pulumi.set(self, "embedding_column", value)

    @property
    @pulumi.getter(name="bruteForceConfig")
    def brute_force_config(self) -> Optional[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgs']]:
        """
        Configuration options for using brute force search, which simply implements the standard linear search in the database for each query. It is primarily meant for benchmarking and to generate the ground truth for approximate search.
        """
        return pulumi.get(self, "brute_force_config")

    @brute_force_config.setter
    def brute_force_config(self, value: Optional[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgs']]):
        pulumi.set(self, "brute_force_config", value)

    @property
    @pulumi.getter(name="crowdingColumn")
    def crowding_column(self) -> Optional[pulumi.Input[str]]:
        """
        Column of crowding. This column contains crowding attribute which is a constraint on a neighbor list produced by nearest neighbor search requiring that no more than some value k' of the k neighbors returned have the same value of crowdingAttribute.
        """
        return pulumi.get(self, "crowding_column")

    @crowding_column.setter
    def crowding_column(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "crowding_column", value)

    @property
    @pulumi.getter(name="distanceMeasureType")
    def distance_measure_type(self) -> Optional[pulumi.Input[str]]:
        """
        The distance measure used in nearest neighbor search.
        For details on allowed values, see the [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.featureOnlineStores.featureViews#DistanceMeasureType).
        Possible values are: `SQUARED_L2_DISTANCE`, `COSINE_DISTANCE`, `DOT_PRODUCT_DISTANCE`.
        """
        return pulumi.get(self, "distance_measure_type")

    @distance_measure_type.setter
    def distance_measure_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "distance_measure_type", value)

    @property
    @pulumi.getter(name="embeddingDimension")
    def embedding_dimension(self) -> Optional[pulumi.Input[int]]:
        """
        The number of dimensions of the input embedding.
        """
        return pulumi.get(self, "embedding_dimension")

    @embedding_dimension.setter
    def embedding_dimension(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "embedding_dimension", value)

    @property
    @pulumi.getter(name="filterColumns")
    def filter_columns(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Columns of features that are used to filter vector search results.
        """
        return pulumi.get(self, "filter_columns")

    @filter_columns.setter
    def filter_columns(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "filter_columns", value)

    @property
    @pulumi.getter(name="treeAhConfig")
    def tree_ah_config(self) -> Optional[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgs']]:
        """
        Configuration options for the tree-AH algorithm (Shallow tree + Asymmetric Hashing). Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        Structure is documented below.
        """
        return pulumi.get(self, "tree_ah_config")

    @tree_ah_config.setter
    def tree_ah_config(self, value: Optional[pulumi.Input['AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgs']]):
        pulumi.set(self, "tree_ah_config", value)


if not MYPY:
    class AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgsDict(TypedDict):
        pass
elif False:
    AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgs:
    def __init__(__self__):
        pass


if not MYPY:
    class AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgsDict(TypedDict):
        leaf_node_embedding_count: NotRequired[pulumi.Input[str]]
        """
        Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
elif False:
    AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgs:
    def __init__(__self__, *,
                 leaf_node_embedding_count: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] leaf_node_embedding_count: Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        if leaf_node_embedding_count is not None:
            pulumi.set(__self__, "leaf_node_embedding_count", leaf_node_embedding_count)

    @property
    @pulumi.getter(name="leafNodeEmbeddingCount")
    def leaf_node_embedding_count(self) -> Optional[pulumi.Input[str]]:
        """
        Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        return pulumi.get(self, "leaf_node_embedding_count")

    @leaf_node_embedding_count.setter
    def leaf_node_embedding_count(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "leaf_node_embedding_count", value)


if not MYPY:
    class AiFeatureOnlineStoreOptimizedArgsDict(TypedDict):
        pass
elif False:
    AiFeatureOnlineStoreOptimizedArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureOnlineStoreOptimizedArgs:
    def __init__(__self__):
        pass


if not MYPY:
    class AiFeatureStoreEncryptionSpecArgsDict(TypedDict):
        kms_key_name: pulumi.Input[str]
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
        """
elif False:
    AiFeatureStoreEncryptionSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEncryptionSpecArgs:
    def __init__(__self__, *,
                 kms_key_name: pulumi.Input[str]):
        """
        :param pulumi.Input[str] kms_key_name: The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> pulumi.Input[str]:
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "kms_key_name", value)


if not MYPY:
    class AiFeatureStoreEntityTypeIamBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AiFeatureStoreEntityTypeIamBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEntityTypeIamBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AiFeatureStoreEntityTypeIamMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AiFeatureStoreEntityTypeIamMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEntityTypeIamMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AiFeatureStoreEntityTypeMonitoringConfigArgsDict(TypedDict):
        categorical_threshold_config: NotRequired[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgsDict']]
        """
        Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
        Structure is documented below.
        """
        import_features_analysis: NotRequired[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgsDict']]
        """
        The config for ImportFeatures Analysis Based Feature Monitoring.
        Structure is documented below.
        """
        numerical_threshold_config: NotRequired[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgsDict']]
        """
        Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
        Structure is documented below.
        """
        snapshot_analysis: NotRequired[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgsDict']]
        """
        The config for Snapshot Analysis Based Feature Monitoring.
        Structure is documented below.
        """
elif False:
    AiFeatureStoreEntityTypeMonitoringConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEntityTypeMonitoringConfigArgs:
    def __init__(__self__, *,
                 categorical_threshold_config: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgs']] = None,
                 import_features_analysis: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgs']] = None,
                 numerical_threshold_config: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgs']] = None,
                 snapshot_analysis: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgs']] = None):
        """
        :param pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgs'] categorical_threshold_config: Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
               Structure is documented below.
        :param pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgs'] import_features_analysis: The config for ImportFeatures Analysis Based Feature Monitoring.
               Structure is documented below.
        :param pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgs'] numerical_threshold_config: Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
               Structure is documented below.
        :param pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgs'] snapshot_analysis: The config for Snapshot Analysis Based Feature Monitoring.
               Structure is documented below.
        """
        if categorical_threshold_config is not None:
            pulumi.set(__self__, "categorical_threshold_config", categorical_threshold_config)
        if import_features_analysis is not None:
            pulumi.set(__self__, "import_features_analysis", import_features_analysis)
        if numerical_threshold_config is not None:
            pulumi.set(__self__, "numerical_threshold_config", numerical_threshold_config)
        if snapshot_analysis is not None:
            pulumi.set(__self__, "snapshot_analysis", snapshot_analysis)

    @property
    @pulumi.getter(name="categoricalThresholdConfig")
    def categorical_threshold_config(self) -> Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgs']]:
        """
        Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
        Structure is documented below.
        """
        return pulumi.get(self, "categorical_threshold_config")

    @categorical_threshold_config.setter
    def categorical_threshold_config(self, value: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgs']]):
        pulumi.set(self, "categorical_threshold_config", value)

    @property
    @pulumi.getter(name="importFeaturesAnalysis")
    def import_features_analysis(self) -> Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgs']]:
        """
        The config for ImportFeatures Analysis Based Feature Monitoring.
        Structure is documented below.
        """
        return pulumi.get(self, "import_features_analysis")

    @import_features_analysis.setter
    def import_features_analysis(self, value: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgs']]):
        pulumi.set(self, "import_features_analysis", value)

    @property
    @pulumi.getter(name="numericalThresholdConfig")
    def numerical_threshold_config(self) -> Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgs']]:
        """
        Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
        Structure is documented below.
        """
        return pulumi.get(self, "numerical_threshold_config")

    @numerical_threshold_config.setter
    def numerical_threshold_config(self, value: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgs']]):
        pulumi.set(self, "numerical_threshold_config", value)

    @property
    @pulumi.getter(name="snapshotAnalysis")
    def snapshot_analysis(self) -> Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgs']]:
        """
        The config for Snapshot Analysis Based Feature Monitoring.
        Structure is documented below.
        """
        return pulumi.get(self, "snapshot_analysis")

    @snapshot_analysis.setter
    def snapshot_analysis(self, value: Optional[pulumi.Input['AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgs']]):
        pulumi.set(self, "snapshot_analysis", value)


if not MYPY:
    class AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgsDict(TypedDict):
        value: pulumi.Input[float]
        """
        Specify a threshold value that can trigger the alert. For categorical feature, the distribution distance is calculated by L-inifinity norm. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
elif False:
    AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgs:
    def __init__(__self__, *,
                 value: pulumi.Input[float]):
        """
        :param pulumi.Input[float] value: Specify a threshold value that can trigger the alert. For categorical feature, the distribution distance is calculated by L-inifinity norm. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def value(self) -> pulumi.Input[float]:
        """
        Specify a threshold value that can trigger the alert. For categorical feature, the distribution distance is calculated by L-inifinity norm. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        return pulumi.get(self, "value")

    @value.setter
    def value(self, value: pulumi.Input[float]):
        pulumi.set(self, "value", value)


if not MYPY:
    class AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgsDict(TypedDict):
        anomaly_detection_baseline: NotRequired[pulumi.Input[str]]
        """
        Defines the baseline to do anomaly detection for feature values imported by each [entityTypes.importFeatureValues][] operation. The value must be one of the values below:
        * LATEST_STATS: Choose the later one statistics generated by either most recent snapshot analysis or previous import features analysis. If non of them exists, skip anomaly detection and only generate a statistics.
        * MOST_RECENT_SNAPSHOT_STATS: Use the statistics generated by the most recent snapshot analysis if exists.
        * PREVIOUS_IMPORT_FEATURES_STATS: Use the statistics generated by the previous import features analysis if exists.
        """
        state: NotRequired[pulumi.Input[str]]
        """
        Whether to enable / disable / inherite default hebavior for import features analysis. The value must be one of the values below:
        * DEFAULT: The default behavior of whether to enable the monitoring. EntityType-level config: disabled.
        * ENABLED: Explicitly enables import features analysis. EntityType-level config: by default enables import features analysis for all Features under it.
        * DISABLED: Explicitly disables import features analysis. EntityType-level config: by default disables import features analysis for all Features under it.
        """
elif False:
    AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgs:
    def __init__(__self__, *,
                 anomaly_detection_baseline: Optional[pulumi.Input[str]] = None,
                 state: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] anomaly_detection_baseline: Defines the baseline to do anomaly detection for feature values imported by each [entityTypes.importFeatureValues][] operation. The value must be one of the values below:
               * LATEST_STATS: Choose the later one statistics generated by either most recent snapshot analysis or previous import features analysis. If non of them exists, skip anomaly detection and only generate a statistics.
               * MOST_RECENT_SNAPSHOT_STATS: Use the statistics generated by the most recent snapshot analysis if exists.
               * PREVIOUS_IMPORT_FEATURES_STATS: Use the statistics generated by the previous import features analysis if exists.
        :param pulumi.Input[str] state: Whether to enable / disable / inherite default hebavior for import features analysis. The value must be one of the values below:
               * DEFAULT: The default behavior of whether to enable the monitoring. EntityType-level config: disabled.
               * ENABLED: Explicitly enables import features analysis. EntityType-level config: by default enables import features analysis for all Features under it.
               * DISABLED: Explicitly disables import features analysis. EntityType-level config: by default disables import features analysis for all Features under it.
        """
        if anomaly_detection_baseline is not None:
            pulumi.set(__self__, "anomaly_detection_baseline", anomaly_detection_baseline)
        if state is not None:
            pulumi.set(__self__, "state", state)

    @property
    @pulumi.getter(name="anomalyDetectionBaseline")
    def anomaly_detection_baseline(self) -> Optional[pulumi.Input[str]]:
        """
        Defines the baseline to do anomaly detection for feature values imported by each [entityTypes.importFeatureValues][] operation. The value must be one of the values below:
        * LATEST_STATS: Choose the later one statistics generated by either most recent snapshot analysis or previous import features analysis. If non of them exists, skip anomaly detection and only generate a statistics.
        * MOST_RECENT_SNAPSHOT_STATS: Use the statistics generated by the most recent snapshot analysis if exists.
        * PREVIOUS_IMPORT_FEATURES_STATS: Use the statistics generated by the previous import features analysis if exists.
        """
        return pulumi.get(self, "anomaly_detection_baseline")

    @anomaly_detection_baseline.setter
    def anomaly_detection_baseline(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "anomaly_detection_baseline", value)

    @property
    @pulumi.getter
    def state(self) -> Optional[pulumi.Input[str]]:
        """
        Whether to enable / disable / inherite default hebavior for import features analysis. The value must be one of the values below:
        * DEFAULT: The default behavior of whether to enable the monitoring. EntityType-level config: disabled.
        * ENABLED: Explicitly enables import features analysis. EntityType-level config: by default enables import features analysis for all Features under it.
        * DISABLED: Explicitly disables import features analysis. EntityType-level config: by default disables import features analysis for all Features under it.
        """
        return pulumi.get(self, "state")

    @state.setter
    def state(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "state", value)


if not MYPY:
    class AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgsDict(TypedDict):
        value: pulumi.Input[float]
        """
        Specify a threshold value that can trigger the alert. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
elif False:
    AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgs:
    def __init__(__self__, *,
                 value: pulumi.Input[float]):
        """
        :param pulumi.Input[float] value: Specify a threshold value that can trigger the alert. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def value(self) -> pulumi.Input[float]:
        """
        Specify a threshold value that can trigger the alert. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        return pulumi.get(self, "value")

    @value.setter
    def value(self, value: pulumi.Input[float]):
        pulumi.set(self, "value", value)


if not MYPY:
    class AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgsDict(TypedDict):
        disabled: NotRequired[pulumi.Input[bool]]
        """
        The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoringInterval for Features under it.
        """
        monitoring_interval: NotRequired[pulumi.Input[str]]
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value is rolled up to full day.
        A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".

        > **Warning:** `monitoring_interval` is deprecated and will be removed in a future release.
        """
        monitoring_interval_days: NotRequired[pulumi.Input[int]]
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days. The default value is 1.
        If both FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days and [FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval][] are set when creating/updating EntityTypes/Features, FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days will be used.
        """
        staleness_days: NotRequired[pulumi.Input[int]]
        """
        Customized export features time window for snapshot analysis. Unit is one day. The default value is 21 days. Minimum value is 1 day. Maximum value is 4000 days.
        """
elif False:
    AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgs:
    def __init__(__self__, *,
                 disabled: Optional[pulumi.Input[bool]] = None,
                 monitoring_interval: Optional[pulumi.Input[str]] = None,
                 monitoring_interval_days: Optional[pulumi.Input[int]] = None,
                 staleness_days: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[bool] disabled: The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoringInterval for Features under it.
        :param pulumi.Input[str] monitoring_interval: Configuration of the snapshot analysis based monitoring pipeline running interval. The value is rolled up to full day.
               A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
               
               > **Warning:** `monitoring_interval` is deprecated and will be removed in a future release.
        :param pulumi.Input[int] monitoring_interval_days: Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days. The default value is 1.
               If both FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days and [FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval][] are set when creating/updating EntityTypes/Features, FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days will be used.
        :param pulumi.Input[int] staleness_days: Customized export features time window for snapshot analysis. Unit is one day. The default value is 21 days. Minimum value is 1 day. Maximum value is 4000 days.
        """
        if disabled is not None:
            pulumi.set(__self__, "disabled", disabled)
        if monitoring_interval is not None:
            warnings.warn("""`monitoring_interval` is deprecated and will be removed in a future release.""", DeprecationWarning)
            pulumi.log.warn("""monitoring_interval is deprecated: `monitoring_interval` is deprecated and will be removed in a future release.""")
        if monitoring_interval is not None:
            pulumi.set(__self__, "monitoring_interval", monitoring_interval)
        if monitoring_interval_days is not None:
            pulumi.set(__self__, "monitoring_interval_days", monitoring_interval_days)
        if staleness_days is not None:
            pulumi.set(__self__, "staleness_days", staleness_days)

    @property
    @pulumi.getter
    def disabled(self) -> Optional[pulumi.Input[bool]]:
        """
        The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoringInterval for Features under it.
        """
        return pulumi.get(self, "disabled")

    @disabled.setter
    def disabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "disabled", value)

    @property
    @pulumi.getter(name="monitoringInterval")
    @_utilities.deprecated("""`monitoring_interval` is deprecated and will be removed in a future release.""")
    def monitoring_interval(self) -> Optional[pulumi.Input[str]]:
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value is rolled up to full day.
        A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".

        > **Warning:** `monitoring_interval` is deprecated and will be removed in a future release.
        """
        return pulumi.get(self, "monitoring_interval")

    @monitoring_interval.setter
    def monitoring_interval(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "monitoring_interval", value)

    @property
    @pulumi.getter(name="monitoringIntervalDays")
    def monitoring_interval_days(self) -> Optional[pulumi.Input[int]]:
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days. The default value is 1.
        If both FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days and [FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval][] are set when creating/updating EntityTypes/Features, FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days will be used.
        """
        return pulumi.get(self, "monitoring_interval_days")

    @monitoring_interval_days.setter
    def monitoring_interval_days(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "monitoring_interval_days", value)

    @property
    @pulumi.getter(name="stalenessDays")
    def staleness_days(self) -> Optional[pulumi.Input[int]]:
        """
        Customized export features time window for snapshot analysis. Unit is one day. The default value is 21 days. Minimum value is 1 day. Maximum value is 4000 days.
        """
        return pulumi.get(self, "staleness_days")

    @staleness_days.setter
    def staleness_days(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "staleness_days", value)


if not MYPY:
    class AiFeatureStoreIamBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AiFeatureStoreIamBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreIamBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AiFeatureStoreIamMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AiFeatureStoreIamMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreIamMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AiFeatureStoreOnlineServingConfigArgsDict(TypedDict):
        fixed_node_count: NotRequired[pulumi.Input[int]]
        """
        The number of nodes for each cluster. The number of nodes will not scale automatically but can be scaled manually by providing different values when updating.
        """
        scaling: NotRequired[pulumi.Input['AiFeatureStoreOnlineServingConfigScalingArgsDict']]
        """
        Online serving scaling configuration. Only one of fixedNodeCount and scaling can be set. Setting one will reset the other.
        Structure is documented below.
        """
elif False:
    AiFeatureStoreOnlineServingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreOnlineServingConfigArgs:
    def __init__(__self__, *,
                 fixed_node_count: Optional[pulumi.Input[int]] = None,
                 scaling: Optional[pulumi.Input['AiFeatureStoreOnlineServingConfigScalingArgs']] = None):
        """
        :param pulumi.Input[int] fixed_node_count: The number of nodes for each cluster. The number of nodes will not scale automatically but can be scaled manually by providing different values when updating.
        :param pulumi.Input['AiFeatureStoreOnlineServingConfigScalingArgs'] scaling: Online serving scaling configuration. Only one of fixedNodeCount and scaling can be set. Setting one will reset the other.
               Structure is documented below.
        """
        if fixed_node_count is not None:
            pulumi.set(__self__, "fixed_node_count", fixed_node_count)
        if scaling is not None:
            pulumi.set(__self__, "scaling", scaling)

    @property
    @pulumi.getter(name="fixedNodeCount")
    def fixed_node_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of nodes for each cluster. The number of nodes will not scale automatically but can be scaled manually by providing different values when updating.
        """
        return pulumi.get(self, "fixed_node_count")

    @fixed_node_count.setter
    def fixed_node_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "fixed_node_count", value)

    @property
    @pulumi.getter
    def scaling(self) -> Optional[pulumi.Input['AiFeatureStoreOnlineServingConfigScalingArgs']]:
        """
        Online serving scaling configuration. Only one of fixedNodeCount and scaling can be set. Setting one will reset the other.
        Structure is documented below.
        """
        return pulumi.get(self, "scaling")

    @scaling.setter
    def scaling(self, value: Optional[pulumi.Input['AiFeatureStoreOnlineServingConfigScalingArgs']]):
        pulumi.set(self, "scaling", value)


if not MYPY:
    class AiFeatureStoreOnlineServingConfigScalingArgsDict(TypedDict):
        max_node_count: pulumi.Input[int]
        """
        The maximum number of nodes to scale up to. Must be greater than minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        """
        min_node_count: pulumi.Input[int]
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
elif False:
    AiFeatureStoreOnlineServingConfigScalingArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiFeatureStoreOnlineServingConfigScalingArgs:
    def __init__(__self__, *,
                 max_node_count: pulumi.Input[int],
                 min_node_count: pulumi.Input[int]):
        """
        :param pulumi.Input[int] max_node_count: The maximum number of nodes to scale up to. Must be greater than minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        :param pulumi.Input[int] min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> pulumi.Input[int]:
        """
        The maximum number of nodes to scale up to. Must be greater than minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        """
        return pulumi.get(self, "max_node_count")

    @max_node_count.setter
    def max_node_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_node_count", value)

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> pulumi.Input[int]:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")

    @min_node_count.setter
    def min_node_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "min_node_count", value)


if not MYPY:
    class AiIndexDeployedIndexArgsDict(TypedDict):
        deployed_index_id: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The ID of the DeployedIndex in the above IndexEndpoint.
        """
        index_endpoint: NotRequired[pulumi.Input[str]]
        """
        (Output)
        A resource name of the IndexEndpoint.
        """
elif False:
    AiIndexDeployedIndexArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexDeployedIndexArgs:
    def __init__(__self__, *,
                 deployed_index_id: Optional[pulumi.Input[str]] = None,
                 index_endpoint: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] deployed_index_id: (Output)
               The ID of the DeployedIndex in the above IndexEndpoint.
        :param pulumi.Input[str] index_endpoint: (Output)
               A resource name of the IndexEndpoint.
        """
        if deployed_index_id is not None:
            pulumi.set(__self__, "deployed_index_id", deployed_index_id)
        if index_endpoint is not None:
            pulumi.set(__self__, "index_endpoint", index_endpoint)

    @property
    @pulumi.getter(name="deployedIndexId")
    def deployed_index_id(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The ID of the DeployedIndex in the above IndexEndpoint.
        """
        return pulumi.get(self, "deployed_index_id")

    @deployed_index_id.setter
    def deployed_index_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "deployed_index_id", value)

    @property
    @pulumi.getter(name="indexEndpoint")
    def index_endpoint(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        A resource name of the IndexEndpoint.
        """
        return pulumi.get(self, "index_endpoint")

    @index_endpoint.setter
    def index_endpoint(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "index_endpoint", value)


if not MYPY:
    class AiIndexEndpointPrivateServiceConnectConfigArgsDict(TypedDict):
        enable_private_service_connect: pulumi.Input[bool]
        """
        If set to true, the IndexEndpoint is created without private service access.
        """
        project_allowlists: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
elif False:
    AiIndexEndpointPrivateServiceConnectConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexEndpointPrivateServiceConnectConfigArgs:
    def __init__(__self__, *,
                 enable_private_service_connect: pulumi.Input[bool],
                 project_allowlists: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[bool] enable_private_service_connect: If set to true, the IndexEndpoint is created without private service access.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] project_allowlists: A list of Projects from which the forwarding rule will target the service attachment.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        if project_allowlists is not None:
            pulumi.set(__self__, "project_allowlists", project_allowlists)

    @property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> pulumi.Input[bool]:
        """
        If set to true, the IndexEndpoint is created without private service access.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @enable_private_service_connect.setter
    def enable_private_service_connect(self, value: pulumi.Input[bool]):
        pulumi.set(self, "enable_private_service_connect", value)

    @property
    @pulumi.getter(name="projectAllowlists")
    def project_allowlists(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlists")

    @project_allowlists.setter
    def project_allowlists(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "project_allowlists", value)


if not MYPY:
    class AiIndexIndexStatArgsDict(TypedDict):
        shards_count: NotRequired[pulumi.Input[int]]
        """
        (Output)
        The number of shards in the Index.
        """
        vectors_count: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The number of vectors in the Index.
        """
elif False:
    AiIndexIndexStatArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexIndexStatArgs:
    def __init__(__self__, *,
                 shards_count: Optional[pulumi.Input[int]] = None,
                 vectors_count: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] shards_count: (Output)
               The number of shards in the Index.
        :param pulumi.Input[str] vectors_count: (Output)
               The number of vectors in the Index.
        """
        if shards_count is not None:
            pulumi.set(__self__, "shards_count", shards_count)
        if vectors_count is not None:
            pulumi.set(__self__, "vectors_count", vectors_count)

    @property
    @pulumi.getter(name="shardsCount")
    def shards_count(self) -> Optional[pulumi.Input[int]]:
        """
        (Output)
        The number of shards in the Index.
        """
        return pulumi.get(self, "shards_count")

    @shards_count.setter
    def shards_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "shards_count", value)

    @property
    @pulumi.getter(name="vectorsCount")
    def vectors_count(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The number of vectors in the Index.
        """
        return pulumi.get(self, "vectors_count")

    @vectors_count.setter
    def vectors_count(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "vectors_count", value)


if not MYPY:
    class AiIndexMetadataArgsDict(TypedDict):
        contents_delta_uri: pulumi.Input[str]
        """
        Allows inserting, updating  or deleting the contents of the Matching Engine Index.
        The string must be a valid Cloud Storage directory path. If this
        field is set when calling IndexService.UpdateIndex, then no other
        Index field can be also updated as part of the same call.
        The expected structure and format of the files this URI points to is
        described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
        """
        config: NotRequired[pulumi.Input['AiIndexMetadataConfigArgsDict']]
        """
        The configuration of the Matching Engine Index.
        Structure is documented below.
        """
        is_complete_overwrite: NotRequired[pulumi.Input[bool]]
        """
        If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
        then existing content of the Index will be replaced by the data from the contentsDeltaUri.
        """
elif False:
    AiIndexMetadataArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexMetadataArgs:
    def __init__(__self__, *,
                 contents_delta_uri: pulumi.Input[str],
                 config: Optional[pulumi.Input['AiIndexMetadataConfigArgs']] = None,
                 is_complete_overwrite: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[str] contents_delta_uri: Allows inserting, updating  or deleting the contents of the Matching Engine Index.
               The string must be a valid Cloud Storage directory path. If this
               field is set when calling IndexService.UpdateIndex, then no other
               Index field can be also updated as part of the same call.
               The expected structure and format of the files this URI points to is
               described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
        :param pulumi.Input['AiIndexMetadataConfigArgs'] config: The configuration of the Matching Engine Index.
               Structure is documented below.
        :param pulumi.Input[bool] is_complete_overwrite: If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
               then existing content of the Index will be replaced by the data from the contentsDeltaUri.
        """
        pulumi.set(__self__, "contents_delta_uri", contents_delta_uri)
        if config is not None:
            pulumi.set(__self__, "config", config)
        if is_complete_overwrite is not None:
            pulumi.set(__self__, "is_complete_overwrite", is_complete_overwrite)

    @property
    @pulumi.getter(name="contentsDeltaUri")
    def contents_delta_uri(self) -> pulumi.Input[str]:
        """
        Allows inserting, updating  or deleting the contents of the Matching Engine Index.
        The string must be a valid Cloud Storage directory path. If this
        field is set when calling IndexService.UpdateIndex, then no other
        Index field can be also updated as part of the same call.
        The expected structure and format of the files this URI points to is
        described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
        """
        return pulumi.get(self, "contents_delta_uri")

    @contents_delta_uri.setter
    def contents_delta_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "contents_delta_uri", value)

    @property
    @pulumi.getter
    def config(self) -> Optional[pulumi.Input['AiIndexMetadataConfigArgs']]:
        """
        The configuration of the Matching Engine Index.
        Structure is documented below.
        """
        return pulumi.get(self, "config")

    @config.setter
    def config(self, value: Optional[pulumi.Input['AiIndexMetadataConfigArgs']]):
        pulumi.set(self, "config", value)

    @property
    @pulumi.getter(name="isCompleteOverwrite")
    def is_complete_overwrite(self) -> Optional[pulumi.Input[bool]]:
        """
        If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
        then existing content of the Index will be replaced by the data from the contentsDeltaUri.
        """
        return pulumi.get(self, "is_complete_overwrite")

    @is_complete_overwrite.setter
    def is_complete_overwrite(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "is_complete_overwrite", value)


if not MYPY:
    class AiIndexMetadataConfigArgsDict(TypedDict):
        dimensions: pulumi.Input[int]
        """
        The number of dimensions of the input vectors.
        """
        algorithm_config: NotRequired[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigArgsDict']]
        """
        The configuration with regard to the algorithms used for efficient search.
        Structure is documented below.
        """
        approximate_neighbors_count: NotRequired[pulumi.Input[int]]
        """
        The default number of neighbors to find via approximate search before exact reordering is
        performed. Exact reordering is a procedure where results returned by an
        approximate search algorithm are reordered via a more expensive distance computation.
        Required if tree-AH algorithm is used.
        """
        distance_measure_type: NotRequired[pulumi.Input[str]]
        """
        The distance measure used in nearest neighbor search. The value must be one of the followings:
        * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
        * L1_DISTANCE: Manhattan (L_1) Distance
        * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
        * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
        """
        feature_norm_type: NotRequired[pulumi.Input[str]]
        """
        Type of normalization to be carried out on each vector. The value must be one of the followings:
        * UNIT_L2_NORM: Unit L2 normalization type
        * NONE: No normalization type is specified.
        """
        shard_size: NotRequired[pulumi.Input[str]]
        """
        Index data is split into equal parts to be processed. These are called "shards".
        The shard size must be specified when creating an index. The value must be one of the followings:
        * SHARD_SIZE_SMALL: Small (2GB)
        * SHARD_SIZE_MEDIUM: Medium (20GB)
        * SHARD_SIZE_LARGE: Large (50GB)
        """
elif False:
    AiIndexMetadataConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexMetadataConfigArgs:
    def __init__(__self__, *,
                 dimensions: pulumi.Input[int],
                 algorithm_config: Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigArgs']] = None,
                 approximate_neighbors_count: Optional[pulumi.Input[int]] = None,
                 distance_measure_type: Optional[pulumi.Input[str]] = None,
                 feature_norm_type: Optional[pulumi.Input[str]] = None,
                 shard_size: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] dimensions: The number of dimensions of the input vectors.
        :param pulumi.Input['AiIndexMetadataConfigAlgorithmConfigArgs'] algorithm_config: The configuration with regard to the algorithms used for efficient search.
               Structure is documented below.
        :param pulumi.Input[int] approximate_neighbors_count: The default number of neighbors to find via approximate search before exact reordering is
               performed. Exact reordering is a procedure where results returned by an
               approximate search algorithm are reordered via a more expensive distance computation.
               Required if tree-AH algorithm is used.
        :param pulumi.Input[str] distance_measure_type: The distance measure used in nearest neighbor search. The value must be one of the followings:
               * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
               * L1_DISTANCE: Manhattan (L_1) Distance
               * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
               * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
        :param pulumi.Input[str] feature_norm_type: Type of normalization to be carried out on each vector. The value must be one of the followings:
               * UNIT_L2_NORM: Unit L2 normalization type
               * NONE: No normalization type is specified.
        :param pulumi.Input[str] shard_size: Index data is split into equal parts to be processed. These are called "shards".
               The shard size must be specified when creating an index. The value must be one of the followings:
               * SHARD_SIZE_SMALL: Small (2GB)
               * SHARD_SIZE_MEDIUM: Medium (20GB)
               * SHARD_SIZE_LARGE: Large (50GB)
        """
        pulumi.set(__self__, "dimensions", dimensions)
        if algorithm_config is not None:
            pulumi.set(__self__, "algorithm_config", algorithm_config)
        if approximate_neighbors_count is not None:
            pulumi.set(__self__, "approximate_neighbors_count", approximate_neighbors_count)
        if distance_measure_type is not None:
            pulumi.set(__self__, "distance_measure_type", distance_measure_type)
        if feature_norm_type is not None:
            pulumi.set(__self__, "feature_norm_type", feature_norm_type)
        if shard_size is not None:
            pulumi.set(__self__, "shard_size", shard_size)

    @property
    @pulumi.getter
    def dimensions(self) -> pulumi.Input[int]:
        """
        The number of dimensions of the input vectors.
        """
        return pulumi.get(self, "dimensions")

    @dimensions.setter
    def dimensions(self, value: pulumi.Input[int]):
        pulumi.set(self, "dimensions", value)

    @property
    @pulumi.getter(name="algorithmConfig")
    def algorithm_config(self) -> Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigArgs']]:
        """
        The configuration with regard to the algorithms used for efficient search.
        Structure is documented below.
        """
        return pulumi.get(self, "algorithm_config")

    @algorithm_config.setter
    def algorithm_config(self, value: Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigArgs']]):
        pulumi.set(self, "algorithm_config", value)

    @property
    @pulumi.getter(name="approximateNeighborsCount")
    def approximate_neighbors_count(self) -> Optional[pulumi.Input[int]]:
        """
        The default number of neighbors to find via approximate search before exact reordering is
        performed. Exact reordering is a procedure where results returned by an
        approximate search algorithm are reordered via a more expensive distance computation.
        Required if tree-AH algorithm is used.
        """
        return pulumi.get(self, "approximate_neighbors_count")

    @approximate_neighbors_count.setter
    def approximate_neighbors_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "approximate_neighbors_count", value)

    @property
    @pulumi.getter(name="distanceMeasureType")
    def distance_measure_type(self) -> Optional[pulumi.Input[str]]:
        """
        The distance measure used in nearest neighbor search. The value must be one of the followings:
        * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
        * L1_DISTANCE: Manhattan (L_1) Distance
        * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
        * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
        """
        return pulumi.get(self, "distance_measure_type")

    @distance_measure_type.setter
    def distance_measure_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "distance_measure_type", value)

    @property
    @pulumi.getter(name="featureNormType")
    def feature_norm_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of normalization to be carried out on each vector. The value must be one of the followings:
        * UNIT_L2_NORM: Unit L2 normalization type
        * NONE: No normalization type is specified.
        """
        return pulumi.get(self, "feature_norm_type")

    @feature_norm_type.setter
    def feature_norm_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "feature_norm_type", value)

    @property
    @pulumi.getter(name="shardSize")
    def shard_size(self) -> Optional[pulumi.Input[str]]:
        """
        Index data is split into equal parts to be processed. These are called "shards".
        The shard size must be specified when creating an index. The value must be one of the followings:
        * SHARD_SIZE_SMALL: Small (2GB)
        * SHARD_SIZE_MEDIUM: Medium (20GB)
        * SHARD_SIZE_LARGE: Large (50GB)
        """
        return pulumi.get(self, "shard_size")

    @shard_size.setter
    def shard_size(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "shard_size", value)


if not MYPY:
    class AiIndexMetadataConfigAlgorithmConfigArgsDict(TypedDict):
        brute_force_config: NotRequired[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgsDict']]
        """
        Configuration options for using brute force search, which simply implements the
        standard linear search in the database for each query.
        """
        tree_ah_config: NotRequired[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgsDict']]
        """
        Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
        Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        Structure is documented below.
        """
elif False:
    AiIndexMetadataConfigAlgorithmConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexMetadataConfigAlgorithmConfigArgs:
    def __init__(__self__, *,
                 brute_force_config: Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs']] = None,
                 tree_ah_config: Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs']] = None):
        """
        :param pulumi.Input['AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs'] brute_force_config: Configuration options for using brute force search, which simply implements the
               standard linear search in the database for each query.
        :param pulumi.Input['AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs'] tree_ah_config: Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
               Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
               Structure is documented below.
        """
        if brute_force_config is not None:
            pulumi.set(__self__, "brute_force_config", brute_force_config)
        if tree_ah_config is not None:
            pulumi.set(__self__, "tree_ah_config", tree_ah_config)

    @property
    @pulumi.getter(name="bruteForceConfig")
    def brute_force_config(self) -> Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs']]:
        """
        Configuration options for using brute force search, which simply implements the
        standard linear search in the database for each query.
        """
        return pulumi.get(self, "brute_force_config")

    @brute_force_config.setter
    def brute_force_config(self, value: Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs']]):
        pulumi.set(self, "brute_force_config", value)

    @property
    @pulumi.getter(name="treeAhConfig")
    def tree_ah_config(self) -> Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs']]:
        """
        Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
        Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        Structure is documented below.
        """
        return pulumi.get(self, "tree_ah_config")

    @tree_ah_config.setter
    def tree_ah_config(self, value: Optional[pulumi.Input['AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs']]):
        pulumi.set(self, "tree_ah_config", value)


if not MYPY:
    class AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgsDict(TypedDict):
        pass
elif False:
    AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs:
    def __init__(__self__):
        pass


if not MYPY:
    class AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgsDict(TypedDict):
        leaf_node_embedding_count: NotRequired[pulumi.Input[int]]
        """
        Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        leaf_nodes_to_search_percent: NotRequired[pulumi.Input[int]]
        """
        The default percentage of leaf nodes that any query may be searched. Must be in
        range 1-100, inclusive. The default value is 10 (means 10%) if not set.
        """
elif False:
    AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs:
    def __init__(__self__, *,
                 leaf_node_embedding_count: Optional[pulumi.Input[int]] = None,
                 leaf_nodes_to_search_percent: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] leaf_node_embedding_count: Number of embeddings on each leaf node. The default value is 1000 if not set.
        :param pulumi.Input[int] leaf_nodes_to_search_percent: The default percentage of leaf nodes that any query may be searched. Must be in
               range 1-100, inclusive. The default value is 10 (means 10%) if not set.
        """
        if leaf_node_embedding_count is not None:
            pulumi.set(__self__, "leaf_node_embedding_count", leaf_node_embedding_count)
        if leaf_nodes_to_search_percent is not None:
            pulumi.set(__self__, "leaf_nodes_to_search_percent", leaf_nodes_to_search_percent)

    @property
    @pulumi.getter(name="leafNodeEmbeddingCount")
    def leaf_node_embedding_count(self) -> Optional[pulumi.Input[int]]:
        """
        Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        return pulumi.get(self, "leaf_node_embedding_count")

    @leaf_node_embedding_count.setter
    def leaf_node_embedding_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "leaf_node_embedding_count", value)

    @property
    @pulumi.getter(name="leafNodesToSearchPercent")
    def leaf_nodes_to_search_percent(self) -> Optional[pulumi.Input[int]]:
        """
        The default percentage of leaf nodes that any query may be searched. Must be in
        range 1-100, inclusive. The default value is 10 (means 10%) if not set.
        """
        return pulumi.get(self, "leaf_nodes_to_search_percent")

    @leaf_nodes_to_search_percent.setter
    def leaf_nodes_to_search_percent(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "leaf_nodes_to_search_percent", value)


if not MYPY:
    class AiMetadataStoreEncryptionSpecArgsDict(TypedDict):
        kms_key_name: NotRequired[pulumi.Input[str]]
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
elif False:
    AiMetadataStoreEncryptionSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiMetadataStoreEncryptionSpecArgs:
    def __init__(__self__, *,
                 kms_key_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
               Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        if kms_key_name is not None:
            pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> Optional[pulumi.Input[str]]:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kms_key_name", value)


if not MYPY:
    class AiMetadataStoreStateArgsDict(TypedDict):
        disk_utilization_bytes: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The disk utilization of the MetadataStore in bytes.
        """
elif False:
    AiMetadataStoreStateArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiMetadataStoreStateArgs:
    def __init__(__self__, *,
                 disk_utilization_bytes: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] disk_utilization_bytes: (Output)
               The disk utilization of the MetadataStore in bytes.
        """
        if disk_utilization_bytes is not None:
            pulumi.set(__self__, "disk_utilization_bytes", disk_utilization_bytes)

    @property
    @pulumi.getter(name="diskUtilizationBytes")
    def disk_utilization_bytes(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The disk utilization of the MetadataStore in bytes.
        """
        return pulumi.get(self, "disk_utilization_bytes")

    @disk_utilization_bytes.setter
    def disk_utilization_bytes(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "disk_utilization_bytes", value)


if not MYPY:
    class AiTensorboardEncryptionSpecArgsDict(TypedDict):
        kms_key_name: pulumi.Input[str]
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
elif False:
    AiTensorboardEncryptionSpecArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AiTensorboardEncryptionSpecArgs:
    def __init__(__self__, *,
                 kms_key_name: pulumi.Input[str]):
        """
        :param pulumi.Input[str] kms_key_name: The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
               Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> pulumi.Input[str]:
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "kms_key_name", value)



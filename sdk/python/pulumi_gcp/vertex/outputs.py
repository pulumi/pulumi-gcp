# coding=utf-8
# *** WARNING: this file was generated by pulumi-language-python. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import builtins as _builtins
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from .. import _utilities
from . import outputs

__all__ = [
    'AiDatasetEncryptionSpec',
    'AiDeploymentResourcePoolDedicatedResources',
    'AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec',
    'AiDeploymentResourcePoolDedicatedResourcesMachineSpec',
    'AiEndpointDeployedModel',
    'AiEndpointDeployedModelAutomaticResource',
    'AiEndpointDeployedModelDedicatedResource',
    'AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec',
    'AiEndpointDeployedModelDedicatedResourceMachineSpec',
    'AiEndpointDeployedModelPrivateEndpoint',
    'AiEndpointEncryptionSpec',
    'AiEndpointIamBindingCondition',
    'AiEndpointIamMemberCondition',
    'AiEndpointPredictRequestResponseLoggingConfig',
    'AiEndpointPredictRequestResponseLoggingConfigBigqueryDestination',
    'AiEndpointPrivateServiceConnectConfig',
    'AiEndpointPrivateServiceConnectConfigPscAutomationConfig',
    'AiEndpointWithModelGardenDeploymentDeployConfig',
    'AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResources',
    'AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpec',
    'AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec',
    'AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity',
    'AiEndpointWithModelGardenDeploymentEndpointConfig',
    'AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfig',
    'AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigs',
    'AiEndpointWithModelGardenDeploymentModelConfig',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpec',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPort',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeader',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeader',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPort',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeader',
    'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket',
    'AiFeatureGroupBigQuery',
    'AiFeatureGroupBigQueryBigQuerySource',
    'AiFeatureGroupIamBindingCondition',
    'AiFeatureGroupIamMemberCondition',
    'AiFeatureOnlineStoreBigtable',
    'AiFeatureOnlineStoreBigtableAutoScaling',
    'AiFeatureOnlineStoreDedicatedServingEndpoint',
    'AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig',
    'AiFeatureOnlineStoreEmbeddingManagement',
    'AiFeatureOnlineStoreFeatureviewBigQuerySource',
    'AiFeatureOnlineStoreFeatureviewFeatureRegistrySource',
    'AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup',
    'AiFeatureOnlineStoreFeatureviewIamBindingCondition',
    'AiFeatureOnlineStoreFeatureviewIamMemberCondition',
    'AiFeatureOnlineStoreFeatureviewSyncConfig',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfig',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfig',
    'AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfig',
    'AiFeatureOnlineStoreIamBindingCondition',
    'AiFeatureOnlineStoreIamMemberCondition',
    'AiFeatureOnlineStoreOptimized',
    'AiFeatureStoreEncryptionSpec',
    'AiFeatureStoreEntityTypeIamBindingCondition',
    'AiFeatureStoreEntityTypeIamMemberCondition',
    'AiFeatureStoreEntityTypeMonitoringConfig',
    'AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfig',
    'AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis',
    'AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfig',
    'AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis',
    'AiFeatureStoreIamBindingCondition',
    'AiFeatureStoreIamMemberCondition',
    'AiFeatureStoreOnlineServingConfig',
    'AiFeatureStoreOnlineServingConfigScaling',
    'AiIndexDeployedIndex',
    'AiIndexEncryptionSpec',
    'AiIndexEndpointDeployedIndexAutomaticResources',
    'AiIndexEndpointDeployedIndexDedicatedResources',
    'AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec',
    'AiIndexEndpointDeployedIndexDeployedIndexAuthConfig',
    'AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider',
    'AiIndexEndpointDeployedIndexPrivateEndpoint',
    'AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint',
    'AiIndexEndpointEncryptionSpec',
    'AiIndexEndpointPrivateServiceConnectConfig',
    'AiIndexIndexStat',
    'AiIndexMetadata',
    'AiIndexMetadataConfig',
    'AiIndexMetadataConfigAlgorithmConfig',
    'AiIndexMetadataConfigAlgorithmConfigBruteForceConfig',
    'AiIndexMetadataConfigAlgorithmConfigTreeAhConfig',
    'AiMetadataStoreEncryptionSpec',
    'AiMetadataStoreState',
    'AiRagEngineConfigRagManagedDbConfig',
    'AiRagEngineConfigRagManagedDbConfigBasic',
    'AiRagEngineConfigRagManagedDbConfigScaled',
    'AiRagEngineConfigRagManagedDbConfigUnprovisioned',
    'AiReasoningEngineEncryptionSpec',
    'AiReasoningEngineSpec',
    'AiReasoningEngineSpecDeploymentSpec',
    'AiReasoningEngineSpecDeploymentSpecEnv',
    'AiReasoningEngineSpecDeploymentSpecSecretEnv',
    'AiReasoningEngineSpecDeploymentSpecSecretEnvSecretRef',
    'AiReasoningEngineSpecPackageSpec',
    'AiReasoningEngineSpecSourceCodeSpec',
    'AiReasoningEngineSpecSourceCodeSpecInlineSource',
    'AiReasoningEngineSpecSourceCodeSpecPythonSpec',
    'AiTensorboardEncryptionSpec',
    'GetAiIndexDeployedIndexResult',
    'GetAiIndexEncryptionSpecResult',
    'GetAiIndexIndexStatResult',
    'GetAiIndexMetadataResult',
    'GetAiIndexMetadataConfigResult',
    'GetAiIndexMetadataConfigAlgorithmConfigResult',
    'GetAiIndexMetadataConfigAlgorithmConfigBruteForceConfigResult',
    'GetAiIndexMetadataConfigAlgorithmConfigTreeAhConfigResult',
]

@pulumi.output_type
class AiDatasetEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiDatasetEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiDatasetEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiDatasetEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: Optional[_builtins.str] = None):
        """
        :param _builtins.str kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
               Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        if kms_key_name is not None:
            pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> Optional[_builtins.str]:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class AiDeploymentResourcePoolDedicatedResources(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "machineSpec":
            suggest = "machine_spec"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"
        elif key == "autoscalingMetricSpecs":
            suggest = "autoscaling_metric_specs"
        elif key == "maxReplicaCount":
            suggest = "max_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiDeploymentResourcePoolDedicatedResources. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiDeploymentResourcePoolDedicatedResources.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiDeploymentResourcePoolDedicatedResources.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 machine_spec: 'outputs.AiDeploymentResourcePoolDedicatedResourcesMachineSpec',
                 min_replica_count: _builtins.int,
                 autoscaling_metric_specs: Optional[Sequence['outputs.AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec']] = None,
                 max_replica_count: Optional[_builtins.int] = None):
        """
        :param 'AiDeploymentResourcePoolDedicatedResourcesMachineSpecArgs' machine_spec: The specification of a single machine used by the prediction
               Structure is documented below.
        :param _builtins.int min_replica_count: The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        :param Sequence['AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpecArgs'] autoscaling_metric_specs: A list of the metric specifications that overrides a resource utilization metric.
               Structure is documented below.
        :param _builtins.int max_replica_count: The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "min_replica_count", min_replica_count)
        if autoscaling_metric_specs is not None:
            pulumi.set(__self__, "autoscaling_metric_specs", autoscaling_metric_specs)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)

    @_builtins.property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> 'outputs.AiDeploymentResourcePoolDedicatedResourcesMachineSpec':
        """
        The specification of a single machine used by the prediction
        Structure is documented below.
        """
        return pulumi.get(self, "machine_spec")

    @_builtins.property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> _builtins.int:
        """
        The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        """
        return pulumi.get(self, "min_replica_count")

    @_builtins.property
    @pulumi.getter(name="autoscalingMetricSpecs")
    def autoscaling_metric_specs(self) -> Optional[Sequence['outputs.AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec']]:
        """
        A list of the metric specifications that overrides a resource utilization metric.
        Structure is documented below.
        """
        return pulumi.get(self, "autoscaling_metric_specs")

    @_builtins.property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[_builtins.int]:
        """
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
        return pulumi.get(self, "max_replica_count")


@pulumi.output_type
class AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metricName":
            suggest = "metric_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metric_name: _builtins.str,
                 target: Optional[_builtins.int] = None):
        """
        :param _builtins.str metric_name: The resource metric name. Supported metrics: For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        :param _builtins.int target: The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        pulumi.set(__self__, "metric_name", metric_name)
        if target is not None:
            pulumi.set(__self__, "target", target)

    @_builtins.property
    @pulumi.getter(name="metricName")
    def metric_name(self) -> _builtins.str:
        """
        The resource metric name. Supported metrics: For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        return pulumi.get(self, "metric_name")

    @_builtins.property
    @pulumi.getter
    def target(self) -> Optional[_builtins.int]:
        """
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        return pulumi.get(self, "target")


@pulumi.output_type
class AiDeploymentResourcePoolDedicatedResourcesMachineSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "acceleratorCount":
            suggest = "accelerator_count"
        elif key == "acceleratorType":
            suggest = "accelerator_type"
        elif key == "machineType":
            suggest = "machine_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiDeploymentResourcePoolDedicatedResourcesMachineSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiDeploymentResourcePoolDedicatedResourcesMachineSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiDeploymentResourcePoolDedicatedResourcesMachineSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 accelerator_count: Optional[_builtins.int] = None,
                 accelerator_type: Optional[_builtins.str] = None,
                 machine_type: Optional[_builtins.str] = None):
        """
        :param _builtins.int accelerator_count: The number of accelerators to attach to the machine.
        :param _builtins.str accelerator_type: The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        :param _builtins.str machine_type: The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)

    @_builtins.property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[_builtins.int]:
        """
        The number of accelerators to attach to the machine.
        """
        return pulumi.get(self, "accelerator_count")

    @_builtins.property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[_builtins.str]:
        """
        The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        """
        return pulumi.get(self, "accelerator_type")

    @_builtins.property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[_builtins.str]:
        """
        The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
        """
        return pulumi.get(self, "machine_type")


@pulumi.output_type
class AiEndpointDeployedModel(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "automaticResources":
            suggest = "automatic_resources"
        elif key == "createTime":
            suggest = "create_time"
        elif key == "dedicatedResources":
            suggest = "dedicated_resources"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "enableAccessLogging":
            suggest = "enable_access_logging"
        elif key == "enableContainerLogging":
            suggest = "enable_container_logging"
        elif key == "modelVersionId":
            suggest = "model_version_id"
        elif key == "privateEndpoints":
            suggest = "private_endpoints"
        elif key == "serviceAccount":
            suggest = "service_account"
        elif key == "sharedResources":
            suggest = "shared_resources"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointDeployedModel. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointDeployedModel.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointDeployedModel.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 automatic_resources: Optional[Sequence['outputs.AiEndpointDeployedModelAutomaticResource']] = None,
                 create_time: Optional[_builtins.str] = None,
                 dedicated_resources: Optional[Sequence['outputs.AiEndpointDeployedModelDedicatedResource']] = None,
                 display_name: Optional[_builtins.str] = None,
                 enable_access_logging: Optional[_builtins.bool] = None,
                 enable_container_logging: Optional[_builtins.bool] = None,
                 id: Optional[_builtins.str] = None,
                 model: Optional[_builtins.str] = None,
                 model_version_id: Optional[_builtins.str] = None,
                 private_endpoints: Optional[Sequence['outputs.AiEndpointDeployedModelPrivateEndpoint']] = None,
                 service_account: Optional[_builtins.str] = None,
                 shared_resources: Optional[_builtins.str] = None):
        """
        :param Sequence['AiEndpointDeployedModelAutomaticResourceArgs'] automatic_resources: (Output)
               A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
               Structure is documented below.
        :param _builtins.str create_time: (Output)
               Output only. Timestamp when the DeployedModel was created.
        :param Sequence['AiEndpointDeployedModelDedicatedResourceArgs'] dedicated_resources: (Output)
               A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
               Structure is documented below.
        :param _builtins.str display_name: Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param _builtins.bool enable_access_logging: (Output)
               These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        :param _builtins.bool enable_container_logging: (Output)
               If true, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
        :param _builtins.str id: (Output)
               The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
        :param _builtins.str model: (Output)
               The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
        :param _builtins.str model_version_id: (Output)
               Output only. The version ID of the model that is deployed.
        :param Sequence['AiEndpointDeployedModelPrivateEndpointArgs'] private_endpoints: (Output)
               Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
               Structure is documented below.
        :param _builtins.str service_account: (Output)
               The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        :param _builtins.str shared_resources: (Output)
               The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
        """
        if automatic_resources is not None:
            pulumi.set(__self__, "automatic_resources", automatic_resources)
        if create_time is not None:
            pulumi.set(__self__, "create_time", create_time)
        if dedicated_resources is not None:
            pulumi.set(__self__, "dedicated_resources", dedicated_resources)
        if display_name is not None:
            pulumi.set(__self__, "display_name", display_name)
        if enable_access_logging is not None:
            pulumi.set(__self__, "enable_access_logging", enable_access_logging)
        if enable_container_logging is not None:
            pulumi.set(__self__, "enable_container_logging", enable_container_logging)
        if id is not None:
            pulumi.set(__self__, "id", id)
        if model is not None:
            pulumi.set(__self__, "model", model)
        if model_version_id is not None:
            pulumi.set(__self__, "model_version_id", model_version_id)
        if private_endpoints is not None:
            pulumi.set(__self__, "private_endpoints", private_endpoints)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if shared_resources is not None:
            pulumi.set(__self__, "shared_resources", shared_resources)

    @_builtins.property
    @pulumi.getter(name="automaticResources")
    def automatic_resources(self) -> Optional[Sequence['outputs.AiEndpointDeployedModelAutomaticResource']]:
        """
        (Output)
        A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
        Structure is documented below.
        """
        return pulumi.get(self, "automatic_resources")

    @_builtins.property
    @pulumi.getter(name="createTime")
    def create_time(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. Timestamp when the DeployedModel was created.
        """
        return pulumi.get(self, "create_time")

    @_builtins.property
    @pulumi.getter(name="dedicatedResources")
    def dedicated_resources(self) -> Optional[Sequence['outputs.AiEndpointDeployedModelDedicatedResource']]:
        """
        (Output)
        A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
        Structure is documented below.
        """
        return pulumi.get(self, "dedicated_resources")

    @_builtins.property
    @pulumi.getter(name="displayName")
    def display_name(self) -> Optional[_builtins.str]:
        """
        Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @_builtins.property
    @pulumi.getter(name="enableAccessLogging")
    def enable_access_logging(self) -> Optional[_builtins.bool]:
        """
        (Output)
        These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        """
        return pulumi.get(self, "enable_access_logging")

    @_builtins.property
    @pulumi.getter(name="enableContainerLogging")
    def enable_container_logging(self) -> Optional[_builtins.bool]:
        """
        (Output)
        If true, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
        """
        return pulumi.get(self, "enable_container_logging")

    @_builtins.property
    @pulumi.getter
    def id(self) -> Optional[_builtins.str]:
        """
        (Output)
        The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
        """
        return pulumi.get(self, "id")

    @_builtins.property
    @pulumi.getter
    def model(self) -> Optional[_builtins.str]:
        """
        (Output)
        The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
        """
        return pulumi.get(self, "model")

    @_builtins.property
    @pulumi.getter(name="modelVersionId")
    def model_version_id(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. The version ID of the model that is deployed.
        """
        return pulumi.get(self, "model_version_id")

    @_builtins.property
    @pulumi.getter(name="privateEndpoints")
    def private_endpoints(self) -> Optional[Sequence['outputs.AiEndpointDeployedModelPrivateEndpoint']]:
        """
        (Output)
        Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
        Structure is documented below.
        """
        return pulumi.get(self, "private_endpoints")

    @_builtins.property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[_builtins.str]:
        """
        (Output)
        The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        """
        return pulumi.get(self, "service_account")

    @_builtins.property
    @pulumi.getter(name="sharedResources")
    def shared_resources(self) -> Optional[_builtins.str]:
        """
        (Output)
        The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
        """
        return pulumi.get(self, "shared_resources")


@pulumi.output_type
class AiEndpointDeployedModelAutomaticResource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxReplicaCount":
            suggest = "max_replica_count"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointDeployedModelAutomaticResource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointDeployedModelAutomaticResource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointDeployedModelAutomaticResource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_replica_count: Optional[_builtins.int] = None,
                 min_replica_count: Optional[_builtins.int] = None):
        """
        :param _builtins.int max_replica_count: (Output)
               The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        :param _builtins.int min_replica_count: (Output)
               The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)
        if min_replica_count is not None:
            pulumi.set(__self__, "min_replica_count", min_replica_count)

    @_builtins.property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[_builtins.int]:
        """
        (Output)
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        return pulumi.get(self, "max_replica_count")

    @_builtins.property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> Optional[_builtins.int]:
        """
        (Output)
        The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        return pulumi.get(self, "min_replica_count")


@pulumi.output_type
class AiEndpointDeployedModelDedicatedResource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "autoscalingMetricSpecs":
            suggest = "autoscaling_metric_specs"
        elif key == "machineSpecs":
            suggest = "machine_specs"
        elif key == "maxReplicaCount":
            suggest = "max_replica_count"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointDeployedModelDedicatedResource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointDeployedModelDedicatedResource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointDeployedModelDedicatedResource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 autoscaling_metric_specs: Optional[Sequence['outputs.AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec']] = None,
                 machine_specs: Optional[Sequence['outputs.AiEndpointDeployedModelDedicatedResourceMachineSpec']] = None,
                 max_replica_count: Optional[_builtins.int] = None,
                 min_replica_count: Optional[_builtins.int] = None):
        """
        :param Sequence['AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpecArgs'] autoscaling_metric_specs: (Output)
               The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
               Structure is documented below.
        :param Sequence['AiEndpointDeployedModelDedicatedResourceMachineSpecArgs'] machine_specs: (Output)
               The specification of a single machine used by the prediction.
               Structure is documented below.
        :param _builtins.int max_replica_count: (Output)
               The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        :param _builtins.int min_replica_count: (Output)
               The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        if autoscaling_metric_specs is not None:
            pulumi.set(__self__, "autoscaling_metric_specs", autoscaling_metric_specs)
        if machine_specs is not None:
            pulumi.set(__self__, "machine_specs", machine_specs)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)
        if min_replica_count is not None:
            pulumi.set(__self__, "min_replica_count", min_replica_count)

    @_builtins.property
    @pulumi.getter(name="autoscalingMetricSpecs")
    def autoscaling_metric_specs(self) -> Optional[Sequence['outputs.AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec']]:
        """
        (Output)
        The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
        Structure is documented below.
        """
        return pulumi.get(self, "autoscaling_metric_specs")

    @_builtins.property
    @pulumi.getter(name="machineSpecs")
    def machine_specs(self) -> Optional[Sequence['outputs.AiEndpointDeployedModelDedicatedResourceMachineSpec']]:
        """
        (Output)
        The specification of a single machine used by the prediction.
        Structure is documented below.
        """
        return pulumi.get(self, "machine_specs")

    @_builtins.property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[_builtins.int]:
        """
        (Output)
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        return pulumi.get(self, "max_replica_count")

    @_builtins.property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> Optional[_builtins.int]:
        """
        (Output)
        The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        return pulumi.get(self, "min_replica_count")


@pulumi.output_type
class AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metricName":
            suggest = "metric_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metric_name: Optional[_builtins.str] = None,
                 target: Optional[_builtins.int] = None):
        """
        :param _builtins.str metric_name: (Output)
               The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        :param _builtins.int target: (Output)
               The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        if metric_name is not None:
            pulumi.set(__self__, "metric_name", metric_name)
        if target is not None:
            pulumi.set(__self__, "target", target)

    @_builtins.property
    @pulumi.getter(name="metricName")
    def metric_name(self) -> Optional[_builtins.str]:
        """
        (Output)
        The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        return pulumi.get(self, "metric_name")

    @_builtins.property
    @pulumi.getter
    def target(self) -> Optional[_builtins.int]:
        """
        (Output)
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        return pulumi.get(self, "target")


@pulumi.output_type
class AiEndpointDeployedModelDedicatedResourceMachineSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "acceleratorCount":
            suggest = "accelerator_count"
        elif key == "acceleratorType":
            suggest = "accelerator_type"
        elif key == "machineType":
            suggest = "machine_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointDeployedModelDedicatedResourceMachineSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointDeployedModelDedicatedResourceMachineSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointDeployedModelDedicatedResourceMachineSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 accelerator_count: Optional[_builtins.int] = None,
                 accelerator_type: Optional[_builtins.str] = None,
                 machine_type: Optional[_builtins.str] = None):
        """
        :param _builtins.int accelerator_count: (Output)
               The number of accelerators to attach to the machine.
        :param _builtins.str accelerator_type: (Output)
               The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        :param _builtins.str machine_type: (Output)
               The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO: Try to better unify the required vs optional.
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)

    @_builtins.property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[_builtins.int]:
        """
        (Output)
        The number of accelerators to attach to the machine.
        """
        return pulumi.get(self, "accelerator_count")

    @_builtins.property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[_builtins.str]:
        """
        (Output)
        The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
        """
        return pulumi.get(self, "accelerator_type")

    @_builtins.property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[_builtins.str]:
        """
        (Output)
        The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO: Try to better unify the required vs optional.
        """
        return pulumi.get(self, "machine_type")


@pulumi.output_type
class AiEndpointDeployedModelPrivateEndpoint(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "explainHttpUri":
            suggest = "explain_http_uri"
        elif key == "healthHttpUri":
            suggest = "health_http_uri"
        elif key == "predictHttpUri":
            suggest = "predict_http_uri"
        elif key == "serviceAttachment":
            suggest = "service_attachment"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointDeployedModelPrivateEndpoint. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointDeployedModelPrivateEndpoint.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointDeployedModelPrivateEndpoint.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 explain_http_uri: Optional[_builtins.str] = None,
                 health_http_uri: Optional[_builtins.str] = None,
                 predict_http_uri: Optional[_builtins.str] = None,
                 service_attachment: Optional[_builtins.str] = None):
        """
        :param _builtins.str explain_http_uri: (Output)
               Output only. Http(s) path to send explain requests.
        :param _builtins.str health_http_uri: (Output)
               Output only. Http(s) path to send health check requests.
        :param _builtins.str predict_http_uri: (Output)
               Output only. Http(s) path to send prediction requests.
        :param _builtins.str service_attachment: (Output)
               Output only. The name of the service attachment resource. Populated if private service connect is enabled.
        """
        if explain_http_uri is not None:
            pulumi.set(__self__, "explain_http_uri", explain_http_uri)
        if health_http_uri is not None:
            pulumi.set(__self__, "health_http_uri", health_http_uri)
        if predict_http_uri is not None:
            pulumi.set(__self__, "predict_http_uri", predict_http_uri)
        if service_attachment is not None:
            pulumi.set(__self__, "service_attachment", service_attachment)

    @_builtins.property
    @pulumi.getter(name="explainHttpUri")
    def explain_http_uri(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. Http(s) path to send explain requests.
        """
        return pulumi.get(self, "explain_http_uri")

    @_builtins.property
    @pulumi.getter(name="healthHttpUri")
    def health_http_uri(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. Http(s) path to send health check requests.
        """
        return pulumi.get(self, "health_http_uri")

    @_builtins.property
    @pulumi.getter(name="predictHttpUri")
    def predict_http_uri(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. Http(s) path to send prediction requests.
        """
        return pulumi.get(self, "predict_http_uri")

    @_builtins.property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. The name of the service attachment resource. Populated if private service connect is enabled.
        """
        return pulumi.get(self, "service_attachment")


@pulumi.output_type
class AiEndpointEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: _builtins.str):
        """
        :param _builtins.str kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> _builtins.str:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class AiEndpointIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiEndpointIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiEndpointPredictRequestResponseLoggingConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigqueryDestination":
            suggest = "bigquery_destination"
        elif key == "samplingRate":
            suggest = "sampling_rate"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointPredictRequestResponseLoggingConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointPredictRequestResponseLoggingConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointPredictRequestResponseLoggingConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_destination: Optional['outputs.AiEndpointPredictRequestResponseLoggingConfigBigqueryDestination'] = None,
                 enabled: Optional[_builtins.bool] = None,
                 sampling_rate: Optional[_builtins.float] = None):
        """
        :param 'AiEndpointPredictRequestResponseLoggingConfigBigqueryDestinationArgs' bigquery_destination: BigQuery table for logging. If only given a project, a new dataset will be created with name `logging_<endpoint-display-name>_<endpoint-id>` where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name `request_response_logging`
               Structure is documented below.
        :param _builtins.bool enabled: If logging is enabled or not.
        :param _builtins.float sampling_rate: Percentage of requests to be logged, expressed as a fraction in range(0,1]
        """
        if bigquery_destination is not None:
            pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        if enabled is not None:
            pulumi.set(__self__, "enabled", enabled)
        if sampling_rate is not None:
            pulumi.set(__self__, "sampling_rate", sampling_rate)

    @_builtins.property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> Optional['outputs.AiEndpointPredictRequestResponseLoggingConfigBigqueryDestination']:
        """
        BigQuery table for logging. If only given a project, a new dataset will be created with name `logging_<endpoint-display-name>_<endpoint-id>` where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name `request_response_logging`
        Structure is documented below.
        """
        return pulumi.get(self, "bigquery_destination")

    @_builtins.property
    @pulumi.getter
    def enabled(self) -> Optional[_builtins.bool]:
        """
        If logging is enabled or not.
        """
        return pulumi.get(self, "enabled")

    @_builtins.property
    @pulumi.getter(name="samplingRate")
    def sampling_rate(self) -> Optional[_builtins.float]:
        """
        Percentage of requests to be logged, expressed as a fraction in range(0,1]
        """
        return pulumi.get(self, "sampling_rate")


@pulumi.output_type
class AiEndpointPredictRequestResponseLoggingConfigBigqueryDestination(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "outputUri":
            suggest = "output_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointPredictRequestResponseLoggingConfigBigqueryDestination. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointPredictRequestResponseLoggingConfigBigqueryDestination.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointPredictRequestResponseLoggingConfigBigqueryDestination.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 output_uri: Optional[_builtins.str] = None):
        """
        :param _builtins.str output_uri: BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: - BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
        """
        if output_uri is not None:
            pulumi.set(__self__, "output_uri", output_uri)

    @_builtins.property
    @pulumi.getter(name="outputUri")
    def output_uri(self) -> Optional[_builtins.str]:
        """
        BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: - BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
        """
        return pulumi.get(self, "output_uri")


@pulumi.output_type
class AiEndpointPrivateServiceConnectConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enablePrivateServiceConnect":
            suggest = "enable_private_service_connect"
        elif key == "enableSecurePrivateServiceConnect":
            suggest = "enable_secure_private_service_connect"
        elif key == "projectAllowlists":
            suggest = "project_allowlists"
        elif key == "pscAutomationConfigs":
            suggest = "psc_automation_configs"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointPrivateServiceConnectConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointPrivateServiceConnectConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointPrivateServiceConnectConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_private_service_connect: _builtins.bool,
                 enable_secure_private_service_connect: Optional[_builtins.bool] = None,
                 project_allowlists: Optional[Sequence[_builtins.str]] = None,
                 psc_automation_configs: Optional[Sequence['outputs.AiEndpointPrivateServiceConnectConfigPscAutomationConfig']] = None):
        """
        :param _builtins.bool enable_private_service_connect: Required. If true, expose the IndexEndpoint via private service connect.
        :param _builtins.bool enable_secure_private_service_connect: If set to true, enable secure private service connect with IAM authorization. Otherwise, private service connect will be done without authorization. Note latency will be slightly increased if authorization is enabled.
        :param Sequence[_builtins.str] project_allowlists: A list of Projects from which the forwarding rule will target the service attachment.
        :param Sequence['AiEndpointPrivateServiceConnectConfigPscAutomationConfigArgs'] psc_automation_configs: List of projects and networks where the PSC endpoints will be created. This field is used by Online Inference(Prediction) only.
               Structure is documented below.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        if enable_secure_private_service_connect is not None:
            pulumi.set(__self__, "enable_secure_private_service_connect", enable_secure_private_service_connect)
        if project_allowlists is not None:
            pulumi.set(__self__, "project_allowlists", project_allowlists)
        if psc_automation_configs is not None:
            pulumi.set(__self__, "psc_automation_configs", psc_automation_configs)

    @_builtins.property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> _builtins.bool:
        """
        Required. If true, expose the IndexEndpoint via private service connect.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @_builtins.property
    @pulumi.getter(name="enableSecurePrivateServiceConnect")
    def enable_secure_private_service_connect(self) -> Optional[_builtins.bool]:
        """
        If set to true, enable secure private service connect with IAM authorization. Otherwise, private service connect will be done without authorization. Note latency will be slightly increased if authorization is enabled.
        """
        return pulumi.get(self, "enable_secure_private_service_connect")

    @_builtins.property
    @pulumi.getter(name="projectAllowlists")
    def project_allowlists(self) -> Optional[Sequence[_builtins.str]]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlists")

    @_builtins.property
    @pulumi.getter(name="pscAutomationConfigs")
    def psc_automation_configs(self) -> Optional[Sequence['outputs.AiEndpointPrivateServiceConnectConfigPscAutomationConfig']]:
        """
        List of projects and networks where the PSC endpoints will be created. This field is used by Online Inference(Prediction) only.
        Structure is documented below.
        """
        return pulumi.get(self, "psc_automation_configs")


@pulumi.output_type
class AiEndpointPrivateServiceConnectConfigPscAutomationConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "projectId":
            suggest = "project_id"
        elif key == "errorMessage":
            suggest = "error_message"
        elif key == "forwardingRule":
            suggest = "forwarding_rule"
        elif key == "ipAddress":
            suggest = "ip_address"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointPrivateServiceConnectConfigPscAutomationConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointPrivateServiceConnectConfigPscAutomationConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointPrivateServiceConnectConfigPscAutomationConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 network: _builtins.str,
                 project_id: _builtins.str,
                 error_message: Optional[_builtins.str] = None,
                 forwarding_rule: Optional[_builtins.str] = None,
                 ip_address: Optional[_builtins.str] = None,
                 state: Optional[_builtins.str] = None):
        """
        :param _builtins.str network: The full name of the Google Compute Engine [network](https://cloud.google.com/compute/docs/networks-and-firewalls#networks). [Format](https://cloud.google.com/compute/docs/reference/rest/v1/networks/get): projects/{project}/global/networks/{network}.
        :param _builtins.str project_id: Project id used to create forwarding rule.
        :param _builtins.str error_message: (Output)
               Error message if the PSC service automation failed.
        :param _builtins.str forwarding_rule: (Output)
               Forwarding rule created by the PSC service automation.
        :param _builtins.str ip_address: (Output)
               IP address rule created by the PSC service automation.
        :param _builtins.str state: (Output)
               The state of the PSC service automation.
        """
        pulumi.set(__self__, "network", network)
        pulumi.set(__self__, "project_id", project_id)
        if error_message is not None:
            pulumi.set(__self__, "error_message", error_message)
        if forwarding_rule is not None:
            pulumi.set(__self__, "forwarding_rule", forwarding_rule)
        if ip_address is not None:
            pulumi.set(__self__, "ip_address", ip_address)
        if state is not None:
            pulumi.set(__self__, "state", state)

    @_builtins.property
    @pulumi.getter
    def network(self) -> _builtins.str:
        """
        The full name of the Google Compute Engine [network](https://cloud.google.com/compute/docs/networks-and-firewalls#networks). [Format](https://cloud.google.com/compute/docs/reference/rest/v1/networks/get): projects/{project}/global/networks/{network}.
        """
        return pulumi.get(self, "network")

    @_builtins.property
    @pulumi.getter(name="projectId")
    def project_id(self) -> _builtins.str:
        """
        Project id used to create forwarding rule.
        """
        return pulumi.get(self, "project_id")

    @_builtins.property
    @pulumi.getter(name="errorMessage")
    def error_message(self) -> Optional[_builtins.str]:
        """
        (Output)
        Error message if the PSC service automation failed.
        """
        return pulumi.get(self, "error_message")

    @_builtins.property
    @pulumi.getter(name="forwardingRule")
    def forwarding_rule(self) -> Optional[_builtins.str]:
        """
        (Output)
        Forwarding rule created by the PSC service automation.
        """
        return pulumi.get(self, "forwarding_rule")

    @_builtins.property
    @pulumi.getter(name="ipAddress")
    def ip_address(self) -> Optional[_builtins.str]:
        """
        (Output)
        IP address rule created by the PSC service automation.
        """
        return pulumi.get(self, "ip_address")

    @_builtins.property
    @pulumi.getter
    def state(self) -> Optional[_builtins.str]:
        """
        (Output)
        The state of the PSC service automation.
        """
        return pulumi.get(self, "state")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentDeployConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "dedicatedResources":
            suggest = "dedicated_resources"
        elif key == "fastTryoutEnabled":
            suggest = "fast_tryout_enabled"
        elif key == "systemLabels":
            suggest = "system_labels"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentDeployConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dedicated_resources: Optional['outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResources'] = None,
                 fast_tryout_enabled: Optional[_builtins.bool] = None,
                 system_labels: Optional[Mapping[str, _builtins.str]] = None):
        """
        :param 'AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesArgs' dedicated_resources: A description of resources that are dedicated to a DeployedModel or
               DeployedIndex, and that need a higher degree of manual configuration.
               Structure is documented below.
        :param _builtins.bool fast_tryout_enabled: If true, enable the QMT fast tryout feature for this model if possible.
        :param Mapping[str, _builtins.str] system_labels: System labels for Model Garden deployments.
               These labels are managed by Google and for tracking purposes only.
        """
        if dedicated_resources is not None:
            pulumi.set(__self__, "dedicated_resources", dedicated_resources)
        if fast_tryout_enabled is not None:
            pulumi.set(__self__, "fast_tryout_enabled", fast_tryout_enabled)
        if system_labels is not None:
            pulumi.set(__self__, "system_labels", system_labels)

    @_builtins.property
    @pulumi.getter(name="dedicatedResources")
    def dedicated_resources(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResources']:
        """
        A description of resources that are dedicated to a DeployedModel or
        DeployedIndex, and that need a higher degree of manual configuration.
        Structure is documented below.
        """
        return pulumi.get(self, "dedicated_resources")

    @_builtins.property
    @pulumi.getter(name="fastTryoutEnabled")
    def fast_tryout_enabled(self) -> Optional[_builtins.bool]:
        """
        If true, enable the QMT fast tryout feature for this model if possible.
        """
        return pulumi.get(self, "fast_tryout_enabled")

    @_builtins.property
    @pulumi.getter(name="systemLabels")
    def system_labels(self) -> Optional[Mapping[str, _builtins.str]]:
        """
        System labels for Model Garden deployments.
        These labels are managed by Google and for tracking purposes only.
        """
        return pulumi.get(self, "system_labels")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResources(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "machineSpec":
            suggest = "machine_spec"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"
        elif key == "autoscalingMetricSpecs":
            suggest = "autoscaling_metric_specs"
        elif key == "maxReplicaCount":
            suggest = "max_replica_count"
        elif key == "requiredReplicaCount":
            suggest = "required_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResources. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResources.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResources.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 machine_spec: 'outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec',
                 min_replica_count: _builtins.int,
                 autoscaling_metric_specs: Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpec']] = None,
                 max_replica_count: Optional[_builtins.int] = None,
                 required_replica_count: Optional[_builtins.int] = None,
                 spot: Optional[_builtins.bool] = None):
        """
        :param 'AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecArgs' machine_spec: Specification of a single machine.
               Structure is documented below.
        :param _builtins.int min_replica_count: The minimum number of machine replicas that will be always deployed on.
               This value must be greater than or equal to 1.
               If traffic increases, it may dynamically be deployed onto more replicas,
               and as traffic decreases, some of these extra replicas may be freed.
        :param Sequence['AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpecArgs'] autoscaling_metric_specs: The metric specifications that overrides a resource
               utilization metric (CPU utilization, accelerator's duty cycle, and so on)
               target value (default to 60 if not set). At most one entry is allowed per
               metric.
               If machine_spec.accelerator_count is
               above 0, the autoscaling will be based on both CPU utilization and
               accelerator's duty cycle metrics and scale up when either metrics exceeds
               its target value while scale down if both metrics are under their target
               value. The default target value is 60 for both metrics.
               If machine_spec.accelerator_count is
               0, the autoscaling will be based on CPU utilization metric only with
               default target value 60 if not explicitly set.
               For example, in the case of Online Prediction, if you want to override
               target CPU utilization to 80, you should set
               autoscaling_metric_specs.metric_name
               to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and
               autoscaling_metric_specs.target to `80`.
               Structure is documented below.
        :param _builtins.int max_replica_count: The maximum number of replicas that may be deployed on when the traffic
               against it increases. If the requested value is too large, the deployment
               will error, but if deployment succeeds then the ability to scale to that
               many replicas is guaranteed (barring service outages). If traffic increases
               beyond what its replicas at maximum may handle, a portion of the traffic
               will be dropped. If this value is not provided, will use
               min_replica_count as the default value.
               The value of this field impacts the charge against Vertex CPU and GPU
               quotas. Specifically, you will be charged for (max_replica_count *
               number of cores in the selected machine type) and (max_replica_count *
               number of GPUs per replica in the selected machine type).
        :param _builtins.int required_replica_count: Number of required available replicas for the deployment to succeed.
               This field is only needed when partial deployment/mutation is
               desired. If set, the deploy/mutate operation will succeed once
               available_replica_count reaches required_replica_count, and the rest of
               the replicas will be retried. If not set, the default
               required_replica_count will be min_replica_count.
        :param _builtins.bool spot: If true, schedule the deployment workload on [spot
               VMs](https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms).
        """
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "min_replica_count", min_replica_count)
        if autoscaling_metric_specs is not None:
            pulumi.set(__self__, "autoscaling_metric_specs", autoscaling_metric_specs)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)
        if required_replica_count is not None:
            pulumi.set(__self__, "required_replica_count", required_replica_count)
        if spot is not None:
            pulumi.set(__self__, "spot", spot)

    @_builtins.property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> 'outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec':
        """
        Specification of a single machine.
        Structure is documented below.
        """
        return pulumi.get(self, "machine_spec")

    @_builtins.property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> _builtins.int:
        """
        The minimum number of machine replicas that will be always deployed on.
        This value must be greater than or equal to 1.
        If traffic increases, it may dynamically be deployed onto more replicas,
        and as traffic decreases, some of these extra replicas may be freed.
        """
        return pulumi.get(self, "min_replica_count")

    @_builtins.property
    @pulumi.getter(name="autoscalingMetricSpecs")
    def autoscaling_metric_specs(self) -> Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpec']]:
        """
        The metric specifications that overrides a resource
        utilization metric (CPU utilization, accelerator's duty cycle, and so on)
        target value (default to 60 if not set). At most one entry is allowed per
        metric.
        If machine_spec.accelerator_count is
        above 0, the autoscaling will be based on both CPU utilization and
        accelerator's duty cycle metrics and scale up when either metrics exceeds
        its target value while scale down if both metrics are under their target
        value. The default target value is 60 for both metrics.
        If machine_spec.accelerator_count is
        0, the autoscaling will be based on CPU utilization metric only with
        default target value 60 if not explicitly set.
        For example, in the case of Online Prediction, if you want to override
        target CPU utilization to 80, you should set
        autoscaling_metric_specs.metric_name
        to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and
        autoscaling_metric_specs.target to `80`.
        Structure is documented below.
        """
        return pulumi.get(self, "autoscaling_metric_specs")

    @_builtins.property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[_builtins.int]:
        """
        The maximum number of replicas that may be deployed on when the traffic
        against it increases. If the requested value is too large, the deployment
        will error, but if deployment succeeds then the ability to scale to that
        many replicas is guaranteed (barring service outages). If traffic increases
        beyond what its replicas at maximum may handle, a portion of the traffic
        will be dropped. If this value is not provided, will use
        min_replica_count as the default value.
        The value of this field impacts the charge against Vertex CPU and GPU
        quotas. Specifically, you will be charged for (max_replica_count *
        number of cores in the selected machine type) and (max_replica_count *
        number of GPUs per replica in the selected machine type).
        """
        return pulumi.get(self, "max_replica_count")

    @_builtins.property
    @pulumi.getter(name="requiredReplicaCount")
    def required_replica_count(self) -> Optional[_builtins.int]:
        """
        Number of required available replicas for the deployment to succeed.
        This field is only needed when partial deployment/mutation is
        desired. If set, the deploy/mutate operation will succeed once
        available_replica_count reaches required_replica_count, and the rest of
        the replicas will be retried. If not set, the default
        required_replica_count will be min_replica_count.
        """
        return pulumi.get(self, "required_replica_count")

    @_builtins.property
    @pulumi.getter
    def spot(self) -> Optional[_builtins.bool]:
        """
        If true, schedule the deployment workload on [spot
        VMs](https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms).
        """
        return pulumi.get(self, "spot")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metricName":
            suggest = "metric_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesAutoscalingMetricSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metric_name: _builtins.str,
                 target: Optional[_builtins.int] = None):
        """
        :param _builtins.str metric_name: The resource metric name.
               Supported metrics:
               * For Online Prediction:
               * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle`
               * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        :param _builtins.int target: The target resource utilization in percentage (1% - 100%) for the given
               metric; once the real usage deviates from the target by a certain
               percentage, the machine replicas change. The default value is 60
               (representing 60%) if not provided.
        """
        pulumi.set(__self__, "metric_name", metric_name)
        if target is not None:
            pulumi.set(__self__, "target", target)

    @_builtins.property
    @pulumi.getter(name="metricName")
    def metric_name(self) -> _builtins.str:
        """
        The resource metric name.
        Supported metrics:
        * For Online Prediction:
        * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle`
        * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        return pulumi.get(self, "metric_name")

    @_builtins.property
    @pulumi.getter
    def target(self) -> Optional[_builtins.int]:
        """
        The target resource utilization in percentage (1% - 100%) for the given
        metric; once the real usage deviates from the target by a certain
        percentage, the machine replicas change. The default value is 60
        (representing 60%) if not provided.
        """
        return pulumi.get(self, "target")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "acceleratorCount":
            suggest = "accelerator_count"
        elif key == "acceleratorType":
            suggest = "accelerator_type"
        elif key == "machineType":
            suggest = "machine_type"
        elif key == "multihostGpuNodeCount":
            suggest = "multihost_gpu_node_count"
        elif key == "reservationAffinity":
            suggest = "reservation_affinity"
        elif key == "tpuTopology":
            suggest = "tpu_topology"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 accelerator_count: Optional[_builtins.int] = None,
                 accelerator_type: Optional[_builtins.str] = None,
                 machine_type: Optional[_builtins.str] = None,
                 multihost_gpu_node_count: Optional[_builtins.int] = None,
                 reservation_affinity: Optional['outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity'] = None,
                 tpu_topology: Optional[_builtins.str] = None):
        """
        :param _builtins.int accelerator_count: The number of accelerators to attach to the machine.
        :param _builtins.str accelerator_type: Possible values:
               ACCELERATOR_TYPE_UNSPECIFIED
               NVIDIA_TESLA_K80
               NVIDIA_TESLA_P100
               NVIDIA_TESLA_V100
               NVIDIA_TESLA_P4
               NVIDIA_TESLA_T4
               NVIDIA_TESLA_A100
               NVIDIA_A100_80GB
               NVIDIA_L4
               NVIDIA_H100_80GB
               NVIDIA_H100_MEGA_80GB
               NVIDIA_H200_141GB
               NVIDIA_B200
               TPU_V2
               TPU_V3
               TPU_V4_POD
               TPU_V5_LITEPOD
        :param _builtins.str machine_type: The type of the machine.
               See the [list of machine types supported for
               prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
               See the [list of machine types supported for custom
               training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
               For DeployedModel this field is optional, and the default
               value is `n1-standard-2`. For BatchPredictionJob or as part of
               WorkerPoolSpec this field is required.
        :param _builtins.int multihost_gpu_node_count: The number of nodes per replica for multihost GPU deployments.
        :param 'AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityArgs' reservation_affinity: A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
               DeployedModel) to draw its Compute Engine resources from a Shared
               Reservation, or exclusively from on-demand capacity.
               Structure is documented below.
        :param _builtins.str tpu_topology: The topology of the TPUs. Corresponds to the TPU topologies available from
               GKE. (Example: tpu_topology: "2x2x1").
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if multihost_gpu_node_count is not None:
            pulumi.set(__self__, "multihost_gpu_node_count", multihost_gpu_node_count)
        if reservation_affinity is not None:
            pulumi.set(__self__, "reservation_affinity", reservation_affinity)
        if tpu_topology is not None:
            pulumi.set(__self__, "tpu_topology", tpu_topology)

    @_builtins.property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[_builtins.int]:
        """
        The number of accelerators to attach to the machine.
        """
        return pulumi.get(self, "accelerator_count")

    @_builtins.property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[_builtins.str]:
        """
        Possible values:
        ACCELERATOR_TYPE_UNSPECIFIED
        NVIDIA_TESLA_K80
        NVIDIA_TESLA_P100
        NVIDIA_TESLA_V100
        NVIDIA_TESLA_P4
        NVIDIA_TESLA_T4
        NVIDIA_TESLA_A100
        NVIDIA_A100_80GB
        NVIDIA_L4
        NVIDIA_H100_80GB
        NVIDIA_H100_MEGA_80GB
        NVIDIA_H200_141GB
        NVIDIA_B200
        TPU_V2
        TPU_V3
        TPU_V4_POD
        TPU_V5_LITEPOD
        """
        return pulumi.get(self, "accelerator_type")

    @_builtins.property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[_builtins.str]:
        """
        The type of the machine.
        See the [list of machine types supported for
        prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
        See the [list of machine types supported for custom
        training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
        For DeployedModel this field is optional, and the default
        value is `n1-standard-2`. For BatchPredictionJob or as part of
        WorkerPoolSpec this field is required.
        """
        return pulumi.get(self, "machine_type")

    @_builtins.property
    @pulumi.getter(name="multihostGpuNodeCount")
    def multihost_gpu_node_count(self) -> Optional[_builtins.int]:
        """
        The number of nodes per replica for multihost GPU deployments.
        """
        return pulumi.get(self, "multihost_gpu_node_count")

    @_builtins.property
    @pulumi.getter(name="reservationAffinity")
    def reservation_affinity(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity']:
        """
        A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
        DeployedModel) to draw its Compute Engine resources from a Shared
        Reservation, or exclusively from on-demand capacity.
        Structure is documented below.
        """
        return pulumi.get(self, "reservation_affinity")

    @_builtins.property
    @pulumi.getter(name="tpuTopology")
    def tpu_topology(self) -> Optional[_builtins.str]:
        """
        The topology of the TPUs. Corresponds to the TPU topologies available from
        GKE. (Example: tpu_topology: "2x2x1").
        """
        return pulumi.get(self, "tpu_topology")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "reservationAffinityType":
            suggest = "reservation_affinity_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 reservation_affinity_type: _builtins.str,
                 key: Optional[_builtins.str] = None,
                 values: Optional[Sequence[_builtins.str]] = None):
        """
        :param _builtins.str reservation_affinity_type: Specifies the reservation affinity type.
               Possible values:
               TYPE_UNSPECIFIED
               NO_RESERVATION
               ANY_RESERVATION
               SPECIFIC_RESERVATION
        :param _builtins.str key: Corresponds to the label key of a reservation resource. To target a
               SPECIFIC_RESERVATION by name, use `compute.googleapis.com/reservation-name`
               as the key and specify the name of your reservation as its value.
        :param Sequence[_builtins.str] values: Corresponds to the label values of a reservation resource. This must be the
               full resource name of the reservation or reservation block.
        """
        pulumi.set(__self__, "reservation_affinity_type", reservation_affinity_type)
        if key is not None:
            pulumi.set(__self__, "key", key)
        if values is not None:
            pulumi.set(__self__, "values", values)

    @_builtins.property
    @pulumi.getter(name="reservationAffinityType")
    def reservation_affinity_type(self) -> _builtins.str:
        """
        Specifies the reservation affinity type.
        Possible values:
        TYPE_UNSPECIFIED
        NO_RESERVATION
        ANY_RESERVATION
        SPECIFIC_RESERVATION
        """
        return pulumi.get(self, "reservation_affinity_type")

    @_builtins.property
    @pulumi.getter
    def key(self) -> Optional[_builtins.str]:
        """
        Corresponds to the label key of a reservation resource. To target a
        SPECIFIC_RESERVATION by name, use `compute.googleapis.com/reservation-name`
        as the key and specify the name of your reservation as its value.
        """
        return pulumi.get(self, "key")

    @_builtins.property
    @pulumi.getter
    def values(self) -> Optional[Sequence[_builtins.str]]:
        """
        Corresponds to the label values of a reservation resource. This must be the
        full resource name of the reservation or reservation block.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentEndpointConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "dedicatedEndpointEnabled":
            suggest = "dedicated_endpoint_enabled"
        elif key == "endpointDisplayName":
            suggest = "endpoint_display_name"
        elif key == "privateServiceConnectConfig":
            suggest = "private_service_connect_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentEndpointConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentEndpointConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentEndpointConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dedicated_endpoint_enabled: Optional[_builtins.bool] = None,
                 endpoint_display_name: Optional[_builtins.str] = None,
                 private_service_connect_config: Optional['outputs.AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfig'] = None):
        """
        :param _builtins.bool dedicated_endpoint_enabled: If true, the endpoint will be exposed through a dedicated
               DNS [Endpoint.dedicated_endpoint_dns]. Your request to the dedicated DNS
               will be isolated from other users' traffic and will have better
               performance and reliability. Note: Once you enabled dedicated endpoint,
               you won't be able to send request to the shared DNS
               {region}-aiplatform.googleapis.com. The limitations will be removed soon.
        :param _builtins.str endpoint_display_name: The user-specified display name of the endpoint. If not set, a
               default name will be used.
        :param 'AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigArgs' private_service_connect_config: The configuration for Private Service Connect (PSC).
               Structure is documented below.
        """
        if dedicated_endpoint_enabled is not None:
            pulumi.set(__self__, "dedicated_endpoint_enabled", dedicated_endpoint_enabled)
        if endpoint_display_name is not None:
            pulumi.set(__self__, "endpoint_display_name", endpoint_display_name)
        if private_service_connect_config is not None:
            pulumi.set(__self__, "private_service_connect_config", private_service_connect_config)

    @_builtins.property
    @pulumi.getter(name="dedicatedEndpointEnabled")
    def dedicated_endpoint_enabled(self) -> Optional[_builtins.bool]:
        """
        If true, the endpoint will be exposed through a dedicated
        DNS [Endpoint.dedicated_endpoint_dns]. Your request to the dedicated DNS
        will be isolated from other users' traffic and will have better
        performance and reliability. Note: Once you enabled dedicated endpoint,
        you won't be able to send request to the shared DNS
        {region}-aiplatform.googleapis.com. The limitations will be removed soon.
        """
        return pulumi.get(self, "dedicated_endpoint_enabled")

    @_builtins.property
    @pulumi.getter(name="endpointDisplayName")
    def endpoint_display_name(self) -> Optional[_builtins.str]:
        """
        The user-specified display name of the endpoint. If not set, a
        default name will be used.
        """
        return pulumi.get(self, "endpoint_display_name")

    @_builtins.property
    @pulumi.getter(name="privateServiceConnectConfig")
    def private_service_connect_config(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfig']:
        """
        The configuration for Private Service Connect (PSC).
        Structure is documented below.
        """
        return pulumi.get(self, "private_service_connect_config")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enablePrivateServiceConnect":
            suggest = "enable_private_service_connect"
        elif key == "projectAllowlists":
            suggest = "project_allowlists"
        elif key == "pscAutomationConfigs":
            suggest = "psc_automation_configs"
        elif key == "serviceAttachment":
            suggest = "service_attachment"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_private_service_connect: _builtins.bool,
                 project_allowlists: Optional[Sequence[_builtins.str]] = None,
                 psc_automation_configs: Optional['outputs.AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigs'] = None,
                 service_attachment: Optional[_builtins.str] = None):
        """
        :param _builtins.bool enable_private_service_connect: Required. If true, expose the IndexEndpoint via private service connect.
        :param Sequence[_builtins.str] project_allowlists: A list of Projects from which the forwarding rule will target the service attachment.
        :param 'AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigsArgs' psc_automation_configs: PSC config that is used to automatically create PSC endpoints in the user projects.
               Structure is documented below.
        :param _builtins.str service_attachment: (Output)
               Output only. The name of the generated service attachment resource.
               This is only populated if the endpoint is deployed with PrivateServiceConnect.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        if project_allowlists is not None:
            pulumi.set(__self__, "project_allowlists", project_allowlists)
        if psc_automation_configs is not None:
            pulumi.set(__self__, "psc_automation_configs", psc_automation_configs)
        if service_attachment is not None:
            pulumi.set(__self__, "service_attachment", service_attachment)

    @_builtins.property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> _builtins.bool:
        """
        Required. If true, expose the IndexEndpoint via private service connect.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @_builtins.property
    @pulumi.getter(name="projectAllowlists")
    def project_allowlists(self) -> Optional[Sequence[_builtins.str]]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlists")

    @_builtins.property
    @pulumi.getter(name="pscAutomationConfigs")
    def psc_automation_configs(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigs']:
        """
        PSC config that is used to automatically create PSC endpoints in the user projects.
        Structure is documented below.
        """
        return pulumi.get(self, "psc_automation_configs")

    @_builtins.property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. The name of the generated service attachment resource.
        This is only populated if the endpoint is deployed with PrivateServiceConnect.
        """
        return pulumi.get(self, "service_attachment")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigs(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "projectId":
            suggest = "project_id"
        elif key == "errorMessage":
            suggest = "error_message"
        elif key == "forwardingRule":
            suggest = "forwarding_rule"
        elif key == "ipAddress":
            suggest = "ip_address"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigs. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigs.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentEndpointConfigPrivateServiceConnectConfigPscAutomationConfigs.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 network: _builtins.str,
                 project_id: _builtins.str,
                 error_message: Optional[_builtins.str] = None,
                 forwarding_rule: Optional[_builtins.str] = None,
                 ip_address: Optional[_builtins.str] = None,
                 state: Optional[_builtins.str] = None):
        """
        :param _builtins.str network: Required. The full name of the Google Compute Engine network.
               Format: projects/{project}/global/networks/{network}.
        :param _builtins.str project_id: Required. Project id used to create forwarding rule.
        :param _builtins.str error_message: (Output)
               Output only. Error message if the PSC service automation failed.
        :param _builtins.str forwarding_rule: (Output)
               Output only. Forwarding rule created by the PSC service automation.
        :param _builtins.str ip_address: (Output)
               Output only. IP address rule created by the PSC service automation.
        :param _builtins.str state: (Output)
               Output only. The state of the PSC service automation.
        """
        pulumi.set(__self__, "network", network)
        pulumi.set(__self__, "project_id", project_id)
        if error_message is not None:
            pulumi.set(__self__, "error_message", error_message)
        if forwarding_rule is not None:
            pulumi.set(__self__, "forwarding_rule", forwarding_rule)
        if ip_address is not None:
            pulumi.set(__self__, "ip_address", ip_address)
        if state is not None:
            pulumi.set(__self__, "state", state)

    @_builtins.property
    @pulumi.getter
    def network(self) -> _builtins.str:
        """
        Required. The full name of the Google Compute Engine network.
        Format: projects/{project}/global/networks/{network}.
        """
        return pulumi.get(self, "network")

    @_builtins.property
    @pulumi.getter(name="projectId")
    def project_id(self) -> _builtins.str:
        """
        Required. Project id used to create forwarding rule.
        """
        return pulumi.get(self, "project_id")

    @_builtins.property
    @pulumi.getter(name="errorMessage")
    def error_message(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. Error message if the PSC service automation failed.
        """
        return pulumi.get(self, "error_message")

    @_builtins.property
    @pulumi.getter(name="forwardingRule")
    def forwarding_rule(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. Forwarding rule created by the PSC service automation.
        """
        return pulumi.get(self, "forwarding_rule")

    @_builtins.property
    @pulumi.getter(name="ipAddress")
    def ip_address(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. IP address rule created by the PSC service automation.
        """
        return pulumi.get(self, "ip_address")

    @_builtins.property
    @pulumi.getter
    def state(self) -> Optional[_builtins.str]:
        """
        (Output)
        Output only. The state of the PSC service automation.
        """
        return pulumi.get(self, "state")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "acceptEula":
            suggest = "accept_eula"
        elif key == "containerSpec":
            suggest = "container_spec"
        elif key == "huggingFaceAccessToken":
            suggest = "hugging_face_access_token"
        elif key == "huggingFaceCacheEnabled":
            suggest = "hugging_face_cache_enabled"
        elif key == "modelDisplayName":
            suggest = "model_display_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 accept_eula: Optional[_builtins.bool] = None,
                 container_spec: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpec'] = None,
                 hugging_face_access_token: Optional[_builtins.str] = None,
                 hugging_face_cache_enabled: Optional[_builtins.bool] = None,
                 model_display_name: Optional[_builtins.str] = None):
        """
        :param _builtins.bool accept_eula: Whether the user accepts the End User License Agreement (EULA)
               for the model.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecArgs' container_spec: Specification of a container for serving predictions. Some fields in this
               message correspond to fields in the [Kubernetes Container v1 core
               specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
               Structure is documented below.
        :param _builtins.str hugging_face_access_token: The Hugging Face read access token used to access the model
               artifacts of gated models.
        :param _builtins.bool hugging_face_cache_enabled: If true, the model will deploy with a cached version instead of directly
               downloading the model artifacts from Hugging Face. This is suitable for
               VPC-SC users with limited internet access.
        :param _builtins.str model_display_name: The user-specified display name of the uploaded model. If not
               set, a default name will be used.
        """
        if accept_eula is not None:
            pulumi.set(__self__, "accept_eula", accept_eula)
        if container_spec is not None:
            pulumi.set(__self__, "container_spec", container_spec)
        if hugging_face_access_token is not None:
            pulumi.set(__self__, "hugging_face_access_token", hugging_face_access_token)
        if hugging_face_cache_enabled is not None:
            pulumi.set(__self__, "hugging_face_cache_enabled", hugging_face_cache_enabled)
        if model_display_name is not None:
            pulumi.set(__self__, "model_display_name", model_display_name)

    @_builtins.property
    @pulumi.getter(name="acceptEula")
    def accept_eula(self) -> Optional[_builtins.bool]:
        """
        Whether the user accepts the End User License Agreement (EULA)
        for the model.
        """
        return pulumi.get(self, "accept_eula")

    @_builtins.property
    @pulumi.getter(name="containerSpec")
    def container_spec(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpec']:
        """
        Specification of a container for serving predictions. Some fields in this
        message correspond to fields in the [Kubernetes Container v1 core
        specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        Structure is documented below.
        """
        return pulumi.get(self, "container_spec")

    @_builtins.property
    @pulumi.getter(name="huggingFaceAccessToken")
    def hugging_face_access_token(self) -> Optional[_builtins.str]:
        """
        The Hugging Face read access token used to access the model
        artifacts of gated models.
        """
        return pulumi.get(self, "hugging_face_access_token")

    @_builtins.property
    @pulumi.getter(name="huggingFaceCacheEnabled")
    def hugging_face_cache_enabled(self) -> Optional[_builtins.bool]:
        """
        If true, the model will deploy with a cached version instead of directly
        downloading the model artifacts from Hugging Face. This is suitable for
        VPC-SC users with limited internet access.
        """
        return pulumi.get(self, "hugging_face_cache_enabled")

    @_builtins.property
    @pulumi.getter(name="modelDisplayName")
    def model_display_name(self) -> Optional[_builtins.str]:
        """
        The user-specified display name of the uploaded model. If not
        set, a default name will be used.
        """
        return pulumi.get(self, "model_display_name")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "imageUri":
            suggest = "image_uri"
        elif key == "deploymentTimeout":
            suggest = "deployment_timeout"
        elif key == "grpcPorts":
            suggest = "grpc_ports"
        elif key == "healthProbe":
            suggest = "health_probe"
        elif key == "healthRoute":
            suggest = "health_route"
        elif key == "livenessProbe":
            suggest = "liveness_probe"
        elif key == "predictRoute":
            suggest = "predict_route"
        elif key == "sharedMemorySizeMb":
            suggest = "shared_memory_size_mb"
        elif key == "startupProbe":
            suggest = "startup_probe"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 image_uri: _builtins.str,
                 args: Optional[Sequence[_builtins.str]] = None,
                 commands: Optional[Sequence[_builtins.str]] = None,
                 deployment_timeout: Optional[_builtins.str] = None,
                 envs: Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv']] = None,
                 grpc_ports: Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPort']] = None,
                 health_probe: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe'] = None,
                 health_route: Optional[_builtins.str] = None,
                 liveness_probe: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe'] = None,
                 ports: Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPort']] = None,
                 predict_route: Optional[_builtins.str] = None,
                 shared_memory_size_mb: Optional[_builtins.str] = None,
                 startup_probe: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe'] = None):
        """
        :param _builtins.str image_uri: URI of the Docker image to be used as the custom container for serving
               predictions. This URI must identify an image in Artifact Registry or
               Container Registry. Learn more about the [container publishing
               requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing),
               including permissions requirements for the Vertex AI Service Agent.
               The container image is ingested upon ModelService.UploadModel, stored
               internally, and this original path is afterwards not used.
               To learn about the requirements for the Docker image itself, see
               [Custom container
               requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#).
               You can use the URI to one of Vertex AI's [pre-built container images for
               prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)
               in this field.
        :param Sequence[_builtins.str] args: Specifies arguments for the command that runs when the container starts.
               This overrides the container's
               [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify
               this field as an array of executable and arguments, similar to a Docker
               `CMD`'s "default parameters" form.
               If you don't specify this field but do specify the
               command field, then the command from the
               `command` field runs without any additional arguments. See the
               [Kubernetes documentation about how the
               `command` and `args` fields interact with a container's `ENTRYPOINT` and
               `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
               If you don't specify this field and don't specify the `command` field,
               then the container's
               [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and
               `CMD` determine what runs based on their default behavior. See the Docker
               documentation about [how `CMD` and `ENTRYPOINT`
               interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
               In this field, you can reference [environment variables
               set by Vertex
               AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
               and environment variables set in the env field.
               You cannot reference environment variables set in the Docker image. In
               order for environment variables to be expanded, reference them by using the
               following syntax:$(VARIABLE_NAME)
               Note that this differs from Bash variable expansion, which does not use
               parentheses. If a variable cannot be resolved, the reference in the input
               string is used unchanged. To avoid variable expansion, you can escape this
               syntax with `$$`; for example:$$(VARIABLE_NAME)
               This field corresponds to the `args` field of the Kubernetes Containers
               [v1 core
               API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param Sequence[_builtins.str] commands: Specifies the command that runs when the container starts. This overrides
               the container's
               [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint).
               Specify this field as an array of executable and arguments, similar to a
               Docker `ENTRYPOINT`'s "exec" form, not its "shell" form.
               If you do not specify this field, then the container's `ENTRYPOINT` runs,
               in conjunction with the args field or the
               container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd),
               if either exists. If this field is not specified and the container does not
               have an `ENTRYPOINT`, then refer to the Docker documentation about [how
               `CMD` and `ENTRYPOINT`
               interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
               If you specify this field, then you can also specify the `args` field to
               provide additional arguments for this command. However, if you specify this
               field, then the container's `CMD` is ignored. See the
               [Kubernetes documentation about how the
               `command` and `args` fields interact with a container's `ENTRYPOINT` and
               `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
               In this field, you can reference [environment variables set by Vertex
               AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
               and environment variables set in the env field.
               You cannot reference environment variables set in the Docker image. In
               order for environment variables to be expanded, reference them by using the
               following syntax:$(VARIABLE_NAME)
               Note that this differs from Bash variable expansion, which does not use
               parentheses. If a variable cannot be resolved, the reference in the input
               string is used unchanged. To avoid variable expansion, you can escape this
               syntax with `$$`; for example:$$(VARIABLE_NAME)
               This field corresponds to the `command` field of the Kubernetes Containers
               [v1 core
               API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param _builtins.str deployment_timeout: Deployment timeout.
               Limit for deployment timeout is 2 hours.
        :param Sequence['AiEndpointWithModelGardenDeploymentModelConfigContainerSpecEnvArgs'] envs: List of environment variables to set in the container. After the container
               starts running, code running in the container can read these environment
               variables.
               Additionally, the command and
               args fields can reference these variables. Later
               entries in this list can also reference earlier entries. For example, the
               following example sets the variable `VAR_2` to have the value `foo bar`:
               ```json
               [
               {
               "name": "VAR_1",
               "value": "foo"
               },
               {
               "name": "VAR_2",
               "value": "$(VAR_1) bar"
               }
               ]
               ```
               If you switch the order of the variables in the example, then the expansion
               does not occur.
               This field corresponds to the `env` field of the Kubernetes Containers
               [v1 core
               API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
               Structure is documented below.
        :param Sequence['AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPortArgs'] grpc_ports: List of ports to expose from the container. Vertex AI sends gRPC
               prediction requests that it receives to the first port on this list. Vertex
               AI also sends liveness and health checks to this port.
               If you do not specify this field, gRPC requests to the container will be
               disabled.
               Vertex AI does not use ports other than the first one listed. This field
               corresponds to the `ports` field of the Kubernetes Containers v1 core API.
               Structure is documented below.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeArgs' health_probe: Probe describes a health check to be performed against a container to
               determine whether it is alive or ready to receive traffic.
               Structure is documented below.
        :param _builtins.str health_route: HTTP path on the container to send health checks to. Vertex AI
               intermittently sends GET requests to this path on the container's IP
               address and port to check that the container is healthy. Read more about
               [health
               checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health).
               For example, if you set this field to `/bar`, then Vertex AI
               intermittently sends a GET request to the `/bar` path on the port of your
               container specified by the first value of this `ModelContainerSpec`'s
               ports field.
               If you don't specify this field, it defaults to the following value when
               you deploy this Model to an Endpoint:/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict
               The placeholders in this value are replaced as follows:
               * ENDPOINT: The last segment (following `endpoints/`)of the
               Endpoint.name][] field of the Endpoint where this Model has been
               deployed. (Vertex AI makes this value available to your container code
               as the [`AIP_ENDPOINT_ID` environment
               variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
               * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`.
               (Vertex AI makes this value available to your container code as the
               [`AIP_DEPLOYED_MODEL_ID` environment
               variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeArgs' liveness_probe: Probe describes a health check to be performed against a container to
               determine whether it is alive or ready to receive traffic.
               Structure is documented below.
        :param Sequence['AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPortArgs'] ports: List of ports to expose from the container. Vertex AI sends any
               prediction requests that it receives to the first port on this list. Vertex
               AI also sends
               [liveness and health
               checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness)
               to this port.
               If you do not specify this field, it defaults to following value:
               ```json
               [
               {
               "containerPort": 8080
               }
               ]
               ```
               Vertex AI does not use ports other than the first one listed. This field
               corresponds to the `ports` field of the Kubernetes Containers
               [v1 core
               API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
               Structure is documented below.
        :param _builtins.str predict_route: HTTP path on the container to send prediction requests to. Vertex AI
               forwards requests sent using
               projects.locations.endpoints.predict to this
               path on the container's IP address and port. Vertex AI then returns the
               container's response in the API response.
               For example, if you set this field to `/foo`, then when Vertex AI
               receives a prediction request, it forwards the request body in a POST
               request to the `/foo` path on the port of your container specified by the
               first value of this `ModelContainerSpec`'s
               ports field.
               If you don't specify this field, it defaults to the following value when
               you deploy this Model to an Endpoint:/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict
               The placeholders in this value are replaced as follows:
               * ENDPOINT: The last segment (following `endpoints/`)of the
               Endpoint.name][] field of the Endpoint where this Model has been
               deployed. (Vertex AI makes this value available to your container code
               as the [`AIP_ENDPOINT_ID` environment
               variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
               * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`.
               (Vertex AI makes this value available to your container code
               as the [`AIP_DEPLOYED_MODEL_ID` environment
               variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        :param _builtins.str shared_memory_size_mb: The amount of the VM memory to reserve as the shared memory for the model
               in megabytes.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeArgs' startup_probe: Probe describes a health check to be performed against a container to
               determine whether it is alive or ready to receive traffic.
               Structure is documented below.
        """
        pulumi.set(__self__, "image_uri", image_uri)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if commands is not None:
            pulumi.set(__self__, "commands", commands)
        if deployment_timeout is not None:
            pulumi.set(__self__, "deployment_timeout", deployment_timeout)
        if envs is not None:
            pulumi.set(__self__, "envs", envs)
        if grpc_ports is not None:
            pulumi.set(__self__, "grpc_ports", grpc_ports)
        if health_probe is not None:
            pulumi.set(__self__, "health_probe", health_probe)
        if health_route is not None:
            pulumi.set(__self__, "health_route", health_route)
        if liveness_probe is not None:
            pulumi.set(__self__, "liveness_probe", liveness_probe)
        if ports is not None:
            pulumi.set(__self__, "ports", ports)
        if predict_route is not None:
            pulumi.set(__self__, "predict_route", predict_route)
        if shared_memory_size_mb is not None:
            pulumi.set(__self__, "shared_memory_size_mb", shared_memory_size_mb)
        if startup_probe is not None:
            pulumi.set(__self__, "startup_probe", startup_probe)

    @_builtins.property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> _builtins.str:
        """
        URI of the Docker image to be used as the custom container for serving
        predictions. This URI must identify an image in Artifact Registry or
        Container Registry. Learn more about the [container publishing
        requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing),
        including permissions requirements for the Vertex AI Service Agent.
        The container image is ingested upon ModelService.UploadModel, stored
        internally, and this original path is afterwards not used.
        To learn about the requirements for the Docker image itself, see
        [Custom container
        requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#).
        You can use the URI to one of Vertex AI's [pre-built container images for
        prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)
        in this field.
        """
        return pulumi.get(self, "image_uri")

    @_builtins.property
    @pulumi.getter
    def args(self) -> Optional[Sequence[_builtins.str]]:
        """
        Specifies arguments for the command that runs when the container starts.
        This overrides the container's
        [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify
        this field as an array of executable and arguments, similar to a Docker
        `CMD`'s "default parameters" form.
        If you don't specify this field but do specify the
        command field, then the command from the
        `command` field runs without any additional arguments. See the
        [Kubernetes documentation about how the
        `command` and `args` fields interact with a container's `ENTRYPOINT` and
        `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
        If you don't specify this field and don't specify the `command` field,
        then the container's
        [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and
        `CMD` determine what runs based on their default behavior. See the Docker
        documentation about [how `CMD` and `ENTRYPOINT`
        interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
        In this field, you can reference [environment variables
        set by Vertex
        AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
        and environment variables set in the env field.
        You cannot reference environment variables set in the Docker image. In
        order for environment variables to be expanded, reference them by using the
        following syntax:$(VARIABLE_NAME)
        Note that this differs from Bash variable expansion, which does not use
        parentheses. If a variable cannot be resolved, the reference in the input
        string is used unchanged. To avoid variable expansion, you can escape this
        syntax with `$$`; for example:$$(VARIABLE_NAME)
        This field corresponds to the `args` field of the Kubernetes Containers
        [v1 core
        API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "args")

    @_builtins.property
    @pulumi.getter
    def commands(self) -> Optional[Sequence[_builtins.str]]:
        """
        Specifies the command that runs when the container starts. This overrides
        the container's
        [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint).
        Specify this field as an array of executable and arguments, similar to a
        Docker `ENTRYPOINT`'s "exec" form, not its "shell" form.
        If you do not specify this field, then the container's `ENTRYPOINT` runs,
        in conjunction with the args field or the
        container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd),
        if either exists. If this field is not specified and the container does not
        have an `ENTRYPOINT`, then refer to the Docker documentation about [how
        `CMD` and `ENTRYPOINT`
        interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
        If you specify this field, then you can also specify the `args` field to
        provide additional arguments for this command. However, if you specify this
        field, then the container's `CMD` is ignored. See the
        [Kubernetes documentation about how the
        `command` and `args` fields interact with a container's `ENTRYPOINT` and
        `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
        In this field, you can reference [environment variables set by Vertex
        AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
        and environment variables set in the env field.
        You cannot reference environment variables set in the Docker image. In
        order for environment variables to be expanded, reference them by using the
        following syntax:$(VARIABLE_NAME)
        Note that this differs from Bash variable expansion, which does not use
        parentheses. If a variable cannot be resolved, the reference in the input
        string is used unchanged. To avoid variable expansion, you can escape this
        syntax with `$$`; for example:$$(VARIABLE_NAME)
        This field corresponds to the `command` field of the Kubernetes Containers
        [v1 core
        API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "commands")

    @_builtins.property
    @pulumi.getter(name="deploymentTimeout")
    def deployment_timeout(self) -> Optional[_builtins.str]:
        """
        Deployment timeout.
        Limit for deployment timeout is 2 hours.
        """
        return pulumi.get(self, "deployment_timeout")

    @_builtins.property
    @pulumi.getter
    def envs(self) -> Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv']]:
        """
        List of environment variables to set in the container. After the container
        starts running, code running in the container can read these environment
        variables.
        Additionally, the command and
        args fields can reference these variables. Later
        entries in this list can also reference earlier entries. For example, the
        following example sets the variable `VAR_2` to have the value `foo bar`:
        ```json
        [
        {
        "name": "VAR_1",
        "value": "foo"
        },
        {
        "name": "VAR_2",
        "value": "$(VAR_1) bar"
        }
        ]
        ```
        If you switch the order of the variables in the example, then the expansion
        does not occur.
        This field corresponds to the `env` field of the Kubernetes Containers
        [v1 core
        API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        Structure is documented below.
        """
        return pulumi.get(self, "envs")

    @_builtins.property
    @pulumi.getter(name="grpcPorts")
    def grpc_ports(self) -> Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPort']]:
        """
        List of ports to expose from the container. Vertex AI sends gRPC
        prediction requests that it receives to the first port on this list. Vertex
        AI also sends liveness and health checks to this port.
        If you do not specify this field, gRPC requests to the container will be
        disabled.
        Vertex AI does not use ports other than the first one listed. This field
        corresponds to the `ports` field of the Kubernetes Containers v1 core API.
        Structure is documented below.
        """
        return pulumi.get(self, "grpc_ports")

    @_builtins.property
    @pulumi.getter(name="healthProbe")
    def health_probe(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe']:
        """
        Probe describes a health check to be performed against a container to
        determine whether it is alive or ready to receive traffic.
        Structure is documented below.
        """
        return pulumi.get(self, "health_probe")

    @_builtins.property
    @pulumi.getter(name="healthRoute")
    def health_route(self) -> Optional[_builtins.str]:
        """
        HTTP path on the container to send health checks to. Vertex AI
        intermittently sends GET requests to this path on the container's IP
        address and port to check that the container is healthy. Read more about
        [health
        checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health).
        For example, if you set this field to `/bar`, then Vertex AI
        intermittently sends a GET request to the `/bar` path on the port of your
        container specified by the first value of this `ModelContainerSpec`'s
        ports field.
        If you don't specify this field, it defaults to the following value when
        you deploy this Model to an Endpoint:/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict
        The placeholders in this value are replaced as follows:
        * ENDPOINT: The last segment (following `endpoints/`)of the
        Endpoint.name][] field of the Endpoint where this Model has been
        deployed. (Vertex AI makes this value available to your container code
        as the [`AIP_ENDPOINT_ID` environment
        variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`.
        (Vertex AI makes this value available to your container code as the
        [`AIP_DEPLOYED_MODEL_ID` environment
        variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        """
        return pulumi.get(self, "health_route")

    @_builtins.property
    @pulumi.getter(name="livenessProbe")
    def liveness_probe(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe']:
        """
        Probe describes a health check to be performed against a container to
        determine whether it is alive or ready to receive traffic.
        Structure is documented below.
        """
        return pulumi.get(self, "liveness_probe")

    @_builtins.property
    @pulumi.getter
    def ports(self) -> Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPort']]:
        """
        List of ports to expose from the container. Vertex AI sends any
        prediction requests that it receives to the first port on this list. Vertex
        AI also sends
        [liveness and health
        checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness)
        to this port.
        If you do not specify this field, it defaults to following value:
        ```json
        [
        {
        "containerPort": 8080
        }
        ]
        ```
        Vertex AI does not use ports other than the first one listed. This field
        corresponds to the `ports` field of the Kubernetes Containers
        [v1 core
        API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        Structure is documented below.
        """
        return pulumi.get(self, "ports")

    @_builtins.property
    @pulumi.getter(name="predictRoute")
    def predict_route(self) -> Optional[_builtins.str]:
        """
        HTTP path on the container to send prediction requests to. Vertex AI
        forwards requests sent using
        projects.locations.endpoints.predict to this
        path on the container's IP address and port. Vertex AI then returns the
        container's response in the API response.
        For example, if you set this field to `/foo`, then when Vertex AI
        receives a prediction request, it forwards the request body in a POST
        request to the `/foo` path on the port of your container specified by the
        first value of this `ModelContainerSpec`'s
        ports field.
        If you don't specify this field, it defaults to the following value when
        you deploy this Model to an Endpoint:/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict
        The placeholders in this value are replaced as follows:
        * ENDPOINT: The last segment (following `endpoints/`)of the
        Endpoint.name][] field of the Endpoint where this Model has been
        deployed. (Vertex AI makes this value available to your container code
        as the [`AIP_ENDPOINT_ID` environment
        variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`.
        (Vertex AI makes this value available to your container code
        as the [`AIP_DEPLOYED_MODEL_ID` environment
        variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        """
        return pulumi.get(self, "predict_route")

    @_builtins.property
    @pulumi.getter(name="sharedMemorySizeMb")
    def shared_memory_size_mb(self) -> Optional[_builtins.str]:
        """
        The amount of the VM memory to reserve as the shared memory for the model
        in megabytes.
        """
        return pulumi.get(self, "shared_memory_size_mb")

    @_builtins.property
    @pulumi.getter(name="startupProbe")
    def startup_probe(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe']:
        """
        Probe describes a health check to be performed against a container to
        determine whether it is alive or ready to receive traffic.
        Structure is documented below.
        """
        return pulumi.get(self, "startup_probe")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecEnv(dict):
    def __init__(__self__, *,
                 name: _builtins.str,
                 value: _builtins.str):
        """
        :param _builtins.str name: Name of the environment variable. Must be a valid C identifier.
        :param _builtins.str value: Variables that reference a $(VAR_NAME) are expanded
               using the previous defined environment variables in the container and
               any service environment variables. If a variable cannot be resolved,
               the reference in the input string will be unchanged. The $(VAR_NAME)
               syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped
               references will never be expanded, regardless of whether the variable
               exists or not.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "value", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> _builtins.str:
        """
        Name of the environment variable. Must be a valid C identifier.
        """
        return pulumi.get(self, "name")

    @_builtins.property
    @pulumi.getter
    def value(self) -> _builtins.str:
        """
        Variables that reference a $(VAR_NAME) are expanded
        using the previous defined environment variables in the container and
        any service environment variables. If a variable cannot be resolved,
        the reference in the input string will be unchanged. The $(VAR_NAME)
        syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped
        references will never be expanded, regardless of whether the variable
        exists or not.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPort(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "containerPort":
            suggest = "container_port"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPort. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPort.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecGrpcPort.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 container_port: Optional[_builtins.int] = None):
        """
        :param _builtins.int container_port: The number of the port to expose on the pod's IP address.
               Must be a valid port number, between 1 and 65535 inclusive.
        """
        if container_port is not None:
            pulumi.set(__self__, "container_port", container_port)

    @_builtins.property
    @pulumi.getter(name="containerPort")
    def container_port(self) -> Optional[_builtins.int]:
        """
        The number of the port to expose on the pod's IP address.
        Must be a valid port number, between 1 and 65535 inclusive.
        """
        return pulumi.get(self, "container_port")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "exec":
            suggest = "exec_"
        elif key == "failureThreshold":
            suggest = "failure_threshold"
        elif key == "httpGet":
            suggest = "http_get"
        elif key == "initialDelaySeconds":
            suggest = "initial_delay_seconds"
        elif key == "periodSeconds":
            suggest = "period_seconds"
        elif key == "successThreshold":
            suggest = "success_threshold"
        elif key == "tcpSocket":
            suggest = "tcp_socket"
        elif key == "timeoutSeconds":
            suggest = "timeout_seconds"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbe.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 exec_: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec'] = None,
                 failure_threshold: Optional[_builtins.int] = None,
                 grpc: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc'] = None,
                 http_get: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet'] = None,
                 initial_delay_seconds: Optional[_builtins.int] = None,
                 period_seconds: Optional[_builtins.int] = None,
                 success_threshold: Optional[_builtins.int] = None,
                 tcp_socket: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket'] = None,
                 timeout_seconds: Optional[_builtins.int] = None):
        """
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExecArgs' exec_: ExecAction specifies a command to execute.
               Structure is documented below.
        :param _builtins.int failure_threshold: Number of consecutive failures before the probe is considered failed.
               Defaults to 3. Minimum value is 1.
               Maps to Kubernetes probe argument 'failureThreshold'.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpcArgs' grpc: GrpcAction checks the health of a container using a gRPC service.
               Structure is documented below.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetArgs' http_get: HttpGetAction describes an action based on HTTP Get requests.
               Structure is documented below.
        :param _builtins.int initial_delay_seconds: Number of seconds to wait before starting the probe. Defaults to 0.
               Minimum value is 0.
               Maps to Kubernetes probe argument 'initialDelaySeconds'.
        :param _builtins.int period_seconds: How often (in seconds) to perform the probe. Default to 10 seconds.
               Minimum value is 1. Must be less than timeout_seconds.
               Maps to Kubernetes probe argument 'periodSeconds'.
        :param _builtins.int success_threshold: Number of consecutive successes before the probe is considered successful.
               Defaults to 1. Minimum value is 1.
               Maps to Kubernetes probe argument 'successThreshold'.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocketArgs' tcp_socket: TcpSocketAction probes the health of a container by opening a TCP socket
               connection.
               Structure is documented below.
        :param _builtins.int timeout_seconds: Number of seconds after which the probe times out. Defaults to 1 second.
               Minimum value is 1. Must be greater or equal to period_seconds.
               Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        if exec_ is not None:
            pulumi.set(__self__, "exec_", exec_)
        if failure_threshold is not None:
            pulumi.set(__self__, "failure_threshold", failure_threshold)
        if grpc is not None:
            pulumi.set(__self__, "grpc", grpc)
        if http_get is not None:
            pulumi.set(__self__, "http_get", http_get)
        if initial_delay_seconds is not None:
            pulumi.set(__self__, "initial_delay_seconds", initial_delay_seconds)
        if period_seconds is not None:
            pulumi.set(__self__, "period_seconds", period_seconds)
        if success_threshold is not None:
            pulumi.set(__self__, "success_threshold", success_threshold)
        if tcp_socket is not None:
            pulumi.set(__self__, "tcp_socket", tcp_socket)
        if timeout_seconds is not None:
            pulumi.set(__self__, "timeout_seconds", timeout_seconds)

    @_builtins.property
    @pulumi.getter(name="exec")
    def exec_(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec']:
        """
        ExecAction specifies a command to execute.
        Structure is documented below.
        """
        return pulumi.get(self, "exec_")

    @_builtins.property
    @pulumi.getter(name="failureThreshold")
    def failure_threshold(self) -> Optional[_builtins.int]:
        """
        Number of consecutive failures before the probe is considered failed.
        Defaults to 3. Minimum value is 1.
        Maps to Kubernetes probe argument 'failureThreshold'.
        """
        return pulumi.get(self, "failure_threshold")

    @_builtins.property
    @pulumi.getter
    def grpc(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc']:
        """
        GrpcAction checks the health of a container using a gRPC service.
        Structure is documented below.
        """
        return pulumi.get(self, "grpc")

    @_builtins.property
    @pulumi.getter(name="httpGet")
    def http_get(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet']:
        """
        HttpGetAction describes an action based on HTTP Get requests.
        Structure is documented below.
        """
        return pulumi.get(self, "http_get")

    @_builtins.property
    @pulumi.getter(name="initialDelaySeconds")
    def initial_delay_seconds(self) -> Optional[_builtins.int]:
        """
        Number of seconds to wait before starting the probe. Defaults to 0.
        Minimum value is 0.
        Maps to Kubernetes probe argument 'initialDelaySeconds'.
        """
        return pulumi.get(self, "initial_delay_seconds")

    @_builtins.property
    @pulumi.getter(name="periodSeconds")
    def period_seconds(self) -> Optional[_builtins.int]:
        """
        How often (in seconds) to perform the probe. Default to 10 seconds.
        Minimum value is 1. Must be less than timeout_seconds.
        Maps to Kubernetes probe argument 'periodSeconds'.
        """
        return pulumi.get(self, "period_seconds")

    @_builtins.property
    @pulumi.getter(name="successThreshold")
    def success_threshold(self) -> Optional[_builtins.int]:
        """
        Number of consecutive successes before the probe is considered successful.
        Defaults to 1. Minimum value is 1.
        Maps to Kubernetes probe argument 'successThreshold'.
        """
        return pulumi.get(self, "success_threshold")

    @_builtins.property
    @pulumi.getter(name="tcpSocket")
    def tcp_socket(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket']:
        """
        TcpSocketAction probes the health of a container by opening a TCP socket
        connection.
        Structure is documented below.
        """
        return pulumi.get(self, "tcp_socket")

    @_builtins.property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> Optional[_builtins.int]:
        """
        Number of seconds after which the probe times out. Defaults to 1 second.
        Minimum value is 1. Must be greater or equal to period_seconds.
        Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        return pulumi.get(self, "timeout_seconds")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeExec(dict):
    def __init__(__self__, *,
                 commands: Optional[Sequence[_builtins.str]] = None):
        """
        :param Sequence[_builtins.str] commands: Command is the command line to execute inside the container, the working
               directory for the command is root ('/') in the container's filesystem.
               The command is simply exec'd, it is not run inside a shell, so
               traditional shell instructions ('|', etc) won't work. To use a shell, you
               need to explicitly call out to that shell. Exit status of 0 is treated as
               live/healthy and non-zero is unhealthy.
        """
        if commands is not None:
            pulumi.set(__self__, "commands", commands)

    @_builtins.property
    @pulumi.getter
    def commands(self) -> Optional[Sequence[_builtins.str]]:
        """
        Command is the command line to execute inside the container, the working
        directory for the command is root ('/') in the container's filesystem.
        The command is simply exec'd, it is not run inside a shell, so
        traditional shell instructions ('|', etc) won't work. To use a shell, you
        need to explicitly call out to that shell. Exit status of 0 is treated as
        live/healthy and non-zero is unhealthy.
        """
        return pulumi.get(self, "commands")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeGrpc(dict):
    def __init__(__self__, *,
                 port: Optional[_builtins.int] = None,
                 service: Optional[_builtins.str] = None):
        """
        :param _builtins.int port: Port number of the gRPC service. Number must be in the range 1 to 65535.
        :param _builtins.str service: Service is the name of the service to place in the gRPC
               HealthCheckRequest. See
               https://github.com/grpc/grpc/blob/master/doc/health-checking.md.
               If this is not specified, the default behavior is defined by gRPC.
        """
        if port is not None:
            pulumi.set(__self__, "port", port)
        if service is not None:
            pulumi.set(__self__, "service", service)

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Port number of the gRPC service. Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")

    @_builtins.property
    @pulumi.getter
    def service(self) -> Optional[_builtins.str]:
        """
        Service is the name of the service to place in the gRPC
        HealthCheckRequest. See
        https://github.com/grpc/grpc/blob/master/doc/health-checking.md.
        If this is not specified, the default behavior is defined by gRPC.
        """
        return pulumi.get(self, "service")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "httpHeaders":
            suggest = "http_headers"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGet.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 host: Optional[_builtins.str] = None,
                 http_headers: Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeader']] = None,
                 path: Optional[_builtins.str] = None,
                 port: Optional[_builtins.int] = None,
                 scheme: Optional[_builtins.str] = None):
        """
        :param _builtins.str host: Host name to connect to, defaults to the model serving container's IP.
               You probably want to set "Host" in httpHeaders instead.
        :param Sequence['AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeaderArgs'] http_headers: Custom headers to set in the request. HTTP allows repeated headers.
               Structure is documented below.
        :param _builtins.str path: Path to access on the HTTP server.
        :param _builtins.int port: Number of the port to access on the container.
               Number must be in the range 1 to 65535.
        :param _builtins.str scheme: Scheme to use for connecting to the host.
               Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".
        """
        if host is not None:
            pulumi.set(__self__, "host", host)
        if http_headers is not None:
            pulumi.set(__self__, "http_headers", http_headers)
        if path is not None:
            pulumi.set(__self__, "path", path)
        if port is not None:
            pulumi.set(__self__, "port", port)
        if scheme is not None:
            pulumi.set(__self__, "scheme", scheme)

    @_builtins.property
    @pulumi.getter
    def host(self) -> Optional[_builtins.str]:
        """
        Host name to connect to, defaults to the model serving container's IP.
        You probably want to set "Host" in httpHeaders instead.
        """
        return pulumi.get(self, "host")

    @_builtins.property
    @pulumi.getter(name="httpHeaders")
    def http_headers(self) -> Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeader']]:
        """
        Custom headers to set in the request. HTTP allows repeated headers.
        Structure is documented below.
        """
        return pulumi.get(self, "http_headers")

    @_builtins.property
    @pulumi.getter
    def path(self) -> Optional[_builtins.str]:
        """
        Path to access on the HTTP server.
        """
        return pulumi.get(self, "path")

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Number of the port to access on the container.
        Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")

    @_builtins.property
    @pulumi.getter
    def scheme(self) -> Optional[_builtins.str]:
        """
        Scheme to use for connecting to the host.
        Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".
        """
        return pulumi.get(self, "scheme")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeHttpGetHttpHeader(dict):
    def __init__(__self__, *,
                 name: Optional[_builtins.str] = None,
                 value: Optional[_builtins.str] = None):
        """
        :param _builtins.str name: The header field name.
               This will be canonicalized upon output, so case-variant names will be
               understood as the same header.
        :param _builtins.str value: The header field value
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if value is not None:
            pulumi.set(__self__, "value", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[_builtins.str]:
        """
        The header field name.
        This will be canonicalized upon output, so case-variant names will be
        understood as the same header.
        """
        return pulumi.get(self, "name")

    @_builtins.property
    @pulumi.getter
    def value(self) -> Optional[_builtins.str]:
        """
        The header field value
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecHealthProbeTcpSocket(dict):
    def __init__(__self__, *,
                 host: Optional[_builtins.str] = None,
                 port: Optional[_builtins.int] = None):
        """
        :param _builtins.str host: Optional: Host name to connect to, defaults to the model serving
               container's IP.
        :param _builtins.int port: Number of the port to access on the container.
               Number must be in the range 1 to 65535.
        """
        if host is not None:
            pulumi.set(__self__, "host", host)
        if port is not None:
            pulumi.set(__self__, "port", port)

    @_builtins.property
    @pulumi.getter
    def host(self) -> Optional[_builtins.str]:
        """
        Optional: Host name to connect to, defaults to the model serving
        container's IP.
        """
        return pulumi.get(self, "host")

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Number of the port to access on the container.
        Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "exec":
            suggest = "exec_"
        elif key == "failureThreshold":
            suggest = "failure_threshold"
        elif key == "httpGet":
            suggest = "http_get"
        elif key == "initialDelaySeconds":
            suggest = "initial_delay_seconds"
        elif key == "periodSeconds":
            suggest = "period_seconds"
        elif key == "successThreshold":
            suggest = "success_threshold"
        elif key == "tcpSocket":
            suggest = "tcp_socket"
        elif key == "timeoutSeconds":
            suggest = "timeout_seconds"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbe.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 exec_: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec'] = None,
                 failure_threshold: Optional[_builtins.int] = None,
                 grpc: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc'] = None,
                 http_get: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet'] = None,
                 initial_delay_seconds: Optional[_builtins.int] = None,
                 period_seconds: Optional[_builtins.int] = None,
                 success_threshold: Optional[_builtins.int] = None,
                 tcp_socket: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket'] = None,
                 timeout_seconds: Optional[_builtins.int] = None):
        """
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExecArgs' exec_: ExecAction specifies a command to execute.
               Structure is documented below.
        :param _builtins.int failure_threshold: Number of consecutive failures before the probe is considered failed.
               Defaults to 3. Minimum value is 1.
               Maps to Kubernetes probe argument 'failureThreshold'.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpcArgs' grpc: GrpcAction checks the health of a container using a gRPC service.
               Structure is documented below.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetArgs' http_get: HttpGetAction describes an action based on HTTP Get requests.
               Structure is documented below.
        :param _builtins.int initial_delay_seconds: Number of seconds to wait before starting the probe. Defaults to 0.
               Minimum value is 0.
               Maps to Kubernetes probe argument 'initialDelaySeconds'.
        :param _builtins.int period_seconds: How often (in seconds) to perform the probe. Default to 10 seconds.
               Minimum value is 1. Must be less than timeout_seconds.
               Maps to Kubernetes probe argument 'periodSeconds'.
        :param _builtins.int success_threshold: Number of consecutive successes before the probe is considered successful.
               Defaults to 1. Minimum value is 1.
               Maps to Kubernetes probe argument 'successThreshold'.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocketArgs' tcp_socket: TcpSocketAction probes the health of a container by opening a TCP socket
               connection.
               Structure is documented below.
        :param _builtins.int timeout_seconds: Number of seconds after which the probe times out. Defaults to 1 second.
               Minimum value is 1. Must be greater or equal to period_seconds.
               Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        if exec_ is not None:
            pulumi.set(__self__, "exec_", exec_)
        if failure_threshold is not None:
            pulumi.set(__self__, "failure_threshold", failure_threshold)
        if grpc is not None:
            pulumi.set(__self__, "grpc", grpc)
        if http_get is not None:
            pulumi.set(__self__, "http_get", http_get)
        if initial_delay_seconds is not None:
            pulumi.set(__self__, "initial_delay_seconds", initial_delay_seconds)
        if period_seconds is not None:
            pulumi.set(__self__, "period_seconds", period_seconds)
        if success_threshold is not None:
            pulumi.set(__self__, "success_threshold", success_threshold)
        if tcp_socket is not None:
            pulumi.set(__self__, "tcp_socket", tcp_socket)
        if timeout_seconds is not None:
            pulumi.set(__self__, "timeout_seconds", timeout_seconds)

    @_builtins.property
    @pulumi.getter(name="exec")
    def exec_(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec']:
        """
        ExecAction specifies a command to execute.
        Structure is documented below.
        """
        return pulumi.get(self, "exec_")

    @_builtins.property
    @pulumi.getter(name="failureThreshold")
    def failure_threshold(self) -> Optional[_builtins.int]:
        """
        Number of consecutive failures before the probe is considered failed.
        Defaults to 3. Minimum value is 1.
        Maps to Kubernetes probe argument 'failureThreshold'.
        """
        return pulumi.get(self, "failure_threshold")

    @_builtins.property
    @pulumi.getter
    def grpc(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc']:
        """
        GrpcAction checks the health of a container using a gRPC service.
        Structure is documented below.
        """
        return pulumi.get(self, "grpc")

    @_builtins.property
    @pulumi.getter(name="httpGet")
    def http_get(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet']:
        """
        HttpGetAction describes an action based on HTTP Get requests.
        Structure is documented below.
        """
        return pulumi.get(self, "http_get")

    @_builtins.property
    @pulumi.getter(name="initialDelaySeconds")
    def initial_delay_seconds(self) -> Optional[_builtins.int]:
        """
        Number of seconds to wait before starting the probe. Defaults to 0.
        Minimum value is 0.
        Maps to Kubernetes probe argument 'initialDelaySeconds'.
        """
        return pulumi.get(self, "initial_delay_seconds")

    @_builtins.property
    @pulumi.getter(name="periodSeconds")
    def period_seconds(self) -> Optional[_builtins.int]:
        """
        How often (in seconds) to perform the probe. Default to 10 seconds.
        Minimum value is 1. Must be less than timeout_seconds.
        Maps to Kubernetes probe argument 'periodSeconds'.
        """
        return pulumi.get(self, "period_seconds")

    @_builtins.property
    @pulumi.getter(name="successThreshold")
    def success_threshold(self) -> Optional[_builtins.int]:
        """
        Number of consecutive successes before the probe is considered successful.
        Defaults to 1. Minimum value is 1.
        Maps to Kubernetes probe argument 'successThreshold'.
        """
        return pulumi.get(self, "success_threshold")

    @_builtins.property
    @pulumi.getter(name="tcpSocket")
    def tcp_socket(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket']:
        """
        TcpSocketAction probes the health of a container by opening a TCP socket
        connection.
        Structure is documented below.
        """
        return pulumi.get(self, "tcp_socket")

    @_builtins.property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> Optional[_builtins.int]:
        """
        Number of seconds after which the probe times out. Defaults to 1 second.
        Minimum value is 1. Must be greater or equal to period_seconds.
        Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        return pulumi.get(self, "timeout_seconds")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeExec(dict):
    def __init__(__self__, *,
                 commands: Optional[Sequence[_builtins.str]] = None):
        """
        :param Sequence[_builtins.str] commands: Command is the command line to execute inside the container, the working
               directory for the command is root ('/') in the container's filesystem.
               The command is simply exec'd, it is not run inside a shell, so
               traditional shell instructions ('|', etc) won't work. To use a shell, you
               need to explicitly call out to that shell. Exit status of 0 is treated as
               live/healthy and non-zero is unhealthy.
        """
        if commands is not None:
            pulumi.set(__self__, "commands", commands)

    @_builtins.property
    @pulumi.getter
    def commands(self) -> Optional[Sequence[_builtins.str]]:
        """
        Command is the command line to execute inside the container, the working
        directory for the command is root ('/') in the container's filesystem.
        The command is simply exec'd, it is not run inside a shell, so
        traditional shell instructions ('|', etc) won't work. To use a shell, you
        need to explicitly call out to that shell. Exit status of 0 is treated as
        live/healthy and non-zero is unhealthy.
        """
        return pulumi.get(self, "commands")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeGrpc(dict):
    def __init__(__self__, *,
                 port: Optional[_builtins.int] = None,
                 service: Optional[_builtins.str] = None):
        """
        :param _builtins.int port: Port number of the gRPC service. Number must be in the range 1 to 65535.
        :param _builtins.str service: Service is the name of the service to place in the gRPC
               HealthCheckRequest. See
               https://github.com/grpc/grpc/blob/master/doc/health-checking.md.
               If this is not specified, the default behavior is defined by gRPC.
        """
        if port is not None:
            pulumi.set(__self__, "port", port)
        if service is not None:
            pulumi.set(__self__, "service", service)

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Port number of the gRPC service. Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")

    @_builtins.property
    @pulumi.getter
    def service(self) -> Optional[_builtins.str]:
        """
        Service is the name of the service to place in the gRPC
        HealthCheckRequest. See
        https://github.com/grpc/grpc/blob/master/doc/health-checking.md.
        If this is not specified, the default behavior is defined by gRPC.
        """
        return pulumi.get(self, "service")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "httpHeaders":
            suggest = "http_headers"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGet.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 host: Optional[_builtins.str] = None,
                 http_headers: Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeader']] = None,
                 path: Optional[_builtins.str] = None,
                 port: Optional[_builtins.int] = None,
                 scheme: Optional[_builtins.str] = None):
        """
        :param _builtins.str host: Host name to connect to, defaults to the model serving container's IP.
               You probably want to set "Host" in httpHeaders instead.
        :param Sequence['AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeaderArgs'] http_headers: Custom headers to set in the request. HTTP allows repeated headers.
               Structure is documented below.
        :param _builtins.str path: Path to access on the HTTP server.
        :param _builtins.int port: Number of the port to access on the container.
               Number must be in the range 1 to 65535.
        :param _builtins.str scheme: Scheme to use for connecting to the host.
               Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".
        """
        if host is not None:
            pulumi.set(__self__, "host", host)
        if http_headers is not None:
            pulumi.set(__self__, "http_headers", http_headers)
        if path is not None:
            pulumi.set(__self__, "path", path)
        if port is not None:
            pulumi.set(__self__, "port", port)
        if scheme is not None:
            pulumi.set(__self__, "scheme", scheme)

    @_builtins.property
    @pulumi.getter
    def host(self) -> Optional[_builtins.str]:
        """
        Host name to connect to, defaults to the model serving container's IP.
        You probably want to set "Host" in httpHeaders instead.
        """
        return pulumi.get(self, "host")

    @_builtins.property
    @pulumi.getter(name="httpHeaders")
    def http_headers(self) -> Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeader']]:
        """
        Custom headers to set in the request. HTTP allows repeated headers.
        Structure is documented below.
        """
        return pulumi.get(self, "http_headers")

    @_builtins.property
    @pulumi.getter
    def path(self) -> Optional[_builtins.str]:
        """
        Path to access on the HTTP server.
        """
        return pulumi.get(self, "path")

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Number of the port to access on the container.
        Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")

    @_builtins.property
    @pulumi.getter
    def scheme(self) -> Optional[_builtins.str]:
        """
        Scheme to use for connecting to the host.
        Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".
        """
        return pulumi.get(self, "scheme")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeHttpGetHttpHeader(dict):
    def __init__(__self__, *,
                 name: Optional[_builtins.str] = None,
                 value: Optional[_builtins.str] = None):
        """
        :param _builtins.str name: The header field name.
               This will be canonicalized upon output, so case-variant names will be
               understood as the same header.
        :param _builtins.str value: The header field value
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if value is not None:
            pulumi.set(__self__, "value", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[_builtins.str]:
        """
        The header field name.
        This will be canonicalized upon output, so case-variant names will be
        understood as the same header.
        """
        return pulumi.get(self, "name")

    @_builtins.property
    @pulumi.getter
    def value(self) -> Optional[_builtins.str]:
        """
        The header field value
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecLivenessProbeTcpSocket(dict):
    def __init__(__self__, *,
                 host: Optional[_builtins.str] = None,
                 port: Optional[_builtins.int] = None):
        """
        :param _builtins.str host: Optional: Host name to connect to, defaults to the model serving
               container's IP.
        :param _builtins.int port: Number of the port to access on the container.
               Number must be in the range 1 to 65535.
        """
        if host is not None:
            pulumi.set(__self__, "host", host)
        if port is not None:
            pulumi.set(__self__, "port", port)

    @_builtins.property
    @pulumi.getter
    def host(self) -> Optional[_builtins.str]:
        """
        Optional: Host name to connect to, defaults to the model serving
        container's IP.
        """
        return pulumi.get(self, "host")

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Number of the port to access on the container.
        Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPort(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "containerPort":
            suggest = "container_port"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPort. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPort.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecPort.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 container_port: Optional[_builtins.int] = None):
        """
        :param _builtins.int container_port: The number of the port to expose on the pod's IP address.
               Must be a valid port number, between 1 and 65535 inclusive.
        """
        if container_port is not None:
            pulumi.set(__self__, "container_port", container_port)

    @_builtins.property
    @pulumi.getter(name="containerPort")
    def container_port(self) -> Optional[_builtins.int]:
        """
        The number of the port to expose on the pod's IP address.
        Must be a valid port number, between 1 and 65535 inclusive.
        """
        return pulumi.get(self, "container_port")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "exec":
            suggest = "exec_"
        elif key == "failureThreshold":
            suggest = "failure_threshold"
        elif key == "httpGet":
            suggest = "http_get"
        elif key == "initialDelaySeconds":
            suggest = "initial_delay_seconds"
        elif key == "periodSeconds":
            suggest = "period_seconds"
        elif key == "successThreshold":
            suggest = "success_threshold"
        elif key == "tcpSocket":
            suggest = "tcp_socket"
        elif key == "timeoutSeconds":
            suggest = "timeout_seconds"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbe.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 exec_: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec'] = None,
                 failure_threshold: Optional[_builtins.int] = None,
                 grpc: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc'] = None,
                 http_get: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet'] = None,
                 initial_delay_seconds: Optional[_builtins.int] = None,
                 period_seconds: Optional[_builtins.int] = None,
                 success_threshold: Optional[_builtins.int] = None,
                 tcp_socket: Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket'] = None,
                 timeout_seconds: Optional[_builtins.int] = None):
        """
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExecArgs' exec_: ExecAction specifies a command to execute.
               Structure is documented below.
        :param _builtins.int failure_threshold: Number of consecutive failures before the probe is considered failed.
               Defaults to 3. Minimum value is 1.
               Maps to Kubernetes probe argument 'failureThreshold'.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpcArgs' grpc: GrpcAction checks the health of a container using a gRPC service.
               Structure is documented below.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetArgs' http_get: HttpGetAction describes an action based on HTTP Get requests.
               Structure is documented below.
        :param _builtins.int initial_delay_seconds: Number of seconds to wait before starting the probe. Defaults to 0.
               Minimum value is 0.
               Maps to Kubernetes probe argument 'initialDelaySeconds'.
        :param _builtins.int period_seconds: How often (in seconds) to perform the probe. Default to 10 seconds.
               Minimum value is 1. Must be less than timeout_seconds.
               Maps to Kubernetes probe argument 'periodSeconds'.
        :param _builtins.int success_threshold: Number of consecutive successes before the probe is considered successful.
               Defaults to 1. Minimum value is 1.
               Maps to Kubernetes probe argument 'successThreshold'.
        :param 'AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocketArgs' tcp_socket: TcpSocketAction probes the health of a container by opening a TCP socket
               connection.
               Structure is documented below.
        :param _builtins.int timeout_seconds: Number of seconds after which the probe times out. Defaults to 1 second.
               Minimum value is 1. Must be greater or equal to period_seconds.
               Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        if exec_ is not None:
            pulumi.set(__self__, "exec_", exec_)
        if failure_threshold is not None:
            pulumi.set(__self__, "failure_threshold", failure_threshold)
        if grpc is not None:
            pulumi.set(__self__, "grpc", grpc)
        if http_get is not None:
            pulumi.set(__self__, "http_get", http_get)
        if initial_delay_seconds is not None:
            pulumi.set(__self__, "initial_delay_seconds", initial_delay_seconds)
        if period_seconds is not None:
            pulumi.set(__self__, "period_seconds", period_seconds)
        if success_threshold is not None:
            pulumi.set(__self__, "success_threshold", success_threshold)
        if tcp_socket is not None:
            pulumi.set(__self__, "tcp_socket", tcp_socket)
        if timeout_seconds is not None:
            pulumi.set(__self__, "timeout_seconds", timeout_seconds)

    @_builtins.property
    @pulumi.getter(name="exec")
    def exec_(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec']:
        """
        ExecAction specifies a command to execute.
        Structure is documented below.
        """
        return pulumi.get(self, "exec_")

    @_builtins.property
    @pulumi.getter(name="failureThreshold")
    def failure_threshold(self) -> Optional[_builtins.int]:
        """
        Number of consecutive failures before the probe is considered failed.
        Defaults to 3. Minimum value is 1.
        Maps to Kubernetes probe argument 'failureThreshold'.
        """
        return pulumi.get(self, "failure_threshold")

    @_builtins.property
    @pulumi.getter
    def grpc(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc']:
        """
        GrpcAction checks the health of a container using a gRPC service.
        Structure is documented below.
        """
        return pulumi.get(self, "grpc")

    @_builtins.property
    @pulumi.getter(name="httpGet")
    def http_get(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet']:
        """
        HttpGetAction describes an action based on HTTP Get requests.
        Structure is documented below.
        """
        return pulumi.get(self, "http_get")

    @_builtins.property
    @pulumi.getter(name="initialDelaySeconds")
    def initial_delay_seconds(self) -> Optional[_builtins.int]:
        """
        Number of seconds to wait before starting the probe. Defaults to 0.
        Minimum value is 0.
        Maps to Kubernetes probe argument 'initialDelaySeconds'.
        """
        return pulumi.get(self, "initial_delay_seconds")

    @_builtins.property
    @pulumi.getter(name="periodSeconds")
    def period_seconds(self) -> Optional[_builtins.int]:
        """
        How often (in seconds) to perform the probe. Default to 10 seconds.
        Minimum value is 1. Must be less than timeout_seconds.
        Maps to Kubernetes probe argument 'periodSeconds'.
        """
        return pulumi.get(self, "period_seconds")

    @_builtins.property
    @pulumi.getter(name="successThreshold")
    def success_threshold(self) -> Optional[_builtins.int]:
        """
        Number of consecutive successes before the probe is considered successful.
        Defaults to 1. Minimum value is 1.
        Maps to Kubernetes probe argument 'successThreshold'.
        """
        return pulumi.get(self, "success_threshold")

    @_builtins.property
    @pulumi.getter(name="tcpSocket")
    def tcp_socket(self) -> Optional['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket']:
        """
        TcpSocketAction probes the health of a container by opening a TCP socket
        connection.
        Structure is documented below.
        """
        return pulumi.get(self, "tcp_socket")

    @_builtins.property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> Optional[_builtins.int]:
        """
        Number of seconds after which the probe times out. Defaults to 1 second.
        Minimum value is 1. Must be greater or equal to period_seconds.
        Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        return pulumi.get(self, "timeout_seconds")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeExec(dict):
    def __init__(__self__, *,
                 commands: Optional[Sequence[_builtins.str]] = None):
        """
        :param Sequence[_builtins.str] commands: Command is the command line to execute inside the container, the working
               directory for the command is root ('/') in the container's filesystem.
               The command is simply exec'd, it is not run inside a shell, so
               traditional shell instructions ('|', etc) won't work. To use a shell, you
               need to explicitly call out to that shell. Exit status of 0 is treated as
               live/healthy and non-zero is unhealthy.
        """
        if commands is not None:
            pulumi.set(__self__, "commands", commands)

    @_builtins.property
    @pulumi.getter
    def commands(self) -> Optional[Sequence[_builtins.str]]:
        """
        Command is the command line to execute inside the container, the working
        directory for the command is root ('/') in the container's filesystem.
        The command is simply exec'd, it is not run inside a shell, so
        traditional shell instructions ('|', etc) won't work. To use a shell, you
        need to explicitly call out to that shell. Exit status of 0 is treated as
        live/healthy and non-zero is unhealthy.
        """
        return pulumi.get(self, "commands")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeGrpc(dict):
    def __init__(__self__, *,
                 port: Optional[_builtins.int] = None,
                 service: Optional[_builtins.str] = None):
        """
        :param _builtins.int port: Port number of the gRPC service. Number must be in the range 1 to 65535.
        :param _builtins.str service: Service is the name of the service to place in the gRPC
               HealthCheckRequest. See
               https://github.com/grpc/grpc/blob/master/doc/health-checking.md.
               If this is not specified, the default behavior is defined by gRPC.
        """
        if port is not None:
            pulumi.set(__self__, "port", port)
        if service is not None:
            pulumi.set(__self__, "service", service)

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Port number of the gRPC service. Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")

    @_builtins.property
    @pulumi.getter
    def service(self) -> Optional[_builtins.str]:
        """
        Service is the name of the service to place in the gRPC
        HealthCheckRequest. See
        https://github.com/grpc/grpc/blob/master/doc/health-checking.md.
        If this is not specified, the default behavior is defined by gRPC.
        """
        return pulumi.get(self, "service")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "httpHeaders":
            suggest = "http_headers"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGet.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 host: Optional[_builtins.str] = None,
                 http_headers: Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeader']] = None,
                 path: Optional[_builtins.str] = None,
                 port: Optional[_builtins.int] = None,
                 scheme: Optional[_builtins.str] = None):
        """
        :param _builtins.str host: Host name to connect to, defaults to the model serving container's IP.
               You probably want to set "Host" in httpHeaders instead.
        :param Sequence['AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeaderArgs'] http_headers: Custom headers to set in the request. HTTP allows repeated headers.
               Structure is documented below.
        :param _builtins.str path: Path to access on the HTTP server.
        :param _builtins.int port: Number of the port to access on the container.
               Number must be in the range 1 to 65535.
        :param _builtins.str scheme: Scheme to use for connecting to the host.
               Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".
        """
        if host is not None:
            pulumi.set(__self__, "host", host)
        if http_headers is not None:
            pulumi.set(__self__, "http_headers", http_headers)
        if path is not None:
            pulumi.set(__self__, "path", path)
        if port is not None:
            pulumi.set(__self__, "port", port)
        if scheme is not None:
            pulumi.set(__self__, "scheme", scheme)

    @_builtins.property
    @pulumi.getter
    def host(self) -> Optional[_builtins.str]:
        """
        Host name to connect to, defaults to the model serving container's IP.
        You probably want to set "Host" in httpHeaders instead.
        """
        return pulumi.get(self, "host")

    @_builtins.property
    @pulumi.getter(name="httpHeaders")
    def http_headers(self) -> Optional[Sequence['outputs.AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeader']]:
        """
        Custom headers to set in the request. HTTP allows repeated headers.
        Structure is documented below.
        """
        return pulumi.get(self, "http_headers")

    @_builtins.property
    @pulumi.getter
    def path(self) -> Optional[_builtins.str]:
        """
        Path to access on the HTTP server.
        """
        return pulumi.get(self, "path")

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Number of the port to access on the container.
        Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")

    @_builtins.property
    @pulumi.getter
    def scheme(self) -> Optional[_builtins.str]:
        """
        Scheme to use for connecting to the host.
        Defaults to HTTP. Acceptable values are "HTTP" or "HTTPS".
        """
        return pulumi.get(self, "scheme")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeHttpGetHttpHeader(dict):
    def __init__(__self__, *,
                 name: Optional[_builtins.str] = None,
                 value: Optional[_builtins.str] = None):
        """
        :param _builtins.str name: The header field name.
               This will be canonicalized upon output, so case-variant names will be
               understood as the same header.
        :param _builtins.str value: The header field value
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if value is not None:
            pulumi.set(__self__, "value", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> Optional[_builtins.str]:
        """
        The header field name.
        This will be canonicalized upon output, so case-variant names will be
        understood as the same header.
        """
        return pulumi.get(self, "name")

    @_builtins.property
    @pulumi.getter
    def value(self) -> Optional[_builtins.str]:
        """
        The header field value
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class AiEndpointWithModelGardenDeploymentModelConfigContainerSpecStartupProbeTcpSocket(dict):
    def __init__(__self__, *,
                 host: Optional[_builtins.str] = None,
                 port: Optional[_builtins.int] = None):
        """
        :param _builtins.str host: Optional: Host name to connect to, defaults to the model serving
               container's IP.
        :param _builtins.int port: Number of the port to access on the container.
               Number must be in the range 1 to 65535.
        """
        if host is not None:
            pulumi.set(__self__, "host", host)
        if port is not None:
            pulumi.set(__self__, "port", port)

    @_builtins.property
    @pulumi.getter
    def host(self) -> Optional[_builtins.str]:
        """
        Optional: Host name to connect to, defaults to the model serving
        container's IP.
        """
        return pulumi.get(self, "host")

    @_builtins.property
    @pulumi.getter
    def port(self) -> Optional[_builtins.int]:
        """
        Number of the port to access on the container.
        Number must be in the range 1 to 65535.
        """
        return pulumi.get(self, "port")


@pulumi.output_type
class AiFeatureGroupBigQuery(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigQuerySource":
            suggest = "big_query_source"
        elif key == "entityIdColumns":
            suggest = "entity_id_columns"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureGroupBigQuery. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureGroupBigQuery.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureGroupBigQuery.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 big_query_source: 'outputs.AiFeatureGroupBigQueryBigQuerySource',
                 entity_id_columns: Optional[Sequence[_builtins.str]] = None):
        """
        :param 'AiFeatureGroupBigQueryBigQuerySourceArgs' big_query_source: The BigQuery source URI that points to either a BigQuery Table or View.
               Structure is documented below.
        :param Sequence[_builtins.str] entity_id_columns: Columns to construct entityId / row keys. If not provided defaults to entityId.
        """
        pulumi.set(__self__, "big_query_source", big_query_source)
        if entity_id_columns is not None:
            pulumi.set(__self__, "entity_id_columns", entity_id_columns)

    @_builtins.property
    @pulumi.getter(name="bigQuerySource")
    def big_query_source(self) -> 'outputs.AiFeatureGroupBigQueryBigQuerySource':
        """
        The BigQuery source URI that points to either a BigQuery Table or View.
        Structure is documented below.
        """
        return pulumi.get(self, "big_query_source")

    @_builtins.property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> Optional[Sequence[_builtins.str]]:
        """
        Columns to construct entityId / row keys. If not provided defaults to entityId.
        """
        return pulumi.get(self, "entity_id_columns")


@pulumi.output_type
class AiFeatureGroupBigQueryBigQuerySource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "inputUri":
            suggest = "input_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureGroupBigQueryBigQuerySource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureGroupBigQueryBigQuerySource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureGroupBigQueryBigQuerySource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 input_uri: _builtins.str):
        """
        :param _builtins.str input_uri: BigQuery URI to a table, up to 2000 characters long. For example: `bq://projectId.bqDatasetId.bqTableId.`
        """
        pulumi.set(__self__, "input_uri", input_uri)

    @_builtins.property
    @pulumi.getter(name="inputUri")
    def input_uri(self) -> _builtins.str:
        """
        BigQuery URI to a table, up to 2000 characters long. For example: `bq://projectId.bqDatasetId.bqTableId.`
        """
        return pulumi.get(self, "input_uri")


@pulumi.output_type
class AiFeatureGroupIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureGroupIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureOnlineStoreBigtable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "autoScaling":
            suggest = "auto_scaling"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreBigtable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreBigtable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreBigtable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 auto_scaling: 'outputs.AiFeatureOnlineStoreBigtableAutoScaling'):
        """
        :param 'AiFeatureOnlineStoreBigtableAutoScalingArgs' auto_scaling: Autoscaling config applied to Bigtable Instance.
               Structure is documented below.
        """
        pulumi.set(__self__, "auto_scaling", auto_scaling)

    @_builtins.property
    @pulumi.getter(name="autoScaling")
    def auto_scaling(self) -> 'outputs.AiFeatureOnlineStoreBigtableAutoScaling':
        """
        Autoscaling config applied to Bigtable Instance.
        Structure is documented below.
        """
        return pulumi.get(self, "auto_scaling")


@pulumi.output_type
class AiFeatureOnlineStoreBigtableAutoScaling(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxNodeCount":
            suggest = "max_node_count"
        elif key == "minNodeCount":
            suggest = "min_node_count"
        elif key == "cpuUtilizationTarget":
            suggest = "cpu_utilization_target"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreBigtableAutoScaling. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreBigtableAutoScaling.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreBigtableAutoScaling.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_node_count: _builtins.int,
                 min_node_count: _builtins.int,
                 cpu_utilization_target: Optional[_builtins.int] = None):
        """
        :param _builtins.int max_node_count: The maximum number of nodes to scale up to. Must be greater than or equal to minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        :param _builtins.int min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        :param _builtins.int cpu_utilization_target: A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)
        if cpu_utilization_target is not None:
            pulumi.set(__self__, "cpu_utilization_target", cpu_utilization_target)

    @_builtins.property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> _builtins.int:
        """
        The maximum number of nodes to scale up to. Must be greater than or equal to minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        """
        return pulumi.get(self, "max_node_count")

    @_builtins.property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> _builtins.int:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")

    @_builtins.property
    @pulumi.getter(name="cpuUtilizationTarget")
    def cpu_utilization_target(self) -> Optional[_builtins.int]:
        """
        A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
        return pulumi.get(self, "cpu_utilization_target")


@pulumi.output_type
class AiFeatureOnlineStoreDedicatedServingEndpoint(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "privateServiceConnectConfig":
            suggest = "private_service_connect_config"
        elif key == "publicEndpointDomainName":
            suggest = "public_endpoint_domain_name"
        elif key == "serviceAttachment":
            suggest = "service_attachment"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreDedicatedServingEndpoint. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreDedicatedServingEndpoint.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreDedicatedServingEndpoint.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 private_service_connect_config: Optional['outputs.AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig'] = None,
                 public_endpoint_domain_name: Optional[_builtins.str] = None,
                 service_attachment: Optional[_builtins.str] = None):
        """
        :param 'AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfigArgs' private_service_connect_config: Private service connect config.
               Structure is documented below.
        :param _builtins.str public_endpoint_domain_name: (Output)
               Domain name to use for this FeatureOnlineStore
        :param _builtins.str service_attachment: (Output)
               Name of the service attachment resource. Applicable only if private service connect is enabled and after FeatureViewSync is created.
        """
        if private_service_connect_config is not None:
            pulumi.set(__self__, "private_service_connect_config", private_service_connect_config)
        if public_endpoint_domain_name is not None:
            pulumi.set(__self__, "public_endpoint_domain_name", public_endpoint_domain_name)
        if service_attachment is not None:
            pulumi.set(__self__, "service_attachment", service_attachment)

    @_builtins.property
    @pulumi.getter(name="privateServiceConnectConfig")
    def private_service_connect_config(self) -> Optional['outputs.AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig']:
        """
        Private service connect config.
        Structure is documented below.
        """
        return pulumi.get(self, "private_service_connect_config")

    @_builtins.property
    @pulumi.getter(name="publicEndpointDomainName")
    def public_endpoint_domain_name(self) -> Optional[_builtins.str]:
        """
        (Output)
        Domain name to use for this FeatureOnlineStore
        """
        return pulumi.get(self, "public_endpoint_domain_name")

    @_builtins.property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> Optional[_builtins.str]:
        """
        (Output)
        Name of the service attachment resource. Applicable only if private service connect is enabled and after FeatureViewSync is created.
        """
        return pulumi.get(self, "service_attachment")


@pulumi.output_type
class AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enablePrivateServiceConnect":
            suggest = "enable_private_service_connect"
        elif key == "projectAllowlists":
            suggest = "project_allowlists"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_private_service_connect: _builtins.bool,
                 project_allowlists: Optional[Sequence[_builtins.str]] = None):
        """
        :param _builtins.bool enable_private_service_connect: If set to true, customers will use private service connection to send request. Otherwise, the connection will set to public endpoint.
        :param Sequence[_builtins.str] project_allowlists: A list of Projects from which the forwarding rule will target the service attachment.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        if project_allowlists is not None:
            pulumi.set(__self__, "project_allowlists", project_allowlists)

    @_builtins.property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> _builtins.bool:
        """
        If set to true, customers will use private service connection to send request. Otherwise, the connection will set to public endpoint.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @_builtins.property
    @pulumi.getter(name="projectAllowlists")
    def project_allowlists(self) -> Optional[Sequence[_builtins.str]]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlists")


@pulumi.output_type
class AiFeatureOnlineStoreEmbeddingManagement(dict):
    def __init__(__self__, *,
                 enabled: Optional[_builtins.bool] = None):
        """
        :param _builtins.bool enabled: Enable embedding management.
        """
        if enabled is not None:
            pulumi.set(__self__, "enabled", enabled)

    @_builtins.property
    @pulumi.getter
    def enabled(self) -> Optional[_builtins.bool]:
        """
        Enable embedding management.
        """
        return pulumi.get(self, "enabled")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewBigQuerySource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "entityIdColumns":
            suggest = "entity_id_columns"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreFeatureviewBigQuerySource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreFeatureviewBigQuerySource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreFeatureviewBigQuerySource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 entity_id_columns: Sequence[_builtins.str],
                 uri: _builtins.str):
        """
        :param Sequence[_builtins.str] entity_id_columns: Columns to construct entityId / row keys. Start by supporting 1 only.
        :param _builtins.str uri: The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        pulumi.set(__self__, "entity_id_columns", entity_id_columns)
        pulumi.set(__self__, "uri", uri)

    @_builtins.property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> Sequence[_builtins.str]:
        """
        Columns to construct entityId / row keys. Start by supporting 1 only.
        """
        return pulumi.get(self, "entity_id_columns")

    @_builtins.property
    @pulumi.getter
    def uri(self) -> _builtins.str:
        """
        The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        return pulumi.get(self, "uri")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewFeatureRegistrySource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "featureGroups":
            suggest = "feature_groups"
        elif key == "projectNumber":
            suggest = "project_number"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreFeatureviewFeatureRegistrySource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreFeatureviewFeatureRegistrySource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreFeatureviewFeatureRegistrySource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 feature_groups: Sequence['outputs.AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup'],
                 project_number: Optional[_builtins.str] = None):
        """
        :param Sequence['AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroupArgs'] feature_groups: List of features that need to be synced to Online Store.
               Structure is documented below.
        :param _builtins.str project_number: The project number of the parent project of the feature Groups.
        """
        pulumi.set(__self__, "feature_groups", feature_groups)
        if project_number is not None:
            pulumi.set(__self__, "project_number", project_number)

    @_builtins.property
    @pulumi.getter(name="featureGroups")
    def feature_groups(self) -> Sequence['outputs.AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup']:
        """
        List of features that need to be synced to Online Store.
        Structure is documented below.
        """
        return pulumi.get(self, "feature_groups")

    @_builtins.property
    @pulumi.getter(name="projectNumber")
    def project_number(self) -> Optional[_builtins.str]:
        """
        The project number of the parent project of the feature Groups.
        """
        return pulumi.get(self, "project_number")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "featureGroupId":
            suggest = "feature_group_id"
        elif key == "featureIds":
            suggest = "feature_ids"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 feature_group_id: _builtins.str,
                 feature_ids: Sequence[_builtins.str]):
        """
        :param _builtins.str feature_group_id: Identifier of the feature group.
        :param Sequence[_builtins.str] feature_ids: Identifiers of features under the feature group.
        """
        pulumi.set(__self__, "feature_group_id", feature_group_id)
        pulumi.set(__self__, "feature_ids", feature_ids)

    @_builtins.property
    @pulumi.getter(name="featureGroupId")
    def feature_group_id(self) -> _builtins.str:
        """
        Identifier of the feature group.
        """
        return pulumi.get(self, "feature_group_id")

    @_builtins.property
    @pulumi.getter(name="featureIds")
    def feature_ids(self) -> Sequence[_builtins.str]:
        """
        Identifiers of features under the feature group.
        """
        return pulumi.get(self, "feature_ids")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewSyncConfig(dict):
    def __init__(__self__, *,
                 continuous: Optional[_builtins.bool] = None,
                 cron: Optional[_builtins.str] = None):
        """
        :param _builtins.bool continuous: If true, syncs the FeatureView in a continuous manner to Online Store.
        :param _builtins.str cron: Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs.
               To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
        """
        if continuous is not None:
            pulumi.set(__self__, "continuous", continuous)
        if cron is not None:
            pulumi.set(__self__, "cron", cron)

    @_builtins.property
    @pulumi.getter
    def continuous(self) -> Optional[_builtins.bool]:
        """
        If true, syncs the FeatureView in a continuous manner to Online Store.
        """
        return pulumi.get(self, "continuous")

    @_builtins.property
    @pulumi.getter
    def cron(self) -> Optional[_builtins.str]:
        """
        Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs.
        To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
        """
        return pulumi.get(self, "cron")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewVectorSearchConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "embeddingColumn":
            suggest = "embedding_column"
        elif key == "bruteForceConfig":
            suggest = "brute_force_config"
        elif key == "crowdingColumn":
            suggest = "crowding_column"
        elif key == "distanceMeasureType":
            suggest = "distance_measure_type"
        elif key == "embeddingDimension":
            suggest = "embedding_dimension"
        elif key == "filterColumns":
            suggest = "filter_columns"
        elif key == "treeAhConfig":
            suggest = "tree_ah_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreFeatureviewVectorSearchConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreFeatureviewVectorSearchConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreFeatureviewVectorSearchConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 embedding_column: _builtins.str,
                 brute_force_config: Optional['outputs.AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfig'] = None,
                 crowding_column: Optional[_builtins.str] = None,
                 distance_measure_type: Optional[_builtins.str] = None,
                 embedding_dimension: Optional[_builtins.int] = None,
                 filter_columns: Optional[Sequence[_builtins.str]] = None,
                 tree_ah_config: Optional['outputs.AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfig'] = None):
        """
        :param _builtins.str embedding_column: Column of embedding. This column contains the source data to create index for vector search.
        :param 'AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfigArgs' brute_force_config: Configuration options for using brute force search, which simply implements the standard linear search in the database for each query. It is primarily meant for benchmarking and to generate the ground truth for approximate search.
        :param _builtins.str crowding_column: Column of crowding. This column contains crowding attribute which is a constraint on a neighbor list produced by nearest neighbor search requiring that no more than some value k' of the k neighbors returned have the same value of crowdingAttribute.
        :param _builtins.str distance_measure_type: The distance measure used in nearest neighbor search.
               For details on allowed values, see the [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.featureOnlineStores.featureViews#DistanceMeasureType).
               Possible values are: `SQUARED_L2_DISTANCE`, `COSINE_DISTANCE`, `DOT_PRODUCT_DISTANCE`.
        :param _builtins.int embedding_dimension: The number of dimensions of the input embedding.
        :param Sequence[_builtins.str] filter_columns: Columns of features that are used to filter vector search results.
        :param 'AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfigArgs' tree_ah_config: Configuration options for the tree-AH algorithm (Shallow tree + Asymmetric Hashing). Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
               Structure is documented below.
        """
        pulumi.set(__self__, "embedding_column", embedding_column)
        if brute_force_config is not None:
            pulumi.set(__self__, "brute_force_config", brute_force_config)
        if crowding_column is not None:
            pulumi.set(__self__, "crowding_column", crowding_column)
        if distance_measure_type is not None:
            pulumi.set(__self__, "distance_measure_type", distance_measure_type)
        if embedding_dimension is not None:
            pulumi.set(__self__, "embedding_dimension", embedding_dimension)
        if filter_columns is not None:
            pulumi.set(__self__, "filter_columns", filter_columns)
        if tree_ah_config is not None:
            pulumi.set(__self__, "tree_ah_config", tree_ah_config)

    @_builtins.property
    @pulumi.getter(name="embeddingColumn")
    def embedding_column(self) -> _builtins.str:
        """
        Column of embedding. This column contains the source data to create index for vector search.
        """
        return pulumi.get(self, "embedding_column")

    @_builtins.property
    @pulumi.getter(name="bruteForceConfig")
    def brute_force_config(self) -> Optional['outputs.AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfig']:
        """
        Configuration options for using brute force search, which simply implements the standard linear search in the database for each query. It is primarily meant for benchmarking and to generate the ground truth for approximate search.
        """
        return pulumi.get(self, "brute_force_config")

    @_builtins.property
    @pulumi.getter(name="crowdingColumn")
    def crowding_column(self) -> Optional[_builtins.str]:
        """
        Column of crowding. This column contains crowding attribute which is a constraint on a neighbor list produced by nearest neighbor search requiring that no more than some value k' of the k neighbors returned have the same value of crowdingAttribute.
        """
        return pulumi.get(self, "crowding_column")

    @_builtins.property
    @pulumi.getter(name="distanceMeasureType")
    def distance_measure_type(self) -> Optional[_builtins.str]:
        """
        The distance measure used in nearest neighbor search.
        For details on allowed values, see the [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.featureOnlineStores.featureViews#DistanceMeasureType).
        Possible values are: `SQUARED_L2_DISTANCE`, `COSINE_DISTANCE`, `DOT_PRODUCT_DISTANCE`.
        """
        return pulumi.get(self, "distance_measure_type")

    @_builtins.property
    @pulumi.getter(name="embeddingDimension")
    def embedding_dimension(self) -> Optional[_builtins.int]:
        """
        The number of dimensions of the input embedding.
        """
        return pulumi.get(self, "embedding_dimension")

    @_builtins.property
    @pulumi.getter(name="filterColumns")
    def filter_columns(self) -> Optional[Sequence[_builtins.str]]:
        """
        Columns of features that are used to filter vector search results.
        """
        return pulumi.get(self, "filter_columns")

    @_builtins.property
    @pulumi.getter(name="treeAhConfig")
    def tree_ah_config(self) -> Optional['outputs.AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfig']:
        """
        Configuration options for the tree-AH algorithm (Shallow tree + Asymmetric Hashing). Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        Structure is documented below.
        """
        return pulumi.get(self, "tree_ah_config")


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewVectorSearchConfigBruteForceConfig(dict):
    def __init__(__self__):
        pass


@pulumi.output_type
class AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "leafNodeEmbeddingCount":
            suggest = "leaf_node_embedding_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureOnlineStoreFeatureviewVectorSearchConfigTreeAhConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 leaf_node_embedding_count: Optional[_builtins.str] = None):
        """
        :param _builtins.str leaf_node_embedding_count: Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        if leaf_node_embedding_count is not None:
            pulumi.set(__self__, "leaf_node_embedding_count", leaf_node_embedding_count)

    @_builtins.property
    @pulumi.getter(name="leafNodeEmbeddingCount")
    def leaf_node_embedding_count(self) -> Optional[_builtins.str]:
        """
        Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        return pulumi.get(self, "leaf_node_embedding_count")


@pulumi.output_type
class AiFeatureOnlineStoreIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureOnlineStoreIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureOnlineStoreOptimized(dict):
    def __init__(__self__):
        pass


@pulumi.output_type
class AiFeatureStoreEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureStoreEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureStoreEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureStoreEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: _builtins.str):
        """
        :param _builtins.str kms_key_name: The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> _builtins.str:
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class AiFeatureStoreEntityTypeIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureStoreEntityTypeIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureStoreEntityTypeMonitoringConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "categoricalThresholdConfig":
            suggest = "categorical_threshold_config"
        elif key == "importFeaturesAnalysis":
            suggest = "import_features_analysis"
        elif key == "numericalThresholdConfig":
            suggest = "numerical_threshold_config"
        elif key == "snapshotAnalysis":
            suggest = "snapshot_analysis"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureStoreEntityTypeMonitoringConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureStoreEntityTypeMonitoringConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureStoreEntityTypeMonitoringConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 categorical_threshold_config: Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfig'] = None,
                 import_features_analysis: Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis'] = None,
                 numerical_threshold_config: Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfig'] = None,
                 snapshot_analysis: Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis'] = None):
        """
        :param 'AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfigArgs' categorical_threshold_config: Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
               Structure is documented below.
        :param 'AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysisArgs' import_features_analysis: The config for ImportFeatures Analysis Based Feature Monitoring.
               Structure is documented below.
        :param 'AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfigArgs' numerical_threshold_config: Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
               Structure is documented below.
        :param 'AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysisArgs' snapshot_analysis: The config for Snapshot Analysis Based Feature Monitoring.
               Structure is documented below.
        """
        if categorical_threshold_config is not None:
            pulumi.set(__self__, "categorical_threshold_config", categorical_threshold_config)
        if import_features_analysis is not None:
            pulumi.set(__self__, "import_features_analysis", import_features_analysis)
        if numerical_threshold_config is not None:
            pulumi.set(__self__, "numerical_threshold_config", numerical_threshold_config)
        if snapshot_analysis is not None:
            pulumi.set(__self__, "snapshot_analysis", snapshot_analysis)

    @_builtins.property
    @pulumi.getter(name="categoricalThresholdConfig")
    def categorical_threshold_config(self) -> Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfig']:
        """
        Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
        Structure is documented below.
        """
        return pulumi.get(self, "categorical_threshold_config")

    @_builtins.property
    @pulumi.getter(name="importFeaturesAnalysis")
    def import_features_analysis(self) -> Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis']:
        """
        The config for ImportFeatures Analysis Based Feature Monitoring.
        Structure is documented below.
        """
        return pulumi.get(self, "import_features_analysis")

    @_builtins.property
    @pulumi.getter(name="numericalThresholdConfig")
    def numerical_threshold_config(self) -> Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfig']:
        """
        Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
        Structure is documented below.
        """
        return pulumi.get(self, "numerical_threshold_config")

    @_builtins.property
    @pulumi.getter(name="snapshotAnalysis")
    def snapshot_analysis(self) -> Optional['outputs.AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis']:
        """
        The config for Snapshot Analysis Based Feature Monitoring.
        Structure is documented below.
        """
        return pulumi.get(self, "snapshot_analysis")


@pulumi.output_type
class AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfig(dict):
    def __init__(__self__, *,
                 value: _builtins.float):
        """
        :param _builtins.float value: Specify a threshold value that can trigger the alert. For categorical feature, the distribution distance is calculated by L-inifinity norm. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        pulumi.set(__self__, "value", value)

    @_builtins.property
    @pulumi.getter
    def value(self) -> _builtins.float:
        """
        Specify a threshold value that can trigger the alert. For categorical feature, the distribution distance is calculated by L-inifinity norm. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "anomalyDetectionBaseline":
            suggest = "anomaly_detection_baseline"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 anomaly_detection_baseline: Optional[_builtins.str] = None,
                 state: Optional[_builtins.str] = None):
        """
        :param _builtins.str anomaly_detection_baseline: Defines the baseline to do anomaly detection for feature values imported by each [entityTypes.importFeatureValues][] operation. The value must be one of the values below:
               * LATEST_STATS: Choose the later one statistics generated by either most recent snapshot analysis or previous import features analysis. If non of them exists, skip anomaly detection and only generate a statistics.
               * MOST_RECENT_SNAPSHOT_STATS: Use the statistics generated by the most recent snapshot analysis if exists.
               * PREVIOUS_IMPORT_FEATURES_STATS: Use the statistics generated by the previous import features analysis if exists.
        :param _builtins.str state: Whether to enable / disable / inherite default hebavior for import features analysis. The value must be one of the values below:
               * DEFAULT: The default behavior of whether to enable the monitoring. EntityType-level config: disabled.
               * ENABLED: Explicitly enables import features analysis. EntityType-level config: by default enables import features analysis for all Features under it.
               * DISABLED: Explicitly disables import features analysis. EntityType-level config: by default disables import features analysis for all Features under it.
        """
        if anomaly_detection_baseline is not None:
            pulumi.set(__self__, "anomaly_detection_baseline", anomaly_detection_baseline)
        if state is not None:
            pulumi.set(__self__, "state", state)

    @_builtins.property
    @pulumi.getter(name="anomalyDetectionBaseline")
    def anomaly_detection_baseline(self) -> Optional[_builtins.str]:
        """
        Defines the baseline to do anomaly detection for feature values imported by each [entityTypes.importFeatureValues][] operation. The value must be one of the values below:
        * LATEST_STATS: Choose the later one statistics generated by either most recent snapshot analysis or previous import features analysis. If non of them exists, skip anomaly detection and only generate a statistics.
        * MOST_RECENT_SNAPSHOT_STATS: Use the statistics generated by the most recent snapshot analysis if exists.
        * PREVIOUS_IMPORT_FEATURES_STATS: Use the statistics generated by the previous import features analysis if exists.
        """
        return pulumi.get(self, "anomaly_detection_baseline")

    @_builtins.property
    @pulumi.getter
    def state(self) -> Optional[_builtins.str]:
        """
        Whether to enable / disable / inherite default hebavior for import features analysis. The value must be one of the values below:
        * DEFAULT: The default behavior of whether to enable the monitoring. EntityType-level config: disabled.
        * ENABLED: Explicitly enables import features analysis. EntityType-level config: by default enables import features analysis for all Features under it.
        * DISABLED: Explicitly disables import features analysis. EntityType-level config: by default disables import features analysis for all Features under it.
        """
        return pulumi.get(self, "state")


@pulumi.output_type
class AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfig(dict):
    def __init__(__self__, *,
                 value: _builtins.float):
        """
        :param _builtins.float value: Specify a threshold value that can trigger the alert. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        pulumi.set(__self__, "value", value)

    @_builtins.property
    @pulumi.getter
    def value(self) -> _builtins.float:
        """
        Specify a threshold value that can trigger the alert. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "monitoringInterval":
            suggest = "monitoring_interval"
        elif key == "monitoringIntervalDays":
            suggest = "monitoring_interval_days"
        elif key == "stalenessDays":
            suggest = "staleness_days"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 disabled: Optional[_builtins.bool] = None,
                 monitoring_interval: Optional[_builtins.str] = None,
                 monitoring_interval_days: Optional[_builtins.int] = None,
                 staleness_days: Optional[_builtins.int] = None):
        """
        :param _builtins.bool disabled: The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoringInterval for Features under it.
        :param _builtins.str monitoring_interval: Configuration of the snapshot analysis based monitoring pipeline running interval. The value is rolled up to full day.
               A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
               
               > **Warning:** `monitoring_interval` is deprecated and will be removed in a future release.
        :param _builtins.int monitoring_interval_days: Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days. The default value is 1.
               If both FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days and [FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval][] are set when creating/updating EntityTypes/Features, FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days will be used.
        :param _builtins.int staleness_days: Customized export features time window for snapshot analysis. Unit is one day. The default value is 21 days. Minimum value is 1 day. Maximum value is 4000 days.
        """
        if disabled is not None:
            pulumi.set(__self__, "disabled", disabled)
        if monitoring_interval is not None:
            pulumi.set(__self__, "monitoring_interval", monitoring_interval)
        if monitoring_interval_days is not None:
            pulumi.set(__self__, "monitoring_interval_days", monitoring_interval_days)
        if staleness_days is not None:
            pulumi.set(__self__, "staleness_days", staleness_days)

    @_builtins.property
    @pulumi.getter
    def disabled(self) -> Optional[_builtins.bool]:
        """
        The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoringInterval for Features under it.
        """
        return pulumi.get(self, "disabled")

    @_builtins.property
    @pulumi.getter(name="monitoringInterval")
    @_utilities.deprecated("""`monitoring_interval` is deprecated and will be removed in a future release.""")
    def monitoring_interval(self) -> Optional[_builtins.str]:
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value is rolled up to full day.
        A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".

        > **Warning:** `monitoring_interval` is deprecated and will be removed in a future release.
        """
        return pulumi.get(self, "monitoring_interval")

    @_builtins.property
    @pulumi.getter(name="monitoringIntervalDays")
    def monitoring_interval_days(self) -> Optional[_builtins.int]:
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days. The default value is 1.
        If both FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days and [FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval][] are set when creating/updating EntityTypes/Features, FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days will be used.
        """
        return pulumi.get(self, "monitoring_interval_days")

    @_builtins.property
    @pulumi.getter(name="stalenessDays")
    def staleness_days(self) -> Optional[_builtins.int]:
        """
        Customized export features time window for snapshot analysis. Unit is one day. The default value is 21 days. Minimum value is 1 day. Maximum value is 4000 days.
        """
        return pulumi.get(self, "staleness_days")


@pulumi.output_type
class AiFeatureStoreIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureStoreIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: _builtins.str,
                 title: _builtins.str,
                 description: Optional[_builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @_builtins.property
    @pulumi.getter
    def expression(self) -> _builtins.str:
        return pulumi.get(self, "expression")

    @_builtins.property
    @pulumi.getter
    def title(self) -> _builtins.str:
        return pulumi.get(self, "title")

    @_builtins.property
    @pulumi.getter
    def description(self) -> Optional[_builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class AiFeatureStoreOnlineServingConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "fixedNodeCount":
            suggest = "fixed_node_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureStoreOnlineServingConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureStoreOnlineServingConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureStoreOnlineServingConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 fixed_node_count: Optional[_builtins.int] = None,
                 scaling: Optional['outputs.AiFeatureStoreOnlineServingConfigScaling'] = None):
        """
        :param _builtins.int fixed_node_count: The number of nodes for each cluster. The number of nodes will not scale automatically but can be scaled manually by providing different values when updating.
        :param 'AiFeatureStoreOnlineServingConfigScalingArgs' scaling: Online serving scaling configuration. Only one of fixedNodeCount and scaling can be set. Setting one will reset the other.
               Structure is documented below.
        """
        if fixed_node_count is not None:
            pulumi.set(__self__, "fixed_node_count", fixed_node_count)
        if scaling is not None:
            pulumi.set(__self__, "scaling", scaling)

    @_builtins.property
    @pulumi.getter(name="fixedNodeCount")
    def fixed_node_count(self) -> Optional[_builtins.int]:
        """
        The number of nodes for each cluster. The number of nodes will not scale automatically but can be scaled manually by providing different values when updating.
        """
        return pulumi.get(self, "fixed_node_count")

    @_builtins.property
    @pulumi.getter
    def scaling(self) -> Optional['outputs.AiFeatureStoreOnlineServingConfigScaling']:
        """
        Online serving scaling configuration. Only one of fixedNodeCount and scaling can be set. Setting one will reset the other.
        Structure is documented below.
        """
        return pulumi.get(self, "scaling")


@pulumi.output_type
class AiFeatureStoreOnlineServingConfigScaling(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxNodeCount":
            suggest = "max_node_count"
        elif key == "minNodeCount":
            suggest = "min_node_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiFeatureStoreOnlineServingConfigScaling. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiFeatureStoreOnlineServingConfigScaling.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiFeatureStoreOnlineServingConfigScaling.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_node_count: _builtins.int,
                 min_node_count: _builtins.int):
        """
        :param _builtins.int max_node_count: The maximum number of nodes to scale up to. Must be greater than minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        :param _builtins.int min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)

    @_builtins.property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> _builtins.int:
        """
        The maximum number of nodes to scale up to. Must be greater than minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
        """
        return pulumi.get(self, "max_node_count")

    @_builtins.property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> _builtins.int:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")


@pulumi.output_type
class AiIndexDeployedIndex(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "deployedIndexId":
            suggest = "deployed_index_id"
        elif key == "indexEndpoint":
            suggest = "index_endpoint"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexDeployedIndex. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexDeployedIndex.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexDeployedIndex.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 deployed_index_id: Optional[_builtins.str] = None,
                 index_endpoint: Optional[_builtins.str] = None):
        """
        :param _builtins.str deployed_index_id: (Output)
               The ID of the DeployedIndex in the above IndexEndpoint.
        :param _builtins.str index_endpoint: (Output)
               A resource name of the IndexEndpoint.
        """
        if deployed_index_id is not None:
            pulumi.set(__self__, "deployed_index_id", deployed_index_id)
        if index_endpoint is not None:
            pulumi.set(__self__, "index_endpoint", index_endpoint)

    @_builtins.property
    @pulumi.getter(name="deployedIndexId")
    def deployed_index_id(self) -> Optional[_builtins.str]:
        """
        (Output)
        The ID of the DeployedIndex in the above IndexEndpoint.
        """
        return pulumi.get(self, "deployed_index_id")

    @_builtins.property
    @pulumi.getter(name="indexEndpoint")
    def index_endpoint(self) -> Optional[_builtins.str]:
        """
        (Output)
        A resource name of the IndexEndpoint.
        """
        return pulumi.get(self, "index_endpoint")


@pulumi.output_type
class AiIndexEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: _builtins.str):
        """
        :param _builtins.str kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> _builtins.str:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class AiIndexEndpointDeployedIndexAutomaticResources(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxReplicaCount":
            suggest = "max_replica_count"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointDeployedIndexAutomaticResources. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointDeployedIndexAutomaticResources.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointDeployedIndexAutomaticResources.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_replica_count: Optional[_builtins.int] = None,
                 min_replica_count: Optional[_builtins.int] = None):
        """
        :param _builtins.int max_replica_count: The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If maxReplicaCount is not set, the default value is minReplicaCount. The max allowed replica count is 1000.
               The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        :param _builtins.int min_replica_count: The minimum number of replicas this DeployedModel will be always deployed on. If minReplicaCount is not set, the default value is 2 (we don't provide SLA when minReplicaCount=1).
               If traffic against it increases, it may dynamically be deployed onto more replicas up to [maxReplicaCount](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/AutomaticResources#FIELDS.max_replica_count), and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)
        if min_replica_count is not None:
            pulumi.set(__self__, "min_replica_count", min_replica_count)

    @_builtins.property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[_builtins.int]:
        """
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If maxReplicaCount is not set, the default value is minReplicaCount. The max allowed replica count is 1000.
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        return pulumi.get(self, "max_replica_count")

    @_builtins.property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> Optional[_builtins.int]:
        """
        The minimum number of replicas this DeployedModel will be always deployed on. If minReplicaCount is not set, the default value is 2 (we don't provide SLA when minReplicaCount=1).
        If traffic against it increases, it may dynamically be deployed onto more replicas up to [maxReplicaCount](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/AutomaticResources#FIELDS.max_replica_count), and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        return pulumi.get(self, "min_replica_count")


@pulumi.output_type
class AiIndexEndpointDeployedIndexDedicatedResources(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "machineSpec":
            suggest = "machine_spec"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"
        elif key == "maxReplicaCount":
            suggest = "max_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointDeployedIndexDedicatedResources. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointDeployedIndexDedicatedResources.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointDeployedIndexDedicatedResources.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 machine_spec: 'outputs.AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec',
                 min_replica_count: _builtins.int,
                 max_replica_count: Optional[_builtins.int] = None):
        """
        :param 'AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpecArgs' machine_spec: The minimum number of replicas this DeployedModel will be always deployed on.
               Structure is documented below.
        :param _builtins.int min_replica_count: The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1.
        :param _builtins.int max_replica_count: The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If maxReplicaCount is not set, the default value is minReplicaCount
        """
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "min_replica_count", min_replica_count)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)

    @_builtins.property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> 'outputs.AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec':
        """
        The minimum number of replicas this DeployedModel will be always deployed on.
        Structure is documented below.
        """
        return pulumi.get(self, "machine_spec")

    @_builtins.property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> _builtins.int:
        """
        The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_replica_count")

    @_builtins.property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[_builtins.int]:
        """
        The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If maxReplicaCount is not set, the default value is minReplicaCount
        """
        return pulumi.get(self, "max_replica_count")


@pulumi.output_type
class AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "machineType":
            suggest = "machine_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 machine_type: Optional[_builtins.str] = None):
        """
        :param _builtins.str machine_type: The type of the machine.
               See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
               See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
               For [DeployedModel](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#DeployedModel) this field is optional, and the default value is n1-standard-2. For [BatchPredictionJob](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchPredictionJob) or as part of [WorkerPoolSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#WorkerPoolSpec) this field is required.
        """
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)

    @_builtins.property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[_builtins.str]:
        """
        The type of the machine.
        See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
        See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
        For [DeployedModel](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#DeployedModel) this field is optional, and the default value is n1-standard-2. For [BatchPredictionJob](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchPredictionJob) or as part of [WorkerPoolSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#WorkerPoolSpec) this field is required.
        """
        return pulumi.get(self, "machine_type")


@pulumi.output_type
class AiIndexEndpointDeployedIndexDeployedIndexAuthConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "authProvider":
            suggest = "auth_provider"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointDeployedIndexDeployedIndexAuthConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointDeployedIndexDeployedIndexAuthConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointDeployedIndexDeployedIndexAuthConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 auth_provider: Optional['outputs.AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider'] = None):
        """
        :param 'AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProviderArgs' auth_provider: Defines the authentication provider that the DeployedIndex uses.
               Structure is documented below.
        """
        if auth_provider is not None:
            pulumi.set(__self__, "auth_provider", auth_provider)

    @_builtins.property
    @pulumi.getter(name="authProvider")
    def auth_provider(self) -> Optional['outputs.AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider']:
        """
        Defines the authentication provider that the DeployedIndex uses.
        Structure is documented below.
        """
        return pulumi.get(self, "auth_provider")


@pulumi.output_type
class AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "allowedIssuers":
            suggest = "allowed_issuers"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 allowed_issuers: Optional[Sequence[_builtins.str]] = None,
                 audiences: Optional[Sequence[_builtins.str]] = None):
        """
        :param Sequence[_builtins.str] allowed_issuers: A list of allowed JWT issuers. Each entry must be a valid Google service account, in the following format: service-account-name@project-id.iam.gserviceaccount.com
        :param Sequence[_builtins.str] audiences: The list of JWT audiences. that are allowed to access. A JWT containing any of these audiences will be accepted.
        """
        if allowed_issuers is not None:
            pulumi.set(__self__, "allowed_issuers", allowed_issuers)
        if audiences is not None:
            pulumi.set(__self__, "audiences", audiences)

    @_builtins.property
    @pulumi.getter(name="allowedIssuers")
    def allowed_issuers(self) -> Optional[Sequence[_builtins.str]]:
        """
        A list of allowed JWT issuers. Each entry must be a valid Google service account, in the following format: service-account-name@project-id.iam.gserviceaccount.com
        """
        return pulumi.get(self, "allowed_issuers")

    @_builtins.property
    @pulumi.getter
    def audiences(self) -> Optional[Sequence[_builtins.str]]:
        """
        The list of JWT audiences. that are allowed to access. A JWT containing any of these audiences will be accepted.
        """
        return pulumi.get(self, "audiences")


@pulumi.output_type
class AiIndexEndpointDeployedIndexPrivateEndpoint(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "matchGrpcAddress":
            suggest = "match_grpc_address"
        elif key == "pscAutomatedEndpoints":
            suggest = "psc_automated_endpoints"
        elif key == "serviceAttachment":
            suggest = "service_attachment"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointDeployedIndexPrivateEndpoint. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointDeployedIndexPrivateEndpoint.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointDeployedIndexPrivateEndpoint.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 match_grpc_address: Optional[_builtins.str] = None,
                 psc_automated_endpoints: Optional[Sequence['outputs.AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint']] = None,
                 service_attachment: Optional[_builtins.str] = None):
        """
        :param _builtins.str match_grpc_address: (Output)
               The ip address used to send match gRPC requests.
        :param Sequence['AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpointArgs'] psc_automated_endpoints: (Output)
               PscAutomatedEndpoints is populated if private service connect is enabled if PscAutomatedConfig is set.
               Structure is documented below.
        :param _builtins.str service_attachment: (Output)
               The name of the service attachment resource. Populated if private service connect is enabled.
        """
        if match_grpc_address is not None:
            pulumi.set(__self__, "match_grpc_address", match_grpc_address)
        if psc_automated_endpoints is not None:
            pulumi.set(__self__, "psc_automated_endpoints", psc_automated_endpoints)
        if service_attachment is not None:
            pulumi.set(__self__, "service_attachment", service_attachment)

    @_builtins.property
    @pulumi.getter(name="matchGrpcAddress")
    def match_grpc_address(self) -> Optional[_builtins.str]:
        """
        (Output)
        The ip address used to send match gRPC requests.
        """
        return pulumi.get(self, "match_grpc_address")

    @_builtins.property
    @pulumi.getter(name="pscAutomatedEndpoints")
    def psc_automated_endpoints(self) -> Optional[Sequence['outputs.AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint']]:
        """
        (Output)
        PscAutomatedEndpoints is populated if private service connect is enabled if PscAutomatedConfig is set.
        Structure is documented below.
        """
        return pulumi.get(self, "psc_automated_endpoints")

    @_builtins.property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> Optional[_builtins.str]:
        """
        (Output)
        The name of the service attachment resource. Populated if private service connect is enabled.
        """
        return pulumi.get(self, "service_attachment")


@pulumi.output_type
class AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "matchAddress":
            suggest = "match_address"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 match_address: Optional[_builtins.str] = None,
                 network: Optional[_builtins.str] = None,
                 project_id: Optional[_builtins.str] = None):
        """
        :param _builtins.str match_address: (Output)
               ip Address created by the automated forwarding rule.
        :param _builtins.str network: (Output)
               Corresponding network in pscAutomationConfigs.
        :param _builtins.str project_id: (Output)
               Corresponding projectId in pscAutomationConfigs
        """
        if match_address is not None:
            pulumi.set(__self__, "match_address", match_address)
        if network is not None:
            pulumi.set(__self__, "network", network)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)

    @_builtins.property
    @pulumi.getter(name="matchAddress")
    def match_address(self) -> Optional[_builtins.str]:
        """
        (Output)
        ip Address created by the automated forwarding rule.
        """
        return pulumi.get(self, "match_address")

    @_builtins.property
    @pulumi.getter
    def network(self) -> Optional[_builtins.str]:
        """
        (Output)
        Corresponding network in pscAutomationConfigs.
        """
        return pulumi.get(self, "network")

    @_builtins.property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[_builtins.str]:
        """
        (Output)
        Corresponding projectId in pscAutomationConfigs
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class AiIndexEndpointEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: _builtins.str):
        """
        :param _builtins.str kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> _builtins.str:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class AiIndexEndpointPrivateServiceConnectConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enablePrivateServiceConnect":
            suggest = "enable_private_service_connect"
        elif key == "projectAllowlists":
            suggest = "project_allowlists"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexEndpointPrivateServiceConnectConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexEndpointPrivateServiceConnectConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexEndpointPrivateServiceConnectConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_private_service_connect: _builtins.bool,
                 project_allowlists: Optional[Sequence[_builtins.str]] = None):
        """
        :param _builtins.bool enable_private_service_connect: If set to true, the IndexEndpoint is created without private service access.
        :param Sequence[_builtins.str] project_allowlists: A list of Projects from which the forwarding rule will target the service attachment.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        if project_allowlists is not None:
            pulumi.set(__self__, "project_allowlists", project_allowlists)

    @_builtins.property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> _builtins.bool:
        """
        If set to true, the IndexEndpoint is created without private service access.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @_builtins.property
    @pulumi.getter(name="projectAllowlists")
    def project_allowlists(self) -> Optional[Sequence[_builtins.str]]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlists")


@pulumi.output_type
class AiIndexIndexStat(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "shardsCount":
            suggest = "shards_count"
        elif key == "vectorsCount":
            suggest = "vectors_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexIndexStat. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexIndexStat.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexIndexStat.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 shards_count: Optional[_builtins.int] = None,
                 vectors_count: Optional[_builtins.str] = None):
        """
        :param _builtins.int shards_count: (Output)
               The number of shards in the Index.
        :param _builtins.str vectors_count: (Output)
               The number of vectors in the Index.
        """
        if shards_count is not None:
            pulumi.set(__self__, "shards_count", shards_count)
        if vectors_count is not None:
            pulumi.set(__self__, "vectors_count", vectors_count)

    @_builtins.property
    @pulumi.getter(name="shardsCount")
    def shards_count(self) -> Optional[_builtins.int]:
        """
        (Output)
        The number of shards in the Index.
        """
        return pulumi.get(self, "shards_count")

    @_builtins.property
    @pulumi.getter(name="vectorsCount")
    def vectors_count(self) -> Optional[_builtins.str]:
        """
        (Output)
        The number of vectors in the Index.
        """
        return pulumi.get(self, "vectors_count")


@pulumi.output_type
class AiIndexMetadata(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "contentsDeltaUri":
            suggest = "contents_delta_uri"
        elif key == "isCompleteOverwrite":
            suggest = "is_complete_overwrite"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexMetadata. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexMetadata.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexMetadata.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 config: 'outputs.AiIndexMetadataConfig',
                 contents_delta_uri: Optional[_builtins.str] = None,
                 is_complete_overwrite: Optional[_builtins.bool] = None):
        """
        :param 'AiIndexMetadataConfigArgs' config: The configuration of the Matching Engine Index.
               Structure is documented below.
        :param _builtins.str contents_delta_uri: Allows inserting, updating  or deleting the contents of the Matching Engine Index.
               The string must be a valid Cloud Storage directory path. If this
               field is set when calling IndexService.UpdateIndex, then no other
               Index field can be also updated as part of the same call.
               The expected structure and format of the files this URI points to is
               described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
        :param _builtins.bool is_complete_overwrite: If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
               then existing content of the Index will be replaced by the data from the contentsDeltaUri.
        """
        pulumi.set(__self__, "config", config)
        if contents_delta_uri is not None:
            pulumi.set(__self__, "contents_delta_uri", contents_delta_uri)
        if is_complete_overwrite is not None:
            pulumi.set(__self__, "is_complete_overwrite", is_complete_overwrite)

    @_builtins.property
    @pulumi.getter
    def config(self) -> 'outputs.AiIndexMetadataConfig':
        """
        The configuration of the Matching Engine Index.
        Structure is documented below.
        """
        return pulumi.get(self, "config")

    @_builtins.property
    @pulumi.getter(name="contentsDeltaUri")
    def contents_delta_uri(self) -> Optional[_builtins.str]:
        """
        Allows inserting, updating  or deleting the contents of the Matching Engine Index.
        The string must be a valid Cloud Storage directory path. If this
        field is set when calling IndexService.UpdateIndex, then no other
        Index field can be also updated as part of the same call.
        The expected structure and format of the files this URI points to is
        described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
        """
        return pulumi.get(self, "contents_delta_uri")

    @_builtins.property
    @pulumi.getter(name="isCompleteOverwrite")
    def is_complete_overwrite(self) -> Optional[_builtins.bool]:
        """
        If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
        then existing content of the Index will be replaced by the data from the contentsDeltaUri.
        """
        return pulumi.get(self, "is_complete_overwrite")


@pulumi.output_type
class AiIndexMetadataConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "algorithmConfig":
            suggest = "algorithm_config"
        elif key == "approximateNeighborsCount":
            suggest = "approximate_neighbors_count"
        elif key == "distanceMeasureType":
            suggest = "distance_measure_type"
        elif key == "featureNormType":
            suggest = "feature_norm_type"
        elif key == "shardSize":
            suggest = "shard_size"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexMetadataConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexMetadataConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexMetadataConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dimensions: _builtins.int,
                 algorithm_config: Optional['outputs.AiIndexMetadataConfigAlgorithmConfig'] = None,
                 approximate_neighbors_count: Optional[_builtins.int] = None,
                 distance_measure_type: Optional[_builtins.str] = None,
                 feature_norm_type: Optional[_builtins.str] = None,
                 shard_size: Optional[_builtins.str] = None):
        """
        :param _builtins.int dimensions: The number of dimensions of the input vectors.
        :param 'AiIndexMetadataConfigAlgorithmConfigArgs' algorithm_config: The configuration with regard to the algorithms used for efficient search. This field may be required based on your configuration.
               Structure is documented below.
        :param _builtins.int approximate_neighbors_count: The default number of neighbors to find via approximate search before exact reordering is
               performed. Exact reordering is a procedure where results returned by an
               approximate search algorithm are reordered via a more expensive distance computation.
               Required if tree-AH algorithm is used.
        :param _builtins.str distance_measure_type: The distance measure used in nearest neighbor search. The value must be one of the followings:
               * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
               * L1_DISTANCE: Manhattan (L_1) Distance
               * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
               * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
        :param _builtins.str feature_norm_type: Type of normalization to be carried out on each vector. The value must be one of the followings:
               * UNIT_L2_NORM: Unit L2 normalization type
               * NONE: No normalization type is specified.
        :param _builtins.str shard_size: Index data is split into equal parts to be processed. These are called "shards".
               The shard size must be specified when creating an index. The value must be one of the followings:
               * SHARD_SIZE_SMALL: Small (2GB)
               * SHARD_SIZE_MEDIUM: Medium (20GB)
               * SHARD_SIZE_LARGE: Large (50GB)
        """
        pulumi.set(__self__, "dimensions", dimensions)
        if algorithm_config is not None:
            pulumi.set(__self__, "algorithm_config", algorithm_config)
        if approximate_neighbors_count is not None:
            pulumi.set(__self__, "approximate_neighbors_count", approximate_neighbors_count)
        if distance_measure_type is not None:
            pulumi.set(__self__, "distance_measure_type", distance_measure_type)
        if feature_norm_type is not None:
            pulumi.set(__self__, "feature_norm_type", feature_norm_type)
        if shard_size is not None:
            pulumi.set(__self__, "shard_size", shard_size)

    @_builtins.property
    @pulumi.getter
    def dimensions(self) -> _builtins.int:
        """
        The number of dimensions of the input vectors.
        """
        return pulumi.get(self, "dimensions")

    @_builtins.property
    @pulumi.getter(name="algorithmConfig")
    def algorithm_config(self) -> Optional['outputs.AiIndexMetadataConfigAlgorithmConfig']:
        """
        The configuration with regard to the algorithms used for efficient search. This field may be required based on your configuration.
        Structure is documented below.
        """
        return pulumi.get(self, "algorithm_config")

    @_builtins.property
    @pulumi.getter(name="approximateNeighborsCount")
    def approximate_neighbors_count(self) -> Optional[_builtins.int]:
        """
        The default number of neighbors to find via approximate search before exact reordering is
        performed. Exact reordering is a procedure where results returned by an
        approximate search algorithm are reordered via a more expensive distance computation.
        Required if tree-AH algorithm is used.
        """
        return pulumi.get(self, "approximate_neighbors_count")

    @_builtins.property
    @pulumi.getter(name="distanceMeasureType")
    def distance_measure_type(self) -> Optional[_builtins.str]:
        """
        The distance measure used in nearest neighbor search. The value must be one of the followings:
        * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
        * L1_DISTANCE: Manhattan (L_1) Distance
        * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
        * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
        """
        return pulumi.get(self, "distance_measure_type")

    @_builtins.property
    @pulumi.getter(name="featureNormType")
    def feature_norm_type(self) -> Optional[_builtins.str]:
        """
        Type of normalization to be carried out on each vector. The value must be one of the followings:
        * UNIT_L2_NORM: Unit L2 normalization type
        * NONE: No normalization type is specified.
        """
        return pulumi.get(self, "feature_norm_type")

    @_builtins.property
    @pulumi.getter(name="shardSize")
    def shard_size(self) -> Optional[_builtins.str]:
        """
        Index data is split into equal parts to be processed. These are called "shards".
        The shard size must be specified when creating an index. The value must be one of the followings:
        * SHARD_SIZE_SMALL: Small (2GB)
        * SHARD_SIZE_MEDIUM: Medium (20GB)
        * SHARD_SIZE_LARGE: Large (50GB)
        """
        return pulumi.get(self, "shard_size")


@pulumi.output_type
class AiIndexMetadataConfigAlgorithmConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bruteForceConfig":
            suggest = "brute_force_config"
        elif key == "treeAhConfig":
            suggest = "tree_ah_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexMetadataConfigAlgorithmConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexMetadataConfigAlgorithmConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexMetadataConfigAlgorithmConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 brute_force_config: Optional['outputs.AiIndexMetadataConfigAlgorithmConfigBruteForceConfig'] = None,
                 tree_ah_config: Optional['outputs.AiIndexMetadataConfigAlgorithmConfigTreeAhConfig'] = None):
        """
        :param 'AiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs' brute_force_config: Configuration options for using brute force search, which simply implements the
               standard linear search in the database for each query.
        :param 'AiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs' tree_ah_config: Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
               Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
               Structure is documented below.
        """
        if brute_force_config is not None:
            pulumi.set(__self__, "brute_force_config", brute_force_config)
        if tree_ah_config is not None:
            pulumi.set(__self__, "tree_ah_config", tree_ah_config)

    @_builtins.property
    @pulumi.getter(name="bruteForceConfig")
    def brute_force_config(self) -> Optional['outputs.AiIndexMetadataConfigAlgorithmConfigBruteForceConfig']:
        """
        Configuration options for using brute force search, which simply implements the
        standard linear search in the database for each query.
        """
        return pulumi.get(self, "brute_force_config")

    @_builtins.property
    @pulumi.getter(name="treeAhConfig")
    def tree_ah_config(self) -> Optional['outputs.AiIndexMetadataConfigAlgorithmConfigTreeAhConfig']:
        """
        Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
        Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        Structure is documented below.
        """
        return pulumi.get(self, "tree_ah_config")


@pulumi.output_type
class AiIndexMetadataConfigAlgorithmConfigBruteForceConfig(dict):
    def __init__(__self__):
        pass


@pulumi.output_type
class AiIndexMetadataConfigAlgorithmConfigTreeAhConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "leafNodeEmbeddingCount":
            suggest = "leaf_node_embedding_count"
        elif key == "leafNodesToSearchPercent":
            suggest = "leaf_nodes_to_search_percent"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiIndexMetadataConfigAlgorithmConfigTreeAhConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiIndexMetadataConfigAlgorithmConfigTreeAhConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiIndexMetadataConfigAlgorithmConfigTreeAhConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 leaf_node_embedding_count: Optional[_builtins.int] = None,
                 leaf_nodes_to_search_percent: Optional[_builtins.int] = None):
        """
        :param _builtins.int leaf_node_embedding_count: Number of embeddings on each leaf node. The default value is 1000 if not set.
        :param _builtins.int leaf_nodes_to_search_percent: The default percentage of leaf nodes that any query may be searched. Must be in
               range 1-100, inclusive. The default value is 10 (means 10%) if not set.
        """
        if leaf_node_embedding_count is not None:
            pulumi.set(__self__, "leaf_node_embedding_count", leaf_node_embedding_count)
        if leaf_nodes_to_search_percent is not None:
            pulumi.set(__self__, "leaf_nodes_to_search_percent", leaf_nodes_to_search_percent)

    @_builtins.property
    @pulumi.getter(name="leafNodeEmbeddingCount")
    def leaf_node_embedding_count(self) -> Optional[_builtins.int]:
        """
        Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        return pulumi.get(self, "leaf_node_embedding_count")

    @_builtins.property
    @pulumi.getter(name="leafNodesToSearchPercent")
    def leaf_nodes_to_search_percent(self) -> Optional[_builtins.int]:
        """
        The default percentage of leaf nodes that any query may be searched. Must be in
        range 1-100, inclusive. The default value is 10 (means 10%) if not set.
        """
        return pulumi.get(self, "leaf_nodes_to_search_percent")


@pulumi.output_type
class AiMetadataStoreEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiMetadataStoreEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiMetadataStoreEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiMetadataStoreEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: Optional[_builtins.str] = None):
        """
        :param _builtins.str kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
               Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        if kms_key_name is not None:
            pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> Optional[_builtins.str]:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class AiMetadataStoreState(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "diskUtilizationBytes":
            suggest = "disk_utilization_bytes"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiMetadataStoreState. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiMetadataStoreState.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiMetadataStoreState.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 disk_utilization_bytes: Optional[_builtins.str] = None):
        """
        :param _builtins.str disk_utilization_bytes: (Output)
               The disk utilization of the MetadataStore in bytes.
        """
        if disk_utilization_bytes is not None:
            pulumi.set(__self__, "disk_utilization_bytes", disk_utilization_bytes)

    @_builtins.property
    @pulumi.getter(name="diskUtilizationBytes")
    def disk_utilization_bytes(self) -> Optional[_builtins.str]:
        """
        (Output)
        The disk utilization of the MetadataStore in bytes.
        """
        return pulumi.get(self, "disk_utilization_bytes")


@pulumi.output_type
class AiRagEngineConfigRagManagedDbConfig(dict):
    def __init__(__self__, *,
                 basic: Optional['outputs.AiRagEngineConfigRagManagedDbConfigBasic'] = None,
                 scaled: Optional['outputs.AiRagEngineConfigRagManagedDbConfigScaled'] = None,
                 unprovisioned: Optional['outputs.AiRagEngineConfigRagManagedDbConfigUnprovisioned'] = None):
        """
        :param 'AiRagEngineConfigRagManagedDbConfigBasicArgs' basic: Basic tier is a cost-effective and low compute tier suitable for the following cases: Experimenting with RagManagedDb, Small data size, Latency insensitive workload, Only using RAG Engine with external vector DBs.
               NOTE: This is the default tier if not explicitly chosen.
        :param 'AiRagEngineConfigRagManagedDbConfigScaledArgs' scaled: Scaled tier offers production grade performance along with autoscaling functionality. It is suitable for customers with large amounts of data or performance sensitive workloads.
        :param 'AiRagEngineConfigRagManagedDbConfigUnprovisionedArgs' unprovisioned: Disables the RAG Engine service and deletes all your data held within this service. This will halt the billing of the service.
               NOTE: Once deleted the data cannot be recovered. To start using RAG Engine again, you will need to update the tier by calling the UpdateRagEngineConfig API.
        """
        if basic is not None:
            pulumi.set(__self__, "basic", basic)
        if scaled is not None:
            pulumi.set(__self__, "scaled", scaled)
        if unprovisioned is not None:
            pulumi.set(__self__, "unprovisioned", unprovisioned)

    @_builtins.property
    @pulumi.getter
    def basic(self) -> Optional['outputs.AiRagEngineConfigRagManagedDbConfigBasic']:
        """
        Basic tier is a cost-effective and low compute tier suitable for the following cases: Experimenting with RagManagedDb, Small data size, Latency insensitive workload, Only using RAG Engine with external vector DBs.
        NOTE: This is the default tier if not explicitly chosen.
        """
        return pulumi.get(self, "basic")

    @_builtins.property
    @pulumi.getter
    def scaled(self) -> Optional['outputs.AiRagEngineConfigRagManagedDbConfigScaled']:
        """
        Scaled tier offers production grade performance along with autoscaling functionality. It is suitable for customers with large amounts of data or performance sensitive workloads.
        """
        return pulumi.get(self, "scaled")

    @_builtins.property
    @pulumi.getter
    def unprovisioned(self) -> Optional['outputs.AiRagEngineConfigRagManagedDbConfigUnprovisioned']:
        """
        Disables the RAG Engine service and deletes all your data held within this service. This will halt the billing of the service.
        NOTE: Once deleted the data cannot be recovered. To start using RAG Engine again, you will need to update the tier by calling the UpdateRagEngineConfig API.
        """
        return pulumi.get(self, "unprovisioned")


@pulumi.output_type
class AiRagEngineConfigRagManagedDbConfigBasic(dict):
    def __init__(__self__):
        pass


@pulumi.output_type
class AiRagEngineConfigRagManagedDbConfigScaled(dict):
    def __init__(__self__):
        pass


@pulumi.output_type
class AiRagEngineConfigRagManagedDbConfigUnprovisioned(dict):
    def __init__(__self__):
        pass


@pulumi.output_type
class AiReasoningEngineEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: _builtins.str):
        """
        :param _builtins.str kms_key_name: Required. The Cloud KMS resource identifier of the customer managed
               encryption key used to protect a resource. Has the form:
               projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key.
               The key needs to be in the same region as where the compute resource
               is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> _builtins.str:
        """
        Required. The Cloud KMS resource identifier of the customer managed
        encryption key used to protect a resource. Has the form:
        projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key.
        The key needs to be in the same region as where the compute resource
        is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class AiReasoningEngineSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "agentFramework":
            suggest = "agent_framework"
        elif key == "classMethods":
            suggest = "class_methods"
        elif key == "deploymentSpec":
            suggest = "deployment_spec"
        elif key == "packageSpec":
            suggest = "package_spec"
        elif key == "serviceAccount":
            suggest = "service_account"
        elif key == "sourceCodeSpec":
            suggest = "source_code_spec"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 agent_framework: Optional[_builtins.str] = None,
                 class_methods: Optional[_builtins.str] = None,
                 deployment_spec: Optional['outputs.AiReasoningEngineSpecDeploymentSpec'] = None,
                 package_spec: Optional['outputs.AiReasoningEngineSpecPackageSpec'] = None,
                 service_account: Optional[_builtins.str] = None,
                 source_code_spec: Optional['outputs.AiReasoningEngineSpecSourceCodeSpec'] = None):
        """
        :param _builtins.str agent_framework: Optional. The OSS agent framework used to develop the agent.
        :param _builtins.str class_methods: Optional. Declarations for object class methods in OpenAPI
               specification format.
        :param 'AiReasoningEngineSpecDeploymentSpecArgs' deployment_spec: Optional. The specification of a Reasoning Engine deployment.
               Structure is documented below.
        :param 'AiReasoningEngineSpecPackageSpecArgs' package_spec: Optional. User provided package spec of the ReasoningEngine.
               Ignored when users directly specify a deployment image through
               deploymentSpec.first_party_image_override, but keeping the
               field_behavior to avoid introducing breaking changes.
               Structure is documented below.
        :param _builtins.str service_account: Optional. The service account that the Reasoning Engine artifact runs
               as. It should have "roles/storage.objectViewer" for reading the user
               project's Cloud Storage and "roles/aiplatform.user" for using Vertex
               extensions. If not specified, the Vertex AI Reasoning Engine service
               Agent in the project will be used.
        :param 'AiReasoningEngineSpecSourceCodeSpecArgs' source_code_spec: Specification for deploying from source code.
               Structure is documented below.
        """
        if agent_framework is not None:
            pulumi.set(__self__, "agent_framework", agent_framework)
        if class_methods is not None:
            pulumi.set(__self__, "class_methods", class_methods)
        if deployment_spec is not None:
            pulumi.set(__self__, "deployment_spec", deployment_spec)
        if package_spec is not None:
            pulumi.set(__self__, "package_spec", package_spec)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if source_code_spec is not None:
            pulumi.set(__self__, "source_code_spec", source_code_spec)

    @_builtins.property
    @pulumi.getter(name="agentFramework")
    def agent_framework(self) -> Optional[_builtins.str]:
        """
        Optional. The OSS agent framework used to develop the agent.
        """
        return pulumi.get(self, "agent_framework")

    @_builtins.property
    @pulumi.getter(name="classMethods")
    def class_methods(self) -> Optional[_builtins.str]:
        """
        Optional. Declarations for object class methods in OpenAPI
        specification format.
        """
        return pulumi.get(self, "class_methods")

    @_builtins.property
    @pulumi.getter(name="deploymentSpec")
    def deployment_spec(self) -> Optional['outputs.AiReasoningEngineSpecDeploymentSpec']:
        """
        Optional. The specification of a Reasoning Engine deployment.
        Structure is documented below.
        """
        return pulumi.get(self, "deployment_spec")

    @_builtins.property
    @pulumi.getter(name="packageSpec")
    def package_spec(self) -> Optional['outputs.AiReasoningEngineSpecPackageSpec']:
        """
        Optional. User provided package spec of the ReasoningEngine.
        Ignored when users directly specify a deployment image through
        deploymentSpec.first_party_image_override, but keeping the
        field_behavior to avoid introducing breaking changes.
        Structure is documented below.
        """
        return pulumi.get(self, "package_spec")

    @_builtins.property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[_builtins.str]:
        """
        Optional. The service account that the Reasoning Engine artifact runs
        as. It should have "roles/storage.objectViewer" for reading the user
        project's Cloud Storage and "roles/aiplatform.user" for using Vertex
        extensions. If not specified, the Vertex AI Reasoning Engine service
        Agent in the project will be used.
        """
        return pulumi.get(self, "service_account")

    @_builtins.property
    @pulumi.getter(name="sourceCodeSpec")
    def source_code_spec(self) -> Optional['outputs.AiReasoningEngineSpecSourceCodeSpec']:
        """
        Specification for deploying from source code.
        Structure is documented below.
        """
        return pulumi.get(self, "source_code_spec")


@pulumi.output_type
class AiReasoningEngineSpecDeploymentSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "containerConcurrency":
            suggest = "container_concurrency"
        elif key == "maxInstances":
            suggest = "max_instances"
        elif key == "minInstances":
            suggest = "min_instances"
        elif key == "resourceLimits":
            suggest = "resource_limits"
        elif key == "secretEnvs":
            suggest = "secret_envs"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineSpecDeploymentSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineSpecDeploymentSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineSpecDeploymentSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 container_concurrency: Optional[_builtins.int] = None,
                 envs: Optional[Sequence['outputs.AiReasoningEngineSpecDeploymentSpecEnv']] = None,
                 max_instances: Optional[_builtins.int] = None,
                 min_instances: Optional[_builtins.int] = None,
                 resource_limits: Optional[Mapping[str, _builtins.str]] = None,
                 secret_envs: Optional[Sequence['outputs.AiReasoningEngineSpecDeploymentSpecSecretEnv']] = None):
        """
        :param _builtins.int container_concurrency: Optional. Concurrency for each container and agent server.
               Recommended value: 2 * cpu + 1. Defaults to 9.
        :param Sequence['AiReasoningEngineSpecDeploymentSpecEnvArgs'] envs: Optional. Environment variables to be set with the Reasoning
               Engine deployment.
               Structure is documented below.
        :param _builtins.int max_instances: Optional. The minimum number of application instances that will be
               kept running at all times. Defaults to 1. Range: [0, 10].
        :param _builtins.int min_instances: Optional. The maximum number of application instances that can be
               launched to handle increased traffic. Defaults to 100.
               Range: [1, 1000]. If VPC-SC or PSC-I is enabled, the acceptable
               range is [1, 100].
        :param Mapping[str, _builtins.str] resource_limits: Optional. Resource limits for each container.
               Only 'cpu' and 'memory' keys are supported.
               Defaults to {"cpu": "4", "memory": "4Gi"}.
               The only supported values for CPU are '1', '2', '4', '6' and '8'.
               For more information, go to
               https://cloud.google.com/run/docs/configuring/cpu.
               The only supported values for memory are '1Gi', '2Gi', ... '32 Gi'.
               For more information, go to
               https://cloud.google.com/run/docs/configuring/memory-limits.
        :param Sequence['AiReasoningEngineSpecDeploymentSpecSecretEnvArgs'] secret_envs: Optional. Environment variables where the value is a secret in
               Cloud Secret Manager. To use this feature, add 'Secret Manager
               Secret Accessor' role (roles/secretmanager.secretAccessor) to AI
               Platform Reasoning Engine service Agent.
               Structure is documented below.
        """
        if container_concurrency is not None:
            pulumi.set(__self__, "container_concurrency", container_concurrency)
        if envs is not None:
            pulumi.set(__self__, "envs", envs)
        if max_instances is not None:
            pulumi.set(__self__, "max_instances", max_instances)
        if min_instances is not None:
            pulumi.set(__self__, "min_instances", min_instances)
        if resource_limits is not None:
            pulumi.set(__self__, "resource_limits", resource_limits)
        if secret_envs is not None:
            pulumi.set(__self__, "secret_envs", secret_envs)

    @_builtins.property
    @pulumi.getter(name="containerConcurrency")
    def container_concurrency(self) -> Optional[_builtins.int]:
        """
        Optional. Concurrency for each container and agent server.
        Recommended value: 2 * cpu + 1. Defaults to 9.
        """
        return pulumi.get(self, "container_concurrency")

    @_builtins.property
    @pulumi.getter
    def envs(self) -> Optional[Sequence['outputs.AiReasoningEngineSpecDeploymentSpecEnv']]:
        """
        Optional. Environment variables to be set with the Reasoning
        Engine deployment.
        Structure is documented below.
        """
        return pulumi.get(self, "envs")

    @_builtins.property
    @pulumi.getter(name="maxInstances")
    def max_instances(self) -> Optional[_builtins.int]:
        """
        Optional. The minimum number of application instances that will be
        kept running at all times. Defaults to 1. Range: [0, 10].
        """
        return pulumi.get(self, "max_instances")

    @_builtins.property
    @pulumi.getter(name="minInstances")
    def min_instances(self) -> Optional[_builtins.int]:
        """
        Optional. The maximum number of application instances that can be
        launched to handle increased traffic. Defaults to 100.
        Range: [1, 1000]. If VPC-SC or PSC-I is enabled, the acceptable
        range is [1, 100].
        """
        return pulumi.get(self, "min_instances")

    @_builtins.property
    @pulumi.getter(name="resourceLimits")
    def resource_limits(self) -> Optional[Mapping[str, _builtins.str]]:
        """
        Optional. Resource limits for each container.
        Only 'cpu' and 'memory' keys are supported.
        Defaults to {"cpu": "4", "memory": "4Gi"}.
        The only supported values for CPU are '1', '2', '4', '6' and '8'.
        For more information, go to
        https://cloud.google.com/run/docs/configuring/cpu.
        The only supported values for memory are '1Gi', '2Gi', ... '32 Gi'.
        For more information, go to
        https://cloud.google.com/run/docs/configuring/memory-limits.
        """
        return pulumi.get(self, "resource_limits")

    @_builtins.property
    @pulumi.getter(name="secretEnvs")
    def secret_envs(self) -> Optional[Sequence['outputs.AiReasoningEngineSpecDeploymentSpecSecretEnv']]:
        """
        Optional. Environment variables where the value is a secret in
        Cloud Secret Manager. To use this feature, add 'Secret Manager
        Secret Accessor' role (roles/secretmanager.secretAccessor) to AI
        Platform Reasoning Engine service Agent.
        Structure is documented below.
        """
        return pulumi.get(self, "secret_envs")


@pulumi.output_type
class AiReasoningEngineSpecDeploymentSpecEnv(dict):
    def __init__(__self__, *,
                 name: _builtins.str,
                 value: _builtins.str):
        """
        :param _builtins.str name: The name of the environment variable. Must be a valid
               C identifier.
        :param _builtins.str value: Variables that reference a $(VAR_NAME) are expanded using
               the previous defined environment variables in the container
               and any service environment variables. If a variable cannot
               be resolved, the reference in the input string will be
               unchanged. The $(VAR_NAME) syntax can be escaped with a
               double $$, ie: $$(VAR_NAME). Escaped references will never
               be expanded, regardless of whether the variable exists
               or not.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "value", value)

    @_builtins.property
    @pulumi.getter
    def name(self) -> _builtins.str:
        """
        The name of the environment variable. Must be a valid
        C identifier.
        """
        return pulumi.get(self, "name")

    @_builtins.property
    @pulumi.getter
    def value(self) -> _builtins.str:
        """
        Variables that reference a $(VAR_NAME) are expanded using
        the previous defined environment variables in the container
        and any service environment variables. If a variable cannot
        be resolved, the reference in the input string will be
        unchanged. The $(VAR_NAME) syntax can be escaped with a
        double $$, ie: $$(VAR_NAME). Escaped references will never
        be expanded, regardless of whether the variable exists
        or not.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class AiReasoningEngineSpecDeploymentSpecSecretEnv(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "secretRef":
            suggest = "secret_ref"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineSpecDeploymentSpecSecretEnv. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineSpecDeploymentSpecSecretEnv.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineSpecDeploymentSpecSecretEnv.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 name: _builtins.str,
                 secret_ref: 'outputs.AiReasoningEngineSpecDeploymentSpecSecretEnvSecretRef'):
        """
        :param _builtins.str name: The name of the environment variable. Must be a valid C
               identifier.
        :param 'AiReasoningEngineSpecDeploymentSpecSecretEnvSecretRefArgs' secret_ref: Reference to a secret stored in the Cloud Secret Manager
               that will provide the value for this environment variable.
               Structure is documented below.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "secret_ref", secret_ref)

    @_builtins.property
    @pulumi.getter
    def name(self) -> _builtins.str:
        """
        The name of the environment variable. Must be a valid C
        identifier.
        """
        return pulumi.get(self, "name")

    @_builtins.property
    @pulumi.getter(name="secretRef")
    def secret_ref(self) -> 'outputs.AiReasoningEngineSpecDeploymentSpecSecretEnvSecretRef':
        """
        Reference to a secret stored in the Cloud Secret Manager
        that will provide the value for this environment variable.
        Structure is documented below.
        """
        return pulumi.get(self, "secret_ref")


@pulumi.output_type
class AiReasoningEngineSpecDeploymentSpecSecretEnvSecretRef(dict):
    def __init__(__self__, *,
                 secret: _builtins.str,
                 version: Optional[_builtins.str] = None):
        """
        :param _builtins.str secret: The name of the secret in Cloud Secret Manager.
               Format: {secret_name}.
        :param _builtins.str version: The Cloud Secret Manager secret version. Can be 'latest'
               for the latest version, an integer for a specific
               version, or a version alias.
        """
        pulumi.set(__self__, "secret", secret)
        if version is not None:
            pulumi.set(__self__, "version", version)

    @_builtins.property
    @pulumi.getter
    def secret(self) -> _builtins.str:
        """
        The name of the secret in Cloud Secret Manager.
        Format: {secret_name}.
        """
        return pulumi.get(self, "secret")

    @_builtins.property
    @pulumi.getter
    def version(self) -> Optional[_builtins.str]:
        """
        The Cloud Secret Manager secret version. Can be 'latest'
        for the latest version, an integer for a specific
        version, or a version alias.
        """
        return pulumi.get(self, "version")


@pulumi.output_type
class AiReasoningEngineSpecPackageSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "dependencyFilesGcsUri":
            suggest = "dependency_files_gcs_uri"
        elif key == "pickleObjectGcsUri":
            suggest = "pickle_object_gcs_uri"
        elif key == "pythonVersion":
            suggest = "python_version"
        elif key == "requirementsGcsUri":
            suggest = "requirements_gcs_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineSpecPackageSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineSpecPackageSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineSpecPackageSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dependency_files_gcs_uri: Optional[_builtins.str] = None,
                 pickle_object_gcs_uri: Optional[_builtins.str] = None,
                 python_version: Optional[_builtins.str] = None,
                 requirements_gcs_uri: Optional[_builtins.str] = None):
        """
        :param _builtins.str dependency_files_gcs_uri: Optional. The Cloud Storage URI of the dependency files in tar.gz
               format.
        :param _builtins.str pickle_object_gcs_uri: Optional. The Cloud Storage URI of the pickled python object.
        :param _builtins.str python_version: Optional. The Python version. Currently support 3.8, 3.9, 3.10,
               3.11, 3.12, 3.13. If not specified, default value is 3.10.
        :param _builtins.str requirements_gcs_uri: Optional. The Cloud Storage URI of the requirements.txtfile
        """
        if dependency_files_gcs_uri is not None:
            pulumi.set(__self__, "dependency_files_gcs_uri", dependency_files_gcs_uri)
        if pickle_object_gcs_uri is not None:
            pulumi.set(__self__, "pickle_object_gcs_uri", pickle_object_gcs_uri)
        if python_version is not None:
            pulumi.set(__self__, "python_version", python_version)
        if requirements_gcs_uri is not None:
            pulumi.set(__self__, "requirements_gcs_uri", requirements_gcs_uri)

    @_builtins.property
    @pulumi.getter(name="dependencyFilesGcsUri")
    def dependency_files_gcs_uri(self) -> Optional[_builtins.str]:
        """
        Optional. The Cloud Storage URI of the dependency files in tar.gz
        format.
        """
        return pulumi.get(self, "dependency_files_gcs_uri")

    @_builtins.property
    @pulumi.getter(name="pickleObjectGcsUri")
    def pickle_object_gcs_uri(self) -> Optional[_builtins.str]:
        """
        Optional. The Cloud Storage URI of the pickled python object.
        """
        return pulumi.get(self, "pickle_object_gcs_uri")

    @_builtins.property
    @pulumi.getter(name="pythonVersion")
    def python_version(self) -> Optional[_builtins.str]:
        """
        Optional. The Python version. Currently support 3.8, 3.9, 3.10,
        3.11, 3.12, 3.13. If not specified, default value is 3.10.
        """
        return pulumi.get(self, "python_version")

    @_builtins.property
    @pulumi.getter(name="requirementsGcsUri")
    def requirements_gcs_uri(self) -> Optional[_builtins.str]:
        """
        Optional. The Cloud Storage URI of the requirements.txtfile
        """
        return pulumi.get(self, "requirements_gcs_uri")


@pulumi.output_type
class AiReasoningEngineSpecSourceCodeSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "inlineSource":
            suggest = "inline_source"
        elif key == "pythonSpec":
            suggest = "python_spec"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineSpecSourceCodeSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineSpecSourceCodeSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineSpecSourceCodeSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 inline_source: Optional['outputs.AiReasoningEngineSpecSourceCodeSpecInlineSource'] = None,
                 python_spec: Optional['outputs.AiReasoningEngineSpecSourceCodeSpecPythonSpec'] = None):
        """
        :param 'AiReasoningEngineSpecSourceCodeSpecInlineSourceArgs' inline_source: Source code is provided directly in the request.
               Structure is documented below.
        :param 'AiReasoningEngineSpecSourceCodeSpecPythonSpecArgs' python_spec: Specification for running a Python application from source.
               Structure is documented below.
        """
        if inline_source is not None:
            pulumi.set(__self__, "inline_source", inline_source)
        if python_spec is not None:
            pulumi.set(__self__, "python_spec", python_spec)

    @_builtins.property
    @pulumi.getter(name="inlineSource")
    def inline_source(self) -> Optional['outputs.AiReasoningEngineSpecSourceCodeSpecInlineSource']:
        """
        Source code is provided directly in the request.
        Structure is documented below.
        """
        return pulumi.get(self, "inline_source")

    @_builtins.property
    @pulumi.getter(name="pythonSpec")
    def python_spec(self) -> Optional['outputs.AiReasoningEngineSpecSourceCodeSpecPythonSpec']:
        """
        Specification for running a Python application from source.
        Structure is documented below.
        """
        return pulumi.get(self, "python_spec")


@pulumi.output_type
class AiReasoningEngineSpecSourceCodeSpecInlineSource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "sourceArchive":
            suggest = "source_archive"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineSpecSourceCodeSpecInlineSource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineSpecSourceCodeSpecInlineSource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineSpecSourceCodeSpecInlineSource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 source_archive: Optional[_builtins.str] = None):
        """
        :param _builtins.str source_archive: Required. Input only.
               The application source code archive, provided as a compressed
               tarball (.tar.gz) file. A base64-encoded string.
        """
        if source_archive is not None:
            pulumi.set(__self__, "source_archive", source_archive)

    @_builtins.property
    @pulumi.getter(name="sourceArchive")
    def source_archive(self) -> Optional[_builtins.str]:
        """
        Required. Input only.
        The application source code archive, provided as a compressed
        tarball (.tar.gz) file. A base64-encoded string.
        """
        return pulumi.get(self, "source_archive")


@pulumi.output_type
class AiReasoningEngineSpecSourceCodeSpecPythonSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "entrypointModule":
            suggest = "entrypoint_module"
        elif key == "entrypointObject":
            suggest = "entrypoint_object"
        elif key == "requirementsFile":
            suggest = "requirements_file"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiReasoningEngineSpecSourceCodeSpecPythonSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiReasoningEngineSpecSourceCodeSpecPythonSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiReasoningEngineSpecSourceCodeSpecPythonSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 entrypoint_module: Optional[_builtins.str] = None,
                 entrypoint_object: Optional[_builtins.str] = None,
                 requirements_file: Optional[_builtins.str] = None,
                 version: Optional[_builtins.str] = None):
        """
        :param _builtins.str entrypoint_module: Optional. The Python module to load as the entrypoint,
               specified as a fully qualified module name. For example:
               path.to.agent. If not specified, defaults to "agent".
               The project root will be added to Python sys.path, allowing
               imports to be specified relative to the root.
        :param _builtins.str entrypoint_object: Optional. The name of the callable object within the
               entrypointModule to use as the application If not specified,
               defaults to "root_agent".
        :param _builtins.str requirements_file: Optional. The path to the requirements file, relative to the
               source root. If not specified, defaults to "requirements.txt".
        :param _builtins.str version: Optional. The version of Python to use. Support version
               includes 3.9, 3.10, 3.11, 3.12, 3.13. If not specified,
               default value is 3.10.
        """
        if entrypoint_module is not None:
            pulumi.set(__self__, "entrypoint_module", entrypoint_module)
        if entrypoint_object is not None:
            pulumi.set(__self__, "entrypoint_object", entrypoint_object)
        if requirements_file is not None:
            pulumi.set(__self__, "requirements_file", requirements_file)
        if version is not None:
            pulumi.set(__self__, "version", version)

    @_builtins.property
    @pulumi.getter(name="entrypointModule")
    def entrypoint_module(self) -> Optional[_builtins.str]:
        """
        Optional. The Python module to load as the entrypoint,
        specified as a fully qualified module name. For example:
        path.to.agent. If not specified, defaults to "agent".
        The project root will be added to Python sys.path, allowing
        imports to be specified relative to the root.
        """
        return pulumi.get(self, "entrypoint_module")

    @_builtins.property
    @pulumi.getter(name="entrypointObject")
    def entrypoint_object(self) -> Optional[_builtins.str]:
        """
        Optional. The name of the callable object within the
        entrypointModule to use as the application If not specified,
        defaults to "root_agent".
        """
        return pulumi.get(self, "entrypoint_object")

    @_builtins.property
    @pulumi.getter(name="requirementsFile")
    def requirements_file(self) -> Optional[_builtins.str]:
        """
        Optional. The path to the requirements file, relative to the
        source root. If not specified, defaults to "requirements.txt".
        """
        return pulumi.get(self, "requirements_file")

    @_builtins.property
    @pulumi.getter
    def version(self) -> Optional[_builtins.str]:
        """
        Optional. The version of Python to use. Support version
        includes 3.9, 3.10, 3.11, 3.12, 3.13. If not specified,
        default value is 3.10.
        """
        return pulumi.get(self, "version")


@pulumi.output_type
class AiTensorboardEncryptionSpec(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AiTensorboardEncryptionSpec. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AiTensorboardEncryptionSpec.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AiTensorboardEncryptionSpec.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: _builtins.str):
        """
        :param _builtins.str kms_key_name: The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
               Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> _builtins.str:
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
        Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class GetAiIndexDeployedIndexResult(dict):
    def __init__(__self__, *,
                 deployed_index_id: _builtins.str,
                 index_endpoint: _builtins.str):
        """
        :param _builtins.str deployed_index_id: The ID of the DeployedIndex in the above IndexEndpoint.
        :param _builtins.str index_endpoint: A resource name of the IndexEndpoint.
        """
        pulumi.set(__self__, "deployed_index_id", deployed_index_id)
        pulumi.set(__self__, "index_endpoint", index_endpoint)

    @_builtins.property
    @pulumi.getter(name="deployedIndexId")
    def deployed_index_id(self) -> _builtins.str:
        """
        The ID of the DeployedIndex in the above IndexEndpoint.
        """
        return pulumi.get(self, "deployed_index_id")

    @_builtins.property
    @pulumi.getter(name="indexEndpoint")
    def index_endpoint(self) -> _builtins.str:
        """
        A resource name of the IndexEndpoint.
        """
        return pulumi.get(self, "index_endpoint")


@pulumi.output_type
class GetAiIndexEncryptionSpecResult(dict):
    def __init__(__self__, *,
                 kms_key_name: _builtins.str):
        """
        :param _builtins.str kms_key_name: Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: 'projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key'. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @_builtins.property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> _builtins.str:
        """
        Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: 'projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key'. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class GetAiIndexIndexStatResult(dict):
    def __init__(__self__, *,
                 shards_count: _builtins.int,
                 vectors_count: _builtins.str):
        """
        :param _builtins.int shards_count: The number of shards in the Index.
        :param _builtins.str vectors_count: The number of vectors in the Index.
        """
        pulumi.set(__self__, "shards_count", shards_count)
        pulumi.set(__self__, "vectors_count", vectors_count)

    @_builtins.property
    @pulumi.getter(name="shardsCount")
    def shards_count(self) -> _builtins.int:
        """
        The number of shards in the Index.
        """
        return pulumi.get(self, "shards_count")

    @_builtins.property
    @pulumi.getter(name="vectorsCount")
    def vectors_count(self) -> _builtins.str:
        """
        The number of vectors in the Index.
        """
        return pulumi.get(self, "vectors_count")


@pulumi.output_type
class GetAiIndexMetadataResult(dict):
    def __init__(__self__, *,
                 configs: Sequence['outputs.GetAiIndexMetadataConfigResult'],
                 contents_delta_uri: _builtins.str,
                 is_complete_overwrite: _builtins.bool):
        """
        :param Sequence['GetAiIndexMetadataConfigArgs'] configs: The configuration of the Matching Engine Index.
        :param _builtins.str contents_delta_uri: Allows inserting, updating  or deleting the contents of the Matching Engine Index.
               The string must be a valid Cloud Storage directory path. If this
               field is set when calling IndexService.UpdateIndex, then no other
               Index field can be also updated as part of the same call.
               The expected structure and format of the files this URI points to is
               described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
        :param _builtins.bool is_complete_overwrite: If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
               then existing content of the Index will be replaced by the data from the contentsDeltaUri.
        """
        pulumi.set(__self__, "configs", configs)
        pulumi.set(__self__, "contents_delta_uri", contents_delta_uri)
        pulumi.set(__self__, "is_complete_overwrite", is_complete_overwrite)

    @_builtins.property
    @pulumi.getter
    def configs(self) -> Sequence['outputs.GetAiIndexMetadataConfigResult']:
        """
        The configuration of the Matching Engine Index.
        """
        return pulumi.get(self, "configs")

    @_builtins.property
    @pulumi.getter(name="contentsDeltaUri")
    def contents_delta_uri(self) -> _builtins.str:
        """
        Allows inserting, updating  or deleting the contents of the Matching Engine Index.
        The string must be a valid Cloud Storage directory path. If this
        field is set when calling IndexService.UpdateIndex, then no other
        Index field can be also updated as part of the same call.
        The expected structure and format of the files this URI points to is
        described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
        """
        return pulumi.get(self, "contents_delta_uri")

    @_builtins.property
    @pulumi.getter(name="isCompleteOverwrite")
    def is_complete_overwrite(self) -> _builtins.bool:
        """
        If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
        then existing content of the Index will be replaced by the data from the contentsDeltaUri.
        """
        return pulumi.get(self, "is_complete_overwrite")


@pulumi.output_type
class GetAiIndexMetadataConfigResult(dict):
    def __init__(__self__, *,
                 algorithm_configs: Sequence['outputs.GetAiIndexMetadataConfigAlgorithmConfigResult'],
                 approximate_neighbors_count: _builtins.int,
                 dimensions: _builtins.int,
                 distance_measure_type: _builtins.str,
                 feature_norm_type: _builtins.str,
                 shard_size: _builtins.str):
        """
        :param Sequence['GetAiIndexMetadataConfigAlgorithmConfigArgs'] algorithm_configs: The configuration with regard to the algorithms used for efficient search. This field may be required based on your configuration.
        :param _builtins.int approximate_neighbors_count: The default number of neighbors to find via approximate search before exact reordering is
               performed. Exact reordering is a procedure where results returned by an
               approximate search algorithm are reordered via a more expensive distance computation.
               Required if tree-AH algorithm is used.
        :param _builtins.int dimensions: The number of dimensions of the input vectors.
        :param _builtins.str distance_measure_type: The distance measure used in nearest neighbor search. The value must be one of the followings:
               * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
               * L1_DISTANCE: Manhattan (L_1) Distance
               * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
               * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
        :param _builtins.str feature_norm_type: Type of normalization to be carried out on each vector. The value must be one of the followings:
               * UNIT_L2_NORM: Unit L2 normalization type
               * NONE: No normalization type is specified.
        :param _builtins.str shard_size: Index data is split into equal parts to be processed. These are called "shards".
               The shard size must be specified when creating an index. The value must be one of the followings:
               * SHARD_SIZE_SMALL: Small (2GB)
               * SHARD_SIZE_MEDIUM: Medium (20GB)
               * SHARD_SIZE_LARGE: Large (50GB)
        """
        pulumi.set(__self__, "algorithm_configs", algorithm_configs)
        pulumi.set(__self__, "approximate_neighbors_count", approximate_neighbors_count)
        pulumi.set(__self__, "dimensions", dimensions)
        pulumi.set(__self__, "distance_measure_type", distance_measure_type)
        pulumi.set(__self__, "feature_norm_type", feature_norm_type)
        pulumi.set(__self__, "shard_size", shard_size)

    @_builtins.property
    @pulumi.getter(name="algorithmConfigs")
    def algorithm_configs(self) -> Sequence['outputs.GetAiIndexMetadataConfigAlgorithmConfigResult']:
        """
        The configuration with regard to the algorithms used for efficient search. This field may be required based on your configuration.
        """
        return pulumi.get(self, "algorithm_configs")

    @_builtins.property
    @pulumi.getter(name="approximateNeighborsCount")
    def approximate_neighbors_count(self) -> _builtins.int:
        """
        The default number of neighbors to find via approximate search before exact reordering is
        performed. Exact reordering is a procedure where results returned by an
        approximate search algorithm are reordered via a more expensive distance computation.
        Required if tree-AH algorithm is used.
        """
        return pulumi.get(self, "approximate_neighbors_count")

    @_builtins.property
    @pulumi.getter
    def dimensions(self) -> _builtins.int:
        """
        The number of dimensions of the input vectors.
        """
        return pulumi.get(self, "dimensions")

    @_builtins.property
    @pulumi.getter(name="distanceMeasureType")
    def distance_measure_type(self) -> _builtins.str:
        """
        The distance measure used in nearest neighbor search. The value must be one of the followings:
        * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
        * L1_DISTANCE: Manhattan (L_1) Distance
        * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
        * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
        """
        return pulumi.get(self, "distance_measure_type")

    @_builtins.property
    @pulumi.getter(name="featureNormType")
    def feature_norm_type(self) -> _builtins.str:
        """
        Type of normalization to be carried out on each vector. The value must be one of the followings:
        * UNIT_L2_NORM: Unit L2 normalization type
        * NONE: No normalization type is specified.
        """
        return pulumi.get(self, "feature_norm_type")

    @_builtins.property
    @pulumi.getter(name="shardSize")
    def shard_size(self) -> _builtins.str:
        """
        Index data is split into equal parts to be processed. These are called "shards".
        The shard size must be specified when creating an index. The value must be one of the followings:
        * SHARD_SIZE_SMALL: Small (2GB)
        * SHARD_SIZE_MEDIUM: Medium (20GB)
        * SHARD_SIZE_LARGE: Large (50GB)
        """
        return pulumi.get(self, "shard_size")


@pulumi.output_type
class GetAiIndexMetadataConfigAlgorithmConfigResult(dict):
    def __init__(__self__, *,
                 brute_force_configs: Sequence['outputs.GetAiIndexMetadataConfigAlgorithmConfigBruteForceConfigResult'],
                 tree_ah_configs: Sequence['outputs.GetAiIndexMetadataConfigAlgorithmConfigTreeAhConfigResult']):
        """
        :param Sequence['GetAiIndexMetadataConfigAlgorithmConfigBruteForceConfigArgs'] brute_force_configs: Configuration options for using brute force search, which simply implements the
               standard linear search in the database for each query.
        :param Sequence['GetAiIndexMetadataConfigAlgorithmConfigTreeAhConfigArgs'] tree_ah_configs: Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
               Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        """
        pulumi.set(__self__, "brute_force_configs", brute_force_configs)
        pulumi.set(__self__, "tree_ah_configs", tree_ah_configs)

    @_builtins.property
    @pulumi.getter(name="bruteForceConfigs")
    def brute_force_configs(self) -> Sequence['outputs.GetAiIndexMetadataConfigAlgorithmConfigBruteForceConfigResult']:
        """
        Configuration options for using brute force search, which simply implements the
        standard linear search in the database for each query.
        """
        return pulumi.get(self, "brute_force_configs")

    @_builtins.property
    @pulumi.getter(name="treeAhConfigs")
    def tree_ah_configs(self) -> Sequence['outputs.GetAiIndexMetadataConfigAlgorithmConfigTreeAhConfigResult']:
        """
        Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
        Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
        """
        return pulumi.get(self, "tree_ah_configs")


@pulumi.output_type
class GetAiIndexMetadataConfigAlgorithmConfigBruteForceConfigResult(dict):
    def __init__(__self__):
        pass


@pulumi.output_type
class GetAiIndexMetadataConfigAlgorithmConfigTreeAhConfigResult(dict):
    def __init__(__self__, *,
                 leaf_node_embedding_count: _builtins.int,
                 leaf_nodes_to_search_percent: _builtins.int):
        """
        :param _builtins.int leaf_node_embedding_count: Number of embeddings on each leaf node. The default value is 1000 if not set.
        :param _builtins.int leaf_nodes_to_search_percent: The default percentage of leaf nodes that any query may be searched. Must be in
               range 1-100, inclusive. The default value is 10 (means 10%) if not set.
        """
        pulumi.set(__self__, "leaf_node_embedding_count", leaf_node_embedding_count)
        pulumi.set(__self__, "leaf_nodes_to_search_percent", leaf_nodes_to_search_percent)

    @_builtins.property
    @pulumi.getter(name="leafNodeEmbeddingCount")
    def leaf_node_embedding_count(self) -> _builtins.int:
        """
        Number of embeddings on each leaf node. The default value is 1000 if not set.
        """
        return pulumi.get(self, "leaf_node_embedding_count")

    @_builtins.property
    @pulumi.getter(name="leafNodesToSearchPercent")
    def leaf_nodes_to_search_percent(self) -> _builtins.int:
        """
        The default percentage of leaf nodes that any query may be searched. Must be in
        range 1-100, inclusive. The default value is 10 (means 10%) if not set.
        """
        return pulumi.get(self, "leaf_nodes_to_search_percent")



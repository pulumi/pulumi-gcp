# coding=utf-8
# *** WARNING: this file was generated by pulumi-language-python. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import builtins
import copy
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from .. import _utilities
from . import outputs

__all__ = [
    'AppProfileDataBoostIsolationReadOnly',
    'AppProfileSingleClusterRouting',
    'AppProfileStandardIsolation',
    'BiReservationPreferredTable',
    'ConnectionAws',
    'ConnectionAwsAccessRole',
    'ConnectionAzure',
    'ConnectionCloudResource',
    'ConnectionCloudSpanner',
    'ConnectionCloudSql',
    'ConnectionCloudSqlCredential',
    'ConnectionIamBindingCondition',
    'ConnectionIamMemberCondition',
    'ConnectionSpark',
    'ConnectionSparkMetastoreServiceConfig',
    'ConnectionSparkSparkHistoryServerConfig',
    'DataTransferConfigEmailPreferences',
    'DataTransferConfigEncryptionConfiguration',
    'DataTransferConfigScheduleOptions',
    'DataTransferConfigSensitiveParams',
    'DatasetAccess',
    'DatasetAccessAuthorizedDataset',
    'DatasetAccessAuthorizedDatasetDataset',
    'DatasetAccessCondition',
    'DatasetAccessDataset',
    'DatasetAccessDatasetDataset',
    'DatasetAccessRoutine',
    'DatasetAccessView',
    'DatasetDefaultEncryptionConfiguration',
    'DatasetExternalCatalogDatasetOptions',
    'DatasetExternalDatasetReference',
    'DatasetIamBindingCondition',
    'DatasetIamMemberCondition',
    'IamBindingCondition',
    'IamMemberCondition',
    'JobCopy',
    'JobCopyDestinationEncryptionConfiguration',
    'JobCopyDestinationTable',
    'JobCopySourceTable',
    'JobExtract',
    'JobExtractSourceModel',
    'JobExtractSourceTable',
    'JobLoad',
    'JobLoadDestinationEncryptionConfiguration',
    'JobLoadDestinationTable',
    'JobLoadParquetOptions',
    'JobLoadTimePartitioning',
    'JobQuery',
    'JobQueryDefaultDataset',
    'JobQueryDestinationEncryptionConfiguration',
    'JobQueryDestinationTable',
    'JobQueryScriptOptions',
    'JobQueryUserDefinedFunctionResource',
    'JobStatus',
    'JobStatusError',
    'JobStatusErrorResult',
    'ReservationAutoscale',
    'ReservationReplicationStatus',
    'ReservationReplicationStatusError',
    'RoutineArgument',
    'RoutineRemoteFunctionOptions',
    'RoutineSparkOptions',
    'TableBiglakeConfiguration',
    'TableEncryptionConfiguration',
    'TableExternalCatalogTableOptions',
    'TableExternalCatalogTableOptionsStorageDescriptor',
    'TableExternalCatalogTableOptionsStorageDescriptorSerdeInfo',
    'TableExternalDataConfiguration',
    'TableExternalDataConfigurationAvroOptions',
    'TableExternalDataConfigurationBigtableOptions',
    'TableExternalDataConfigurationBigtableOptionsColumnFamily',
    'TableExternalDataConfigurationBigtableOptionsColumnFamilyColumn',
    'TableExternalDataConfigurationCsvOptions',
    'TableExternalDataConfigurationGoogleSheetsOptions',
    'TableExternalDataConfigurationHivePartitioningOptions',
    'TableExternalDataConfigurationJsonOptions',
    'TableExternalDataConfigurationParquetOptions',
    'TableMaterializedView',
    'TableRangePartitioning',
    'TableRangePartitioningRange',
    'TableSchemaForeignTypeInfo',
    'TableTableConstraints',
    'TableTableConstraintsForeignKey',
    'TableTableConstraintsForeignKeyColumnReferences',
    'TableTableConstraintsForeignKeyReferencedTable',
    'TableTableConstraintsPrimaryKey',
    'TableTableReplicationInfo',
    'TableTimePartitioning',
    'TableView',
    'GetDatasetAccessResult',
    'GetDatasetAccessConditionResult',
    'GetDatasetAccessDatasetResult',
    'GetDatasetAccessDatasetDatasetResult',
    'GetDatasetAccessRoutineResult',
    'GetDatasetAccessViewResult',
    'GetDatasetDefaultEncryptionConfigurationResult',
    'GetDatasetExternalCatalogDatasetOptionResult',
    'GetDatasetExternalDatasetReferenceResult',
    'GetTableBiglakeConfigurationResult',
    'GetTableEncryptionConfigurationResult',
    'GetTableExternalCatalogTableOptionResult',
    'GetTableExternalCatalogTableOptionStorageDescriptorResult',
    'GetTableExternalCatalogTableOptionStorageDescriptorSerdeInfoResult',
    'GetTableExternalDataConfigurationResult',
    'GetTableExternalDataConfigurationAvroOptionResult',
    'GetTableExternalDataConfigurationBigtableOptionResult',
    'GetTableExternalDataConfigurationBigtableOptionColumnFamilyResult',
    'GetTableExternalDataConfigurationBigtableOptionColumnFamilyColumnResult',
    'GetTableExternalDataConfigurationCsvOptionResult',
    'GetTableExternalDataConfigurationGoogleSheetsOptionResult',
    'GetTableExternalDataConfigurationHivePartitioningOptionResult',
    'GetTableExternalDataConfigurationJsonOptionResult',
    'GetTableExternalDataConfigurationParquetOptionResult',
    'GetTableMaterializedViewResult',
    'GetTableRangePartitioningResult',
    'GetTableRangePartitioningRangeResult',
    'GetTableSchemaForeignTypeInfoResult',
    'GetTableTableConstraintResult',
    'GetTableTableConstraintForeignKeyResult',
    'GetTableTableConstraintForeignKeyColumnReferenceResult',
    'GetTableTableConstraintForeignKeyReferencedTableResult',
    'GetTableTableConstraintPrimaryKeyResult',
    'GetTableTableReplicationInfoResult',
    'GetTableTimePartitioningResult',
    'GetTableViewResult',
    'GetTablesTableResult',
]

@pulumi.output_type
class AppProfileDataBoostIsolationReadOnly(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "computeBillingOwner":
            suggest = "compute_billing_owner"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AppProfileDataBoostIsolationReadOnly. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AppProfileDataBoostIsolationReadOnly.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AppProfileDataBoostIsolationReadOnly.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 compute_billing_owner: builtins.str):
        """
        :param builtins.str compute_billing_owner: The Compute Billing Owner for this Data Boost App Profile.
               Possible values are: `HOST_PAYS`.
        """
        pulumi.set(__self__, "compute_billing_owner", compute_billing_owner)

    @property
    @pulumi.getter(name="computeBillingOwner")
    def compute_billing_owner(self) -> builtins.str:
        """
        The Compute Billing Owner for this Data Boost App Profile.
        Possible values are: `HOST_PAYS`.
        """
        return pulumi.get(self, "compute_billing_owner")


@pulumi.output_type
class AppProfileSingleClusterRouting(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "clusterId":
            suggest = "cluster_id"
        elif key == "allowTransactionalWrites":
            suggest = "allow_transactional_writes"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in AppProfileSingleClusterRouting. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        AppProfileSingleClusterRouting.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        AppProfileSingleClusterRouting.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 cluster_id: builtins.str,
                 allow_transactional_writes: Optional[builtins.bool] = None):
        """
        :param builtins.str cluster_id: The cluster to which read/write requests should be routed.
        :param builtins.bool allow_transactional_writes: If true, CheckAndMutateRow and ReadModifyWriteRow requests are allowed by this app profile.
               It is unsafe to send these requests to the same table/row/column in multiple clusters.
        """
        pulumi.set(__self__, "cluster_id", cluster_id)
        if allow_transactional_writes is not None:
            pulumi.set(__self__, "allow_transactional_writes", allow_transactional_writes)

    @property
    @pulumi.getter(name="clusterId")
    def cluster_id(self) -> builtins.str:
        """
        The cluster to which read/write requests should be routed.
        """
        return pulumi.get(self, "cluster_id")

    @property
    @pulumi.getter(name="allowTransactionalWrites")
    def allow_transactional_writes(self) -> Optional[builtins.bool]:
        """
        If true, CheckAndMutateRow and ReadModifyWriteRow requests are allowed by this app profile.
        It is unsafe to send these requests to the same table/row/column in multiple clusters.
        """
        return pulumi.get(self, "allow_transactional_writes")


@pulumi.output_type
class AppProfileStandardIsolation(dict):
    def __init__(__self__, *,
                 priority: builtins.str):
        """
        :param builtins.str priority: The priority of requests sent using this app profile.
               Possible values are: `PRIORITY_LOW`, `PRIORITY_MEDIUM`, `PRIORITY_HIGH`.
        """
        pulumi.set(__self__, "priority", priority)

    @property
    @pulumi.getter
    def priority(self) -> builtins.str:
        """
        The priority of requests sent using this app profile.
        Possible values are: `PRIORITY_LOW`, `PRIORITY_MEDIUM`, `PRIORITY_HIGH`.
        """
        return pulumi.get(self, "priority")


@pulumi.output_type
class BiReservationPreferredTable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"
        elif key == "tableId":
            suggest = "table_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in BiReservationPreferredTable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        BiReservationPreferredTable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        BiReservationPreferredTable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: Optional[builtins.str] = None,
                 project_id: Optional[builtins.str] = None,
                 table_id: Optional[builtins.str] = None):
        """
        :param builtins.str dataset_id: The ID of the dataset in the above project.
        :param builtins.str project_id: The assigned project ID of the project.
        :param builtins.str table_id: The ID of the table in the above dataset.
        """
        if dataset_id is not None:
            pulumi.set(__self__, "dataset_id", dataset_id)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)
        if table_id is not None:
            pulumi.set(__self__, "table_id", table_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> Optional[builtins.str]:
        """
        The ID of the dataset in the above project.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[builtins.str]:
        """
        The assigned project ID of the project.
        """
        return pulumi.get(self, "project_id")

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> Optional[builtins.str]:
        """
        The ID of the table in the above dataset.
        """
        return pulumi.get(self, "table_id")


@pulumi.output_type
class ConnectionAws(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "accessRole":
            suggest = "access_role"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionAws. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionAws.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionAws.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 access_role: 'outputs.ConnectionAwsAccessRole'):
        """
        :param 'ConnectionAwsAccessRoleArgs' access_role: Authentication using Google owned service account to assume into customer's AWS IAM Role.
               Structure is documented below.
        """
        pulumi.set(__self__, "access_role", access_role)

    @property
    @pulumi.getter(name="accessRole")
    def access_role(self) -> 'outputs.ConnectionAwsAccessRole':
        """
        Authentication using Google owned service account to assume into customer's AWS IAM Role.
        Structure is documented below.
        """
        return pulumi.get(self, "access_role")


@pulumi.output_type
class ConnectionAwsAccessRole(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "iamRoleId":
            suggest = "iam_role_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionAwsAccessRole. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionAwsAccessRole.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionAwsAccessRole.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 iam_role_id: builtins.str,
                 identity: Optional[builtins.str] = None):
        """
        :param builtins.str iam_role_id: The user’s AWS IAM Role that trusts the Google-owned AWS IAM user Connection.
        :param builtins.str identity: (Output)
               A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's AWS IAM Role.
        """
        pulumi.set(__self__, "iam_role_id", iam_role_id)
        if identity is not None:
            pulumi.set(__self__, "identity", identity)

    @property
    @pulumi.getter(name="iamRoleId")
    def iam_role_id(self) -> builtins.str:
        """
        The user’s AWS IAM Role that trusts the Google-owned AWS IAM user Connection.
        """
        return pulumi.get(self, "iam_role_id")

    @property
    @pulumi.getter
    def identity(self) -> Optional[builtins.str]:
        """
        (Output)
        A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's AWS IAM Role.
        """
        return pulumi.get(self, "identity")


@pulumi.output_type
class ConnectionAzure(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "customerTenantId":
            suggest = "customer_tenant_id"
        elif key == "clientId":
            suggest = "client_id"
        elif key == "federatedApplicationClientId":
            suggest = "federated_application_client_id"
        elif key == "objectId":
            suggest = "object_id"
        elif key == "redirectUri":
            suggest = "redirect_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionAzure. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionAzure.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionAzure.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 customer_tenant_id: builtins.str,
                 application: Optional[builtins.str] = None,
                 client_id: Optional[builtins.str] = None,
                 federated_application_client_id: Optional[builtins.str] = None,
                 identity: Optional[builtins.str] = None,
                 object_id: Optional[builtins.str] = None,
                 redirect_uri: Optional[builtins.str] = None):
        """
        :param builtins.str customer_tenant_id: The id of customer's directory that host the data.
        :param builtins.str application: (Output)
               The name of the Azure Active Directory Application.
        :param builtins.str client_id: (Output)
               The client id of the Azure Active Directory Application.
        :param builtins.str federated_application_client_id: The Azure Application (client) ID where the federated credentials will be hosted.
        :param builtins.str identity: (Output)
               A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's Azure Active Directory Application.
        :param builtins.str object_id: (Output)
               The object id of the Azure Active Directory Application.
        :param builtins.str redirect_uri: (Output)
               The URL user will be redirected to after granting consent during connection setup.
        """
        pulumi.set(__self__, "customer_tenant_id", customer_tenant_id)
        if application is not None:
            pulumi.set(__self__, "application", application)
        if client_id is not None:
            pulumi.set(__self__, "client_id", client_id)
        if federated_application_client_id is not None:
            pulumi.set(__self__, "federated_application_client_id", federated_application_client_id)
        if identity is not None:
            pulumi.set(__self__, "identity", identity)
        if object_id is not None:
            pulumi.set(__self__, "object_id", object_id)
        if redirect_uri is not None:
            pulumi.set(__self__, "redirect_uri", redirect_uri)

    @property
    @pulumi.getter(name="customerTenantId")
    def customer_tenant_id(self) -> builtins.str:
        """
        The id of customer's directory that host the data.
        """
        return pulumi.get(self, "customer_tenant_id")

    @property
    @pulumi.getter
    def application(self) -> Optional[builtins.str]:
        """
        (Output)
        The name of the Azure Active Directory Application.
        """
        return pulumi.get(self, "application")

    @property
    @pulumi.getter(name="clientId")
    def client_id(self) -> Optional[builtins.str]:
        """
        (Output)
        The client id of the Azure Active Directory Application.
        """
        return pulumi.get(self, "client_id")

    @property
    @pulumi.getter(name="federatedApplicationClientId")
    def federated_application_client_id(self) -> Optional[builtins.str]:
        """
        The Azure Application (client) ID where the federated credentials will be hosted.
        """
        return pulumi.get(self, "federated_application_client_id")

    @property
    @pulumi.getter
    def identity(self) -> Optional[builtins.str]:
        """
        (Output)
        A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's Azure Active Directory Application.
        """
        return pulumi.get(self, "identity")

    @property
    @pulumi.getter(name="objectId")
    def object_id(self) -> Optional[builtins.str]:
        """
        (Output)
        The object id of the Azure Active Directory Application.
        """
        return pulumi.get(self, "object_id")

    @property
    @pulumi.getter(name="redirectUri")
    def redirect_uri(self) -> Optional[builtins.str]:
        """
        (Output)
        The URL user will be redirected to after granting consent during connection setup.
        """
        return pulumi.get(self, "redirect_uri")


@pulumi.output_type
class ConnectionCloudResource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "serviceAccountId":
            suggest = "service_account_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionCloudResource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionCloudResource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionCloudResource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 service_account_id: Optional[builtins.str] = None):
        """
        :param builtins.str service_account_id: (Output)
               The account ID of the service created for the purpose of this connection.
        """
        if service_account_id is not None:
            pulumi.set(__self__, "service_account_id", service_account_id)

    @property
    @pulumi.getter(name="serviceAccountId")
    def service_account_id(self) -> Optional[builtins.str]:
        """
        (Output)
        The account ID of the service created for the purpose of this connection.
        """
        return pulumi.get(self, "service_account_id")


@pulumi.output_type
class ConnectionCloudSpanner(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "databaseRole":
            suggest = "database_role"
        elif key == "maxParallelism":
            suggest = "max_parallelism"
        elif key == "useDataBoost":
            suggest = "use_data_boost"
        elif key == "useParallelism":
            suggest = "use_parallelism"
        elif key == "useServerlessAnalytics":
            suggest = "use_serverless_analytics"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionCloudSpanner. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionCloudSpanner.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionCloudSpanner.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 database: builtins.str,
                 database_role: Optional[builtins.str] = None,
                 max_parallelism: Optional[builtins.int] = None,
                 use_data_boost: Optional[builtins.bool] = None,
                 use_parallelism: Optional[builtins.bool] = None,
                 use_serverless_analytics: Optional[builtins.bool] = None):
        """
        :param builtins.str database: Cloud Spanner database in the form `project/instance/database'.
        :param builtins.str database_role: Cloud Spanner database role for fine-grained access control. The Cloud Spanner admin should have provisioned the database role with appropriate permissions, such as `SELECT` and `INSERT`. Other users should only use roles provided by their Cloud Spanner admins. The database role name must start with a letter, and can only contain letters, numbers, and underscores. For more details, see https://cloud.google.com/spanner/docs/fgac-about.
        :param builtins.int max_parallelism: Allows setting max parallelism per query when executing on Spanner independent compute resources. If unspecified, default values of parallelism are chosen that are dependent on the Cloud Spanner instance configuration. `useParallelism` and `useDataBoost` must be set when setting max parallelism.
        :param builtins.bool use_data_boost: If set, the request will be executed via Spanner independent compute resources. `use_parallelism` must be set when using data boost.
        :param builtins.bool use_parallelism: If parallelism should be used when reading from Cloud Spanner.
        :param builtins.bool use_serverless_analytics: (Optional, Deprecated)
               If the serverless analytics service should be used to read data from Cloud Spanner. `useParallelism` must be set when using serverless analytics.
               
               > **Warning:** `useServerlessAnalytics` is deprecated and will be removed in a future major release. Use `useDataBoost` instead.
        """
        pulumi.set(__self__, "database", database)
        if database_role is not None:
            pulumi.set(__self__, "database_role", database_role)
        if max_parallelism is not None:
            pulumi.set(__self__, "max_parallelism", max_parallelism)
        if use_data_boost is not None:
            pulumi.set(__self__, "use_data_boost", use_data_boost)
        if use_parallelism is not None:
            pulumi.set(__self__, "use_parallelism", use_parallelism)
        if use_serverless_analytics is not None:
            pulumi.set(__self__, "use_serverless_analytics", use_serverless_analytics)

    @property
    @pulumi.getter
    def database(self) -> builtins.str:
        """
        Cloud Spanner database in the form `project/instance/database'.
        """
        return pulumi.get(self, "database")

    @property
    @pulumi.getter(name="databaseRole")
    def database_role(self) -> Optional[builtins.str]:
        """
        Cloud Spanner database role for fine-grained access control. The Cloud Spanner admin should have provisioned the database role with appropriate permissions, such as `SELECT` and `INSERT`. Other users should only use roles provided by their Cloud Spanner admins. The database role name must start with a letter, and can only contain letters, numbers, and underscores. For more details, see https://cloud.google.com/spanner/docs/fgac-about.
        """
        return pulumi.get(self, "database_role")

    @property
    @pulumi.getter(name="maxParallelism")
    def max_parallelism(self) -> Optional[builtins.int]:
        """
        Allows setting max parallelism per query when executing on Spanner independent compute resources. If unspecified, default values of parallelism are chosen that are dependent on the Cloud Spanner instance configuration. `useParallelism` and `useDataBoost` must be set when setting max parallelism.
        """
        return pulumi.get(self, "max_parallelism")

    @property
    @pulumi.getter(name="useDataBoost")
    def use_data_boost(self) -> Optional[builtins.bool]:
        """
        If set, the request will be executed via Spanner independent compute resources. `use_parallelism` must be set when using data boost.
        """
        return pulumi.get(self, "use_data_boost")

    @property
    @pulumi.getter(name="useParallelism")
    def use_parallelism(self) -> Optional[builtins.bool]:
        """
        If parallelism should be used when reading from Cloud Spanner.
        """
        return pulumi.get(self, "use_parallelism")

    @property
    @pulumi.getter(name="useServerlessAnalytics")
    @_utilities.deprecated("""`useServerlessAnalytics` is deprecated and will be removed in a future major release. Use `useDataBoost` instead.""")
    def use_serverless_analytics(self) -> Optional[builtins.bool]:
        """
        (Optional, Deprecated)
        If the serverless analytics service should be used to read data from Cloud Spanner. `useParallelism` must be set when using serverless analytics.

        > **Warning:** `useServerlessAnalytics` is deprecated and will be removed in a future major release. Use `useDataBoost` instead.
        """
        return pulumi.get(self, "use_serverless_analytics")


@pulumi.output_type
class ConnectionCloudSql(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "instanceId":
            suggest = "instance_id"
        elif key == "serviceAccountId":
            suggest = "service_account_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionCloudSql. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionCloudSql.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionCloudSql.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 credential: 'outputs.ConnectionCloudSqlCredential',
                 database: builtins.str,
                 instance_id: builtins.str,
                 type: builtins.str,
                 service_account_id: Optional[builtins.str] = None):
        """
        :param 'ConnectionCloudSqlCredentialArgs' credential: Cloud SQL properties.
               Structure is documented below.
        :param builtins.str database: Database name.
        :param builtins.str instance_id: Cloud SQL instance ID in the form project:location:instance.
        :param builtins.str type: Type of the Cloud SQL database.
               Possible values are: `DATABASE_TYPE_UNSPECIFIED`, `POSTGRES`, `MYSQL`.
        :param builtins.str service_account_id: (Output)
               When the connection is used in the context of an operation in BigQuery, this service account will serve as the identity being used for connecting to the CloudSQL instance specified in this connection.
        """
        pulumi.set(__self__, "credential", credential)
        pulumi.set(__self__, "database", database)
        pulumi.set(__self__, "instance_id", instance_id)
        pulumi.set(__self__, "type", type)
        if service_account_id is not None:
            pulumi.set(__self__, "service_account_id", service_account_id)

    @property
    @pulumi.getter
    def credential(self) -> 'outputs.ConnectionCloudSqlCredential':
        """
        Cloud SQL properties.
        Structure is documented below.
        """
        return pulumi.get(self, "credential")

    @property
    @pulumi.getter
    def database(self) -> builtins.str:
        """
        Database name.
        """
        return pulumi.get(self, "database")

    @property
    @pulumi.getter(name="instanceId")
    def instance_id(self) -> builtins.str:
        """
        Cloud SQL instance ID in the form project:location:instance.
        """
        return pulumi.get(self, "instance_id")

    @property
    @pulumi.getter
    def type(self) -> builtins.str:
        """
        Type of the Cloud SQL database.
        Possible values are: `DATABASE_TYPE_UNSPECIFIED`, `POSTGRES`, `MYSQL`.
        """
        return pulumi.get(self, "type")

    @property
    @pulumi.getter(name="serviceAccountId")
    def service_account_id(self) -> Optional[builtins.str]:
        """
        (Output)
        When the connection is used in the context of an operation in BigQuery, this service account will serve as the identity being used for connecting to the CloudSQL instance specified in this connection.
        """
        return pulumi.get(self, "service_account_id")


@pulumi.output_type
class ConnectionCloudSqlCredential(dict):
    def __init__(__self__, *,
                 password: builtins.str,
                 username: builtins.str):
        """
        :param builtins.str password: Password for database.
               **Note**: This property is sensitive and will not be displayed in the plan.
        :param builtins.str username: Username for database.
        """
        pulumi.set(__self__, "password", password)
        pulumi.set(__self__, "username", username)

    @property
    @pulumi.getter
    def password(self) -> builtins.str:
        """
        Password for database.
        **Note**: This property is sensitive and will not be displayed in the plan.
        """
        return pulumi.get(self, "password")

    @property
    @pulumi.getter
    def username(self) -> builtins.str:
        """
        Username for database.
        """
        return pulumi.get(self, "username")


@pulumi.output_type
class ConnectionIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: builtins.str,
                 title: builtins.str,
                 description: Optional[builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def title(self) -> builtins.str:
        return pulumi.get(self, "title")

    @property
    @pulumi.getter
    def description(self) -> Optional[builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class ConnectionIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: builtins.str,
                 title: builtins.str,
                 description: Optional[builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def title(self) -> builtins.str:
        return pulumi.get(self, "title")

    @property
    @pulumi.getter
    def description(self) -> Optional[builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class ConnectionSpark(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metastoreServiceConfig":
            suggest = "metastore_service_config"
        elif key == "serviceAccountId":
            suggest = "service_account_id"
        elif key == "sparkHistoryServerConfig":
            suggest = "spark_history_server_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionSpark. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionSpark.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionSpark.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metastore_service_config: Optional['outputs.ConnectionSparkMetastoreServiceConfig'] = None,
                 service_account_id: Optional[builtins.str] = None,
                 spark_history_server_config: Optional['outputs.ConnectionSparkSparkHistoryServerConfig'] = None):
        """
        :param 'ConnectionSparkMetastoreServiceConfigArgs' metastore_service_config: Dataproc Metastore Service configuration for the connection.
               Structure is documented below.
        :param builtins.str service_account_id: (Output)
               The account ID of the service created for the purpose of this connection.
        :param 'ConnectionSparkSparkHistoryServerConfigArgs' spark_history_server_config: Spark History Server configuration for the connection.
               Structure is documented below.
        """
        if metastore_service_config is not None:
            pulumi.set(__self__, "metastore_service_config", metastore_service_config)
        if service_account_id is not None:
            pulumi.set(__self__, "service_account_id", service_account_id)
        if spark_history_server_config is not None:
            pulumi.set(__self__, "spark_history_server_config", spark_history_server_config)

    @property
    @pulumi.getter(name="metastoreServiceConfig")
    def metastore_service_config(self) -> Optional['outputs.ConnectionSparkMetastoreServiceConfig']:
        """
        Dataproc Metastore Service configuration for the connection.
        Structure is documented below.
        """
        return pulumi.get(self, "metastore_service_config")

    @property
    @pulumi.getter(name="serviceAccountId")
    def service_account_id(self) -> Optional[builtins.str]:
        """
        (Output)
        The account ID of the service created for the purpose of this connection.
        """
        return pulumi.get(self, "service_account_id")

    @property
    @pulumi.getter(name="sparkHistoryServerConfig")
    def spark_history_server_config(self) -> Optional['outputs.ConnectionSparkSparkHistoryServerConfig']:
        """
        Spark History Server configuration for the connection.
        Structure is documented below.
        """
        return pulumi.get(self, "spark_history_server_config")


@pulumi.output_type
class ConnectionSparkMetastoreServiceConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metastoreService":
            suggest = "metastore_service"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionSparkMetastoreServiceConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionSparkMetastoreServiceConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionSparkMetastoreServiceConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metastore_service: Optional[builtins.str] = None):
        """
        :param builtins.str metastore_service: Resource name of an existing Dataproc Metastore service in the form of projects/[projectId]/locations/[region]/services/[serviceId].
        """
        if metastore_service is not None:
            pulumi.set(__self__, "metastore_service", metastore_service)

    @property
    @pulumi.getter(name="metastoreService")
    def metastore_service(self) -> Optional[builtins.str]:
        """
        Resource name of an existing Dataproc Metastore service in the form of projects/[projectId]/locations/[region]/services/[serviceId].
        """
        return pulumi.get(self, "metastore_service")


@pulumi.output_type
class ConnectionSparkSparkHistoryServerConfig(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "dataprocCluster":
            suggest = "dataproc_cluster"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ConnectionSparkSparkHistoryServerConfig. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ConnectionSparkSparkHistoryServerConfig.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ConnectionSparkSparkHistoryServerConfig.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataproc_cluster: Optional[builtins.str] = None):
        """
        :param builtins.str dataproc_cluster: Resource name of an existing Dataproc Cluster to act as a Spark History Server for the connection if the form of projects/[projectId]/regions/[region]/clusters/[cluster_name].
        """
        if dataproc_cluster is not None:
            pulumi.set(__self__, "dataproc_cluster", dataproc_cluster)

    @property
    @pulumi.getter(name="dataprocCluster")
    def dataproc_cluster(self) -> Optional[builtins.str]:
        """
        Resource name of an existing Dataproc Cluster to act as a Spark History Server for the connection if the form of projects/[projectId]/regions/[region]/clusters/[cluster_name].
        """
        return pulumi.get(self, "dataproc_cluster")


@pulumi.output_type
class DataTransferConfigEmailPreferences(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enableFailureEmail":
            suggest = "enable_failure_email"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DataTransferConfigEmailPreferences. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DataTransferConfigEmailPreferences.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DataTransferConfigEmailPreferences.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_failure_email: builtins.bool):
        """
        :param builtins.bool enable_failure_email: If true, email notifications will be sent on transfer run failures.
        """
        pulumi.set(__self__, "enable_failure_email", enable_failure_email)

    @property
    @pulumi.getter(name="enableFailureEmail")
    def enable_failure_email(self) -> builtins.bool:
        """
        If true, email notifications will be sent on transfer run failures.
        """
        return pulumi.get(self, "enable_failure_email")


@pulumi.output_type
class DataTransferConfigEncryptionConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DataTransferConfigEncryptionConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DataTransferConfigEncryptionConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DataTransferConfigEncryptionConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: builtins.str):
        """
        :param builtins.str kms_key_name: The name of the KMS key used for encrypting BigQuery data.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        The name of the KMS key used for encrypting BigQuery data.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class DataTransferConfigScheduleOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "disableAutoScheduling":
            suggest = "disable_auto_scheduling"
        elif key == "endTime":
            suggest = "end_time"
        elif key == "startTime":
            suggest = "start_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DataTransferConfigScheduleOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DataTransferConfigScheduleOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DataTransferConfigScheduleOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 disable_auto_scheduling: Optional[builtins.bool] = None,
                 end_time: Optional[builtins.str] = None,
                 start_time: Optional[builtins.str] = None):
        """
        :param builtins.bool disable_auto_scheduling: If true, automatic scheduling of data transfer runs for this
               configuration will be disabled. The runs can be started on ad-hoc
               basis using transferConfigs.startManualRuns API. When automatic
               scheduling is disabled, the TransferConfig.schedule field will
               be ignored.
        :param builtins.str end_time: Defines time to stop scheduling transfer runs. A transfer run cannot be
               scheduled at or after the end time. The end time can be changed at any
               moment. The time when a data transfer can be triggered manually is not
               limited by this option.
        :param builtins.str start_time: Specifies time to start scheduling transfer runs. The first run will be
               scheduled at or after the start time according to a recurrence pattern
               defined in the schedule string. The start time can be changed at any
               moment. The time when a data transfer can be triggered manually is not
               limited by this option.
        """
        if disable_auto_scheduling is not None:
            pulumi.set(__self__, "disable_auto_scheduling", disable_auto_scheduling)
        if end_time is not None:
            pulumi.set(__self__, "end_time", end_time)
        if start_time is not None:
            pulumi.set(__self__, "start_time", start_time)

    @property
    @pulumi.getter(name="disableAutoScheduling")
    def disable_auto_scheduling(self) -> Optional[builtins.bool]:
        """
        If true, automatic scheduling of data transfer runs for this
        configuration will be disabled. The runs can be started on ad-hoc
        basis using transferConfigs.startManualRuns API. When automatic
        scheduling is disabled, the TransferConfig.schedule field will
        be ignored.
        """
        return pulumi.get(self, "disable_auto_scheduling")

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> Optional[builtins.str]:
        """
        Defines time to stop scheduling transfer runs. A transfer run cannot be
        scheduled at or after the end time. The end time can be changed at any
        moment. The time when a data transfer can be triggered manually is not
        limited by this option.
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> Optional[builtins.str]:
        """
        Specifies time to start scheduling transfer runs. The first run will be
        scheduled at or after the start time according to a recurrence pattern
        defined in the schedule string. The start time can be changed at any
        moment. The time when a data transfer can be triggered manually is not
        limited by this option.
        """
        return pulumi.get(self, "start_time")


@pulumi.output_type
class DataTransferConfigSensitiveParams(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "secretAccessKey":
            suggest = "secret_access_key"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DataTransferConfigSensitiveParams. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DataTransferConfigSensitiveParams.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DataTransferConfigSensitiveParams.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 secret_access_key: Optional[builtins.str] = None):
        """
        :param builtins.str secret_access_key: The Secret Access Key of the AWS account transferring data from.
        """
        if secret_access_key is not None:
            pulumi.set(__self__, "secret_access_key", secret_access_key)

    @property
    @pulumi.getter(name="secretAccessKey")
    def secret_access_key(self) -> Optional[builtins.str]:
        """
        The Secret Access Key of the AWS account transferring data from.
        """
        return pulumi.get(self, "secret_access_key")


@pulumi.output_type
class DatasetAccess(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "groupByEmail":
            suggest = "group_by_email"
        elif key == "iamMember":
            suggest = "iam_member"
        elif key == "specialGroup":
            suggest = "special_group"
        elif key == "userByEmail":
            suggest = "user_by_email"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetAccess. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetAccess.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetAccess.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 condition: Optional['outputs.DatasetAccessCondition'] = None,
                 dataset: Optional['outputs.DatasetAccessDataset'] = None,
                 domain: Optional[builtins.str] = None,
                 group_by_email: Optional[builtins.str] = None,
                 iam_member: Optional[builtins.str] = None,
                 role: Optional[builtins.str] = None,
                 routine: Optional['outputs.DatasetAccessRoutine'] = None,
                 special_group: Optional[builtins.str] = None,
                 user_by_email: Optional[builtins.str] = None,
                 view: Optional['outputs.DatasetAccessView'] = None):
        """
        :param 'DatasetAccessConditionArgs' condition: Condition for the binding. If CEL expression in this field is true, this
               access binding will be considered.
               Structure is documented below.
        :param 'DatasetAccessDatasetArgs' dataset: Grants all resources of particular types in a particular dataset read access to the current dataset.
               Structure is documented below.
        :param builtins.str domain: A domain to grant access to. Any users signed in with the
               domain specified will be granted the specified access
        :param builtins.str group_by_email: An email address of a Google Group to grant access to.
        :param builtins.str iam_member: Some other type of member that appears in the IAM Policy but isn't a user,
               group, domain, or special group. For example: `allUsers`
        :param builtins.str role: Describes the rights granted to the user specified by the other
               member of the access object. Basic, predefined, and custom roles
               are supported. Predefined roles that have equivalent basic roles
               are swapped by the API to their basic counterparts. See
               [official docs](https://cloud.google.com/bigquery/docs/access-control).
        :param 'DatasetAccessRoutineArgs' routine: A routine from a different dataset to grant access to. Queries
               executed against that routine will have read access to tables in
               this dataset. The role field is not required when this field is
               set. If that routine is updated by any user, access to the routine
               needs to be granted again via an update operation.
               Structure is documented below.
        :param builtins.str special_group: A special group to grant access to. Possible values include:
               * `projectOwners`: Owners of the enclosing project.
               * `projectReaders`: Readers of the enclosing project.
               * `projectWriters`: Writers of the enclosing project.
               * `allAuthenticatedUsers`: All authenticated BigQuery users.
        :param builtins.str user_by_email: An email address of a user to grant access to. For example:
               fred@example.com
        :param 'DatasetAccessViewArgs' view: A view from a different dataset to grant access to. Queries
               executed against that view will have read access to tables in
               this dataset. The role field is not required when this field is
               set. If that view is updated by any user, access to the view
               needs to be granted again via an update operation.
               Structure is documented below.
        """
        if condition is not None:
            pulumi.set(__self__, "condition", condition)
        if dataset is not None:
            pulumi.set(__self__, "dataset", dataset)
        if domain is not None:
            pulumi.set(__self__, "domain", domain)
        if group_by_email is not None:
            pulumi.set(__self__, "group_by_email", group_by_email)
        if iam_member is not None:
            pulumi.set(__self__, "iam_member", iam_member)
        if role is not None:
            pulumi.set(__self__, "role", role)
        if routine is not None:
            pulumi.set(__self__, "routine", routine)
        if special_group is not None:
            pulumi.set(__self__, "special_group", special_group)
        if user_by_email is not None:
            pulumi.set(__self__, "user_by_email", user_by_email)
        if view is not None:
            pulumi.set(__self__, "view", view)

    @property
    @pulumi.getter
    def condition(self) -> Optional['outputs.DatasetAccessCondition']:
        """
        Condition for the binding. If CEL expression in this field is true, this
        access binding will be considered.
        Structure is documented below.
        """
        return pulumi.get(self, "condition")

    @property
    @pulumi.getter
    def dataset(self) -> Optional['outputs.DatasetAccessDataset']:
        """
        Grants all resources of particular types in a particular dataset read access to the current dataset.
        Structure is documented below.
        """
        return pulumi.get(self, "dataset")

    @property
    @pulumi.getter
    def domain(self) -> Optional[builtins.str]:
        """
        A domain to grant access to. Any users signed in with the
        domain specified will be granted the specified access
        """
        return pulumi.get(self, "domain")

    @property
    @pulumi.getter(name="groupByEmail")
    def group_by_email(self) -> Optional[builtins.str]:
        """
        An email address of a Google Group to grant access to.
        """
        return pulumi.get(self, "group_by_email")

    @property
    @pulumi.getter(name="iamMember")
    def iam_member(self) -> Optional[builtins.str]:
        """
        Some other type of member that appears in the IAM Policy but isn't a user,
        group, domain, or special group. For example: `allUsers`
        """
        return pulumi.get(self, "iam_member")

    @property
    @pulumi.getter
    def role(self) -> Optional[builtins.str]:
        """
        Describes the rights granted to the user specified by the other
        member of the access object. Basic, predefined, and custom roles
        are supported. Predefined roles that have equivalent basic roles
        are swapped by the API to their basic counterparts. See
        [official docs](https://cloud.google.com/bigquery/docs/access-control).
        """
        return pulumi.get(self, "role")

    @property
    @pulumi.getter
    def routine(self) -> Optional['outputs.DatasetAccessRoutine']:
        """
        A routine from a different dataset to grant access to. Queries
        executed against that routine will have read access to tables in
        this dataset. The role field is not required when this field is
        set. If that routine is updated by any user, access to the routine
        needs to be granted again via an update operation.
        Structure is documented below.
        """
        return pulumi.get(self, "routine")

    @property
    @pulumi.getter(name="specialGroup")
    def special_group(self) -> Optional[builtins.str]:
        """
        A special group to grant access to. Possible values include:
        * `projectOwners`: Owners of the enclosing project.
        * `projectReaders`: Readers of the enclosing project.
        * `projectWriters`: Writers of the enclosing project.
        * `allAuthenticatedUsers`: All authenticated BigQuery users.
        """
        return pulumi.get(self, "special_group")

    @property
    @pulumi.getter(name="userByEmail")
    def user_by_email(self) -> Optional[builtins.str]:
        """
        An email address of a user to grant access to. For example:
        fred@example.com
        """
        return pulumi.get(self, "user_by_email")

    @property
    @pulumi.getter
    def view(self) -> Optional['outputs.DatasetAccessView']:
        """
        A view from a different dataset to grant access to. Queries
        executed against that view will have read access to tables in
        this dataset. The role field is not required when this field is
        set. If that view is updated by any user, access to the view
        needs to be granted again via an update operation.
        Structure is documented below.
        """
        return pulumi.get(self, "view")


@pulumi.output_type
class DatasetAccessAuthorizedDataset(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "targetTypes":
            suggest = "target_types"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetAccessAuthorizedDataset. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetAccessAuthorizedDataset.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetAccessAuthorizedDataset.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset: 'outputs.DatasetAccessAuthorizedDatasetDataset',
                 target_types: Sequence[builtins.str]):
        """
        :param 'DatasetAccessAuthorizedDatasetDatasetArgs' dataset: The dataset this entry applies to
               Structure is documented below.
        :param Sequence[builtins.str] target_types: Which resources in the dataset this entry applies to. Currently, only views are supported,
               but additional target types may be added in the future. Possible values: VIEWS
        """
        pulumi.set(__self__, "dataset", dataset)
        pulumi.set(__self__, "target_types", target_types)

    @property
    @pulumi.getter
    def dataset(self) -> 'outputs.DatasetAccessAuthorizedDatasetDataset':
        """
        The dataset this entry applies to
        Structure is documented below.
        """
        return pulumi.get(self, "dataset")

    @property
    @pulumi.getter(name="targetTypes")
    def target_types(self) -> Sequence[builtins.str]:
        """
        Which resources in the dataset this entry applies to. Currently, only views are supported,
        but additional target types may be added in the future. Possible values: VIEWS
        """
        return pulumi.get(self, "target_types")


@pulumi.output_type
class DatasetAccessAuthorizedDatasetDataset(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetAccessAuthorizedDatasetDataset. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetAccessAuthorizedDatasetDataset.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetAccessAuthorizedDatasetDataset.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str):
        """
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class DatasetAccessCondition(dict):
    def __init__(__self__, *,
                 expression: builtins.str,
                 description: Optional[builtins.str] = None,
                 location: Optional[builtins.str] = None,
                 title: Optional[builtins.str] = None):
        """
        :param builtins.str expression: Textual representation of an expression in Common Expression Language syntax.
        :param builtins.str description: Description of the expression. This is a longer text which describes the expression,
               e.g. when hovered over it in a UI.
        :param builtins.str location: String indicating the location of the expression for error reporting, e.g. a file
               name and a position in the file.
        :param builtins.str title: Title for the expression, i.e. a short string describing its purpose.
               This can be used e.g. in UIs which allow to enter the expression.
        """
        pulumi.set(__self__, "expression", expression)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if location is not None:
            pulumi.set(__self__, "location", location)
        if title is not None:
            pulumi.set(__self__, "title", title)

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        """
        Textual representation of an expression in Common Expression Language syntax.
        """
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def description(self) -> Optional[builtins.str]:
        """
        Description of the expression. This is a longer text which describes the expression,
        e.g. when hovered over it in a UI.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter
    def location(self) -> Optional[builtins.str]:
        """
        String indicating the location of the expression for error reporting, e.g. a file
        name and a position in the file.
        """
        return pulumi.get(self, "location")

    @property
    @pulumi.getter
    def title(self) -> Optional[builtins.str]:
        """
        Title for the expression, i.e. a short string describing its purpose.
        This can be used e.g. in UIs which allow to enter the expression.
        """
        return pulumi.get(self, "title")


@pulumi.output_type
class DatasetAccessDataset(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "targetTypes":
            suggest = "target_types"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetAccessDataset. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetAccessDataset.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetAccessDataset.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset: 'outputs.DatasetAccessDatasetDataset',
                 target_types: Sequence[builtins.str]):
        """
        :param 'DatasetAccessDatasetDatasetArgs' dataset: The dataset this entry applies to
               Structure is documented below.
        :param Sequence[builtins.str] target_types: Which resources in the dataset this entry applies to. Currently, only views are supported,
               but additional target types may be added in the future. Possible values: VIEWS
        """
        pulumi.set(__self__, "dataset", dataset)
        pulumi.set(__self__, "target_types", target_types)

    @property
    @pulumi.getter
    def dataset(self) -> 'outputs.DatasetAccessDatasetDataset':
        """
        The dataset this entry applies to
        Structure is documented below.
        """
        return pulumi.get(self, "dataset")

    @property
    @pulumi.getter(name="targetTypes")
    def target_types(self) -> Sequence[builtins.str]:
        """
        Which resources in the dataset this entry applies to. Currently, only views are supported,
        but additional target types may be added in the future. Possible values: VIEWS
        """
        return pulumi.get(self, "target_types")


@pulumi.output_type
class DatasetAccessDatasetDataset(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetAccessDatasetDataset. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetAccessDatasetDataset.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetAccessDatasetDataset.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str):
        """
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class DatasetAccessRoutine(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"
        elif key == "routineId":
            suggest = "routine_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetAccessRoutine. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetAccessRoutine.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetAccessRoutine.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str,
                 routine_id: builtins.str):
        """
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        :param builtins.str routine_id: The ID of the routine. The ID must contain only letters (a-z,
               A-Z), numbers (0-9), or underscores (_). The maximum length
               is 256 characters.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)
        pulumi.set(__self__, "routine_id", routine_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")

    @property
    @pulumi.getter(name="routineId")
    def routine_id(self) -> builtins.str:
        """
        The ID of the routine. The ID must contain only letters (a-z,
        A-Z), numbers (0-9), or underscores (_). The maximum length
        is 256 characters.
        """
        return pulumi.get(self, "routine_id")


@pulumi.output_type
class DatasetAccessView(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"
        elif key == "tableId":
            suggest = "table_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetAccessView. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetAccessView.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetAccessView.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str,
                 table_id: builtins.str):
        """
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        :param builtins.str table_id: The ID of the table. The ID must contain only letters (a-z,
               A-Z), numbers (0-9), or underscores (_). The maximum length
               is 1,024 characters.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)
        pulumi.set(__self__, "table_id", table_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The ID of the table. The ID must contain only letters (a-z,
        A-Z), numbers (0-9), or underscores (_). The maximum length
        is 1,024 characters.
        """
        return pulumi.get(self, "table_id")


@pulumi.output_type
class DatasetDefaultEncryptionConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetDefaultEncryptionConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetDefaultEncryptionConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetDefaultEncryptionConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: builtins.str):
        """
        :param builtins.str kms_key_name: Describes the Cloud KMS encryption key that will be used to protect destination
               BigQuery table. The BigQuery Service Account associated with your project requires
               access to this encryption key.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        Describes the Cloud KMS encryption key that will be used to protect destination
        BigQuery table. The BigQuery Service Account associated with your project requires
        access to this encryption key.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class DatasetExternalCatalogDatasetOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "defaultStorageLocationUri":
            suggest = "default_storage_location_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetExternalCatalogDatasetOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetExternalCatalogDatasetOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetExternalCatalogDatasetOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 default_storage_location_uri: Optional[builtins.str] = None,
                 parameters: Optional[Mapping[str, builtins.str]] = None):
        """
        :param builtins.str default_storage_location_uri: The storage location URI for all tables in the dataset. Equivalent to hive metastore's
               database locationUri. Maximum length of 1024 characters.
        :param Mapping[str, builtins.str] parameters: A map of key value pairs defining the parameters and properties of the open source schema.
               Maximum size of 2Mib.
        """
        if default_storage_location_uri is not None:
            pulumi.set(__self__, "default_storage_location_uri", default_storage_location_uri)
        if parameters is not None:
            pulumi.set(__self__, "parameters", parameters)

    @property
    @pulumi.getter(name="defaultStorageLocationUri")
    def default_storage_location_uri(self) -> Optional[builtins.str]:
        """
        The storage location URI for all tables in the dataset. Equivalent to hive metastore's
        database locationUri. Maximum length of 1024 characters.
        """
        return pulumi.get(self, "default_storage_location_uri")

    @property
    @pulumi.getter
    def parameters(self) -> Optional[Mapping[str, builtins.str]]:
        """
        A map of key value pairs defining the parameters and properties of the open source schema.
        Maximum size of 2Mib.
        """
        return pulumi.get(self, "parameters")


@pulumi.output_type
class DatasetExternalDatasetReference(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "externalSource":
            suggest = "external_source"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in DatasetExternalDatasetReference. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        DatasetExternalDatasetReference.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        DatasetExternalDatasetReference.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 connection: builtins.str,
                 external_source: builtins.str):
        """
        :param builtins.str connection: The connection id that is used to access the externalSource.
               Format: projects/{projectId}/locations/{locationId}/connections/{connectionId}
        :param builtins.str external_source: External source that backs this dataset.
        """
        pulumi.set(__self__, "connection", connection)
        pulumi.set(__self__, "external_source", external_source)

    @property
    @pulumi.getter
    def connection(self) -> builtins.str:
        """
        The connection id that is used to access the externalSource.
        Format: projects/{projectId}/locations/{locationId}/connections/{connectionId}
        """
        return pulumi.get(self, "connection")

    @property
    @pulumi.getter(name="externalSource")
    def external_source(self) -> builtins.str:
        """
        External source that backs this dataset.
        """
        return pulumi.get(self, "external_source")


@pulumi.output_type
class DatasetIamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: builtins.str,
                 title: builtins.str,
                 description: Optional[builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def title(self) -> builtins.str:
        return pulumi.get(self, "title")

    @property
    @pulumi.getter
    def description(self) -> Optional[builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class DatasetIamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: builtins.str,
                 title: builtins.str,
                 description: Optional[builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def title(self) -> builtins.str:
        return pulumi.get(self, "title")

    @property
    @pulumi.getter
    def description(self) -> Optional[builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class IamBindingCondition(dict):
    def __init__(__self__, *,
                 expression: builtins.str,
                 title: builtins.str,
                 description: Optional[builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def title(self) -> builtins.str:
        return pulumi.get(self, "title")

    @property
    @pulumi.getter
    def description(self) -> Optional[builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class IamMemberCondition(dict):
    def __init__(__self__, *,
                 expression: builtins.str,
                 title: builtins.str,
                 description: Optional[builtins.str] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def title(self) -> builtins.str:
        return pulumi.get(self, "title")

    @property
    @pulumi.getter
    def description(self) -> Optional[builtins.str]:
        return pulumi.get(self, "description")


@pulumi.output_type
class JobCopy(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "sourceTables":
            suggest = "source_tables"
        elif key == "createDisposition":
            suggest = "create_disposition"
        elif key == "destinationEncryptionConfiguration":
            suggest = "destination_encryption_configuration"
        elif key == "destinationTable":
            suggest = "destination_table"
        elif key == "writeDisposition":
            suggest = "write_disposition"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobCopy. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobCopy.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobCopy.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 source_tables: Sequence['outputs.JobCopySourceTable'],
                 create_disposition: Optional[builtins.str] = None,
                 destination_encryption_configuration: Optional['outputs.JobCopyDestinationEncryptionConfiguration'] = None,
                 destination_table: Optional['outputs.JobCopyDestinationTable'] = None,
                 write_disposition: Optional[builtins.str] = None):
        """
        :param Sequence['JobCopySourceTableArgs'] source_tables: Source tables to copy.
               Structure is documented below.
        :param builtins.str create_disposition: Specifies whether the job is allowed to create new tables. The following values are supported:
               CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
               CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
               Creation, truncation and append actions occur as one atomic update upon job completion
               Default value is `CREATE_IF_NEEDED`.
               Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
        :param 'JobCopyDestinationEncryptionConfigurationArgs' destination_encryption_configuration: Custom encryption configuration (e.g., Cloud KMS keys)
               Structure is documented below.
        :param 'JobCopyDestinationTableArgs' destination_table: The destination table.
               Structure is documented below.
        :param builtins.str write_disposition: Specifies the action that occurs if the destination table already exists. The following values are supported:
               WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
               WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
               WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
               Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
               Creation, truncation and append actions occur as one atomic update upon job completion.
               Default value is `WRITE_EMPTY`.
               Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
        """
        pulumi.set(__self__, "source_tables", source_tables)
        if create_disposition is not None:
            pulumi.set(__self__, "create_disposition", create_disposition)
        if destination_encryption_configuration is not None:
            pulumi.set(__self__, "destination_encryption_configuration", destination_encryption_configuration)
        if destination_table is not None:
            pulumi.set(__self__, "destination_table", destination_table)
        if write_disposition is not None:
            pulumi.set(__self__, "write_disposition", write_disposition)

    @property
    @pulumi.getter(name="sourceTables")
    def source_tables(self) -> Sequence['outputs.JobCopySourceTable']:
        """
        Source tables to copy.
        Structure is documented below.
        """
        return pulumi.get(self, "source_tables")

    @property
    @pulumi.getter(name="createDisposition")
    def create_disposition(self) -> Optional[builtins.str]:
        """
        Specifies whether the job is allowed to create new tables. The following values are supported:
        CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
        CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
        Creation, truncation and append actions occur as one atomic update upon job completion
        Default value is `CREATE_IF_NEEDED`.
        Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
        """
        return pulumi.get(self, "create_disposition")

    @property
    @pulumi.getter(name="destinationEncryptionConfiguration")
    def destination_encryption_configuration(self) -> Optional['outputs.JobCopyDestinationEncryptionConfiguration']:
        """
        Custom encryption configuration (e.g., Cloud KMS keys)
        Structure is documented below.
        """
        return pulumi.get(self, "destination_encryption_configuration")

    @property
    @pulumi.getter(name="destinationTable")
    def destination_table(self) -> Optional['outputs.JobCopyDestinationTable']:
        """
        The destination table.
        Structure is documented below.
        """
        return pulumi.get(self, "destination_table")

    @property
    @pulumi.getter(name="writeDisposition")
    def write_disposition(self) -> Optional[builtins.str]:
        """
        Specifies the action that occurs if the destination table already exists. The following values are supported:
        WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
        WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
        WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
        Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
        Creation, truncation and append actions occur as one atomic update upon job completion.
        Default value is `WRITE_EMPTY`.
        Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
        """
        return pulumi.get(self, "write_disposition")


@pulumi.output_type
class JobCopyDestinationEncryptionConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"
        elif key == "kmsKeyVersion":
            suggest = "kms_key_version"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobCopyDestinationEncryptionConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobCopyDestinationEncryptionConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobCopyDestinationEncryptionConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: builtins.str,
                 kms_key_version: Optional[builtins.str] = None):
        """
        :param builtins.str kms_key_name: Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
               The BigQuery Service Account associated with your project requires access to this encryption key.
        :param builtins.str kms_key_version: (Output)
               Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)
        if kms_key_version is not None:
            pulumi.set(__self__, "kms_key_version", kms_key_version)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
        The BigQuery Service Account associated with your project requires access to this encryption key.
        """
        return pulumi.get(self, "kms_key_name")

    @property
    @pulumi.getter(name="kmsKeyVersion")
    def kms_key_version(self) -> Optional[builtins.str]:
        """
        (Output)
        Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
        """
        return pulumi.get(self, "kms_key_version")


@pulumi.output_type
class JobCopyDestinationTable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "tableId":
            suggest = "table_id"
        elif key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobCopyDestinationTable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobCopyDestinationTable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobCopyDestinationTable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 table_id: builtins.str,
                 dataset_id: Optional[builtins.str] = None,
                 project_id: Optional[builtins.str] = None):
        """
        :param builtins.str table_id: The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
               or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "table_id", table_id)
        if dataset_id is not None:
            pulumi.set(__self__, "dataset_id", dataset_id)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
        or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        """
        return pulumi.get(self, "table_id")

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> Optional[builtins.str]:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[builtins.str]:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class JobCopySourceTable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "tableId":
            suggest = "table_id"
        elif key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobCopySourceTable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobCopySourceTable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobCopySourceTable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 table_id: builtins.str,
                 dataset_id: Optional[builtins.str] = None,
                 project_id: Optional[builtins.str] = None):
        """
        :param builtins.str table_id: The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
               or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "table_id", table_id)
        if dataset_id is not None:
            pulumi.set(__self__, "dataset_id", dataset_id)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
        or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        """
        return pulumi.get(self, "table_id")

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> Optional[builtins.str]:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[builtins.str]:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class JobExtract(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "destinationUris":
            suggest = "destination_uris"
        elif key == "destinationFormat":
            suggest = "destination_format"
        elif key == "fieldDelimiter":
            suggest = "field_delimiter"
        elif key == "printHeader":
            suggest = "print_header"
        elif key == "sourceModel":
            suggest = "source_model"
        elif key == "sourceTable":
            suggest = "source_table"
        elif key == "useAvroLogicalTypes":
            suggest = "use_avro_logical_types"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobExtract. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobExtract.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobExtract.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 destination_uris: Sequence[builtins.str],
                 compression: Optional[builtins.str] = None,
                 destination_format: Optional[builtins.str] = None,
                 field_delimiter: Optional[builtins.str] = None,
                 print_header: Optional[builtins.bool] = None,
                 source_model: Optional['outputs.JobExtractSourceModel'] = None,
                 source_table: Optional['outputs.JobExtractSourceTable'] = None,
                 use_avro_logical_types: Optional[builtins.bool] = None):
        """
        :param Sequence[builtins.str] destination_uris: A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
        :param builtins.str compression: The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
               The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
        :param builtins.str destination_format: The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
               The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
               The default value for models is SAVED_MODEL.
        :param builtins.str field_delimiter: When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
               Default is ','
        :param builtins.bool print_header: Whether to print out a header row in the results. Default is true.
        :param 'JobExtractSourceModelArgs' source_model: A reference to the model being exported.
               Structure is documented below.
        :param 'JobExtractSourceTableArgs' source_table: A reference to the table being exported.
               Structure is documented below.
        :param builtins.bool use_avro_logical_types: Whether to use logical types when extracting to AVRO format.
        """
        pulumi.set(__self__, "destination_uris", destination_uris)
        if compression is not None:
            pulumi.set(__self__, "compression", compression)
        if destination_format is not None:
            pulumi.set(__self__, "destination_format", destination_format)
        if field_delimiter is not None:
            pulumi.set(__self__, "field_delimiter", field_delimiter)
        if print_header is not None:
            pulumi.set(__self__, "print_header", print_header)
        if source_model is not None:
            pulumi.set(__self__, "source_model", source_model)
        if source_table is not None:
            pulumi.set(__self__, "source_table", source_table)
        if use_avro_logical_types is not None:
            pulumi.set(__self__, "use_avro_logical_types", use_avro_logical_types)

    @property
    @pulumi.getter(name="destinationUris")
    def destination_uris(self) -> Sequence[builtins.str]:
        """
        A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
        """
        return pulumi.get(self, "destination_uris")

    @property
    @pulumi.getter
    def compression(self) -> Optional[builtins.str]:
        """
        The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
        The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
        """
        return pulumi.get(self, "compression")

    @property
    @pulumi.getter(name="destinationFormat")
    def destination_format(self) -> Optional[builtins.str]:
        """
        The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
        The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
        The default value for models is SAVED_MODEL.
        """
        return pulumi.get(self, "destination_format")

    @property
    @pulumi.getter(name="fieldDelimiter")
    def field_delimiter(self) -> Optional[builtins.str]:
        """
        When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
        Default is ','
        """
        return pulumi.get(self, "field_delimiter")

    @property
    @pulumi.getter(name="printHeader")
    def print_header(self) -> Optional[builtins.bool]:
        """
        Whether to print out a header row in the results. Default is true.
        """
        return pulumi.get(self, "print_header")

    @property
    @pulumi.getter(name="sourceModel")
    def source_model(self) -> Optional['outputs.JobExtractSourceModel']:
        """
        A reference to the model being exported.
        Structure is documented below.
        """
        return pulumi.get(self, "source_model")

    @property
    @pulumi.getter(name="sourceTable")
    def source_table(self) -> Optional['outputs.JobExtractSourceTable']:
        """
        A reference to the table being exported.
        Structure is documented below.
        """
        return pulumi.get(self, "source_table")

    @property
    @pulumi.getter(name="useAvroLogicalTypes")
    def use_avro_logical_types(self) -> Optional[builtins.bool]:
        """
        Whether to use logical types when extracting to AVRO format.
        """
        return pulumi.get(self, "use_avro_logical_types")


@pulumi.output_type
class JobExtractSourceModel(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "modelId":
            suggest = "model_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobExtractSourceModel. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobExtractSourceModel.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobExtractSourceModel.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 model_id: builtins.str,
                 project_id: builtins.str):
        """
        :param builtins.str dataset_id: The ID of the dataset containing this model.
        :param builtins.str model_id: The ID of the model.
               
               - - -
        :param builtins.str project_id: The ID of the project containing this model.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "model_id", model_id)
        pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The ID of the dataset containing this model.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="modelId")
    def model_id(self) -> builtins.str:
        """
        The ID of the model.

        - - -
        """
        return pulumi.get(self, "model_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this model.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class JobExtractSourceTable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "tableId":
            suggest = "table_id"
        elif key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobExtractSourceTable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobExtractSourceTable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobExtractSourceTable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 table_id: builtins.str,
                 dataset_id: Optional[builtins.str] = None,
                 project_id: Optional[builtins.str] = None):
        """
        :param builtins.str table_id: The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
               or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "table_id", table_id)
        if dataset_id is not None:
            pulumi.set(__self__, "dataset_id", dataset_id)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
        or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        """
        return pulumi.get(self, "table_id")

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> Optional[builtins.str]:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[builtins.str]:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class JobLoad(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "destinationTable":
            suggest = "destination_table"
        elif key == "sourceUris":
            suggest = "source_uris"
        elif key == "allowJaggedRows":
            suggest = "allow_jagged_rows"
        elif key == "allowQuotedNewlines":
            suggest = "allow_quoted_newlines"
        elif key == "createDisposition":
            suggest = "create_disposition"
        elif key == "destinationEncryptionConfiguration":
            suggest = "destination_encryption_configuration"
        elif key == "fieldDelimiter":
            suggest = "field_delimiter"
        elif key == "ignoreUnknownValues":
            suggest = "ignore_unknown_values"
        elif key == "jsonExtension":
            suggest = "json_extension"
        elif key == "maxBadRecords":
            suggest = "max_bad_records"
        elif key == "nullMarker":
            suggest = "null_marker"
        elif key == "parquetOptions":
            suggest = "parquet_options"
        elif key == "projectionFields":
            suggest = "projection_fields"
        elif key == "schemaUpdateOptions":
            suggest = "schema_update_options"
        elif key == "skipLeadingRows":
            suggest = "skip_leading_rows"
        elif key == "sourceFormat":
            suggest = "source_format"
        elif key == "timePartitioning":
            suggest = "time_partitioning"
        elif key == "writeDisposition":
            suggest = "write_disposition"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobLoad. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobLoad.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobLoad.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 destination_table: 'outputs.JobLoadDestinationTable',
                 source_uris: Sequence[builtins.str],
                 allow_jagged_rows: Optional[builtins.bool] = None,
                 allow_quoted_newlines: Optional[builtins.bool] = None,
                 autodetect: Optional[builtins.bool] = None,
                 create_disposition: Optional[builtins.str] = None,
                 destination_encryption_configuration: Optional['outputs.JobLoadDestinationEncryptionConfiguration'] = None,
                 encoding: Optional[builtins.str] = None,
                 field_delimiter: Optional[builtins.str] = None,
                 ignore_unknown_values: Optional[builtins.bool] = None,
                 json_extension: Optional[builtins.str] = None,
                 max_bad_records: Optional[builtins.int] = None,
                 null_marker: Optional[builtins.str] = None,
                 parquet_options: Optional['outputs.JobLoadParquetOptions'] = None,
                 projection_fields: Optional[Sequence[builtins.str]] = None,
                 quote: Optional[builtins.str] = None,
                 schema_update_options: Optional[Sequence[builtins.str]] = None,
                 skip_leading_rows: Optional[builtins.int] = None,
                 source_format: Optional[builtins.str] = None,
                 time_partitioning: Optional['outputs.JobLoadTimePartitioning'] = None,
                 write_disposition: Optional[builtins.str] = None):
        """
        :param 'JobLoadDestinationTableArgs' destination_table: The destination table to load the data into.
               Structure is documented below.
        :param Sequence[builtins.str] source_uris: The fully-qualified URIs that point to your data in Google Cloud.
               For Google Cloud Storage URIs: Each URI can contain one '\\*' wildcard character
               and it must come after the 'bucket' name. Size limits related to load jobs apply
               to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
               specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
               For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '\\*' wildcard character is not allowed.
        :param builtins.bool allow_jagged_rows: Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
               If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
               an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
        :param builtins.bool allow_quoted_newlines: Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
               The default value is false.
        :param builtins.bool autodetect: Indicates if we should automatically infer the options and schema for CSV and JSON sources.
        :param builtins.str create_disposition: Specifies whether the job is allowed to create new tables. The following values are supported:
               CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
               CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
               Creation, truncation and append actions occur as one atomic update upon job completion
               Default value is `CREATE_IF_NEEDED`.
               Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
        :param 'JobLoadDestinationEncryptionConfigurationArgs' destination_encryption_configuration: Custom encryption configuration (e.g., Cloud KMS keys)
               Structure is documented below.
        :param builtins.str encoding: The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
               The default value is UTF-8. BigQuery decodes the data after the raw, binary data
               has been split using the values of the quote and fieldDelimiter properties.
        :param builtins.str field_delimiter: The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
               To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
               the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
               data in its raw, binary state. BigQuery also supports the escape sequence "\\t" to specify a tab separator.
               The default value is a comma (',').
        :param builtins.bool ignore_unknown_values: Indicates if BigQuery should allow extra values that are not represented in the table schema.
               If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
               and if there are too many bad records, an invalid error is returned in the job result.
               The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
               CSV: Trailing columns
               JSON: Named values that don't match any column names
        :param builtins.str json_extension: If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
               For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
               GeoJSON: set to GEOJSON.
        :param builtins.int max_bad_records: The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
               an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
        :param builtins.str null_marker: Specifies a string that represents a null value in a CSV file. The default value is the empty string. If you set this
               property to a custom value, BigQuery throws an error if an
               empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
               an empty value.
        :param 'JobLoadParquetOptionsArgs' parquet_options: Parquet Options for load and make external tables.
               Structure is documented below.
        :param Sequence[builtins.str] projection_fields: If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
               Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
               If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
        :param builtins.str quote: The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
               and then uses the first byte of the encoded string to split the data in its raw, binary state.
               The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
               If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
        :param Sequence[builtins.str] schema_update_options: Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
               supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
               when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
               For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
               ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
               ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
        :param builtins.int skip_leading_rows: The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
               The default value is 0. This property is useful if you have header rows in the file that should be skipped.
               When autodetect is on, the behavior is the following:
               skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
               the row is read as data. Otherwise data is read starting from the second row.
               skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
               skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
               row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
        :param builtins.str source_format: The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
               For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
               For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
               The default value is CSV.
        :param 'JobLoadTimePartitioningArgs' time_partitioning: Time-based partitioning specification for the destination table.
               Structure is documented below.
        :param builtins.str write_disposition: Specifies the action that occurs if the destination table already exists. The following values are supported:
               WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
               WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
               WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
               Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
               Creation, truncation and append actions occur as one atomic update upon job completion.
               Default value is `WRITE_EMPTY`.
               Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
        """
        pulumi.set(__self__, "destination_table", destination_table)
        pulumi.set(__self__, "source_uris", source_uris)
        if allow_jagged_rows is not None:
            pulumi.set(__self__, "allow_jagged_rows", allow_jagged_rows)
        if allow_quoted_newlines is not None:
            pulumi.set(__self__, "allow_quoted_newlines", allow_quoted_newlines)
        if autodetect is not None:
            pulumi.set(__self__, "autodetect", autodetect)
        if create_disposition is not None:
            pulumi.set(__self__, "create_disposition", create_disposition)
        if destination_encryption_configuration is not None:
            pulumi.set(__self__, "destination_encryption_configuration", destination_encryption_configuration)
        if encoding is not None:
            pulumi.set(__self__, "encoding", encoding)
        if field_delimiter is not None:
            pulumi.set(__self__, "field_delimiter", field_delimiter)
        if ignore_unknown_values is not None:
            pulumi.set(__self__, "ignore_unknown_values", ignore_unknown_values)
        if json_extension is not None:
            pulumi.set(__self__, "json_extension", json_extension)
        if max_bad_records is not None:
            pulumi.set(__self__, "max_bad_records", max_bad_records)
        if null_marker is not None:
            pulumi.set(__self__, "null_marker", null_marker)
        if parquet_options is not None:
            pulumi.set(__self__, "parquet_options", parquet_options)
        if projection_fields is not None:
            pulumi.set(__self__, "projection_fields", projection_fields)
        if quote is not None:
            pulumi.set(__self__, "quote", quote)
        if schema_update_options is not None:
            pulumi.set(__self__, "schema_update_options", schema_update_options)
        if skip_leading_rows is not None:
            pulumi.set(__self__, "skip_leading_rows", skip_leading_rows)
        if source_format is not None:
            pulumi.set(__self__, "source_format", source_format)
        if time_partitioning is not None:
            pulumi.set(__self__, "time_partitioning", time_partitioning)
        if write_disposition is not None:
            pulumi.set(__self__, "write_disposition", write_disposition)

    @property
    @pulumi.getter(name="destinationTable")
    def destination_table(self) -> 'outputs.JobLoadDestinationTable':
        """
        The destination table to load the data into.
        Structure is documented below.
        """
        return pulumi.get(self, "destination_table")

    @property
    @pulumi.getter(name="sourceUris")
    def source_uris(self) -> Sequence[builtins.str]:
        """
        The fully-qualified URIs that point to your data in Google Cloud.
        For Google Cloud Storage URIs: Each URI can contain one '\\*' wildcard character
        and it must come after the 'bucket' name. Size limits related to load jobs apply
        to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
        specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
        For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '\\*' wildcard character is not allowed.
        """
        return pulumi.get(self, "source_uris")

    @property
    @pulumi.getter(name="allowJaggedRows")
    def allow_jagged_rows(self) -> Optional[builtins.bool]:
        """
        Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
        If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
        an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
        """
        return pulumi.get(self, "allow_jagged_rows")

    @property
    @pulumi.getter(name="allowQuotedNewlines")
    def allow_quoted_newlines(self) -> Optional[builtins.bool]:
        """
        Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
        The default value is false.
        """
        return pulumi.get(self, "allow_quoted_newlines")

    @property
    @pulumi.getter
    def autodetect(self) -> Optional[builtins.bool]:
        """
        Indicates if we should automatically infer the options and schema for CSV and JSON sources.
        """
        return pulumi.get(self, "autodetect")

    @property
    @pulumi.getter(name="createDisposition")
    def create_disposition(self) -> Optional[builtins.str]:
        """
        Specifies whether the job is allowed to create new tables. The following values are supported:
        CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
        CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
        Creation, truncation and append actions occur as one atomic update upon job completion
        Default value is `CREATE_IF_NEEDED`.
        Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
        """
        return pulumi.get(self, "create_disposition")

    @property
    @pulumi.getter(name="destinationEncryptionConfiguration")
    def destination_encryption_configuration(self) -> Optional['outputs.JobLoadDestinationEncryptionConfiguration']:
        """
        Custom encryption configuration (e.g., Cloud KMS keys)
        Structure is documented below.
        """
        return pulumi.get(self, "destination_encryption_configuration")

    @property
    @pulumi.getter
    def encoding(self) -> Optional[builtins.str]:
        """
        The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
        The default value is UTF-8. BigQuery decodes the data after the raw, binary data
        has been split using the values of the quote and fieldDelimiter properties.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="fieldDelimiter")
    def field_delimiter(self) -> Optional[builtins.str]:
        """
        The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
        To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
        the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
        data in its raw, binary state. BigQuery also supports the escape sequence "\\t" to specify a tab separator.
        The default value is a comma (',').
        """
        return pulumi.get(self, "field_delimiter")

    @property
    @pulumi.getter(name="ignoreUnknownValues")
    def ignore_unknown_values(self) -> Optional[builtins.bool]:
        """
        Indicates if BigQuery should allow extra values that are not represented in the table schema.
        If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
        and if there are too many bad records, an invalid error is returned in the job result.
        The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
        CSV: Trailing columns
        JSON: Named values that don't match any column names
        """
        return pulumi.get(self, "ignore_unknown_values")

    @property
    @pulumi.getter(name="jsonExtension")
    def json_extension(self) -> Optional[builtins.str]:
        """
        If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
        For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
        GeoJSON: set to GEOJSON.
        """
        return pulumi.get(self, "json_extension")

    @property
    @pulumi.getter(name="maxBadRecords")
    def max_bad_records(self) -> Optional[builtins.int]:
        """
        The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
        an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
        """
        return pulumi.get(self, "max_bad_records")

    @property
    @pulumi.getter(name="nullMarker")
    def null_marker(self) -> Optional[builtins.str]:
        """
        Specifies a string that represents a null value in a CSV file. The default value is the empty string. If you set this
        property to a custom value, BigQuery throws an error if an
        empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
        an empty value.
        """
        return pulumi.get(self, "null_marker")

    @property
    @pulumi.getter(name="parquetOptions")
    def parquet_options(self) -> Optional['outputs.JobLoadParquetOptions']:
        """
        Parquet Options for load and make external tables.
        Structure is documented below.
        """
        return pulumi.get(self, "parquet_options")

    @property
    @pulumi.getter(name="projectionFields")
    def projection_fields(self) -> Optional[Sequence[builtins.str]]:
        """
        If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
        Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
        If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
        """
        return pulumi.get(self, "projection_fields")

    @property
    @pulumi.getter
    def quote(self) -> Optional[builtins.str]:
        """
        The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
        and then uses the first byte of the encoded string to split the data in its raw, binary state.
        The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
        If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
        """
        return pulumi.get(self, "quote")

    @property
    @pulumi.getter(name="schemaUpdateOptions")
    def schema_update_options(self) -> Optional[Sequence[builtins.str]]:
        """
        Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
        supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
        when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
        For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
        ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
        ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
        """
        return pulumi.get(self, "schema_update_options")

    @property
    @pulumi.getter(name="skipLeadingRows")
    def skip_leading_rows(self) -> Optional[builtins.int]:
        """
        The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
        The default value is 0. This property is useful if you have header rows in the file that should be skipped.
        When autodetect is on, the behavior is the following:
        skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
        the row is read as data. Otherwise data is read starting from the second row.
        skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
        skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
        row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
        """
        return pulumi.get(self, "skip_leading_rows")

    @property
    @pulumi.getter(name="sourceFormat")
    def source_format(self) -> Optional[builtins.str]:
        """
        The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
        For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
        For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
        The default value is CSV.
        """
        return pulumi.get(self, "source_format")

    @property
    @pulumi.getter(name="timePartitioning")
    def time_partitioning(self) -> Optional['outputs.JobLoadTimePartitioning']:
        """
        Time-based partitioning specification for the destination table.
        Structure is documented below.
        """
        return pulumi.get(self, "time_partitioning")

    @property
    @pulumi.getter(name="writeDisposition")
    def write_disposition(self) -> Optional[builtins.str]:
        """
        Specifies the action that occurs if the destination table already exists. The following values are supported:
        WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
        WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
        WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
        Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
        Creation, truncation and append actions occur as one atomic update upon job completion.
        Default value is `WRITE_EMPTY`.
        Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
        """
        return pulumi.get(self, "write_disposition")


@pulumi.output_type
class JobLoadDestinationEncryptionConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"
        elif key == "kmsKeyVersion":
            suggest = "kms_key_version"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobLoadDestinationEncryptionConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobLoadDestinationEncryptionConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobLoadDestinationEncryptionConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: builtins.str,
                 kms_key_version: Optional[builtins.str] = None):
        """
        :param builtins.str kms_key_name: Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
               The BigQuery Service Account associated with your project requires access to this encryption key.
        :param builtins.str kms_key_version: (Output)
               Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)
        if kms_key_version is not None:
            pulumi.set(__self__, "kms_key_version", kms_key_version)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
        The BigQuery Service Account associated with your project requires access to this encryption key.
        """
        return pulumi.get(self, "kms_key_name")

    @property
    @pulumi.getter(name="kmsKeyVersion")
    def kms_key_version(self) -> Optional[builtins.str]:
        """
        (Output)
        Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
        """
        return pulumi.get(self, "kms_key_version")


@pulumi.output_type
class JobLoadDestinationTable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "tableId":
            suggest = "table_id"
        elif key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobLoadDestinationTable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobLoadDestinationTable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobLoadDestinationTable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 table_id: builtins.str,
                 dataset_id: Optional[builtins.str] = None,
                 project_id: Optional[builtins.str] = None):
        """
        :param builtins.str table_id: The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
               or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "table_id", table_id)
        if dataset_id is not None:
            pulumi.set(__self__, "dataset_id", dataset_id)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
        or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        """
        return pulumi.get(self, "table_id")

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> Optional[builtins.str]:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[builtins.str]:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class JobLoadParquetOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enableListInference":
            suggest = "enable_list_inference"
        elif key == "enumAsString":
            suggest = "enum_as_string"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobLoadParquetOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobLoadParquetOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobLoadParquetOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_list_inference: Optional[builtins.bool] = None,
                 enum_as_string: Optional[builtins.bool] = None):
        """
        :param builtins.bool enable_list_inference: If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
        :param builtins.bool enum_as_string: If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
        """
        if enable_list_inference is not None:
            pulumi.set(__self__, "enable_list_inference", enable_list_inference)
        if enum_as_string is not None:
            pulumi.set(__self__, "enum_as_string", enum_as_string)

    @property
    @pulumi.getter(name="enableListInference")
    def enable_list_inference(self) -> Optional[builtins.bool]:
        """
        If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
        """
        return pulumi.get(self, "enable_list_inference")

    @property
    @pulumi.getter(name="enumAsString")
    def enum_as_string(self) -> Optional[builtins.bool]:
        """
        If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
        """
        return pulumi.get(self, "enum_as_string")


@pulumi.output_type
class JobLoadTimePartitioning(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "expirationMs":
            suggest = "expiration_ms"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobLoadTimePartitioning. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobLoadTimePartitioning.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobLoadTimePartitioning.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 type: builtins.str,
                 expiration_ms: Optional[builtins.str] = None,
                 field: Optional[builtins.str] = None):
        """
        :param builtins.str type: The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
               but in OnePlatform the field will be treated as unset.
        :param builtins.str expiration_ms: Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
        :param builtins.str field: If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
               The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
               A wrapper is used here because an empty string is an invalid value.
        """
        pulumi.set(__self__, "type", type)
        if expiration_ms is not None:
            pulumi.set(__self__, "expiration_ms", expiration_ms)
        if field is not None:
            pulumi.set(__self__, "field", field)

    @property
    @pulumi.getter
    def type(self) -> builtins.str:
        """
        The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
        but in OnePlatform the field will be treated as unset.
        """
        return pulumi.get(self, "type")

    @property
    @pulumi.getter(name="expirationMs")
    def expiration_ms(self) -> Optional[builtins.str]:
        """
        Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
        """
        return pulumi.get(self, "expiration_ms")

    @property
    @pulumi.getter
    def field(self) -> Optional[builtins.str]:
        """
        If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
        The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
        A wrapper is used here because an empty string is an invalid value.
        """
        return pulumi.get(self, "field")


@pulumi.output_type
class JobQuery(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "allowLargeResults":
            suggest = "allow_large_results"
        elif key == "createDisposition":
            suggest = "create_disposition"
        elif key == "defaultDataset":
            suggest = "default_dataset"
        elif key == "destinationEncryptionConfiguration":
            suggest = "destination_encryption_configuration"
        elif key == "destinationTable":
            suggest = "destination_table"
        elif key == "flattenResults":
            suggest = "flatten_results"
        elif key == "maximumBillingTier":
            suggest = "maximum_billing_tier"
        elif key == "maximumBytesBilled":
            suggest = "maximum_bytes_billed"
        elif key == "parameterMode":
            suggest = "parameter_mode"
        elif key == "schemaUpdateOptions":
            suggest = "schema_update_options"
        elif key == "scriptOptions":
            suggest = "script_options"
        elif key == "useLegacySql":
            suggest = "use_legacy_sql"
        elif key == "useQueryCache":
            suggest = "use_query_cache"
        elif key == "userDefinedFunctionResources":
            suggest = "user_defined_function_resources"
        elif key == "writeDisposition":
            suggest = "write_disposition"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobQuery. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobQuery.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobQuery.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 query: builtins.str,
                 allow_large_results: Optional[builtins.bool] = None,
                 continuous: Optional[builtins.bool] = None,
                 create_disposition: Optional[builtins.str] = None,
                 default_dataset: Optional['outputs.JobQueryDefaultDataset'] = None,
                 destination_encryption_configuration: Optional['outputs.JobQueryDestinationEncryptionConfiguration'] = None,
                 destination_table: Optional['outputs.JobQueryDestinationTable'] = None,
                 flatten_results: Optional[builtins.bool] = None,
                 maximum_billing_tier: Optional[builtins.int] = None,
                 maximum_bytes_billed: Optional[builtins.str] = None,
                 parameter_mode: Optional[builtins.str] = None,
                 priority: Optional[builtins.str] = None,
                 schema_update_options: Optional[Sequence[builtins.str]] = None,
                 script_options: Optional['outputs.JobQueryScriptOptions'] = None,
                 use_legacy_sql: Optional[builtins.bool] = None,
                 use_query_cache: Optional[builtins.bool] = None,
                 user_defined_function_resources: Optional[Sequence['outputs.JobQueryUserDefinedFunctionResource']] = None,
                 write_disposition: Optional[builtins.str] = None):
        """
        :param builtins.str query: SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
               *NOTE*: queries containing [DML language](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language)
               (`DELETE`, `UPDATE`, `MERGE`, `INSERT`) must specify `create_disposition = ""` and `write_disposition = ""`.
        :param builtins.bool allow_large_results: If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
               Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
               However, you must still set destinationTable when result size exceeds the allowed maximum response size.
        :param builtins.bool continuous: Whether to run the query as continuous or a regular query.
        :param builtins.str create_disposition: Specifies whether the job is allowed to create new tables. The following values are supported:
               CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
               CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
               Creation, truncation and append actions occur as one atomic update upon job completion
               Default value is `CREATE_IF_NEEDED`.
               Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
        :param 'JobQueryDefaultDatasetArgs' default_dataset: Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
               Structure is documented below.
        :param 'JobQueryDestinationEncryptionConfigurationArgs' destination_encryption_configuration: Custom encryption configuration (e.g., Cloud KMS keys)
               Structure is documented below.
        :param 'JobQueryDestinationTableArgs' destination_table: Describes the table where the query results should be stored.
               This property must be set for large results that exceed the maximum response size.
               For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
               Structure is documented below.
        :param builtins.bool flatten_results: If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
               allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
        :param builtins.int maximum_billing_tier: Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
               If unspecified, this will be set to your project default.
        :param builtins.str maximum_bytes_billed: Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
               If unspecified, this will be set to your project default.
        :param builtins.str parameter_mode: Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
        :param builtins.str priority: Specifies a priority for the query.
               Default value is `INTERACTIVE`.
               Possible values are: `INTERACTIVE`, `BATCH`.
        :param Sequence[builtins.str] schema_update_options: Allows the schema of the destination table to be updated as a side effect of the query job.
               Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
               when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
               specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
               One or more of the following values are specified:
               ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
               ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
        :param 'JobQueryScriptOptionsArgs' script_options: Options controlling the execution of scripts.
               Structure is documented below.
        :param builtins.bool use_legacy_sql: Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
               If set to false, the query will use BigQuery's standard SQL.
        :param builtins.bool use_query_cache: Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
               tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
               The default value is true.
        :param Sequence['JobQueryUserDefinedFunctionResourceArgs'] user_defined_function_resources: Describes user-defined function resources used in the query.
               Structure is documented below.
        :param builtins.str write_disposition: Specifies the action that occurs if the destination table already exists. The following values are supported:
               WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
               WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
               WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
               Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
               Creation, truncation and append actions occur as one atomic update upon job completion.
               Default value is `WRITE_EMPTY`.
               Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
        """
        pulumi.set(__self__, "query", query)
        if allow_large_results is not None:
            pulumi.set(__self__, "allow_large_results", allow_large_results)
        if continuous is not None:
            pulumi.set(__self__, "continuous", continuous)
        if create_disposition is not None:
            pulumi.set(__self__, "create_disposition", create_disposition)
        if default_dataset is not None:
            pulumi.set(__self__, "default_dataset", default_dataset)
        if destination_encryption_configuration is not None:
            pulumi.set(__self__, "destination_encryption_configuration", destination_encryption_configuration)
        if destination_table is not None:
            pulumi.set(__self__, "destination_table", destination_table)
        if flatten_results is not None:
            pulumi.set(__self__, "flatten_results", flatten_results)
        if maximum_billing_tier is not None:
            pulumi.set(__self__, "maximum_billing_tier", maximum_billing_tier)
        if maximum_bytes_billed is not None:
            pulumi.set(__self__, "maximum_bytes_billed", maximum_bytes_billed)
        if parameter_mode is not None:
            pulumi.set(__self__, "parameter_mode", parameter_mode)
        if priority is not None:
            pulumi.set(__self__, "priority", priority)
        if schema_update_options is not None:
            pulumi.set(__self__, "schema_update_options", schema_update_options)
        if script_options is not None:
            pulumi.set(__self__, "script_options", script_options)
        if use_legacy_sql is not None:
            pulumi.set(__self__, "use_legacy_sql", use_legacy_sql)
        if use_query_cache is not None:
            pulumi.set(__self__, "use_query_cache", use_query_cache)
        if user_defined_function_resources is not None:
            pulumi.set(__self__, "user_defined_function_resources", user_defined_function_resources)
        if write_disposition is not None:
            pulumi.set(__self__, "write_disposition", write_disposition)

    @property
    @pulumi.getter
    def query(self) -> builtins.str:
        """
        SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
        *NOTE*: queries containing [DML language](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language)
        (`DELETE`, `UPDATE`, `MERGE`, `INSERT`) must specify `create_disposition = ""` and `write_disposition = ""`.
        """
        return pulumi.get(self, "query")

    @property
    @pulumi.getter(name="allowLargeResults")
    def allow_large_results(self) -> Optional[builtins.bool]:
        """
        If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
        Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
        However, you must still set destinationTable when result size exceeds the allowed maximum response size.
        """
        return pulumi.get(self, "allow_large_results")

    @property
    @pulumi.getter
    def continuous(self) -> Optional[builtins.bool]:
        """
        Whether to run the query as continuous or a regular query.
        """
        return pulumi.get(self, "continuous")

    @property
    @pulumi.getter(name="createDisposition")
    def create_disposition(self) -> Optional[builtins.str]:
        """
        Specifies whether the job is allowed to create new tables. The following values are supported:
        CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
        CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
        Creation, truncation and append actions occur as one atomic update upon job completion
        Default value is `CREATE_IF_NEEDED`.
        Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
        """
        return pulumi.get(self, "create_disposition")

    @property
    @pulumi.getter(name="defaultDataset")
    def default_dataset(self) -> Optional['outputs.JobQueryDefaultDataset']:
        """
        Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
        Structure is documented below.
        """
        return pulumi.get(self, "default_dataset")

    @property
    @pulumi.getter(name="destinationEncryptionConfiguration")
    def destination_encryption_configuration(self) -> Optional['outputs.JobQueryDestinationEncryptionConfiguration']:
        """
        Custom encryption configuration (e.g., Cloud KMS keys)
        Structure is documented below.
        """
        return pulumi.get(self, "destination_encryption_configuration")

    @property
    @pulumi.getter(name="destinationTable")
    def destination_table(self) -> Optional['outputs.JobQueryDestinationTable']:
        """
        Describes the table where the query results should be stored.
        This property must be set for large results that exceed the maximum response size.
        For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
        Structure is documented below.
        """
        return pulumi.get(self, "destination_table")

    @property
    @pulumi.getter(name="flattenResults")
    def flatten_results(self) -> Optional[builtins.bool]:
        """
        If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
        allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
        """
        return pulumi.get(self, "flatten_results")

    @property
    @pulumi.getter(name="maximumBillingTier")
    def maximum_billing_tier(self) -> Optional[builtins.int]:
        """
        Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
        If unspecified, this will be set to your project default.
        """
        return pulumi.get(self, "maximum_billing_tier")

    @property
    @pulumi.getter(name="maximumBytesBilled")
    def maximum_bytes_billed(self) -> Optional[builtins.str]:
        """
        Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
        If unspecified, this will be set to your project default.
        """
        return pulumi.get(self, "maximum_bytes_billed")

    @property
    @pulumi.getter(name="parameterMode")
    def parameter_mode(self) -> Optional[builtins.str]:
        """
        Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
        """
        return pulumi.get(self, "parameter_mode")

    @property
    @pulumi.getter
    def priority(self) -> Optional[builtins.str]:
        """
        Specifies a priority for the query.
        Default value is `INTERACTIVE`.
        Possible values are: `INTERACTIVE`, `BATCH`.
        """
        return pulumi.get(self, "priority")

    @property
    @pulumi.getter(name="schemaUpdateOptions")
    def schema_update_options(self) -> Optional[Sequence[builtins.str]]:
        """
        Allows the schema of the destination table to be updated as a side effect of the query job.
        Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
        when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
        specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
        One or more of the following values are specified:
        ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
        ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
        """
        return pulumi.get(self, "schema_update_options")

    @property
    @pulumi.getter(name="scriptOptions")
    def script_options(self) -> Optional['outputs.JobQueryScriptOptions']:
        """
        Options controlling the execution of scripts.
        Structure is documented below.
        """
        return pulumi.get(self, "script_options")

    @property
    @pulumi.getter(name="useLegacySql")
    def use_legacy_sql(self) -> Optional[builtins.bool]:
        """
        Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
        If set to false, the query will use BigQuery's standard SQL.
        """
        return pulumi.get(self, "use_legacy_sql")

    @property
    @pulumi.getter(name="useQueryCache")
    def use_query_cache(self) -> Optional[builtins.bool]:
        """
        Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
        tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
        The default value is true.
        """
        return pulumi.get(self, "use_query_cache")

    @property
    @pulumi.getter(name="userDefinedFunctionResources")
    def user_defined_function_resources(self) -> Optional[Sequence['outputs.JobQueryUserDefinedFunctionResource']]:
        """
        Describes user-defined function resources used in the query.
        Structure is documented below.
        """
        return pulumi.get(self, "user_defined_function_resources")

    @property
    @pulumi.getter(name="writeDisposition")
    def write_disposition(self) -> Optional[builtins.str]:
        """
        Specifies the action that occurs if the destination table already exists. The following values are supported:
        WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
        WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
        WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
        Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
        Creation, truncation and append actions occur as one atomic update upon job completion.
        Default value is `WRITE_EMPTY`.
        Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
        """
        return pulumi.get(self, "write_disposition")


@pulumi.output_type
class JobQueryDefaultDataset(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobQueryDefaultDataset. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobQueryDefaultDataset.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobQueryDefaultDataset.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: Optional[builtins.str] = None):
        """
        :param builtins.str dataset_id: The dataset. Can be specified `{{dataset_id}}` if `project_id` is also set,
               or of the form `projects/{{project}}/datasets/{{dataset_id}}` if not.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The dataset. Can be specified `{{dataset_id}}` if `project_id` is also set,
        or of the form `projects/{{project}}/datasets/{{dataset_id}}` if not.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[builtins.str]:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class JobQueryDestinationEncryptionConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"
        elif key == "kmsKeyVersion":
            suggest = "kms_key_version"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobQueryDestinationEncryptionConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobQueryDestinationEncryptionConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobQueryDestinationEncryptionConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: builtins.str,
                 kms_key_version: Optional[builtins.str] = None):
        """
        :param builtins.str kms_key_name: Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
               The BigQuery Service Account associated with your project requires access to this encryption key.
        :param builtins.str kms_key_version: (Output)
               Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)
        if kms_key_version is not None:
            pulumi.set(__self__, "kms_key_version", kms_key_version)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
        The BigQuery Service Account associated with your project requires access to this encryption key.
        """
        return pulumi.get(self, "kms_key_name")

    @property
    @pulumi.getter(name="kmsKeyVersion")
    def kms_key_version(self) -> Optional[builtins.str]:
        """
        (Output)
        Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
        """
        return pulumi.get(self, "kms_key_version")


@pulumi.output_type
class JobQueryDestinationTable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "tableId":
            suggest = "table_id"
        elif key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobQueryDestinationTable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobQueryDestinationTable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobQueryDestinationTable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 table_id: builtins.str,
                 dataset_id: Optional[builtins.str] = None,
                 project_id: Optional[builtins.str] = None):
        """
        :param builtins.str table_id: The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
               or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "table_id", table_id)
        if dataset_id is not None:
            pulumi.set(__self__, "dataset_id", dataset_id)
        if project_id is not None:
            pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The table. Can be specified `{{table_id}}` if `project_id` and `dataset_id` are also set,
        or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
        """
        return pulumi.get(self, "table_id")

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> Optional[builtins.str]:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> Optional[builtins.str]:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class JobQueryScriptOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "keyResultStatement":
            suggest = "key_result_statement"
        elif key == "statementByteBudget":
            suggest = "statement_byte_budget"
        elif key == "statementTimeoutMs":
            suggest = "statement_timeout_ms"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobQueryScriptOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobQueryScriptOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobQueryScriptOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 key_result_statement: Optional[builtins.str] = None,
                 statement_byte_budget: Optional[builtins.str] = None,
                 statement_timeout_ms: Optional[builtins.str] = None):
        """
        :param builtins.str key_result_statement: Determines which statement in the script represents the "key result",
               used to populate the schema and query results of the script job.
               Possible values are: `LAST`, `FIRST_SELECT`.
        :param builtins.str statement_byte_budget: Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
        :param builtins.str statement_timeout_ms: Timeout period for each statement in a script.
        """
        if key_result_statement is not None:
            pulumi.set(__self__, "key_result_statement", key_result_statement)
        if statement_byte_budget is not None:
            pulumi.set(__self__, "statement_byte_budget", statement_byte_budget)
        if statement_timeout_ms is not None:
            pulumi.set(__self__, "statement_timeout_ms", statement_timeout_ms)

    @property
    @pulumi.getter(name="keyResultStatement")
    def key_result_statement(self) -> Optional[builtins.str]:
        """
        Determines which statement in the script represents the "key result",
        used to populate the schema and query results of the script job.
        Possible values are: `LAST`, `FIRST_SELECT`.
        """
        return pulumi.get(self, "key_result_statement")

    @property
    @pulumi.getter(name="statementByteBudget")
    def statement_byte_budget(self) -> Optional[builtins.str]:
        """
        Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
        """
        return pulumi.get(self, "statement_byte_budget")

    @property
    @pulumi.getter(name="statementTimeoutMs")
    def statement_timeout_ms(self) -> Optional[builtins.str]:
        """
        Timeout period for each statement in a script.
        """
        return pulumi.get(self, "statement_timeout_ms")


@pulumi.output_type
class JobQueryUserDefinedFunctionResource(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "inlineCode":
            suggest = "inline_code"
        elif key == "resourceUri":
            suggest = "resource_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobQueryUserDefinedFunctionResource. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobQueryUserDefinedFunctionResource.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobQueryUserDefinedFunctionResource.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 inline_code: Optional[builtins.str] = None,
                 resource_uri: Optional[builtins.str] = None):
        """
        :param builtins.str inline_code: An inline resource that contains code for a user-defined function (UDF).
               Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
        :param builtins.str resource_uri: A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
        """
        if inline_code is not None:
            pulumi.set(__self__, "inline_code", inline_code)
        if resource_uri is not None:
            pulumi.set(__self__, "resource_uri", resource_uri)

    @property
    @pulumi.getter(name="inlineCode")
    def inline_code(self) -> Optional[builtins.str]:
        """
        An inline resource that contains code for a user-defined function (UDF).
        Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
        """
        return pulumi.get(self, "inline_code")

    @property
    @pulumi.getter(name="resourceUri")
    def resource_uri(self) -> Optional[builtins.str]:
        """
        A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
        """
        return pulumi.get(self, "resource_uri")


@pulumi.output_type
class JobStatus(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "errorResults":
            suggest = "error_results"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in JobStatus. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        JobStatus.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        JobStatus.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 error_results: Optional[Sequence['outputs.JobStatusErrorResult']] = None,
                 errors: Optional[Sequence['outputs.JobStatusError']] = None,
                 state: Optional[builtins.str] = None):
        """
        :param Sequence['JobStatusErrorResultArgs'] error_results: (Output)
               Final error result of the job. If present, indicates that the job has completed and was unsuccessful.
               Structure is documented below.
        :param Sequence['JobStatusErrorArgs'] errors: (Output)
               The first errors encountered during the running of the job. The final message
               includes the number of errors that caused the process to stop. Errors here do
               not necessarily mean that the job has not completed or was unsuccessful.
               Structure is documented below.
        :param builtins.str state: (Output)
               Running state of the job. Valid states include 'PENDING', 'RUNNING', and 'DONE'.
        """
        if error_results is not None:
            pulumi.set(__self__, "error_results", error_results)
        if errors is not None:
            pulumi.set(__self__, "errors", errors)
        if state is not None:
            pulumi.set(__self__, "state", state)

    @property
    @pulumi.getter(name="errorResults")
    def error_results(self) -> Optional[Sequence['outputs.JobStatusErrorResult']]:
        """
        (Output)
        Final error result of the job. If present, indicates that the job has completed and was unsuccessful.
        Structure is documented below.
        """
        return pulumi.get(self, "error_results")

    @property
    @pulumi.getter
    def errors(self) -> Optional[Sequence['outputs.JobStatusError']]:
        """
        (Output)
        The first errors encountered during the running of the job. The final message
        includes the number of errors that caused the process to stop. Errors here do
        not necessarily mean that the job has not completed or was unsuccessful.
        Structure is documented below.
        """
        return pulumi.get(self, "errors")

    @property
    @pulumi.getter
    def state(self) -> Optional[builtins.str]:
        """
        (Output)
        Running state of the job. Valid states include 'PENDING', 'RUNNING', and 'DONE'.
        """
        return pulumi.get(self, "state")


@pulumi.output_type
class JobStatusError(dict):
    def __init__(__self__, *,
                 location: Optional[builtins.str] = None,
                 message: Optional[builtins.str] = None,
                 reason: Optional[builtins.str] = None):
        """
        :param builtins.str location: Specifies where the error occurred, if present.
        :param builtins.str message: A human-readable description of the error.
        :param builtins.str reason: A short error code that summarizes the error.
        """
        if location is not None:
            pulumi.set(__self__, "location", location)
        if message is not None:
            pulumi.set(__self__, "message", message)
        if reason is not None:
            pulumi.set(__self__, "reason", reason)

    @property
    @pulumi.getter
    def location(self) -> Optional[builtins.str]:
        """
        Specifies where the error occurred, if present.
        """
        return pulumi.get(self, "location")

    @property
    @pulumi.getter
    def message(self) -> Optional[builtins.str]:
        """
        A human-readable description of the error.
        """
        return pulumi.get(self, "message")

    @property
    @pulumi.getter
    def reason(self) -> Optional[builtins.str]:
        """
        A short error code that summarizes the error.
        """
        return pulumi.get(self, "reason")


@pulumi.output_type
class JobStatusErrorResult(dict):
    def __init__(__self__, *,
                 location: Optional[builtins.str] = None,
                 message: Optional[builtins.str] = None,
                 reason: Optional[builtins.str] = None):
        """
        :param builtins.str location: Specifies where the error occurred, if present.
        :param builtins.str message: A human-readable description of the error.
        :param builtins.str reason: A short error code that summarizes the error.
        """
        if location is not None:
            pulumi.set(__self__, "location", location)
        if message is not None:
            pulumi.set(__self__, "message", message)
        if reason is not None:
            pulumi.set(__self__, "reason", reason)

    @property
    @pulumi.getter
    def location(self) -> Optional[builtins.str]:
        """
        Specifies where the error occurred, if present.
        """
        return pulumi.get(self, "location")

    @property
    @pulumi.getter
    def message(self) -> Optional[builtins.str]:
        """
        A human-readable description of the error.
        """
        return pulumi.get(self, "message")

    @property
    @pulumi.getter
    def reason(self) -> Optional[builtins.str]:
        """
        A short error code that summarizes the error.
        """
        return pulumi.get(self, "reason")


@pulumi.output_type
class ReservationAutoscale(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "currentSlots":
            suggest = "current_slots"
        elif key == "maxSlots":
            suggest = "max_slots"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ReservationAutoscale. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ReservationAutoscale.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ReservationAutoscale.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 current_slots: Optional[builtins.int] = None,
                 max_slots: Optional[builtins.int] = None):
        """
        :param builtins.int current_slots: (Output)
               The slot capacity added to this reservation when autoscale happens. Will be between [0, max_slots].
        :param builtins.int max_slots: Number of slots to be scaled when needed.
        """
        if current_slots is not None:
            pulumi.set(__self__, "current_slots", current_slots)
        if max_slots is not None:
            pulumi.set(__self__, "max_slots", max_slots)

    @property
    @pulumi.getter(name="currentSlots")
    def current_slots(self) -> Optional[builtins.int]:
        """
        (Output)
        The slot capacity added to this reservation when autoscale happens. Will be between [0, max_slots].
        """
        return pulumi.get(self, "current_slots")

    @property
    @pulumi.getter(name="maxSlots")
    def max_slots(self) -> Optional[builtins.int]:
        """
        Number of slots to be scaled when needed.
        """
        return pulumi.get(self, "max_slots")


@pulumi.output_type
class ReservationReplicationStatus(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "lastErrorTime":
            suggest = "last_error_time"
        elif key == "lastReplicationTime":
            suggest = "last_replication_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in ReservationReplicationStatus. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        ReservationReplicationStatus.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        ReservationReplicationStatus.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 errors: Optional[Sequence['outputs.ReservationReplicationStatusError']] = None,
                 last_error_time: Optional[builtins.str] = None,
                 last_replication_time: Optional[builtins.str] = None):
        """
        :param Sequence['ReservationReplicationStatusErrorArgs'] errors: (Output)
               The last error encountered while trying to replicate changes from the primary to the
               secondary. This field is only available if the replication has not succeeded since.
               Structure is documented below.
        :param builtins.str last_error_time: (Output)
               The time at which the last error was encountered while trying to replicate changes from
               the primary to the secondary. This field is only available if the replication has not
               succeeded since.
        :param builtins.str last_replication_time: (Output)
               A timestamp corresponding to the last change on the primary that was successfully
               replicated to the secondary.
        """
        if errors is not None:
            pulumi.set(__self__, "errors", errors)
        if last_error_time is not None:
            pulumi.set(__self__, "last_error_time", last_error_time)
        if last_replication_time is not None:
            pulumi.set(__self__, "last_replication_time", last_replication_time)

    @property
    @pulumi.getter
    def errors(self) -> Optional[Sequence['outputs.ReservationReplicationStatusError']]:
        """
        (Output)
        The last error encountered while trying to replicate changes from the primary to the
        secondary. This field is only available if the replication has not succeeded since.
        Structure is documented below.
        """
        return pulumi.get(self, "errors")

    @property
    @pulumi.getter(name="lastErrorTime")
    def last_error_time(self) -> Optional[builtins.str]:
        """
        (Output)
        The time at which the last error was encountered while trying to replicate changes from
        the primary to the secondary. This field is only available if the replication has not
        succeeded since.
        """
        return pulumi.get(self, "last_error_time")

    @property
    @pulumi.getter(name="lastReplicationTime")
    def last_replication_time(self) -> Optional[builtins.str]:
        """
        (Output)
        A timestamp corresponding to the last change on the primary that was successfully
        replicated to the secondary.
        """
        return pulumi.get(self, "last_replication_time")


@pulumi.output_type
class ReservationReplicationStatusError(dict):
    def __init__(__self__, *,
                 code: Optional[builtins.int] = None,
                 message: Optional[builtins.str] = None):
        """
        :param builtins.int code: (Output)
               The status code, which should be an enum value of [google.rpc.Code](https://cloud.google.com/bigquery/docs/reference/reservations/rpc/google.rpc#google.rpc.Code).
        :param builtins.str message: (Output)
               A developer-facing error message, which should be in English.
        """
        if code is not None:
            pulumi.set(__self__, "code", code)
        if message is not None:
            pulumi.set(__self__, "message", message)

    @property
    @pulumi.getter
    def code(self) -> Optional[builtins.int]:
        """
        (Output)
        The status code, which should be an enum value of [google.rpc.Code](https://cloud.google.com/bigquery/docs/reference/reservations/rpc/google.rpc#google.rpc.Code).
        """
        return pulumi.get(self, "code")

    @property
    @pulumi.getter
    def message(self) -> Optional[builtins.str]:
        """
        (Output)
        A developer-facing error message, which should be in English.
        """
        return pulumi.get(self, "message")


@pulumi.output_type
class RoutineArgument(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "argumentKind":
            suggest = "argument_kind"
        elif key == "dataType":
            suggest = "data_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in RoutineArgument. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        RoutineArgument.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        RoutineArgument.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 argument_kind: Optional[builtins.str] = None,
                 data_type: Optional[builtins.str] = None,
                 mode: Optional[builtins.str] = None,
                 name: Optional[builtins.str] = None):
        """
        :param builtins.str argument_kind: Defaults to FIXED_TYPE.
               Default value is `FIXED_TYPE`.
               Possible values are: `FIXED_TYPE`, `ANY_TYPE`.
        :param builtins.str data_type: A JSON schema for the data type. Required unless argumentKind = ANY_TYPE.
               ~>**NOTE**: Because this field expects a JSON string, any changes to the string
               will create a diff, even if the JSON itself hasn't changed. If the API returns
               a different value for the same schema, e.g. it switched the order of values
               or replaced STRUCT field type with RECORD field type, we currently cannot
               suppress the recurring diff this causes. As a workaround, we recommend using
               the schema as returned by the API.
        :param builtins.str mode: Specifies whether the argument is input or output. Can be set for procedures only.
               Possible values are: `IN`, `OUT`, `INOUT`.
        :param builtins.str name: The name of this argument. Can be absent for function return argument.
        """
        if argument_kind is not None:
            pulumi.set(__self__, "argument_kind", argument_kind)
        if data_type is not None:
            pulumi.set(__self__, "data_type", data_type)
        if mode is not None:
            pulumi.set(__self__, "mode", mode)
        if name is not None:
            pulumi.set(__self__, "name", name)

    @property
    @pulumi.getter(name="argumentKind")
    def argument_kind(self) -> Optional[builtins.str]:
        """
        Defaults to FIXED_TYPE.
        Default value is `FIXED_TYPE`.
        Possible values are: `FIXED_TYPE`, `ANY_TYPE`.
        """
        return pulumi.get(self, "argument_kind")

    @property
    @pulumi.getter(name="dataType")
    def data_type(self) -> Optional[builtins.str]:
        """
        A JSON schema for the data type. Required unless argumentKind = ANY_TYPE.
        ~>**NOTE**: Because this field expects a JSON string, any changes to the string
        will create a diff, even if the JSON itself hasn't changed. If the API returns
        a different value for the same schema, e.g. it switched the order of values
        or replaced STRUCT field type with RECORD field type, we currently cannot
        suppress the recurring diff this causes. As a workaround, we recommend using
        the schema as returned by the API.
        """
        return pulumi.get(self, "data_type")

    @property
    @pulumi.getter
    def mode(self) -> Optional[builtins.str]:
        """
        Specifies whether the argument is input or output. Can be set for procedures only.
        Possible values are: `IN`, `OUT`, `INOUT`.
        """
        return pulumi.get(self, "mode")

    @property
    @pulumi.getter
    def name(self) -> Optional[builtins.str]:
        """
        The name of this argument. Can be absent for function return argument.
        """
        return pulumi.get(self, "name")


@pulumi.output_type
class RoutineRemoteFunctionOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxBatchingRows":
            suggest = "max_batching_rows"
        elif key == "userDefinedContext":
            suggest = "user_defined_context"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in RoutineRemoteFunctionOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        RoutineRemoteFunctionOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        RoutineRemoteFunctionOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 connection: Optional[builtins.str] = None,
                 endpoint: Optional[builtins.str] = None,
                 max_batching_rows: Optional[builtins.str] = None,
                 user_defined_context: Optional[Mapping[str, builtins.str]] = None):
        """
        :param builtins.str connection: Fully qualified name of the user-provided connection object which holds
               the authentication information to send requests to the remote service.
               Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
        :param builtins.str endpoint: Endpoint of the user-provided remote service, e.g.
               `https://us-east1-my_gcf_project.cloudfunctions.net/remote_add`
        :param builtins.str max_batching_rows: Max number of rows in each batch sent to the remote service. If absent or if 0,
               BigQuery dynamically decides the number of rows in a batch.
        :param Mapping[str, builtins.str] user_defined_context: User-defined context as a set of key/value pairs, which will be sent as function
               invocation context together with batched arguments in the requests to the remote
               service. The total number of bytes of keys and values must be less than 8KB.
               An object containing a list of "key": value pairs. Example:
               `{ "name": "wrench", "mass": "1.3kg", "count": "3" }`.
        """
        if connection is not None:
            pulumi.set(__self__, "connection", connection)
        if endpoint is not None:
            pulumi.set(__self__, "endpoint", endpoint)
        if max_batching_rows is not None:
            pulumi.set(__self__, "max_batching_rows", max_batching_rows)
        if user_defined_context is not None:
            pulumi.set(__self__, "user_defined_context", user_defined_context)

    @property
    @pulumi.getter
    def connection(self) -> Optional[builtins.str]:
        """
        Fully qualified name of the user-provided connection object which holds
        the authentication information to send requests to the remote service.
        Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
        """
        return pulumi.get(self, "connection")

    @property
    @pulumi.getter
    def endpoint(self) -> Optional[builtins.str]:
        """
        Endpoint of the user-provided remote service, e.g.
        `https://us-east1-my_gcf_project.cloudfunctions.net/remote_add`
        """
        return pulumi.get(self, "endpoint")

    @property
    @pulumi.getter(name="maxBatchingRows")
    def max_batching_rows(self) -> Optional[builtins.str]:
        """
        Max number of rows in each batch sent to the remote service. If absent or if 0,
        BigQuery dynamically decides the number of rows in a batch.
        """
        return pulumi.get(self, "max_batching_rows")

    @property
    @pulumi.getter(name="userDefinedContext")
    def user_defined_context(self) -> Optional[Mapping[str, builtins.str]]:
        """
        User-defined context as a set of key/value pairs, which will be sent as function
        invocation context together with batched arguments in the requests to the remote
        service. The total number of bytes of keys and values must be less than 8KB.
        An object containing a list of "key": value pairs. Example:
        `{ "name": "wrench", "mass": "1.3kg", "count": "3" }`.
        """
        return pulumi.get(self, "user_defined_context")


@pulumi.output_type
class RoutineSparkOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "archiveUris":
            suggest = "archive_uris"
        elif key == "containerImage":
            suggest = "container_image"
        elif key == "fileUris":
            suggest = "file_uris"
        elif key == "jarUris":
            suggest = "jar_uris"
        elif key == "mainClass":
            suggest = "main_class"
        elif key == "mainFileUri":
            suggest = "main_file_uri"
        elif key == "pyFileUris":
            suggest = "py_file_uris"
        elif key == "runtimeVersion":
            suggest = "runtime_version"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in RoutineSparkOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        RoutineSparkOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        RoutineSparkOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 archive_uris: Optional[Sequence[builtins.str]] = None,
                 connection: Optional[builtins.str] = None,
                 container_image: Optional[builtins.str] = None,
                 file_uris: Optional[Sequence[builtins.str]] = None,
                 jar_uris: Optional[Sequence[builtins.str]] = None,
                 main_class: Optional[builtins.str] = None,
                 main_file_uri: Optional[builtins.str] = None,
                 properties: Optional[Mapping[str, builtins.str]] = None,
                 py_file_uris: Optional[Sequence[builtins.str]] = None,
                 runtime_version: Optional[builtins.str] = None):
        """
        :param Sequence[builtins.str] archive_uris: Archive files to be extracted into the working directory of each executor. For more information about Apache Spark, see Apache Spark.
        :param builtins.str connection: Fully qualified name of the user-provided Spark connection object.
               Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
        :param builtins.str container_image: Custom container image for the runtime environment.
        :param Sequence[builtins.str] file_uris: Files to be placed in the working directory of each executor. For more information about Apache Spark, see Apache Spark.
        :param Sequence[builtins.str] jar_uris: JARs to include on the driver and executor CLASSPATH. For more information about Apache Spark, see Apache Spark.
        :param builtins.str main_class: The fully qualified name of a class in jarUris, for example, com.example.wordcount.
               Exactly one of mainClass and main_jar_uri field should be set for Java/Scala language type.
        :param builtins.str main_file_uri: The main file/jar URI of the Spark application.
               Exactly one of the definitionBody field and the mainFileUri field must be set for Python.
               Exactly one of mainClass and mainFileUri field should be set for Java/Scala language type.
        :param Mapping[str, builtins.str] properties: Configuration properties as a set of key/value pairs, which will be passed on to the Spark application.
               For more information, see Apache Spark and the procedure option list.
               An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
        :param Sequence[builtins.str] py_file_uris: Python files to be placed on the PYTHONPATH for PySpark application. Supported file types: .py, .egg, and .zip. For more information about Apache Spark, see Apache Spark.
        :param builtins.str runtime_version: Runtime version. If not specified, the default runtime version is used.
        """
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if connection is not None:
            pulumi.set(__self__, "connection", connection)
        if container_image is not None:
            pulumi.set(__self__, "container_image", container_image)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if jar_uris is not None:
            pulumi.set(__self__, "jar_uris", jar_uris)
        if main_class is not None:
            pulumi.set(__self__, "main_class", main_class)
        if main_file_uri is not None:
            pulumi.set(__self__, "main_file_uri", main_file_uri)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if py_file_uris is not None:
            pulumi.set(__self__, "py_file_uris", py_file_uris)
        if runtime_version is not None:
            pulumi.set(__self__, "runtime_version", runtime_version)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[Sequence[builtins.str]]:
        """
        Archive files to be extracted into the working directory of each executor. For more information about Apache Spark, see Apache Spark.
        """
        return pulumi.get(self, "archive_uris")

    @property
    @pulumi.getter
    def connection(self) -> Optional[builtins.str]:
        """
        Fully qualified name of the user-provided Spark connection object.
        Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
        """
        return pulumi.get(self, "connection")

    @property
    @pulumi.getter(name="containerImage")
    def container_image(self) -> Optional[builtins.str]:
        """
        Custom container image for the runtime environment.
        """
        return pulumi.get(self, "container_image")

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[Sequence[builtins.str]]:
        """
        Files to be placed in the working directory of each executor. For more information about Apache Spark, see Apache Spark.
        """
        return pulumi.get(self, "file_uris")

    @property
    @pulumi.getter(name="jarUris")
    def jar_uris(self) -> Optional[Sequence[builtins.str]]:
        """
        JARs to include on the driver and executor CLASSPATH. For more information about Apache Spark, see Apache Spark.
        """
        return pulumi.get(self, "jar_uris")

    @property
    @pulumi.getter(name="mainClass")
    def main_class(self) -> Optional[builtins.str]:
        """
        The fully qualified name of a class in jarUris, for example, com.example.wordcount.
        Exactly one of mainClass and main_jar_uri field should be set for Java/Scala language type.
        """
        return pulumi.get(self, "main_class")

    @property
    @pulumi.getter(name="mainFileUri")
    def main_file_uri(self) -> Optional[builtins.str]:
        """
        The main file/jar URI of the Spark application.
        Exactly one of the definitionBody field and the mainFileUri field must be set for Python.
        Exactly one of mainClass and mainFileUri field should be set for Java/Scala language type.
        """
        return pulumi.get(self, "main_file_uri")

    @property
    @pulumi.getter
    def properties(self) -> Optional[Mapping[str, builtins.str]]:
        """
        Configuration properties as a set of key/value pairs, which will be passed on to the Spark application.
        For more information, see Apache Spark and the procedure option list.
        An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
        """
        return pulumi.get(self, "properties")

    @property
    @pulumi.getter(name="pyFileUris")
    def py_file_uris(self) -> Optional[Sequence[builtins.str]]:
        """
        Python files to be placed on the PYTHONPATH for PySpark application. Supported file types: .py, .egg, and .zip. For more information about Apache Spark, see Apache Spark.
        """
        return pulumi.get(self, "py_file_uris")

    @property
    @pulumi.getter(name="runtimeVersion")
    def runtime_version(self) -> Optional[builtins.str]:
        """
        Runtime version. If not specified, the default runtime version is used.
        """
        return pulumi.get(self, "runtime_version")


@pulumi.output_type
class TableBiglakeConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "connectionId":
            suggest = "connection_id"
        elif key == "fileFormat":
            suggest = "file_format"
        elif key == "storageUri":
            suggest = "storage_uri"
        elif key == "tableFormat":
            suggest = "table_format"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableBiglakeConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableBiglakeConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableBiglakeConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 connection_id: builtins.str,
                 file_format: builtins.str,
                 storage_uri: builtins.str,
                 table_format: builtins.str):
        """
        :param builtins.str connection_id: The connection specifying the credentials to be used to
               read and write to external storage, such as Cloud Storage. The connection_id can
               have the form "&lt;project\\_id&gt;.&lt;location\\_id&gt;.&lt;connection\\_id&gt;" or
               projects/&lt;project\\_id&gt;/locations/&lt;location\\_id&gt;/connections/&lt;connection\\_id&gt;".
        :param builtins.str file_format: The file format the table data is stored in.
        :param builtins.str storage_uri: The fully qualified location prefix of the external folder where table data
               is stored. The '*' wildcard character is not allowed. The URI should be in the format "gs://bucket/path_to_table/"
        :param builtins.str table_format: The table format the metadata only snapshots are stored in.
        """
        pulumi.set(__self__, "connection_id", connection_id)
        pulumi.set(__self__, "file_format", file_format)
        pulumi.set(__self__, "storage_uri", storage_uri)
        pulumi.set(__self__, "table_format", table_format)

    @property
    @pulumi.getter(name="connectionId")
    def connection_id(self) -> builtins.str:
        """
        The connection specifying the credentials to be used to
        read and write to external storage, such as Cloud Storage. The connection_id can
        have the form "&lt;project\\_id&gt;.&lt;location\\_id&gt;.&lt;connection\\_id&gt;" or
        projects/&lt;project\\_id&gt;/locations/&lt;location\\_id&gt;/connections/&lt;connection\\_id&gt;".
        """
        return pulumi.get(self, "connection_id")

    @property
    @pulumi.getter(name="fileFormat")
    def file_format(self) -> builtins.str:
        """
        The file format the table data is stored in.
        """
        return pulumi.get(self, "file_format")

    @property
    @pulumi.getter(name="storageUri")
    def storage_uri(self) -> builtins.str:
        """
        The fully qualified location prefix of the external folder where table data
        is stored. The '*' wildcard character is not allowed. The URI should be in the format "gs://bucket/path_to_table/"
        """
        return pulumi.get(self, "storage_uri")

    @property
    @pulumi.getter(name="tableFormat")
    def table_format(self) -> builtins.str:
        """
        The table format the metadata only snapshots are stored in.
        """
        return pulumi.get(self, "table_format")


@pulumi.output_type
class TableEncryptionConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"
        elif key == "kmsKeyVersion":
            suggest = "kms_key_version"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableEncryptionConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableEncryptionConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableEncryptionConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: builtins.str,
                 kms_key_version: Optional[builtins.str] = None):
        """
        :param builtins.str kms_key_name: The self link or full name of a key which should be used to
               encrypt this table.  Note that the default bigquery service account will need to have
               encrypt/decrypt permissions on this key - you may want to see the
               `bigquery_get_default_service_account` datasource and the
               `kms.CryptoKeyIAMBinding` resource.
        :param builtins.str kms_key_version: The self link or full name of the kms key version used to encrypt this table.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)
        if kms_key_version is not None:
            pulumi.set(__self__, "kms_key_version", kms_key_version)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        The self link or full name of a key which should be used to
        encrypt this table.  Note that the default bigquery service account will need to have
        encrypt/decrypt permissions on this key - you may want to see the
        `bigquery_get_default_service_account` datasource and the
        `kms.CryptoKeyIAMBinding` resource.
        """
        return pulumi.get(self, "kms_key_name")

    @property
    @pulumi.getter(name="kmsKeyVersion")
    def kms_key_version(self) -> Optional[builtins.str]:
        """
        The self link or full name of the kms key version used to encrypt this table.
        """
        return pulumi.get(self, "kms_key_version")


@pulumi.output_type
class TableExternalCatalogTableOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "connectionId":
            suggest = "connection_id"
        elif key == "storageDescriptor":
            suggest = "storage_descriptor"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalCatalogTableOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalCatalogTableOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalCatalogTableOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 connection_id: Optional[builtins.str] = None,
                 parameters: Optional[Mapping[str, builtins.str]] = None,
                 storage_descriptor: Optional['outputs.TableExternalCatalogTableOptionsStorageDescriptor'] = None):
        """
        :param builtins.str connection_id: The connection specifying the credentials to be
               used to read external storage, such as Azure Blob, Cloud Storage, or S3. The
               connection is needed to read the open source table from BigQuery Engine. The
               connection_id can have the form `<project_id>.<location_id>.<connection_id>`
               or `projects/<project_id>/locations/<location_id>/connections/<connection_id>`.
        :param Mapping[str, builtins.str] parameters: A map of key value pairs defining the parameters and
               properties of the open source table. Corresponds with hive meta store table
               parameters. Maximum size of 4Mib.
        :param 'TableExternalCatalogTableOptionsStorageDescriptorArgs' storage_descriptor: A storage descriptor containing information
               about the physical storage of this table. Structure is documented below.
        """
        if connection_id is not None:
            pulumi.set(__self__, "connection_id", connection_id)
        if parameters is not None:
            pulumi.set(__self__, "parameters", parameters)
        if storage_descriptor is not None:
            pulumi.set(__self__, "storage_descriptor", storage_descriptor)

    @property
    @pulumi.getter(name="connectionId")
    def connection_id(self) -> Optional[builtins.str]:
        """
        The connection specifying the credentials to be
        used to read external storage, such as Azure Blob, Cloud Storage, or S3. The
        connection is needed to read the open source table from BigQuery Engine. The
        connection_id can have the form `<project_id>.<location_id>.<connection_id>`
        or `projects/<project_id>/locations/<location_id>/connections/<connection_id>`.
        """
        return pulumi.get(self, "connection_id")

    @property
    @pulumi.getter
    def parameters(self) -> Optional[Mapping[str, builtins.str]]:
        """
        A map of key value pairs defining the parameters and
        properties of the open source table. Corresponds with hive meta store table
        parameters. Maximum size of 4Mib.
        """
        return pulumi.get(self, "parameters")

    @property
    @pulumi.getter(name="storageDescriptor")
    def storage_descriptor(self) -> Optional['outputs.TableExternalCatalogTableOptionsStorageDescriptor']:
        """
        A storage descriptor containing information
        about the physical storage of this table. Structure is documented below.
        """
        return pulumi.get(self, "storage_descriptor")


@pulumi.output_type
class TableExternalCatalogTableOptionsStorageDescriptor(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "inputFormat":
            suggest = "input_format"
        elif key == "locationUri":
            suggest = "location_uri"
        elif key == "outputFormat":
            suggest = "output_format"
        elif key == "serdeInfo":
            suggest = "serde_info"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalCatalogTableOptionsStorageDescriptor. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalCatalogTableOptionsStorageDescriptor.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalCatalogTableOptionsStorageDescriptor.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 input_format: Optional[builtins.str] = None,
                 location_uri: Optional[builtins.str] = None,
                 output_format: Optional[builtins.str] = None,
                 serde_info: Optional['outputs.TableExternalCatalogTableOptionsStorageDescriptorSerdeInfo'] = None):
        """
        :param builtins.str input_format: Specifies the fully qualified class name of the
               InputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"). The
               maximum length is 128 characters.
        :param builtins.str location_uri: The physical location of the table (e.g.
               'gs://spark-dataproc-data/pangea-data/case_sensitive/' or
               'gs://spark-dataproc-data/pangea-data/*'). The maximum length is 2056 bytes.
        :param builtins.str output_format: Specifies the fully qualified class name of the
               OutputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat"). The
               maximum length is 128 characters.
        :param 'TableExternalCatalogTableOptionsStorageDescriptorSerdeInfoArgs' serde_info: Serializer and deserializer information. Structure
               is documented below.
        """
        if input_format is not None:
            pulumi.set(__self__, "input_format", input_format)
        if location_uri is not None:
            pulumi.set(__self__, "location_uri", location_uri)
        if output_format is not None:
            pulumi.set(__self__, "output_format", output_format)
        if serde_info is not None:
            pulumi.set(__self__, "serde_info", serde_info)

    @property
    @pulumi.getter(name="inputFormat")
    def input_format(self) -> Optional[builtins.str]:
        """
        Specifies the fully qualified class name of the
        InputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"). The
        maximum length is 128 characters.
        """
        return pulumi.get(self, "input_format")

    @property
    @pulumi.getter(name="locationUri")
    def location_uri(self) -> Optional[builtins.str]:
        """
        The physical location of the table (e.g.
        'gs://spark-dataproc-data/pangea-data/case_sensitive/' or
        'gs://spark-dataproc-data/pangea-data/*'). The maximum length is 2056 bytes.
        """
        return pulumi.get(self, "location_uri")

    @property
    @pulumi.getter(name="outputFormat")
    def output_format(self) -> Optional[builtins.str]:
        """
        Specifies the fully qualified class name of the
        OutputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat"). The
        maximum length is 128 characters.
        """
        return pulumi.get(self, "output_format")

    @property
    @pulumi.getter(name="serdeInfo")
    def serde_info(self) -> Optional['outputs.TableExternalCatalogTableOptionsStorageDescriptorSerdeInfo']:
        """
        Serializer and deserializer information. Structure
        is documented below.
        """
        return pulumi.get(self, "serde_info")


@pulumi.output_type
class TableExternalCatalogTableOptionsStorageDescriptorSerdeInfo(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "serializationLibrary":
            suggest = "serialization_library"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalCatalogTableOptionsStorageDescriptorSerdeInfo. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalCatalogTableOptionsStorageDescriptorSerdeInfo.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalCatalogTableOptionsStorageDescriptorSerdeInfo.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 serialization_library: builtins.str,
                 name: Optional[builtins.str] = None,
                 parameters: Optional[Mapping[str, builtins.str]] = None):
        """
        :param builtins.str serialization_library: Specifies a fully-qualified class name of
               the serialization library that is responsible for the translation of data
               between table representation and the underlying low-level input and output
               format structures. The maximum length is 256 characters.
        :param builtins.str name: Name of the SerDe. The maximum length is 256 characters.
        :param Mapping[str, builtins.str] parameters: Key-value pairs that define the initialization
               parameters for the serialization library. Maximum size 10 Kib.
        """
        pulumi.set(__self__, "serialization_library", serialization_library)
        if name is not None:
            pulumi.set(__self__, "name", name)
        if parameters is not None:
            pulumi.set(__self__, "parameters", parameters)

    @property
    @pulumi.getter(name="serializationLibrary")
    def serialization_library(self) -> builtins.str:
        """
        Specifies a fully-qualified class name of
        the serialization library that is responsible for the translation of data
        between table representation and the underlying low-level input and output
        format structures. The maximum length is 256 characters.
        """
        return pulumi.get(self, "serialization_library")

    @property
    @pulumi.getter
    def name(self) -> Optional[builtins.str]:
        """
        Name of the SerDe. The maximum length is 256 characters.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def parameters(self) -> Optional[Mapping[str, builtins.str]]:
        """
        Key-value pairs that define the initialization
        parameters for the serialization library. Maximum size 10 Kib.
        """
        return pulumi.get(self, "parameters")


@pulumi.output_type
class TableExternalDataConfiguration(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "sourceUris":
            suggest = "source_uris"
        elif key == "avroOptions":
            suggest = "avro_options"
        elif key == "bigtableOptions":
            suggest = "bigtable_options"
        elif key == "connectionId":
            suggest = "connection_id"
        elif key == "csvOptions":
            suggest = "csv_options"
        elif key == "fileSetSpecType":
            suggest = "file_set_spec_type"
        elif key == "googleSheetsOptions":
            suggest = "google_sheets_options"
        elif key == "hivePartitioningOptions":
            suggest = "hive_partitioning_options"
        elif key == "ignoreUnknownValues":
            suggest = "ignore_unknown_values"
        elif key == "jsonExtension":
            suggest = "json_extension"
        elif key == "jsonOptions":
            suggest = "json_options"
        elif key == "maxBadRecords":
            suggest = "max_bad_records"
        elif key == "metadataCacheMode":
            suggest = "metadata_cache_mode"
        elif key == "objectMetadata":
            suggest = "object_metadata"
        elif key == "parquetOptions":
            suggest = "parquet_options"
        elif key == "referenceFileSchemaUri":
            suggest = "reference_file_schema_uri"
        elif key == "sourceFormat":
            suggest = "source_format"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfiguration. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfiguration.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfiguration.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 autodetect: builtins.bool,
                 source_uris: Sequence[builtins.str],
                 avro_options: Optional['outputs.TableExternalDataConfigurationAvroOptions'] = None,
                 bigtable_options: Optional['outputs.TableExternalDataConfigurationBigtableOptions'] = None,
                 compression: Optional[builtins.str] = None,
                 connection_id: Optional[builtins.str] = None,
                 csv_options: Optional['outputs.TableExternalDataConfigurationCsvOptions'] = None,
                 file_set_spec_type: Optional[builtins.str] = None,
                 google_sheets_options: Optional['outputs.TableExternalDataConfigurationGoogleSheetsOptions'] = None,
                 hive_partitioning_options: Optional['outputs.TableExternalDataConfigurationHivePartitioningOptions'] = None,
                 ignore_unknown_values: Optional[builtins.bool] = None,
                 json_extension: Optional[builtins.str] = None,
                 json_options: Optional['outputs.TableExternalDataConfigurationJsonOptions'] = None,
                 max_bad_records: Optional[builtins.int] = None,
                 metadata_cache_mode: Optional[builtins.str] = None,
                 object_metadata: Optional[builtins.str] = None,
                 parquet_options: Optional['outputs.TableExternalDataConfigurationParquetOptions'] = None,
                 reference_file_schema_uri: Optional[builtins.str] = None,
                 schema: Optional[builtins.str] = None,
                 source_format: Optional[builtins.str] = None):
        """
        :param builtins.bool autodetect: Let BigQuery try to autodetect the schema
               and format of the table.
        :param Sequence[builtins.str] source_uris: A list of the fully-qualified URIs that point to
               your data in Google Cloud.
        :param 'TableExternalDataConfigurationAvroOptionsArgs' avro_options: Additional options if `source_format` is set to
               "AVRO".  Structure is documented below.
        :param 'TableExternalDataConfigurationBigtableOptionsArgs' bigtable_options: Additional properties to set if
               `source_format` is set to "BIGTABLE". Structure is documented below.
        :param builtins.str compression: The compression type of the data source.
               Valid values are "NONE" or "GZIP".
        :param builtins.str connection_id: The connection specifying the credentials to be used to read
               external storage, such as Azure Blob, Cloud Storage, or S3. The `connection_id` can have
               the form `{{project}}.{{location}}.{{connection_id}}`
               or `projects/{{project}}/locations/{{location}}/connections/{{connection_id}}`.
               
               ~>**NOTE:** If you set `external_data_configuration.connection_id`, the
               table schema must be specified using the top-level `schema` field
               documented above.
        :param 'TableExternalDataConfigurationCsvOptionsArgs' csv_options: Additional properties to set if
               `source_format` is set to "CSV". Structure is documented below.
        :param builtins.str file_set_spec_type: Specifies how source URIs are interpreted for constructing the file set to load.
               By default source URIs are expanded against the underlying storage.
               Other options include specifying manifest files. Only applicable to object storage systems. Docs
        :param 'TableExternalDataConfigurationGoogleSheetsOptionsArgs' google_sheets_options: Additional options if
               `source_format` is set to "GOOGLE_SHEETS". Structure is
               documented below.
        :param 'TableExternalDataConfigurationHivePartitioningOptionsArgs' hive_partitioning_options: When set, configures hive partitioning
               support. Not all storage formats support hive partitioning -- requesting hive
               partitioning on an unsupported format will lead to an error, as will providing
               an invalid specification. Structure is documented below.
        :param builtins.bool ignore_unknown_values: Indicates if BigQuery should
               allow extra values that are not represented in the table schema.
               If true, the extra values are ignored. If false, records with
               extra columns are treated as bad records, and if there are too
               many bad records, an invalid error is returned in the job result.
               The default value is false.
        :param builtins.str json_extension: Used to indicate that a JSON variant, rather than normal JSON, is being used as the sourceFormat. This should only be used in combination with the `JSON` source format. Valid values are: `GEOJSON`.
        :param 'TableExternalDataConfigurationJsonOptionsArgs' json_options: Additional properties to set if
               `source_format` is set to "JSON". Structure is documented below.
        :param builtins.int max_bad_records: The maximum number of bad records that
               BigQuery can ignore when reading data.
        :param builtins.str metadata_cache_mode: Metadata Cache Mode for the table. Set this to enable caching of metadata from external data source. Valid values are `AUTOMATIC` and `MANUAL`.
        :param builtins.str object_metadata: Object Metadata is used to create Object Tables. Object Tables contain a listing of objects (with their metadata) found at the sourceUris. If `object_metadata` is set, `source_format` should be omitted.
        :param 'TableExternalDataConfigurationParquetOptionsArgs' parquet_options: Additional properties to set if
               `source_format` is set to "PARQUET". Structure is documented below.
        :param builtins.str reference_file_schema_uri: When creating an external table, the user can provide a reference file with the table schema. This is enabled for the following formats: AVRO, PARQUET, ORC.
        :param builtins.str schema: A JSON schema for the external table. Schema is required
               for CSV and JSON formats if autodetect is not on. Schema is disallowed
               for Google Cloud Bigtable, Cloud Datastore backups, Avro, Iceberg, ORC and Parquet formats.
               ~>**NOTE:** Because this field expects a JSON string, any changes to the
               string will create a diff, even if the JSON itself hasn't changed.
               Furthermore drift for this field cannot not be detected because BigQuery
               only uses this schema to compute the effective schema for the table, therefore
               any changes on the configured value will force the table to be recreated.
               This schema is effectively only applied when creating a table from an external
               datasource, after creation the computed schema will be stored in
               `google_bigquery_table.schema`
               
               ~>**NOTE:** If you set `external_data_configuration.connection_id`, the
               table schema must be specified using the top-level `schema` field
               documented above.
        :param builtins.str source_format: The data format. Please see sourceFormat under
               [ExternalDataConfiguration](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration)
               in Bigquery's public API documentation for supported formats. To use "GOOGLE_SHEETS"
               the `scopes` must include "https://www.googleapis.com/auth/drive.readonly".
        """
        pulumi.set(__self__, "autodetect", autodetect)
        pulumi.set(__self__, "source_uris", source_uris)
        if avro_options is not None:
            pulumi.set(__self__, "avro_options", avro_options)
        if bigtable_options is not None:
            pulumi.set(__self__, "bigtable_options", bigtable_options)
        if compression is not None:
            pulumi.set(__self__, "compression", compression)
        if connection_id is not None:
            pulumi.set(__self__, "connection_id", connection_id)
        if csv_options is not None:
            pulumi.set(__self__, "csv_options", csv_options)
        if file_set_spec_type is not None:
            pulumi.set(__self__, "file_set_spec_type", file_set_spec_type)
        if google_sheets_options is not None:
            pulumi.set(__self__, "google_sheets_options", google_sheets_options)
        if hive_partitioning_options is not None:
            pulumi.set(__self__, "hive_partitioning_options", hive_partitioning_options)
        if ignore_unknown_values is not None:
            pulumi.set(__self__, "ignore_unknown_values", ignore_unknown_values)
        if json_extension is not None:
            pulumi.set(__self__, "json_extension", json_extension)
        if json_options is not None:
            pulumi.set(__self__, "json_options", json_options)
        if max_bad_records is not None:
            pulumi.set(__self__, "max_bad_records", max_bad_records)
        if metadata_cache_mode is not None:
            pulumi.set(__self__, "metadata_cache_mode", metadata_cache_mode)
        if object_metadata is not None:
            pulumi.set(__self__, "object_metadata", object_metadata)
        if parquet_options is not None:
            pulumi.set(__self__, "parquet_options", parquet_options)
        if reference_file_schema_uri is not None:
            pulumi.set(__self__, "reference_file_schema_uri", reference_file_schema_uri)
        if schema is not None:
            pulumi.set(__self__, "schema", schema)
        if source_format is not None:
            pulumi.set(__self__, "source_format", source_format)

    @property
    @pulumi.getter
    def autodetect(self) -> builtins.bool:
        """
        Let BigQuery try to autodetect the schema
        and format of the table.
        """
        return pulumi.get(self, "autodetect")

    @property
    @pulumi.getter(name="sourceUris")
    def source_uris(self) -> Sequence[builtins.str]:
        """
        A list of the fully-qualified URIs that point to
        your data in Google Cloud.
        """
        return pulumi.get(self, "source_uris")

    @property
    @pulumi.getter(name="avroOptions")
    def avro_options(self) -> Optional['outputs.TableExternalDataConfigurationAvroOptions']:
        """
        Additional options if `source_format` is set to
        "AVRO".  Structure is documented below.
        """
        return pulumi.get(self, "avro_options")

    @property
    @pulumi.getter(name="bigtableOptions")
    def bigtable_options(self) -> Optional['outputs.TableExternalDataConfigurationBigtableOptions']:
        """
        Additional properties to set if
        `source_format` is set to "BIGTABLE". Structure is documented below.
        """
        return pulumi.get(self, "bigtable_options")

    @property
    @pulumi.getter
    def compression(self) -> Optional[builtins.str]:
        """
        The compression type of the data source.
        Valid values are "NONE" or "GZIP".
        """
        return pulumi.get(self, "compression")

    @property
    @pulumi.getter(name="connectionId")
    def connection_id(self) -> Optional[builtins.str]:
        """
        The connection specifying the credentials to be used to read
        external storage, such as Azure Blob, Cloud Storage, or S3. The `connection_id` can have
        the form `{{project}}.{{location}}.{{connection_id}}`
        or `projects/{{project}}/locations/{{location}}/connections/{{connection_id}}`.

        ~>**NOTE:** If you set `external_data_configuration.connection_id`, the
        table schema must be specified using the top-level `schema` field
        documented above.
        """
        return pulumi.get(self, "connection_id")

    @property
    @pulumi.getter(name="csvOptions")
    def csv_options(self) -> Optional['outputs.TableExternalDataConfigurationCsvOptions']:
        """
        Additional properties to set if
        `source_format` is set to "CSV". Structure is documented below.
        """
        return pulumi.get(self, "csv_options")

    @property
    @pulumi.getter(name="fileSetSpecType")
    def file_set_spec_type(self) -> Optional[builtins.str]:
        """
        Specifies how source URIs are interpreted for constructing the file set to load.
        By default source URIs are expanded against the underlying storage.
        Other options include specifying manifest files. Only applicable to object storage systems. Docs
        """
        return pulumi.get(self, "file_set_spec_type")

    @property
    @pulumi.getter(name="googleSheetsOptions")
    def google_sheets_options(self) -> Optional['outputs.TableExternalDataConfigurationGoogleSheetsOptions']:
        """
        Additional options if
        `source_format` is set to "GOOGLE_SHEETS". Structure is
        documented below.
        """
        return pulumi.get(self, "google_sheets_options")

    @property
    @pulumi.getter(name="hivePartitioningOptions")
    def hive_partitioning_options(self) -> Optional['outputs.TableExternalDataConfigurationHivePartitioningOptions']:
        """
        When set, configures hive partitioning
        support. Not all storage formats support hive partitioning -- requesting hive
        partitioning on an unsupported format will lead to an error, as will providing
        an invalid specification. Structure is documented below.
        """
        return pulumi.get(self, "hive_partitioning_options")

    @property
    @pulumi.getter(name="ignoreUnknownValues")
    def ignore_unknown_values(self) -> Optional[builtins.bool]:
        """
        Indicates if BigQuery should
        allow extra values that are not represented in the table schema.
        If true, the extra values are ignored. If false, records with
        extra columns are treated as bad records, and if there are too
        many bad records, an invalid error is returned in the job result.
        The default value is false.
        """
        return pulumi.get(self, "ignore_unknown_values")

    @property
    @pulumi.getter(name="jsonExtension")
    def json_extension(self) -> Optional[builtins.str]:
        """
        Used to indicate that a JSON variant, rather than normal JSON, is being used as the sourceFormat. This should only be used in combination with the `JSON` source format. Valid values are: `GEOJSON`.
        """
        return pulumi.get(self, "json_extension")

    @property
    @pulumi.getter(name="jsonOptions")
    def json_options(self) -> Optional['outputs.TableExternalDataConfigurationJsonOptions']:
        """
        Additional properties to set if
        `source_format` is set to "JSON". Structure is documented below.
        """
        return pulumi.get(self, "json_options")

    @property
    @pulumi.getter(name="maxBadRecords")
    def max_bad_records(self) -> Optional[builtins.int]:
        """
        The maximum number of bad records that
        BigQuery can ignore when reading data.
        """
        return pulumi.get(self, "max_bad_records")

    @property
    @pulumi.getter(name="metadataCacheMode")
    def metadata_cache_mode(self) -> Optional[builtins.str]:
        """
        Metadata Cache Mode for the table. Set this to enable caching of metadata from external data source. Valid values are `AUTOMATIC` and `MANUAL`.
        """
        return pulumi.get(self, "metadata_cache_mode")

    @property
    @pulumi.getter(name="objectMetadata")
    def object_metadata(self) -> Optional[builtins.str]:
        """
        Object Metadata is used to create Object Tables. Object Tables contain a listing of objects (with their metadata) found at the sourceUris. If `object_metadata` is set, `source_format` should be omitted.
        """
        return pulumi.get(self, "object_metadata")

    @property
    @pulumi.getter(name="parquetOptions")
    def parquet_options(self) -> Optional['outputs.TableExternalDataConfigurationParquetOptions']:
        """
        Additional properties to set if
        `source_format` is set to "PARQUET". Structure is documented below.
        """
        return pulumi.get(self, "parquet_options")

    @property
    @pulumi.getter(name="referenceFileSchemaUri")
    def reference_file_schema_uri(self) -> Optional[builtins.str]:
        """
        When creating an external table, the user can provide a reference file with the table schema. This is enabled for the following formats: AVRO, PARQUET, ORC.
        """
        return pulumi.get(self, "reference_file_schema_uri")

    @property
    @pulumi.getter
    def schema(self) -> Optional[builtins.str]:
        """
        A JSON schema for the external table. Schema is required
        for CSV and JSON formats if autodetect is not on. Schema is disallowed
        for Google Cloud Bigtable, Cloud Datastore backups, Avro, Iceberg, ORC and Parquet formats.
        ~>**NOTE:** Because this field expects a JSON string, any changes to the
        string will create a diff, even if the JSON itself hasn't changed.
        Furthermore drift for this field cannot not be detected because BigQuery
        only uses this schema to compute the effective schema for the table, therefore
        any changes on the configured value will force the table to be recreated.
        This schema is effectively only applied when creating a table from an external
        datasource, after creation the computed schema will be stored in
        `google_bigquery_table.schema`

        ~>**NOTE:** If you set `external_data_configuration.connection_id`, the
        table schema must be specified using the top-level `schema` field
        documented above.
        """
        return pulumi.get(self, "schema")

    @property
    @pulumi.getter(name="sourceFormat")
    def source_format(self) -> Optional[builtins.str]:
        """
        The data format. Please see sourceFormat under
        [ExternalDataConfiguration](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration)
        in Bigquery's public API documentation for supported formats. To use "GOOGLE_SHEETS"
        the `scopes` must include "https://www.googleapis.com/auth/drive.readonly".
        """
        return pulumi.get(self, "source_format")


@pulumi.output_type
class TableExternalDataConfigurationAvroOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "useAvroLogicalTypes":
            suggest = "use_avro_logical_types"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationAvroOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationAvroOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationAvroOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 use_avro_logical_types: builtins.bool):
        """
        :param builtins.bool use_avro_logical_types: If is set to true, indicates whether
               to interpret logical types as the corresponding BigQuery data type
               (for example, TIMESTAMP), instead of using the raw type (for example, INTEGER).
        """
        pulumi.set(__self__, "use_avro_logical_types", use_avro_logical_types)

    @property
    @pulumi.getter(name="useAvroLogicalTypes")
    def use_avro_logical_types(self) -> builtins.bool:
        """
        If is set to true, indicates whether
        to interpret logical types as the corresponding BigQuery data type
        (for example, TIMESTAMP), instead of using the raw type (for example, INTEGER).
        """
        return pulumi.get(self, "use_avro_logical_types")


@pulumi.output_type
class TableExternalDataConfigurationBigtableOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "columnFamilies":
            suggest = "column_families"
        elif key == "ignoreUnspecifiedColumnFamilies":
            suggest = "ignore_unspecified_column_families"
        elif key == "outputColumnFamiliesAsJson":
            suggest = "output_column_families_as_json"
        elif key == "readRowkeyAsString":
            suggest = "read_rowkey_as_string"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationBigtableOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationBigtableOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationBigtableOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 column_families: Optional[Sequence['outputs.TableExternalDataConfigurationBigtableOptionsColumnFamily']] = None,
                 ignore_unspecified_column_families: Optional[builtins.bool] = None,
                 output_column_families_as_json: Optional[builtins.bool] = None,
                 read_rowkey_as_string: Optional[builtins.bool] = None):
        """
        :param Sequence['TableExternalDataConfigurationBigtableOptionsColumnFamilyArgs'] column_families: A list of column families to expose in the table schema along with their types. This list restricts the column families that can be referenced in queries and specifies their value types. You can use this list to do type conversions - see the 'type' field for more details. If you leave this list empty, all column families are present in the table schema and their values are read as BYTES. During a query only the column families referenced in that query are read from Bigtable.  Structure is documented below.
        :param builtins.bool ignore_unspecified_column_families: If field is true, then the column families that are not specified in columnFamilies list are not exposed in the table schema. Otherwise, they are read with BYTES type values. The default value is false.
        :param builtins.bool output_column_families_as_json: If field is true, then each column family will be read as a single JSON column. Otherwise they are read as a repeated cell structure containing timestamp/value tuples. The default value is false.
        :param builtins.bool read_rowkey_as_string: If field is true, then the rowkey column families will be read and converted to string. Otherwise they are read with BYTES type values and users need to manually cast them with CAST if necessary. The default value is false.
        """
        if column_families is not None:
            pulumi.set(__self__, "column_families", column_families)
        if ignore_unspecified_column_families is not None:
            pulumi.set(__self__, "ignore_unspecified_column_families", ignore_unspecified_column_families)
        if output_column_families_as_json is not None:
            pulumi.set(__self__, "output_column_families_as_json", output_column_families_as_json)
        if read_rowkey_as_string is not None:
            pulumi.set(__self__, "read_rowkey_as_string", read_rowkey_as_string)

    @property
    @pulumi.getter(name="columnFamilies")
    def column_families(self) -> Optional[Sequence['outputs.TableExternalDataConfigurationBigtableOptionsColumnFamily']]:
        """
        A list of column families to expose in the table schema along with their types. This list restricts the column families that can be referenced in queries and specifies their value types. You can use this list to do type conversions - see the 'type' field for more details. If you leave this list empty, all column families are present in the table schema and their values are read as BYTES. During a query only the column families referenced in that query are read from Bigtable.  Structure is documented below.
        """
        return pulumi.get(self, "column_families")

    @property
    @pulumi.getter(name="ignoreUnspecifiedColumnFamilies")
    def ignore_unspecified_column_families(self) -> Optional[builtins.bool]:
        """
        If field is true, then the column families that are not specified in columnFamilies list are not exposed in the table schema. Otherwise, they are read with BYTES type values. The default value is false.
        """
        return pulumi.get(self, "ignore_unspecified_column_families")

    @property
    @pulumi.getter(name="outputColumnFamiliesAsJson")
    def output_column_families_as_json(self) -> Optional[builtins.bool]:
        """
        If field is true, then each column family will be read as a single JSON column. Otherwise they are read as a repeated cell structure containing timestamp/value tuples. The default value is false.
        """
        return pulumi.get(self, "output_column_families_as_json")

    @property
    @pulumi.getter(name="readRowkeyAsString")
    def read_rowkey_as_string(self) -> Optional[builtins.bool]:
        """
        If field is true, then the rowkey column families will be read and converted to string. Otherwise they are read with BYTES type values and users need to manually cast them with CAST if necessary. The default value is false.
        """
        return pulumi.get(self, "read_rowkey_as_string")


@pulumi.output_type
class TableExternalDataConfigurationBigtableOptionsColumnFamily(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "familyId":
            suggest = "family_id"
        elif key == "onlyReadLatest":
            suggest = "only_read_latest"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationBigtableOptionsColumnFamily. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationBigtableOptionsColumnFamily.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationBigtableOptionsColumnFamily.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 columns: Optional[Sequence['outputs.TableExternalDataConfigurationBigtableOptionsColumnFamilyColumn']] = None,
                 encoding: Optional[builtins.str] = None,
                 family_id: Optional[builtins.str] = None,
                 only_read_latest: Optional[builtins.bool] = None,
                 type: Optional[builtins.str] = None):
        """
        :param Sequence['TableExternalDataConfigurationBigtableOptionsColumnFamilyColumnArgs'] columns: A List of columns that should be exposed as individual fields as opposed to a list of (column name, value) pairs. All columns whose qualifier matches a qualifier in this list can be accessed as Other columns can be accessed as a list through column field.  Structure is documented below.
        :param builtins.str encoding: The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. This can be overridden for a specific column by listing that column in 'columns' and specifying an encoding for it.
        :param builtins.str family_id: Identifier of the column family.
        :param builtins.bool only_read_latest: If this is set only the latest version of value are exposed for all columns in this column family. This can be overridden for a specific column by listing that column in 'columns' and specifying a different setting for that column.
        :param builtins.str type: The type to convert the value in cells of this column family. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON". Default type is BYTES. This can be overridden for a specific column by listing that column in 'columns' and specifying a type for it.
        """
        if columns is not None:
            pulumi.set(__self__, "columns", columns)
        if encoding is not None:
            pulumi.set(__self__, "encoding", encoding)
        if family_id is not None:
            pulumi.set(__self__, "family_id", family_id)
        if only_read_latest is not None:
            pulumi.set(__self__, "only_read_latest", only_read_latest)
        if type is not None:
            pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def columns(self) -> Optional[Sequence['outputs.TableExternalDataConfigurationBigtableOptionsColumnFamilyColumn']]:
        """
        A List of columns that should be exposed as individual fields as opposed to a list of (column name, value) pairs. All columns whose qualifier matches a qualifier in this list can be accessed as Other columns can be accessed as a list through column field.  Structure is documented below.
        """
        return pulumi.get(self, "columns")

    @property
    @pulumi.getter
    def encoding(self) -> Optional[builtins.str]:
        """
        The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. This can be overridden for a specific column by listing that column in 'columns' and specifying an encoding for it.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="familyId")
    def family_id(self) -> Optional[builtins.str]:
        """
        Identifier of the column family.
        """
        return pulumi.get(self, "family_id")

    @property
    @pulumi.getter(name="onlyReadLatest")
    def only_read_latest(self) -> Optional[builtins.bool]:
        """
        If this is set only the latest version of value are exposed for all columns in this column family. This can be overridden for a specific column by listing that column in 'columns' and specifying a different setting for that column.
        """
        return pulumi.get(self, "only_read_latest")

    @property
    @pulumi.getter
    def type(self) -> Optional[builtins.str]:
        """
        The type to convert the value in cells of this column family. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON". Default type is BYTES. This can be overridden for a specific column by listing that column in 'columns' and specifying a type for it.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class TableExternalDataConfigurationBigtableOptionsColumnFamilyColumn(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "fieldName":
            suggest = "field_name"
        elif key == "onlyReadLatest":
            suggest = "only_read_latest"
        elif key == "qualifierEncoded":
            suggest = "qualifier_encoded"
        elif key == "qualifierString":
            suggest = "qualifier_string"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationBigtableOptionsColumnFamilyColumn. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationBigtableOptionsColumnFamilyColumn.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationBigtableOptionsColumnFamilyColumn.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 encoding: Optional[builtins.str] = None,
                 field_name: Optional[builtins.str] = None,
                 only_read_latest: Optional[builtins.bool] = None,
                 qualifier_encoded: Optional[builtins.str] = None,
                 qualifier_string: Optional[builtins.str] = None,
                 type: Optional[builtins.str] = None):
        """
        :param builtins.str encoding: The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. 'encoding' can also be set at the column family level. However, the setting at this level takes precedence if 'encoding' is set at both levels.
        :param builtins.str field_name: If the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as the column field name and is used as field name in queries.
        :param builtins.bool only_read_latest: If this is set, only the latest version of value in this column are exposed. 'onlyReadLatest' can also be set at the column family level. However, the setting at this level takes precedence if 'onlyReadLatest' is set at both levels.
        :param builtins.str qualifier_encoded: Qualifier of the column. Columns in the parent column family that has this exact qualifier are exposed as . field. If the qualifier is valid UTF-8 string, it can be specified in the qualifierString field. Otherwise, a base-64 encoded value must be set to qualifierEncoded. The column field name is the same as the column qualifier. However, if the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as fieldName.
        :param builtins.str qualifier_string: Qualifier string.
        :param builtins.str type: The type to convert the value in cells of this column. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON", Default type is "BYTES". 'type' can also be set at the column family level. However, the setting at this level takes precedence if 'type' is set at both levels.
        """
        if encoding is not None:
            pulumi.set(__self__, "encoding", encoding)
        if field_name is not None:
            pulumi.set(__self__, "field_name", field_name)
        if only_read_latest is not None:
            pulumi.set(__self__, "only_read_latest", only_read_latest)
        if qualifier_encoded is not None:
            pulumi.set(__self__, "qualifier_encoded", qualifier_encoded)
        if qualifier_string is not None:
            pulumi.set(__self__, "qualifier_string", qualifier_string)
        if type is not None:
            pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def encoding(self) -> Optional[builtins.str]:
        """
        The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. 'encoding' can also be set at the column family level. However, the setting at this level takes precedence if 'encoding' is set at both levels.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="fieldName")
    def field_name(self) -> Optional[builtins.str]:
        """
        If the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as the column field name and is used as field name in queries.
        """
        return pulumi.get(self, "field_name")

    @property
    @pulumi.getter(name="onlyReadLatest")
    def only_read_latest(self) -> Optional[builtins.bool]:
        """
        If this is set, only the latest version of value in this column are exposed. 'onlyReadLatest' can also be set at the column family level. However, the setting at this level takes precedence if 'onlyReadLatest' is set at both levels.
        """
        return pulumi.get(self, "only_read_latest")

    @property
    @pulumi.getter(name="qualifierEncoded")
    def qualifier_encoded(self) -> Optional[builtins.str]:
        """
        Qualifier of the column. Columns in the parent column family that has this exact qualifier are exposed as . field. If the qualifier is valid UTF-8 string, it can be specified in the qualifierString field. Otherwise, a base-64 encoded value must be set to qualifierEncoded. The column field name is the same as the column qualifier. However, if the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as fieldName.
        """
        return pulumi.get(self, "qualifier_encoded")

    @property
    @pulumi.getter(name="qualifierString")
    def qualifier_string(self) -> Optional[builtins.str]:
        """
        Qualifier string.
        """
        return pulumi.get(self, "qualifier_string")

    @property
    @pulumi.getter
    def type(self) -> Optional[builtins.str]:
        """
        The type to convert the value in cells of this column. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON", Default type is "BYTES". 'type' can also be set at the column family level. However, the setting at this level takes precedence if 'type' is set at both levels.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class TableExternalDataConfigurationCsvOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "allowJaggedRows":
            suggest = "allow_jagged_rows"
        elif key == "allowQuotedNewlines":
            suggest = "allow_quoted_newlines"
        elif key == "fieldDelimiter":
            suggest = "field_delimiter"
        elif key == "skipLeadingRows":
            suggest = "skip_leading_rows"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationCsvOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationCsvOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationCsvOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 quote: builtins.str,
                 allow_jagged_rows: Optional[builtins.bool] = None,
                 allow_quoted_newlines: Optional[builtins.bool] = None,
                 encoding: Optional[builtins.str] = None,
                 field_delimiter: Optional[builtins.str] = None,
                 skip_leading_rows: Optional[builtins.int] = None):
        """
        :param builtins.str quote: The value that is used to quote data sections in a
               CSV file. If your data does not contain quoted sections, set the
               property value to an empty string. If your data contains quoted newline
               characters, you must also set the `allow_quoted_newlines` property to true.
               The API-side default is `"`, specified in the provider escaped as `\\"`. Due to
               limitations with default values, this value is required to be
               explicitly set.
        :param builtins.bool allow_jagged_rows: Indicates if BigQuery should accept rows
               that are missing trailing optional columns.
        :param builtins.bool allow_quoted_newlines: Indicates if BigQuery should allow
               quoted data sections that contain newline characters in a CSV file.
               The default value is false.
        :param builtins.str encoding: The character encoding of the data. The supported
               values are UTF-8 or ISO-8859-1.
        :param builtins.str field_delimiter: The separator for fields in a CSV file.
        :param builtins.int skip_leading_rows: The number of rows at the top of a CSV
               file that BigQuery will skip when reading the data.
        """
        pulumi.set(__self__, "quote", quote)
        if allow_jagged_rows is not None:
            pulumi.set(__self__, "allow_jagged_rows", allow_jagged_rows)
        if allow_quoted_newlines is not None:
            pulumi.set(__self__, "allow_quoted_newlines", allow_quoted_newlines)
        if encoding is not None:
            pulumi.set(__self__, "encoding", encoding)
        if field_delimiter is not None:
            pulumi.set(__self__, "field_delimiter", field_delimiter)
        if skip_leading_rows is not None:
            pulumi.set(__self__, "skip_leading_rows", skip_leading_rows)

    @property
    @pulumi.getter
    def quote(self) -> builtins.str:
        """
        The value that is used to quote data sections in a
        CSV file. If your data does not contain quoted sections, set the
        property value to an empty string. If your data contains quoted newline
        characters, you must also set the `allow_quoted_newlines` property to true.
        The API-side default is `"`, specified in the provider escaped as `\\"`. Due to
        limitations with default values, this value is required to be
        explicitly set.
        """
        return pulumi.get(self, "quote")

    @property
    @pulumi.getter(name="allowJaggedRows")
    def allow_jagged_rows(self) -> Optional[builtins.bool]:
        """
        Indicates if BigQuery should accept rows
        that are missing trailing optional columns.
        """
        return pulumi.get(self, "allow_jagged_rows")

    @property
    @pulumi.getter(name="allowQuotedNewlines")
    def allow_quoted_newlines(self) -> Optional[builtins.bool]:
        """
        Indicates if BigQuery should allow
        quoted data sections that contain newline characters in a CSV file.
        The default value is false.
        """
        return pulumi.get(self, "allow_quoted_newlines")

    @property
    @pulumi.getter
    def encoding(self) -> Optional[builtins.str]:
        """
        The character encoding of the data. The supported
        values are UTF-8 or ISO-8859-1.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="fieldDelimiter")
    def field_delimiter(self) -> Optional[builtins.str]:
        """
        The separator for fields in a CSV file.
        """
        return pulumi.get(self, "field_delimiter")

    @property
    @pulumi.getter(name="skipLeadingRows")
    def skip_leading_rows(self) -> Optional[builtins.int]:
        """
        The number of rows at the top of a CSV
        file that BigQuery will skip when reading the data.
        """
        return pulumi.get(self, "skip_leading_rows")


@pulumi.output_type
class TableExternalDataConfigurationGoogleSheetsOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "skipLeadingRows":
            suggest = "skip_leading_rows"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationGoogleSheetsOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationGoogleSheetsOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationGoogleSheetsOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 range: Optional[builtins.str] = None,
                 skip_leading_rows: Optional[builtins.int] = None):
        """
        :param builtins.str range: Range of a sheet to query from. Only used when
               non-empty. At least one of `range` or `skip_leading_rows` must be set.
               Typical format: "sheet_name!top_left_cell_id:bottom_right_cell_id"
               For example: "sheet1!A1:B20"
        :param builtins.int skip_leading_rows: The number of rows at the top of the sheet
               that BigQuery will skip when reading the data. At least one of `range` or
               `skip_leading_rows` must be set.
        """
        if range is not None:
            pulumi.set(__self__, "range", range)
        if skip_leading_rows is not None:
            pulumi.set(__self__, "skip_leading_rows", skip_leading_rows)

    @property
    @pulumi.getter
    def range(self) -> Optional[builtins.str]:
        """
        Range of a sheet to query from. Only used when
        non-empty. At least one of `range` or `skip_leading_rows` must be set.
        Typical format: "sheet_name!top_left_cell_id:bottom_right_cell_id"
        For example: "sheet1!A1:B20"
        """
        return pulumi.get(self, "range")

    @property
    @pulumi.getter(name="skipLeadingRows")
    def skip_leading_rows(self) -> Optional[builtins.int]:
        """
        The number of rows at the top of the sheet
        that BigQuery will skip when reading the data. At least one of `range` or
        `skip_leading_rows` must be set.
        """
        return pulumi.get(self, "skip_leading_rows")


@pulumi.output_type
class TableExternalDataConfigurationHivePartitioningOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "requirePartitionFilter":
            suggest = "require_partition_filter"
        elif key == "sourceUriPrefix":
            suggest = "source_uri_prefix"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationHivePartitioningOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationHivePartitioningOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationHivePartitioningOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 mode: Optional[builtins.str] = None,
                 require_partition_filter: Optional[builtins.bool] = None,
                 source_uri_prefix: Optional[builtins.str] = None):
        """
        :param builtins.str mode: When set, what mode of hive partitioning to use when
               reading data. The following modes are supported.
               * AUTO: automatically infer partition key name(s) and type(s).
               * STRINGS: automatically infer partition key name(s). All types are
               Not all storage formats support hive partitioning. Requesting hive
               partitioning on an unsupported format will lead to an error.
               Currently supported formats are: JSON, CSV, ORC, Avro and Parquet.
               * CUSTOM: when set to `CUSTOM`, you must encode the partition key schema within the `source_uri_prefix` by setting `source_uri_prefix` to `gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}`.
        :param builtins.bool require_partition_filter: If set to true, queries over this table
               require a partition filter that can be used for partition elimination to be
               specified.
        :param builtins.str source_uri_prefix: When hive partition detection is requested,
               a common for all source uris must be required. The prefix must end immediately
               before the partition key encoding begins. For example, consider files following
               this data layout. `gs://bucket/path_to_table/dt=2019-06-01/country=USA/id=7/file.avro`
               `gs://bucket/path_to_table/dt=2019-05-31/country=CA/id=3/file.avro` When hive
               partitioning is requested with either AUTO or STRINGS detection, the common prefix
               can be either of `gs://bucket/path_to_table` or `gs://bucket/path_to_table/`.
               Note that when `mode` is set to `CUSTOM`, you must encode the partition key schema within the `source_uri_prefix` by setting `source_uri_prefix` to `gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}`.
        """
        if mode is not None:
            pulumi.set(__self__, "mode", mode)
        if require_partition_filter is not None:
            pulumi.set(__self__, "require_partition_filter", require_partition_filter)
        if source_uri_prefix is not None:
            pulumi.set(__self__, "source_uri_prefix", source_uri_prefix)

    @property
    @pulumi.getter
    def mode(self) -> Optional[builtins.str]:
        """
        When set, what mode of hive partitioning to use when
        reading data. The following modes are supported.
        * AUTO: automatically infer partition key name(s) and type(s).
        * STRINGS: automatically infer partition key name(s). All types are
        Not all storage formats support hive partitioning. Requesting hive
        partitioning on an unsupported format will lead to an error.
        Currently supported formats are: JSON, CSV, ORC, Avro and Parquet.
        * CUSTOM: when set to `CUSTOM`, you must encode the partition key schema within the `source_uri_prefix` by setting `source_uri_prefix` to `gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}`.
        """
        return pulumi.get(self, "mode")

    @property
    @pulumi.getter(name="requirePartitionFilter")
    def require_partition_filter(self) -> Optional[builtins.bool]:
        """
        If set to true, queries over this table
        require a partition filter that can be used for partition elimination to be
        specified.
        """
        return pulumi.get(self, "require_partition_filter")

    @property
    @pulumi.getter(name="sourceUriPrefix")
    def source_uri_prefix(self) -> Optional[builtins.str]:
        """
        When hive partition detection is requested,
        a common for all source uris must be required. The prefix must end immediately
        before the partition key encoding begins. For example, consider files following
        this data layout. `gs://bucket/path_to_table/dt=2019-06-01/country=USA/id=7/file.avro`
        `gs://bucket/path_to_table/dt=2019-05-31/country=CA/id=3/file.avro` When hive
        partitioning is requested with either AUTO or STRINGS detection, the common prefix
        can be either of `gs://bucket/path_to_table` or `gs://bucket/path_to_table/`.
        Note that when `mode` is set to `CUSTOM`, you must encode the partition key schema within the `source_uri_prefix` by setting `source_uri_prefix` to `gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}`.
        """
        return pulumi.get(self, "source_uri_prefix")


@pulumi.output_type
class TableExternalDataConfigurationJsonOptions(dict):
    def __init__(__self__, *,
                 encoding: Optional[builtins.str] = None):
        """
        :param builtins.str encoding: The character encoding of the data. The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is UTF-8.
        """
        if encoding is not None:
            pulumi.set(__self__, "encoding", encoding)

    @property
    @pulumi.getter
    def encoding(self) -> Optional[builtins.str]:
        """
        The character encoding of the data. The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is UTF-8.
        """
        return pulumi.get(self, "encoding")


@pulumi.output_type
class TableExternalDataConfigurationParquetOptions(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enableListInference":
            suggest = "enable_list_inference"
        elif key == "enumAsString":
            suggest = "enum_as_string"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableExternalDataConfigurationParquetOptions. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableExternalDataConfigurationParquetOptions.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableExternalDataConfigurationParquetOptions.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_list_inference: Optional[builtins.bool] = None,
                 enum_as_string: Optional[builtins.bool] = None):
        """
        :param builtins.bool enable_list_inference: Indicates whether to use schema inference specifically for Parquet LIST logical type.
        :param builtins.bool enum_as_string: Indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
        """
        if enable_list_inference is not None:
            pulumi.set(__self__, "enable_list_inference", enable_list_inference)
        if enum_as_string is not None:
            pulumi.set(__self__, "enum_as_string", enum_as_string)

    @property
    @pulumi.getter(name="enableListInference")
    def enable_list_inference(self) -> Optional[builtins.bool]:
        """
        Indicates whether to use schema inference specifically for Parquet LIST logical type.
        """
        return pulumi.get(self, "enable_list_inference")

    @property
    @pulumi.getter(name="enumAsString")
    def enum_as_string(self) -> Optional[builtins.bool]:
        """
        Indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
        """
        return pulumi.get(self, "enum_as_string")


@pulumi.output_type
class TableMaterializedView(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "allowNonIncrementalDefinition":
            suggest = "allow_non_incremental_definition"
        elif key == "enableRefresh":
            suggest = "enable_refresh"
        elif key == "refreshIntervalMs":
            suggest = "refresh_interval_ms"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableMaterializedView. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableMaterializedView.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableMaterializedView.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 query: builtins.str,
                 allow_non_incremental_definition: Optional[builtins.bool] = None,
                 enable_refresh: Optional[builtins.bool] = None,
                 refresh_interval_ms: Optional[builtins.int] = None):
        """
        :param builtins.str query: A query whose result is persisted.
        :param builtins.bool allow_non_incremental_definition: Allow non incremental materialized view definition.
               The default value is false.
        :param builtins.bool enable_refresh: Specifies whether to use BigQuery's automatic refresh for this materialized view when the base table is updated.
               The default value is true.
        :param builtins.int refresh_interval_ms: The maximum frequency at which this materialized view will be refreshed.
               The default value is 1800000
        """
        pulumi.set(__self__, "query", query)
        if allow_non_incremental_definition is not None:
            pulumi.set(__self__, "allow_non_incremental_definition", allow_non_incremental_definition)
        if enable_refresh is not None:
            pulumi.set(__self__, "enable_refresh", enable_refresh)
        if refresh_interval_ms is not None:
            pulumi.set(__self__, "refresh_interval_ms", refresh_interval_ms)

    @property
    @pulumi.getter
    def query(self) -> builtins.str:
        """
        A query whose result is persisted.
        """
        return pulumi.get(self, "query")

    @property
    @pulumi.getter(name="allowNonIncrementalDefinition")
    def allow_non_incremental_definition(self) -> Optional[builtins.bool]:
        """
        Allow non incremental materialized view definition.
        The default value is false.
        """
        return pulumi.get(self, "allow_non_incremental_definition")

    @property
    @pulumi.getter(name="enableRefresh")
    def enable_refresh(self) -> Optional[builtins.bool]:
        """
        Specifies whether to use BigQuery's automatic refresh for this materialized view when the base table is updated.
        The default value is true.
        """
        return pulumi.get(self, "enable_refresh")

    @property
    @pulumi.getter(name="refreshIntervalMs")
    def refresh_interval_ms(self) -> Optional[builtins.int]:
        """
        The maximum frequency at which this materialized view will be refreshed.
        The default value is 1800000
        """
        return pulumi.get(self, "refresh_interval_ms")


@pulumi.output_type
class TableRangePartitioning(dict):
    def __init__(__self__, *,
                 field: builtins.str,
                 range: 'outputs.TableRangePartitioningRange'):
        """
        :param builtins.str field: The field used to determine how to create a range-based
               partition.
        :param 'TableRangePartitioningRangeArgs' range: Information required to partition based on ranges.
               Structure is documented below.
        """
        pulumi.set(__self__, "field", field)
        pulumi.set(__self__, "range", range)

    @property
    @pulumi.getter
    def field(self) -> builtins.str:
        """
        The field used to determine how to create a range-based
        partition.
        """
        return pulumi.get(self, "field")

    @property
    @pulumi.getter
    def range(self) -> 'outputs.TableRangePartitioningRange':
        """
        Information required to partition based on ranges.
        Structure is documented below.
        """
        return pulumi.get(self, "range")


@pulumi.output_type
class TableRangePartitioningRange(dict):
    def __init__(__self__, *,
                 end: builtins.int,
                 interval: builtins.int,
                 start: builtins.int):
        """
        :param builtins.int end: End of the range partitioning, exclusive.
        :param builtins.int interval: The width of each range within the partition.
        :param builtins.int start: Start of the range partitioning, inclusive.
        """
        pulumi.set(__self__, "end", end)
        pulumi.set(__self__, "interval", interval)
        pulumi.set(__self__, "start", start)

    @property
    @pulumi.getter
    def end(self) -> builtins.int:
        """
        End of the range partitioning, exclusive.
        """
        return pulumi.get(self, "end")

    @property
    @pulumi.getter
    def interval(self) -> builtins.int:
        """
        The width of each range within the partition.
        """
        return pulumi.get(self, "interval")

    @property
    @pulumi.getter
    def start(self) -> builtins.int:
        """
        Start of the range partitioning, inclusive.
        """
        return pulumi.get(self, "start")


@pulumi.output_type
class TableSchemaForeignTypeInfo(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "typeSystem":
            suggest = "type_system"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableSchemaForeignTypeInfo. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableSchemaForeignTypeInfo.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableSchemaForeignTypeInfo.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 type_system: builtins.str):
        """
        :param builtins.str type_system: Specifies the system which defines the foreign data
               type.
        """
        pulumi.set(__self__, "type_system", type_system)

    @property
    @pulumi.getter(name="typeSystem")
    def type_system(self) -> builtins.str:
        """
        Specifies the system which defines the foreign data
        type.
        """
        return pulumi.get(self, "type_system")


@pulumi.output_type
class TableTableConstraints(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "foreignKeys":
            suggest = "foreign_keys"
        elif key == "primaryKey":
            suggest = "primary_key"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableTableConstraints. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableTableConstraints.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableTableConstraints.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 foreign_keys: Optional[Sequence['outputs.TableTableConstraintsForeignKey']] = None,
                 primary_key: Optional['outputs.TableTableConstraintsPrimaryKey'] = None):
        """
        :param Sequence['TableTableConstraintsForeignKeyArgs'] foreign_keys: Present only if the table has a foreign key.
               The foreign key is not enforced.
               Structure is documented below.
        :param 'TableTableConstraintsPrimaryKeyArgs' primary_key: Represents the primary key constraint
               on a table's columns. Present only if the table has a primary key.
               The primary key is not enforced.
               Structure is documented below.
        """
        if foreign_keys is not None:
            pulumi.set(__self__, "foreign_keys", foreign_keys)
        if primary_key is not None:
            pulumi.set(__self__, "primary_key", primary_key)

    @property
    @pulumi.getter(name="foreignKeys")
    def foreign_keys(self) -> Optional[Sequence['outputs.TableTableConstraintsForeignKey']]:
        """
        Present only if the table has a foreign key.
        The foreign key is not enforced.
        Structure is documented below.
        """
        return pulumi.get(self, "foreign_keys")

    @property
    @pulumi.getter(name="primaryKey")
    def primary_key(self) -> Optional['outputs.TableTableConstraintsPrimaryKey']:
        """
        Represents the primary key constraint
        on a table's columns. Present only if the table has a primary key.
        The primary key is not enforced.
        Structure is documented below.
        """
        return pulumi.get(self, "primary_key")


@pulumi.output_type
class TableTableConstraintsForeignKey(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "columnReferences":
            suggest = "column_references"
        elif key == "referencedTable":
            suggest = "referenced_table"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableTableConstraintsForeignKey. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableTableConstraintsForeignKey.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableTableConstraintsForeignKey.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 column_references: 'outputs.TableTableConstraintsForeignKeyColumnReferences',
                 referenced_table: 'outputs.TableTableConstraintsForeignKeyReferencedTable',
                 name: Optional[builtins.str] = None):
        """
        :param 'TableTableConstraintsForeignKeyColumnReferencesArgs' column_references: The pair of the foreign key column and primary key column.
               Structure is documented below.
        :param 'TableTableConstraintsForeignKeyReferencedTableArgs' referenced_table: The table that holds the primary key
               and is referenced by this foreign key.
               Structure is documented below.
        :param builtins.str name: Set only if the foreign key constraint is named.
        """
        pulumi.set(__self__, "column_references", column_references)
        pulumi.set(__self__, "referenced_table", referenced_table)
        if name is not None:
            pulumi.set(__self__, "name", name)

    @property
    @pulumi.getter(name="columnReferences")
    def column_references(self) -> 'outputs.TableTableConstraintsForeignKeyColumnReferences':
        """
        The pair of the foreign key column and primary key column.
        Structure is documented below.
        """
        return pulumi.get(self, "column_references")

    @property
    @pulumi.getter(name="referencedTable")
    def referenced_table(self) -> 'outputs.TableTableConstraintsForeignKeyReferencedTable':
        """
        The table that holds the primary key
        and is referenced by this foreign key.
        Structure is documented below.
        """
        return pulumi.get(self, "referenced_table")

    @property
    @pulumi.getter
    def name(self) -> Optional[builtins.str]:
        """
        Set only if the foreign key constraint is named.
        """
        return pulumi.get(self, "name")


@pulumi.output_type
class TableTableConstraintsForeignKeyColumnReferences(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "referencedColumn":
            suggest = "referenced_column"
        elif key == "referencingColumn":
            suggest = "referencing_column"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableTableConstraintsForeignKeyColumnReferences. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableTableConstraintsForeignKeyColumnReferences.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableTableConstraintsForeignKeyColumnReferences.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 referenced_column: builtins.str,
                 referencing_column: builtins.str):
        """
        :param builtins.str referenced_column: The column in the primary key that are
               referenced by the referencingColumn
        :param builtins.str referencing_column: The column that composes the foreign key.
        """
        pulumi.set(__self__, "referenced_column", referenced_column)
        pulumi.set(__self__, "referencing_column", referencing_column)

    @property
    @pulumi.getter(name="referencedColumn")
    def referenced_column(self) -> builtins.str:
        """
        The column in the primary key that are
        referenced by the referencingColumn
        """
        return pulumi.get(self, "referenced_column")

    @property
    @pulumi.getter(name="referencingColumn")
    def referencing_column(self) -> builtins.str:
        """
        The column that composes the foreign key.
        """
        return pulumi.get(self, "referencing_column")


@pulumi.output_type
class TableTableConstraintsForeignKeyReferencedTable(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "datasetId":
            suggest = "dataset_id"
        elif key == "projectId":
            suggest = "project_id"
        elif key == "tableId":
            suggest = "table_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableTableConstraintsForeignKeyReferencedTable. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableTableConstraintsForeignKeyReferencedTable.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableTableConstraintsForeignKeyReferencedTable.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str,
                 table_id: builtins.str):
        """
        :param builtins.str dataset_id: The ID of the dataset containing this table.
        :param builtins.str project_id: The ID of the project containing this table.
        :param builtins.str table_id: The ID of the table. The ID must contain only
               letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum
               length is 1,024 characters. Certain operations allow suffixing of
               the table ID with a partition decorator, such as
               sample_table$20190123.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)
        pulumi.set(__self__, "table_id", table_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The ID of the dataset containing this table.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The ID of the table. The ID must contain only
        letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum
        length is 1,024 characters. Certain operations allow suffixing of
        the table ID with a partition decorator, such as
        sample_table$20190123.
        """
        return pulumi.get(self, "table_id")


@pulumi.output_type
class TableTableConstraintsPrimaryKey(dict):
    def __init__(__self__, *,
                 columns: Sequence[builtins.str]):
        """
        :param Sequence[builtins.str] columns: The columns that are composed of the primary key constraint.
        """
        pulumi.set(__self__, "columns", columns)

    @property
    @pulumi.getter
    def columns(self) -> Sequence[builtins.str]:
        """
        The columns that are composed of the primary key constraint.
        """
        return pulumi.get(self, "columns")


@pulumi.output_type
class TableTableReplicationInfo(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "sourceDatasetId":
            suggest = "source_dataset_id"
        elif key == "sourceProjectId":
            suggest = "source_project_id"
        elif key == "sourceTableId":
            suggest = "source_table_id"
        elif key == "replicationIntervalMs":
            suggest = "replication_interval_ms"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableTableReplicationInfo. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableTableReplicationInfo.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableTableReplicationInfo.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 source_dataset_id: builtins.str,
                 source_project_id: builtins.str,
                 source_table_id: builtins.str,
                 replication_interval_ms: Optional[builtins.int] = None):
        """
        :param builtins.str source_dataset_id: The ID of the source dataset.
        :param builtins.str source_project_id: The ID of the source project.
        :param builtins.str source_table_id: The ID of the source materialized view.
        :param builtins.int replication_interval_ms: The interval at which the source
               materialized view is polled for updates. The default is 300000.
        """
        pulumi.set(__self__, "source_dataset_id", source_dataset_id)
        pulumi.set(__self__, "source_project_id", source_project_id)
        pulumi.set(__self__, "source_table_id", source_table_id)
        if replication_interval_ms is not None:
            pulumi.set(__self__, "replication_interval_ms", replication_interval_ms)

    @property
    @pulumi.getter(name="sourceDatasetId")
    def source_dataset_id(self) -> builtins.str:
        """
        The ID of the source dataset.
        """
        return pulumi.get(self, "source_dataset_id")

    @property
    @pulumi.getter(name="sourceProjectId")
    def source_project_id(self) -> builtins.str:
        """
        The ID of the source project.
        """
        return pulumi.get(self, "source_project_id")

    @property
    @pulumi.getter(name="sourceTableId")
    def source_table_id(self) -> builtins.str:
        """
        The ID of the source materialized view.
        """
        return pulumi.get(self, "source_table_id")

    @property
    @pulumi.getter(name="replicationIntervalMs")
    def replication_interval_ms(self) -> Optional[builtins.int]:
        """
        The interval at which the source
        materialized view is polled for updates. The default is 300000.
        """
        return pulumi.get(self, "replication_interval_ms")


@pulumi.output_type
class TableTimePartitioning(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "expirationMs":
            suggest = "expiration_ms"
        elif key == "requirePartitionFilter":
            suggest = "require_partition_filter"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableTimePartitioning. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableTimePartitioning.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableTimePartitioning.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 type: builtins.str,
                 expiration_ms: Optional[builtins.int] = None,
                 field: Optional[builtins.str] = None,
                 require_partition_filter: Optional[builtins.bool] = None):
        """
        :param builtins.str type: The supported types are DAY, HOUR, MONTH, and YEAR,
               which will generate one partition per day, hour, month, and year, respectively.
        :param builtins.int expiration_ms: Number of milliseconds for which to keep the
               storage for a partition.
        :param builtins.str field: The field used to determine how to create a time-based
               partition. If time-based partitioning is enabled without this value, the
               table is partitioned based on the load time.
        :param builtins.bool require_partition_filter: If set to true, queries over this table
               require a partition filter that can be used for partition elimination to be
               specified. `require_partition_filter` is deprecated and will be removed in
               a future major release. Use the top level field with the same name instead.
        """
        pulumi.set(__self__, "type", type)
        if expiration_ms is not None:
            pulumi.set(__self__, "expiration_ms", expiration_ms)
        if field is not None:
            pulumi.set(__self__, "field", field)
        if require_partition_filter is not None:
            pulumi.set(__self__, "require_partition_filter", require_partition_filter)

    @property
    @pulumi.getter
    def type(self) -> builtins.str:
        """
        The supported types are DAY, HOUR, MONTH, and YEAR,
        which will generate one partition per day, hour, month, and year, respectively.
        """
        return pulumi.get(self, "type")

    @property
    @pulumi.getter(name="expirationMs")
    def expiration_ms(self) -> Optional[builtins.int]:
        """
        Number of milliseconds for which to keep the
        storage for a partition.
        """
        return pulumi.get(self, "expiration_ms")

    @property
    @pulumi.getter
    def field(self) -> Optional[builtins.str]:
        """
        The field used to determine how to create a time-based
        partition. If time-based partitioning is enabled without this value, the
        table is partitioned based on the load time.
        """
        return pulumi.get(self, "field")

    @property
    @pulumi.getter(name="requirePartitionFilter")
    @_utilities.deprecated("""This field is deprecated and will be removed in a future major release; please use the top level field with the same name instead.""")
    def require_partition_filter(self) -> Optional[builtins.bool]:
        """
        If set to true, queries over this table
        require a partition filter that can be used for partition elimination to be
        specified. `require_partition_filter` is deprecated and will be removed in
        a future major release. Use the top level field with the same name instead.
        """
        return pulumi.get(self, "require_partition_filter")


@pulumi.output_type
class TableView(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "useLegacySql":
            suggest = "use_legacy_sql"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableView. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableView.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableView.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 query: builtins.str,
                 use_legacy_sql: Optional[builtins.bool] = None):
        """
        :param builtins.str query: A query that BigQuery executes when the view is referenced.
        :param builtins.bool use_legacy_sql: Specifies whether to use BigQuery's legacy SQL for this view.
               The default value is true. If set to false, the view will use BigQuery's standard SQL.
        """
        pulumi.set(__self__, "query", query)
        if use_legacy_sql is not None:
            pulumi.set(__self__, "use_legacy_sql", use_legacy_sql)

    @property
    @pulumi.getter
    def query(self) -> builtins.str:
        """
        A query that BigQuery executes when the view is referenced.
        """
        return pulumi.get(self, "query")

    @property
    @pulumi.getter(name="useLegacySql")
    def use_legacy_sql(self) -> Optional[builtins.bool]:
        """
        Specifies whether to use BigQuery's legacy SQL for this view.
        The default value is true. If set to false, the view will use BigQuery's standard SQL.
        """
        return pulumi.get(self, "use_legacy_sql")


@pulumi.output_type
class GetDatasetAccessResult(dict):
    def __init__(__self__, *,
                 conditions: Sequence['outputs.GetDatasetAccessConditionResult'],
                 datasets: Sequence['outputs.GetDatasetAccessDatasetResult'],
                 domain: builtins.str,
                 group_by_email: builtins.str,
                 iam_member: builtins.str,
                 role: builtins.str,
                 routines: Sequence['outputs.GetDatasetAccessRoutineResult'],
                 special_group: builtins.str,
                 user_by_email: builtins.str,
                 views: Sequence['outputs.GetDatasetAccessViewResult']):
        """
        :param Sequence['GetDatasetAccessConditionArgs'] conditions: Condition for the binding. If CEL expression in this field is true, this
               access binding will be considered.
        :param Sequence['GetDatasetAccessDatasetArgs'] datasets: Grants all resources of particular types in a particular dataset read access to the current dataset.
        :param builtins.str domain: A domain to grant access to. Any users signed in with the
               domain specified will be granted the specified access
        :param builtins.str group_by_email: An email address of a Google Group to grant access to.
        :param builtins.str iam_member: Some other type of member that appears in the IAM Policy but isn't a user,
               group, domain, or special group. For example: 'allUsers'
        :param builtins.str role: Describes the rights granted to the user specified by the other
               member of the access object. Basic, predefined, and custom roles
               are supported. Predefined roles that have equivalent basic roles
               are swapped by the API to their basic counterparts. See
               [official docs](https://cloud.google.com/bigquery/docs/access-control).
        :param Sequence['GetDatasetAccessRoutineArgs'] routines: A routine from a different dataset to grant access to. Queries
               executed against that routine will have read access to tables in
               this dataset. The role field is not required when this field is
               set. If that routine is updated by any user, access to the routine
               needs to be granted again via an update operation.
        :param builtins.str special_group: A special group to grant access to. Possible values include:
               * 'projectOwners': Owners of the enclosing project.
               * 'projectReaders': Readers of the enclosing project.
               * 'projectWriters': Writers of the enclosing project.
               * 'allAuthenticatedUsers': All authenticated BigQuery users.
        :param builtins.str user_by_email: An email address of a user to grant access to. For example:
               fred@example.com
        :param Sequence['GetDatasetAccessViewArgs'] views: A view from a different dataset to grant access to. Queries
               executed against that view will have read access to tables in
               this dataset. The role field is not required when this field is
               set. If that view is updated by any user, access to the view
               needs to be granted again via an update operation.
        """
        pulumi.set(__self__, "conditions", conditions)
        pulumi.set(__self__, "datasets", datasets)
        pulumi.set(__self__, "domain", domain)
        pulumi.set(__self__, "group_by_email", group_by_email)
        pulumi.set(__self__, "iam_member", iam_member)
        pulumi.set(__self__, "role", role)
        pulumi.set(__self__, "routines", routines)
        pulumi.set(__self__, "special_group", special_group)
        pulumi.set(__self__, "user_by_email", user_by_email)
        pulumi.set(__self__, "views", views)

    @property
    @pulumi.getter
    def conditions(self) -> Sequence['outputs.GetDatasetAccessConditionResult']:
        """
        Condition for the binding. If CEL expression in this field is true, this
        access binding will be considered.
        """
        return pulumi.get(self, "conditions")

    @property
    @pulumi.getter
    def datasets(self) -> Sequence['outputs.GetDatasetAccessDatasetResult']:
        """
        Grants all resources of particular types in a particular dataset read access to the current dataset.
        """
        return pulumi.get(self, "datasets")

    @property
    @pulumi.getter
    def domain(self) -> builtins.str:
        """
        A domain to grant access to. Any users signed in with the
        domain specified will be granted the specified access
        """
        return pulumi.get(self, "domain")

    @property
    @pulumi.getter(name="groupByEmail")
    def group_by_email(self) -> builtins.str:
        """
        An email address of a Google Group to grant access to.
        """
        return pulumi.get(self, "group_by_email")

    @property
    @pulumi.getter(name="iamMember")
    def iam_member(self) -> builtins.str:
        """
        Some other type of member that appears in the IAM Policy but isn't a user,
        group, domain, or special group. For example: 'allUsers'
        """
        return pulumi.get(self, "iam_member")

    @property
    @pulumi.getter
    def role(self) -> builtins.str:
        """
        Describes the rights granted to the user specified by the other
        member of the access object. Basic, predefined, and custom roles
        are supported. Predefined roles that have equivalent basic roles
        are swapped by the API to their basic counterparts. See
        [official docs](https://cloud.google.com/bigquery/docs/access-control).
        """
        return pulumi.get(self, "role")

    @property
    @pulumi.getter
    def routines(self) -> Sequence['outputs.GetDatasetAccessRoutineResult']:
        """
        A routine from a different dataset to grant access to. Queries
        executed against that routine will have read access to tables in
        this dataset. The role field is not required when this field is
        set. If that routine is updated by any user, access to the routine
        needs to be granted again via an update operation.
        """
        return pulumi.get(self, "routines")

    @property
    @pulumi.getter(name="specialGroup")
    def special_group(self) -> builtins.str:
        """
        A special group to grant access to. Possible values include:
        * 'projectOwners': Owners of the enclosing project.
        * 'projectReaders': Readers of the enclosing project.
        * 'projectWriters': Writers of the enclosing project.
        * 'allAuthenticatedUsers': All authenticated BigQuery users.
        """
        return pulumi.get(self, "special_group")

    @property
    @pulumi.getter(name="userByEmail")
    def user_by_email(self) -> builtins.str:
        """
        An email address of a user to grant access to. For example:
        fred@example.com
        """
        return pulumi.get(self, "user_by_email")

    @property
    @pulumi.getter
    def views(self) -> Sequence['outputs.GetDatasetAccessViewResult']:
        """
        A view from a different dataset to grant access to. Queries
        executed against that view will have read access to tables in
        this dataset. The role field is not required when this field is
        set. If that view is updated by any user, access to the view
        needs to be granted again via an update operation.
        """
        return pulumi.get(self, "views")


@pulumi.output_type
class GetDatasetAccessConditionResult(dict):
    def __init__(__self__, *,
                 description: builtins.str,
                 expression: builtins.str,
                 location: builtins.str,
                 title: builtins.str):
        """
        :param builtins.str description: Description of the expression. This is a longer text which describes the expression,
               e.g. when hovered over it in a UI.
        :param builtins.str expression: Textual representation of an expression in Common Expression Language syntax.
        :param builtins.str location: String indicating the location of the expression for error reporting, e.g. a file
               name and a position in the file.
        :param builtins.str title: Title for the expression, i.e. a short string describing its purpose.
               This can be used e.g. in UIs which allow to enter the expression.
        """
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "location", location)
        pulumi.set(__self__, "title", title)

    @property
    @pulumi.getter
    def description(self) -> builtins.str:
        """
        Description of the expression. This is a longer text which describes the expression,
        e.g. when hovered over it in a UI.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter
    def expression(self) -> builtins.str:
        """
        Textual representation of an expression in Common Expression Language syntax.
        """
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def location(self) -> builtins.str:
        """
        String indicating the location of the expression for error reporting, e.g. a file
        name and a position in the file.
        """
        return pulumi.get(self, "location")

    @property
    @pulumi.getter
    def title(self) -> builtins.str:
        """
        Title for the expression, i.e. a short string describing its purpose.
        This can be used e.g. in UIs which allow to enter the expression.
        """
        return pulumi.get(self, "title")


@pulumi.output_type
class GetDatasetAccessDatasetResult(dict):
    def __init__(__self__, *,
                 datasets: Sequence['outputs.GetDatasetAccessDatasetDatasetResult'],
                 target_types: Sequence[builtins.str]):
        """
        :param Sequence['GetDatasetAccessDatasetDatasetArgs'] datasets: The dataset this entry applies to
        :param Sequence[builtins.str] target_types: Which resources in the dataset this entry applies to. Currently, only views are supported,
               but additional target types may be added in the future. Possible values: VIEWS
        """
        pulumi.set(__self__, "datasets", datasets)
        pulumi.set(__self__, "target_types", target_types)

    @property
    @pulumi.getter
    def datasets(self) -> Sequence['outputs.GetDatasetAccessDatasetDatasetResult']:
        """
        The dataset this entry applies to
        """
        return pulumi.get(self, "datasets")

    @property
    @pulumi.getter(name="targetTypes")
    def target_types(self) -> Sequence[builtins.str]:
        """
        Which resources in the dataset this entry applies to. Currently, only views are supported,
        but additional target types may be added in the future. Possible values: VIEWS
        """
        return pulumi.get(self, "target_types")


@pulumi.output_type
class GetDatasetAccessDatasetDatasetResult(dict):
    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str):
        """
        :param builtins.str dataset_id: The dataset ID.
        :param builtins.str project_id: The ID of the project containing this table.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The dataset ID.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")


@pulumi.output_type
class GetDatasetAccessRoutineResult(dict):
    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str,
                 routine_id: builtins.str):
        """
        :param builtins.str dataset_id: The dataset ID.
        :param builtins.str project_id: The ID of the project containing this table.
        :param builtins.str routine_id: The ID of the routine. The ID must contain only letters (a-z,
               A-Z), numbers (0-9), or underscores (_). The maximum length
               is 256 characters.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)
        pulumi.set(__self__, "routine_id", routine_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The dataset ID.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")

    @property
    @pulumi.getter(name="routineId")
    def routine_id(self) -> builtins.str:
        """
        The ID of the routine. The ID must contain only letters (a-z,
        A-Z), numbers (0-9), or underscores (_). The maximum length
        is 256 characters.
        """
        return pulumi.get(self, "routine_id")


@pulumi.output_type
class GetDatasetAccessViewResult(dict):
    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str,
                 table_id: builtins.str):
        """
        :param builtins.str dataset_id: The dataset ID.
        :param builtins.str project_id: The ID of the project containing this table.
        :param builtins.str table_id: The ID of the table. The ID must contain only letters (a-z,
               A-Z), numbers (0-9), or underscores (_). The maximum length
               is 1,024 characters.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)
        pulumi.set(__self__, "table_id", table_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The dataset ID.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The ID of the table. The ID must contain only letters (a-z,
        A-Z), numbers (0-9), or underscores (_). The maximum length
        is 1,024 characters.
        """
        return pulumi.get(self, "table_id")


@pulumi.output_type
class GetDatasetDefaultEncryptionConfigurationResult(dict):
    def __init__(__self__, *,
                 kms_key_name: builtins.str):
        """
        :param builtins.str kms_key_name: Describes the Cloud KMS encryption key that will be used to protect destination
               BigQuery table. The BigQuery Service Account associated with your project requires
               access to this encryption key.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        Describes the Cloud KMS encryption key that will be used to protect destination
        BigQuery table. The BigQuery Service Account associated with your project requires
        access to this encryption key.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class GetDatasetExternalCatalogDatasetOptionResult(dict):
    def __init__(__self__, *,
                 default_storage_location_uri: builtins.str,
                 parameters: Mapping[str, builtins.str]):
        """
        :param builtins.str default_storage_location_uri: The storage location URI for all tables in the dataset. Equivalent to hive metastore's
               database locationUri. Maximum length of 1024 characters.
        :param Mapping[str, builtins.str] parameters: A map of key value pairs defining the parameters and properties of the open source schema.
               Maximum size of 2Mib.
        """
        pulumi.set(__self__, "default_storage_location_uri", default_storage_location_uri)
        pulumi.set(__self__, "parameters", parameters)

    @property
    @pulumi.getter(name="defaultStorageLocationUri")
    def default_storage_location_uri(self) -> builtins.str:
        """
        The storage location URI for all tables in the dataset. Equivalent to hive metastore's
        database locationUri. Maximum length of 1024 characters.
        """
        return pulumi.get(self, "default_storage_location_uri")

    @property
    @pulumi.getter
    def parameters(self) -> Mapping[str, builtins.str]:
        """
        A map of key value pairs defining the parameters and properties of the open source schema.
        Maximum size of 2Mib.
        """
        return pulumi.get(self, "parameters")


@pulumi.output_type
class GetDatasetExternalDatasetReferenceResult(dict):
    def __init__(__self__, *,
                 connection: builtins.str,
                 external_source: builtins.str):
        """
        :param builtins.str connection: The connection id that is used to access the externalSource.
               Format: projects/{projectId}/locations/{locationId}/connections/{connectionId}
        :param builtins.str external_source: External source that backs this dataset.
        """
        pulumi.set(__self__, "connection", connection)
        pulumi.set(__self__, "external_source", external_source)

    @property
    @pulumi.getter
    def connection(self) -> builtins.str:
        """
        The connection id that is used to access the externalSource.
        Format: projects/{projectId}/locations/{locationId}/connections/{connectionId}
        """
        return pulumi.get(self, "connection")

    @property
    @pulumi.getter(name="externalSource")
    def external_source(self) -> builtins.str:
        """
        External source that backs this dataset.
        """
        return pulumi.get(self, "external_source")


@pulumi.output_type
class GetTableBiglakeConfigurationResult(dict):
    def __init__(__self__, *,
                 connection_id: builtins.str,
                 file_format: builtins.str,
                 storage_uri: builtins.str,
                 table_format: builtins.str):
        """
        :param builtins.str connection_id: The connection specifying the credentials to be used to read and write to external storage, such as Cloud Storage. The connection_id can have the form "&lt;project\\_id&gt;.&lt;location\\_id&gt;.&lt;connection\\_id&gt;" or "projects/&lt;project\\_id&gt;/locations/&lt;location\\_id&gt;/connections/&lt;connection\\_id&gt;".
        :param builtins.str file_format: The file format the data is stored in.
        :param builtins.str storage_uri: The fully qualified location prefix of the external folder where table data is stored. The '*' wildcard character is not allowed. The URI should be in the format "gs://bucket/path_to_table/"
        :param builtins.str table_format: The table format the metadata only snapshots are stored in.
        """
        pulumi.set(__self__, "connection_id", connection_id)
        pulumi.set(__self__, "file_format", file_format)
        pulumi.set(__self__, "storage_uri", storage_uri)
        pulumi.set(__self__, "table_format", table_format)

    @property
    @pulumi.getter(name="connectionId")
    def connection_id(self) -> builtins.str:
        """
        The connection specifying the credentials to be used to read and write to external storage, such as Cloud Storage. The connection_id can have the form "&lt;project\\_id&gt;.&lt;location\\_id&gt;.&lt;connection\\_id&gt;" or "projects/&lt;project\\_id&gt;/locations/&lt;location\\_id&gt;/connections/&lt;connection\\_id&gt;".
        """
        return pulumi.get(self, "connection_id")

    @property
    @pulumi.getter(name="fileFormat")
    def file_format(self) -> builtins.str:
        """
        The file format the data is stored in.
        """
        return pulumi.get(self, "file_format")

    @property
    @pulumi.getter(name="storageUri")
    def storage_uri(self) -> builtins.str:
        """
        The fully qualified location prefix of the external folder where table data is stored. The '*' wildcard character is not allowed. The URI should be in the format "gs://bucket/path_to_table/"
        """
        return pulumi.get(self, "storage_uri")

    @property
    @pulumi.getter(name="tableFormat")
    def table_format(self) -> builtins.str:
        """
        The table format the metadata only snapshots are stored in.
        """
        return pulumi.get(self, "table_format")


@pulumi.output_type
class GetTableEncryptionConfigurationResult(dict):
    def __init__(__self__, *,
                 kms_key_name: builtins.str,
                 kms_key_version: builtins.str):
        """
        :param builtins.str kms_key_name: The self link or full name of a key which should be used to encrypt this table. Note that the default bigquery service account will need to have encrypt/decrypt permissions on this key - you may want to see the bigquery_get_default_service_account datasource and the kms.CryptoKeyIAMBinding resource.
        :param builtins.str kms_key_version: The self link or full name of the kms key version used to encrypt this table.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)
        pulumi.set(__self__, "kms_key_version", kms_key_version)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> builtins.str:
        """
        The self link or full name of a key which should be used to encrypt this table. Note that the default bigquery service account will need to have encrypt/decrypt permissions on this key - you may want to see the bigquery_get_default_service_account datasource and the kms.CryptoKeyIAMBinding resource.
        """
        return pulumi.get(self, "kms_key_name")

    @property
    @pulumi.getter(name="kmsKeyVersion")
    def kms_key_version(self) -> builtins.str:
        """
        The self link or full name of the kms key version used to encrypt this table.
        """
        return pulumi.get(self, "kms_key_version")


@pulumi.output_type
class GetTableExternalCatalogTableOptionResult(dict):
    def __init__(__self__, *,
                 connection_id: builtins.str,
                 parameters: Mapping[str, builtins.str],
                 storage_descriptors: Sequence['outputs.GetTableExternalCatalogTableOptionStorageDescriptorResult']):
        """
        :param builtins.str connection_id: The connection specifying the credentials to be used to read external storage, such as Azure Blob, Cloud Storage, or S3. The connection is needed to read the open source table from BigQuery Engine. The connection_id can have the form <project_id>.<location_id>.<connection_id> or projects/<project_id>/locations/<location_id>/connections/<connection_id>.
        :param Mapping[str, builtins.str] parameters: A map of key value pairs defining the parameters and properties of the open source table. Corresponds with hive meta store table parameters. Maximum size of 4Mib.
        :param Sequence['GetTableExternalCatalogTableOptionStorageDescriptorArgs'] storage_descriptors: A storage descriptor containing information about the physical storage of this table.
        """
        pulumi.set(__self__, "connection_id", connection_id)
        pulumi.set(__self__, "parameters", parameters)
        pulumi.set(__self__, "storage_descriptors", storage_descriptors)

    @property
    @pulumi.getter(name="connectionId")
    def connection_id(self) -> builtins.str:
        """
        The connection specifying the credentials to be used to read external storage, such as Azure Blob, Cloud Storage, or S3. The connection is needed to read the open source table from BigQuery Engine. The connection_id can have the form <project_id>.<location_id>.<connection_id> or projects/<project_id>/locations/<location_id>/connections/<connection_id>.
        """
        return pulumi.get(self, "connection_id")

    @property
    @pulumi.getter
    def parameters(self) -> Mapping[str, builtins.str]:
        """
        A map of key value pairs defining the parameters and properties of the open source table. Corresponds with hive meta store table parameters. Maximum size of 4Mib.
        """
        return pulumi.get(self, "parameters")

    @property
    @pulumi.getter(name="storageDescriptors")
    def storage_descriptors(self) -> Sequence['outputs.GetTableExternalCatalogTableOptionStorageDescriptorResult']:
        """
        A storage descriptor containing information about the physical storage of this table.
        """
        return pulumi.get(self, "storage_descriptors")


@pulumi.output_type
class GetTableExternalCatalogTableOptionStorageDescriptorResult(dict):
    def __init__(__self__, *,
                 input_format: builtins.str,
                 location_uri: builtins.str,
                 output_format: builtins.str,
                 serde_infos: Sequence['outputs.GetTableExternalCatalogTableOptionStorageDescriptorSerdeInfoResult']):
        """
        :param builtins.str input_format: Specifies the fully qualified class name of the InputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"). The maximum length is 128 characters.
        :param builtins.str location_uri: The physical location of the table (e.g. 'gs://spark-dataproc-data/pangea-data/case_sensitive/' or 'gs://spark-dataproc-data/pangea-data/*'). The maximum length is 2056 bytes.
        :param builtins.str output_format: Specifies the fully qualified class name of the OutputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat"). The maximum length is 128 characters.
        :param Sequence['GetTableExternalCatalogTableOptionStorageDescriptorSerdeInfoArgs'] serde_infos: Serializer and deserializer information.
        """
        pulumi.set(__self__, "input_format", input_format)
        pulumi.set(__self__, "location_uri", location_uri)
        pulumi.set(__self__, "output_format", output_format)
        pulumi.set(__self__, "serde_infos", serde_infos)

    @property
    @pulumi.getter(name="inputFormat")
    def input_format(self) -> builtins.str:
        """
        Specifies the fully qualified class name of the InputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"). The maximum length is 128 characters.
        """
        return pulumi.get(self, "input_format")

    @property
    @pulumi.getter(name="locationUri")
    def location_uri(self) -> builtins.str:
        """
        The physical location of the table (e.g. 'gs://spark-dataproc-data/pangea-data/case_sensitive/' or 'gs://spark-dataproc-data/pangea-data/*'). The maximum length is 2056 bytes.
        """
        return pulumi.get(self, "location_uri")

    @property
    @pulumi.getter(name="outputFormat")
    def output_format(self) -> builtins.str:
        """
        Specifies the fully qualified class name of the OutputFormat (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat"). The maximum length is 128 characters.
        """
        return pulumi.get(self, "output_format")

    @property
    @pulumi.getter(name="serdeInfos")
    def serde_infos(self) -> Sequence['outputs.GetTableExternalCatalogTableOptionStorageDescriptorSerdeInfoResult']:
        """
        Serializer and deserializer information.
        """
        return pulumi.get(self, "serde_infos")


@pulumi.output_type
class GetTableExternalCatalogTableOptionStorageDescriptorSerdeInfoResult(dict):
    def __init__(__self__, *,
                 name: builtins.str,
                 parameters: Mapping[str, builtins.str],
                 serialization_library: builtins.str):
        """
        :param builtins.str name: Name of the SerDe. The maximum length is 256 characters.
        :param Mapping[str, builtins.str] parameters: Key-value pairs that define the initialization parameters for the serialization library. Maximum size 10 Kib.
        :param builtins.str serialization_library: Specifies a fully-qualified class name of the serialization library that is responsible for the translation of data between table representation and the underlying low-level input and output format structures. The maximum length is 256 characters.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "parameters", parameters)
        pulumi.set(__self__, "serialization_library", serialization_library)

    @property
    @pulumi.getter
    def name(self) -> builtins.str:
        """
        Name of the SerDe. The maximum length is 256 characters.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def parameters(self) -> Mapping[str, builtins.str]:
        """
        Key-value pairs that define the initialization parameters for the serialization library. Maximum size 10 Kib.
        """
        return pulumi.get(self, "parameters")

    @property
    @pulumi.getter(name="serializationLibrary")
    def serialization_library(self) -> builtins.str:
        """
        Specifies a fully-qualified class name of the serialization library that is responsible for the translation of data between table representation and the underlying low-level input and output format structures. The maximum length is 256 characters.
        """
        return pulumi.get(self, "serialization_library")


@pulumi.output_type
class GetTableExternalDataConfigurationResult(dict):
    def __init__(__self__, *,
                 autodetect: builtins.bool,
                 avro_options: Sequence['outputs.GetTableExternalDataConfigurationAvroOptionResult'],
                 bigtable_options: Sequence['outputs.GetTableExternalDataConfigurationBigtableOptionResult'],
                 compression: builtins.str,
                 connection_id: builtins.str,
                 csv_options: Sequence['outputs.GetTableExternalDataConfigurationCsvOptionResult'],
                 file_set_spec_type: builtins.str,
                 google_sheets_options: Sequence['outputs.GetTableExternalDataConfigurationGoogleSheetsOptionResult'],
                 hive_partitioning_options: Sequence['outputs.GetTableExternalDataConfigurationHivePartitioningOptionResult'],
                 ignore_unknown_values: builtins.bool,
                 json_extension: builtins.str,
                 json_options: Sequence['outputs.GetTableExternalDataConfigurationJsonOptionResult'],
                 max_bad_records: builtins.int,
                 metadata_cache_mode: builtins.str,
                 object_metadata: builtins.str,
                 parquet_options: Sequence['outputs.GetTableExternalDataConfigurationParquetOptionResult'],
                 reference_file_schema_uri: builtins.str,
                 schema: builtins.str,
                 source_format: builtins.str,
                 source_uris: Sequence[builtins.str]):
        """
        :param builtins.bool autodetect: Let BigQuery try to autodetect the schema and format of the table.
        :param Sequence['GetTableExternalDataConfigurationAvroOptionArgs'] avro_options: Additional options if source_format is set to "AVRO"
        :param Sequence['GetTableExternalDataConfigurationBigtableOptionArgs'] bigtable_options: Additional options if sourceFormat is set to BIGTABLE.
        :param builtins.str compression: The compression type of the data source. Valid values are "NONE" or "GZIP".
        :param builtins.str connection_id: The connection specifying the credentials to be used to read external storage, such as Azure Blob, Cloud Storage, or S3. The connectionId can have the form "<project>.<location>.<connection_id>" or "projects/<project>/locations/<location>/connections/<connection_id>".
        :param Sequence['GetTableExternalDataConfigurationCsvOptionArgs'] csv_options: Additional properties to set if source_format is set to "CSV".
        :param builtins.str file_set_spec_type: Specifies how source URIs are interpreted for constructing the file set to load.  By default source URIs are expanded against the underlying storage.  Other options include specifying manifest files. Only applicable to object storage systems.
        :param Sequence['GetTableExternalDataConfigurationGoogleSheetsOptionArgs'] google_sheets_options: Additional options if source_format is set to "GOOGLE_SHEETS".
        :param Sequence['GetTableExternalDataConfigurationHivePartitioningOptionArgs'] hive_partitioning_options: When set, configures hive partitioning support. Not all storage formats support hive partitioning -- requesting hive partitioning on an unsupported format will lead to an error, as will providing an invalid specification.
        :param builtins.bool ignore_unknown_values: Indicates if BigQuery should allow extra values that are not represented in the table schema. If true, the extra values are ignored. If false, records with extra columns are treated as bad records, and if there are too many bad records, an invalid error is returned in the job result. The default value is false.
        :param builtins.str json_extension: Load option to be used together with sourceFormat newline-delimited JSON to indicate that a variant of JSON is being loaded. To load newline-delimited GeoJSON, specify GEOJSON (and sourceFormat must be set to NEWLINE_DELIMITED_JSON).
        :param Sequence['GetTableExternalDataConfigurationJsonOptionArgs'] json_options: Additional properties to set if sourceFormat is set to JSON.
        :param builtins.int max_bad_records: The maximum number of bad records that BigQuery can ignore when reading data.
        :param builtins.str metadata_cache_mode: Metadata Cache Mode for the table. Set this to enable caching of metadata from external data source.
        :param builtins.str object_metadata: Object Metadata is used to create Object Tables. Object Tables contain a listing of objects (with their metadata) found at the sourceUris. If ObjectMetadata is set, sourceFormat should be omitted.
        :param Sequence['GetTableExternalDataConfigurationParquetOptionArgs'] parquet_options: Additional properties to set if sourceFormat is set to PARQUET.
        :param builtins.str reference_file_schema_uri: When creating an external table, the user can provide a reference file with the table schema. This is enabled for the following formats: AVRO, PARQUET, ORC.
        :param builtins.str schema: A JSON schema for the external table. Schema is required for CSV and JSON formats and is disallowed for Google Cloud Bigtable, Cloud Datastore backups, and Avro formats when using external tables.
        :param builtins.str source_format: Please see sourceFormat under ExternalDataConfiguration in Bigquery's public API documentation (https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration) for supported formats. To use "GOOGLE_SHEETS" the scopes must include "googleapis.com/auth/drive.readonly".
        :param Sequence[builtins.str] source_uris: A list of the fully-qualified URIs that point to your data in Google Cloud.
        """
        pulumi.set(__self__, "autodetect", autodetect)
        pulumi.set(__self__, "avro_options", avro_options)
        pulumi.set(__self__, "bigtable_options", bigtable_options)
        pulumi.set(__self__, "compression", compression)
        pulumi.set(__self__, "connection_id", connection_id)
        pulumi.set(__self__, "csv_options", csv_options)
        pulumi.set(__self__, "file_set_spec_type", file_set_spec_type)
        pulumi.set(__self__, "google_sheets_options", google_sheets_options)
        pulumi.set(__self__, "hive_partitioning_options", hive_partitioning_options)
        pulumi.set(__self__, "ignore_unknown_values", ignore_unknown_values)
        pulumi.set(__self__, "json_extension", json_extension)
        pulumi.set(__self__, "json_options", json_options)
        pulumi.set(__self__, "max_bad_records", max_bad_records)
        pulumi.set(__self__, "metadata_cache_mode", metadata_cache_mode)
        pulumi.set(__self__, "object_metadata", object_metadata)
        pulumi.set(__self__, "parquet_options", parquet_options)
        pulumi.set(__self__, "reference_file_schema_uri", reference_file_schema_uri)
        pulumi.set(__self__, "schema", schema)
        pulumi.set(__self__, "source_format", source_format)
        pulumi.set(__self__, "source_uris", source_uris)

    @property
    @pulumi.getter
    def autodetect(self) -> builtins.bool:
        """
        Let BigQuery try to autodetect the schema and format of the table.
        """
        return pulumi.get(self, "autodetect")

    @property
    @pulumi.getter(name="avroOptions")
    def avro_options(self) -> Sequence['outputs.GetTableExternalDataConfigurationAvroOptionResult']:
        """
        Additional options if source_format is set to "AVRO"
        """
        return pulumi.get(self, "avro_options")

    @property
    @pulumi.getter(name="bigtableOptions")
    def bigtable_options(self) -> Sequence['outputs.GetTableExternalDataConfigurationBigtableOptionResult']:
        """
        Additional options if sourceFormat is set to BIGTABLE.
        """
        return pulumi.get(self, "bigtable_options")

    @property
    @pulumi.getter
    def compression(self) -> builtins.str:
        """
        The compression type of the data source. Valid values are "NONE" or "GZIP".
        """
        return pulumi.get(self, "compression")

    @property
    @pulumi.getter(name="connectionId")
    def connection_id(self) -> builtins.str:
        """
        The connection specifying the credentials to be used to read external storage, such as Azure Blob, Cloud Storage, or S3. The connectionId can have the form "<project>.<location>.<connection_id>" or "projects/<project>/locations/<location>/connections/<connection_id>".
        """
        return pulumi.get(self, "connection_id")

    @property
    @pulumi.getter(name="csvOptions")
    def csv_options(self) -> Sequence['outputs.GetTableExternalDataConfigurationCsvOptionResult']:
        """
        Additional properties to set if source_format is set to "CSV".
        """
        return pulumi.get(self, "csv_options")

    @property
    @pulumi.getter(name="fileSetSpecType")
    def file_set_spec_type(self) -> builtins.str:
        """
        Specifies how source URIs are interpreted for constructing the file set to load.  By default source URIs are expanded against the underlying storage.  Other options include specifying manifest files. Only applicable to object storage systems.
        """
        return pulumi.get(self, "file_set_spec_type")

    @property
    @pulumi.getter(name="googleSheetsOptions")
    def google_sheets_options(self) -> Sequence['outputs.GetTableExternalDataConfigurationGoogleSheetsOptionResult']:
        """
        Additional options if source_format is set to "GOOGLE_SHEETS".
        """
        return pulumi.get(self, "google_sheets_options")

    @property
    @pulumi.getter(name="hivePartitioningOptions")
    def hive_partitioning_options(self) -> Sequence['outputs.GetTableExternalDataConfigurationHivePartitioningOptionResult']:
        """
        When set, configures hive partitioning support. Not all storage formats support hive partitioning -- requesting hive partitioning on an unsupported format will lead to an error, as will providing an invalid specification.
        """
        return pulumi.get(self, "hive_partitioning_options")

    @property
    @pulumi.getter(name="ignoreUnknownValues")
    def ignore_unknown_values(self) -> builtins.bool:
        """
        Indicates if BigQuery should allow extra values that are not represented in the table schema. If true, the extra values are ignored. If false, records with extra columns are treated as bad records, and if there are too many bad records, an invalid error is returned in the job result. The default value is false.
        """
        return pulumi.get(self, "ignore_unknown_values")

    @property
    @pulumi.getter(name="jsonExtension")
    def json_extension(self) -> builtins.str:
        """
        Load option to be used together with sourceFormat newline-delimited JSON to indicate that a variant of JSON is being loaded. To load newline-delimited GeoJSON, specify GEOJSON (and sourceFormat must be set to NEWLINE_DELIMITED_JSON).
        """
        return pulumi.get(self, "json_extension")

    @property
    @pulumi.getter(name="jsonOptions")
    def json_options(self) -> Sequence['outputs.GetTableExternalDataConfigurationJsonOptionResult']:
        """
        Additional properties to set if sourceFormat is set to JSON.
        """
        return pulumi.get(self, "json_options")

    @property
    @pulumi.getter(name="maxBadRecords")
    def max_bad_records(self) -> builtins.int:
        """
        The maximum number of bad records that BigQuery can ignore when reading data.
        """
        return pulumi.get(self, "max_bad_records")

    @property
    @pulumi.getter(name="metadataCacheMode")
    def metadata_cache_mode(self) -> builtins.str:
        """
        Metadata Cache Mode for the table. Set this to enable caching of metadata from external data source.
        """
        return pulumi.get(self, "metadata_cache_mode")

    @property
    @pulumi.getter(name="objectMetadata")
    def object_metadata(self) -> builtins.str:
        """
        Object Metadata is used to create Object Tables. Object Tables contain a listing of objects (with their metadata) found at the sourceUris. If ObjectMetadata is set, sourceFormat should be omitted.
        """
        return pulumi.get(self, "object_metadata")

    @property
    @pulumi.getter(name="parquetOptions")
    def parquet_options(self) -> Sequence['outputs.GetTableExternalDataConfigurationParquetOptionResult']:
        """
        Additional properties to set if sourceFormat is set to PARQUET.
        """
        return pulumi.get(self, "parquet_options")

    @property
    @pulumi.getter(name="referenceFileSchemaUri")
    def reference_file_schema_uri(self) -> builtins.str:
        """
        When creating an external table, the user can provide a reference file with the table schema. This is enabled for the following formats: AVRO, PARQUET, ORC.
        """
        return pulumi.get(self, "reference_file_schema_uri")

    @property
    @pulumi.getter
    def schema(self) -> builtins.str:
        """
        A JSON schema for the external table. Schema is required for CSV and JSON formats and is disallowed for Google Cloud Bigtable, Cloud Datastore backups, and Avro formats when using external tables.
        """
        return pulumi.get(self, "schema")

    @property
    @pulumi.getter(name="sourceFormat")
    def source_format(self) -> builtins.str:
        """
        Please see sourceFormat under ExternalDataConfiguration in Bigquery's public API documentation (https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration) for supported formats. To use "GOOGLE_SHEETS" the scopes must include "googleapis.com/auth/drive.readonly".
        """
        return pulumi.get(self, "source_format")

    @property
    @pulumi.getter(name="sourceUris")
    def source_uris(self) -> Sequence[builtins.str]:
        """
        A list of the fully-qualified URIs that point to your data in Google Cloud.
        """
        return pulumi.get(self, "source_uris")


@pulumi.output_type
class GetTableExternalDataConfigurationAvroOptionResult(dict):
    def __init__(__self__, *,
                 use_avro_logical_types: builtins.bool):
        """
        :param builtins.bool use_avro_logical_types: If sourceFormat is set to "AVRO", indicates whether to interpret logical types as the corresponding BigQuery data type (for example, TIMESTAMP), instead of using the raw type (for example, INTEGER).
        """
        pulumi.set(__self__, "use_avro_logical_types", use_avro_logical_types)

    @property
    @pulumi.getter(name="useAvroLogicalTypes")
    def use_avro_logical_types(self) -> builtins.bool:
        """
        If sourceFormat is set to "AVRO", indicates whether to interpret logical types as the corresponding BigQuery data type (for example, TIMESTAMP), instead of using the raw type (for example, INTEGER).
        """
        return pulumi.get(self, "use_avro_logical_types")


@pulumi.output_type
class GetTableExternalDataConfigurationBigtableOptionResult(dict):
    def __init__(__self__, *,
                 column_families: Sequence['outputs.GetTableExternalDataConfigurationBigtableOptionColumnFamilyResult'],
                 ignore_unspecified_column_families: builtins.bool,
                 output_column_families_as_json: builtins.bool,
                 read_rowkey_as_string: builtins.bool):
        """
        :param Sequence['GetTableExternalDataConfigurationBigtableOptionColumnFamilyArgs'] column_families: A list of column families to expose in the table schema along with their types. This list restricts the column families that can be referenced in queries and specifies their value types. You can use this list to do type conversions - see the 'type' field for more details. If you leave this list empty, all column families are present in the table schema and their values are read as BYTES. During a query only the column families referenced in that query are read from Bigtable.
        :param builtins.bool ignore_unspecified_column_families: If field is true, then the column families that are not specified in columnFamilies list are not exposed in the table schema. Otherwise, they are read with BYTES type values. The default value is false.
        :param builtins.bool output_column_families_as_json: If field is true, then each column family will be read as a single JSON column. Otherwise they are read as a repeated cell structure containing timestamp/value tuples. The default value is false.
        :param builtins.bool read_rowkey_as_string: If field is true, then the rowkey column families will be read and converted to string. Otherwise they are read with BYTES type values and users need to manually cast them with CAST if necessary. The default value is false.
        """
        pulumi.set(__self__, "column_families", column_families)
        pulumi.set(__self__, "ignore_unspecified_column_families", ignore_unspecified_column_families)
        pulumi.set(__self__, "output_column_families_as_json", output_column_families_as_json)
        pulumi.set(__self__, "read_rowkey_as_string", read_rowkey_as_string)

    @property
    @pulumi.getter(name="columnFamilies")
    def column_families(self) -> Sequence['outputs.GetTableExternalDataConfigurationBigtableOptionColumnFamilyResult']:
        """
        A list of column families to expose in the table schema along with their types. This list restricts the column families that can be referenced in queries and specifies their value types. You can use this list to do type conversions - see the 'type' field for more details. If you leave this list empty, all column families are present in the table schema and their values are read as BYTES. During a query only the column families referenced in that query are read from Bigtable.
        """
        return pulumi.get(self, "column_families")

    @property
    @pulumi.getter(name="ignoreUnspecifiedColumnFamilies")
    def ignore_unspecified_column_families(self) -> builtins.bool:
        """
        If field is true, then the column families that are not specified in columnFamilies list are not exposed in the table schema. Otherwise, they are read with BYTES type values. The default value is false.
        """
        return pulumi.get(self, "ignore_unspecified_column_families")

    @property
    @pulumi.getter(name="outputColumnFamiliesAsJson")
    def output_column_families_as_json(self) -> builtins.bool:
        """
        If field is true, then each column family will be read as a single JSON column. Otherwise they are read as a repeated cell structure containing timestamp/value tuples. The default value is false.
        """
        return pulumi.get(self, "output_column_families_as_json")

    @property
    @pulumi.getter(name="readRowkeyAsString")
    def read_rowkey_as_string(self) -> builtins.bool:
        """
        If field is true, then the rowkey column families will be read and converted to string. Otherwise they are read with BYTES type values and users need to manually cast them with CAST if necessary. The default value is false.
        """
        return pulumi.get(self, "read_rowkey_as_string")


@pulumi.output_type
class GetTableExternalDataConfigurationBigtableOptionColumnFamilyResult(dict):
    def __init__(__self__, *,
                 columns: Sequence['outputs.GetTableExternalDataConfigurationBigtableOptionColumnFamilyColumnResult'],
                 encoding: builtins.str,
                 family_id: builtins.str,
                 only_read_latest: builtins.bool,
                 type: builtins.str):
        """
        :param Sequence['GetTableExternalDataConfigurationBigtableOptionColumnFamilyColumnArgs'] columns: A List of columns that should be exposed as individual fields as opposed to a list of (column name, value) pairs. All columns whose qualifier matches a qualifier in this list can be accessed as Other columns can be accessed as a list through column field
        :param builtins.str encoding: The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. This can be overridden for a specific column by listing that column in 'columns' and specifying an encoding for it.
        :param builtins.str family_id: Identifier of the column family.
        :param builtins.bool only_read_latest: If this is set only the latest version of value are exposed for all columns in this column family. This can be overridden for a specific column by listing that column in 'columns' and specifying a different setting for that column.
        :param builtins.str type: The type to convert the value in cells of this column family. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON". Default type is BYTES. This can be overridden for a specific column by listing that column in 'columns' and specifying a type for it.
        """
        pulumi.set(__self__, "columns", columns)
        pulumi.set(__self__, "encoding", encoding)
        pulumi.set(__self__, "family_id", family_id)
        pulumi.set(__self__, "only_read_latest", only_read_latest)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def columns(self) -> Sequence['outputs.GetTableExternalDataConfigurationBigtableOptionColumnFamilyColumnResult']:
        """
        A List of columns that should be exposed as individual fields as opposed to a list of (column name, value) pairs. All columns whose qualifier matches a qualifier in this list can be accessed as Other columns can be accessed as a list through column field
        """
        return pulumi.get(self, "columns")

    @property
    @pulumi.getter
    def encoding(self) -> builtins.str:
        """
        The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. This can be overridden for a specific column by listing that column in 'columns' and specifying an encoding for it.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="familyId")
    def family_id(self) -> builtins.str:
        """
        Identifier of the column family.
        """
        return pulumi.get(self, "family_id")

    @property
    @pulumi.getter(name="onlyReadLatest")
    def only_read_latest(self) -> builtins.bool:
        """
        If this is set only the latest version of value are exposed for all columns in this column family. This can be overridden for a specific column by listing that column in 'columns' and specifying a different setting for that column.
        """
        return pulumi.get(self, "only_read_latest")

    @property
    @pulumi.getter
    def type(self) -> builtins.str:
        """
        The type to convert the value in cells of this column family. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON". Default type is BYTES. This can be overridden for a specific column by listing that column in 'columns' and specifying a type for it.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class GetTableExternalDataConfigurationBigtableOptionColumnFamilyColumnResult(dict):
    def __init__(__self__, *,
                 encoding: builtins.str,
                 field_name: builtins.str,
                 only_read_latest: builtins.bool,
                 qualifier_encoded: builtins.str,
                 qualifier_string: builtins.str,
                 type: builtins.str):
        """
        :param builtins.str encoding: The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. 'encoding' can also be set at the column family level. However, the setting at this level takes precedence if 'encoding' is set at both levels.
        :param builtins.str field_name: If the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as the column field name and is used as field name in queries.
        :param builtins.bool only_read_latest: If this is set, only the latest version of value in this column are exposed. 'onlyReadLatest' can also be set at the column family level. However, the setting at this level takes precedence if 'onlyReadLatest' is set at both levels.
        :param builtins.str qualifier_encoded: Qualifier of the column. Columns in the parent column family that has this exact qualifier are exposed as . field. If the qualifier is valid UTF-8 string, it can be specified in the qualifierString field. Otherwise, a base-64 encoded value must be set to qualifierEncoded. The column field name is the same as the column qualifier. However, if the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as fieldName.
        :param builtins.str qualifier_string: Qualifier string.
        :param builtins.str type: The type to convert the value in cells of this column. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON", Default type is "BYTES". 'type' can also be set at the column family level. However, the setting at this level takes precedence if 'type' is set at both levels.
        """
        pulumi.set(__self__, "encoding", encoding)
        pulumi.set(__self__, "field_name", field_name)
        pulumi.set(__self__, "only_read_latest", only_read_latest)
        pulumi.set(__self__, "qualifier_encoded", qualifier_encoded)
        pulumi.set(__self__, "qualifier_string", qualifier_string)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def encoding(self) -> builtins.str:
        """
        The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. 'encoding' can also be set at the column family level. However, the setting at this level takes precedence if 'encoding' is set at both levels.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="fieldName")
    def field_name(self) -> builtins.str:
        """
        If the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as the column field name and is used as field name in queries.
        """
        return pulumi.get(self, "field_name")

    @property
    @pulumi.getter(name="onlyReadLatest")
    def only_read_latest(self) -> builtins.bool:
        """
        If this is set, only the latest version of value in this column are exposed. 'onlyReadLatest' can also be set at the column family level. However, the setting at this level takes precedence if 'onlyReadLatest' is set at both levels.
        """
        return pulumi.get(self, "only_read_latest")

    @property
    @pulumi.getter(name="qualifierEncoded")
    def qualifier_encoded(self) -> builtins.str:
        """
        Qualifier of the column. Columns in the parent column family that has this exact qualifier are exposed as . field. If the qualifier is valid UTF-8 string, it can be specified in the qualifierString field. Otherwise, a base-64 encoded value must be set to qualifierEncoded. The column field name is the same as the column qualifier. However, if the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as fieldName.
        """
        return pulumi.get(self, "qualifier_encoded")

    @property
    @pulumi.getter(name="qualifierString")
    def qualifier_string(self) -> builtins.str:
        """
        Qualifier string.
        """
        return pulumi.get(self, "qualifier_string")

    @property
    @pulumi.getter
    def type(self) -> builtins.str:
        """
        The type to convert the value in cells of this column. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON", Default type is "BYTES". 'type' can also be set at the column family level. However, the setting at this level takes precedence if 'type' is set at both levels.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class GetTableExternalDataConfigurationCsvOptionResult(dict):
    def __init__(__self__, *,
                 allow_jagged_rows: builtins.bool,
                 allow_quoted_newlines: builtins.bool,
                 encoding: builtins.str,
                 field_delimiter: builtins.str,
                 quote: builtins.str,
                 skip_leading_rows: builtins.int):
        """
        :param builtins.bool allow_jagged_rows: Indicates if BigQuery should accept rows that are missing trailing optional columns.
        :param builtins.bool allow_quoted_newlines: Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file. The default value is false.
        :param builtins.str encoding: The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
        :param builtins.str field_delimiter: The separator for fields in a CSV file.
        :param builtins.int skip_leading_rows: The number of rows at the top of a CSV file that BigQuery will skip when reading the data.
        """
        pulumi.set(__self__, "allow_jagged_rows", allow_jagged_rows)
        pulumi.set(__self__, "allow_quoted_newlines", allow_quoted_newlines)
        pulumi.set(__self__, "encoding", encoding)
        pulumi.set(__self__, "field_delimiter", field_delimiter)
        pulumi.set(__self__, "quote", quote)
        pulumi.set(__self__, "skip_leading_rows", skip_leading_rows)

    @property
    @pulumi.getter(name="allowJaggedRows")
    def allow_jagged_rows(self) -> builtins.bool:
        """
        Indicates if BigQuery should accept rows that are missing trailing optional columns.
        """
        return pulumi.get(self, "allow_jagged_rows")

    @property
    @pulumi.getter(name="allowQuotedNewlines")
    def allow_quoted_newlines(self) -> builtins.bool:
        """
        Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file. The default value is false.
        """
        return pulumi.get(self, "allow_quoted_newlines")

    @property
    @pulumi.getter
    def encoding(self) -> builtins.str:
        """
        The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="fieldDelimiter")
    def field_delimiter(self) -> builtins.str:
        """
        The separator for fields in a CSV file.
        """
        return pulumi.get(self, "field_delimiter")

    @property
    @pulumi.getter
    def quote(self) -> builtins.str:
        return pulumi.get(self, "quote")

    @property
    @pulumi.getter(name="skipLeadingRows")
    def skip_leading_rows(self) -> builtins.int:
        """
        The number of rows at the top of a CSV file that BigQuery will skip when reading the data.
        """
        return pulumi.get(self, "skip_leading_rows")


@pulumi.output_type
class GetTableExternalDataConfigurationGoogleSheetsOptionResult(dict):
    def __init__(__self__, *,
                 range: builtins.str,
                 skip_leading_rows: builtins.int):
        """
        :param builtins.str range: Range of a sheet to query from. Only used when non-empty. At least one of range or skip_leading_rows must be set. Typical format: "sheet_name!top_left_cell_id:bottom_right_cell_id" For example: "sheet1!A1:B20
        :param builtins.int skip_leading_rows: The number of rows at the top of the sheet that BigQuery will skip when reading the data. At least one of range or skip_leading_rows must be set.
        """
        pulumi.set(__self__, "range", range)
        pulumi.set(__self__, "skip_leading_rows", skip_leading_rows)

    @property
    @pulumi.getter
    def range(self) -> builtins.str:
        """
        Range of a sheet to query from. Only used when non-empty. At least one of range or skip_leading_rows must be set. Typical format: "sheet_name!top_left_cell_id:bottom_right_cell_id" For example: "sheet1!A1:B20
        """
        return pulumi.get(self, "range")

    @property
    @pulumi.getter(name="skipLeadingRows")
    def skip_leading_rows(self) -> builtins.int:
        """
        The number of rows at the top of the sheet that BigQuery will skip when reading the data. At least one of range or skip_leading_rows must be set.
        """
        return pulumi.get(self, "skip_leading_rows")


@pulumi.output_type
class GetTableExternalDataConfigurationHivePartitioningOptionResult(dict):
    def __init__(__self__, *,
                 mode: builtins.str,
                 require_partition_filter: builtins.bool,
                 source_uri_prefix: builtins.str):
        """
        :param builtins.str mode: When set, what mode of hive partitioning to use when reading data.
        :param builtins.bool require_partition_filter: If set to true, queries over this table require a partition filter that can be used for partition elimination to be specified.
        :param builtins.str source_uri_prefix: When hive partition detection is requested, a common for all source uris must be required. The prefix must end immediately before the partition key encoding begins.
        """
        pulumi.set(__self__, "mode", mode)
        pulumi.set(__self__, "require_partition_filter", require_partition_filter)
        pulumi.set(__self__, "source_uri_prefix", source_uri_prefix)

    @property
    @pulumi.getter
    def mode(self) -> builtins.str:
        """
        When set, what mode of hive partitioning to use when reading data.
        """
        return pulumi.get(self, "mode")

    @property
    @pulumi.getter(name="requirePartitionFilter")
    def require_partition_filter(self) -> builtins.bool:
        """
        If set to true, queries over this table require a partition filter that can be used for partition elimination to be specified.
        """
        return pulumi.get(self, "require_partition_filter")

    @property
    @pulumi.getter(name="sourceUriPrefix")
    def source_uri_prefix(self) -> builtins.str:
        """
        When hive partition detection is requested, a common for all source uris must be required. The prefix must end immediately before the partition key encoding begins.
        """
        return pulumi.get(self, "source_uri_prefix")


@pulumi.output_type
class GetTableExternalDataConfigurationJsonOptionResult(dict):
    def __init__(__self__, *,
                 encoding: builtins.str):
        """
        :param builtins.str encoding: The character encoding of the data. The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is UTF-8.
        """
        pulumi.set(__self__, "encoding", encoding)

    @property
    @pulumi.getter
    def encoding(self) -> builtins.str:
        """
        The character encoding of the data. The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is UTF-8.
        """
        return pulumi.get(self, "encoding")


@pulumi.output_type
class GetTableExternalDataConfigurationParquetOptionResult(dict):
    def __init__(__self__, *,
                 enable_list_inference: builtins.bool,
                 enum_as_string: builtins.bool):
        """
        :param builtins.bool enable_list_inference: Indicates whether to use schema inference specifically for Parquet LIST logical type.
        :param builtins.bool enum_as_string: Indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
        """
        pulumi.set(__self__, "enable_list_inference", enable_list_inference)
        pulumi.set(__self__, "enum_as_string", enum_as_string)

    @property
    @pulumi.getter(name="enableListInference")
    def enable_list_inference(self) -> builtins.bool:
        """
        Indicates whether to use schema inference specifically for Parquet LIST logical type.
        """
        return pulumi.get(self, "enable_list_inference")

    @property
    @pulumi.getter(name="enumAsString")
    def enum_as_string(self) -> builtins.bool:
        """
        Indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
        """
        return pulumi.get(self, "enum_as_string")


@pulumi.output_type
class GetTableMaterializedViewResult(dict):
    def __init__(__self__, *,
                 allow_non_incremental_definition: builtins.bool,
                 enable_refresh: builtins.bool,
                 query: builtins.str,
                 refresh_interval_ms: builtins.int):
        """
        :param builtins.bool allow_non_incremental_definition: Allow non incremental materialized view definition. The default value is false.
        :param builtins.bool enable_refresh: Specifies if BigQuery should automatically refresh materialized view when the base table is updated. The default is true.
        :param builtins.str query: A query whose result is persisted.
        :param builtins.int refresh_interval_ms: Specifies maximum frequency at which this materialized view will be refreshed. The default is 1800000.
        """
        pulumi.set(__self__, "allow_non_incremental_definition", allow_non_incremental_definition)
        pulumi.set(__self__, "enable_refresh", enable_refresh)
        pulumi.set(__self__, "query", query)
        pulumi.set(__self__, "refresh_interval_ms", refresh_interval_ms)

    @property
    @pulumi.getter(name="allowNonIncrementalDefinition")
    def allow_non_incremental_definition(self) -> builtins.bool:
        """
        Allow non incremental materialized view definition. The default value is false.
        """
        return pulumi.get(self, "allow_non_incremental_definition")

    @property
    @pulumi.getter(name="enableRefresh")
    def enable_refresh(self) -> builtins.bool:
        """
        Specifies if BigQuery should automatically refresh materialized view when the base table is updated. The default is true.
        """
        return pulumi.get(self, "enable_refresh")

    @property
    @pulumi.getter
    def query(self) -> builtins.str:
        """
        A query whose result is persisted.
        """
        return pulumi.get(self, "query")

    @property
    @pulumi.getter(name="refreshIntervalMs")
    def refresh_interval_ms(self) -> builtins.int:
        """
        Specifies maximum frequency at which this materialized view will be refreshed. The default is 1800000.
        """
        return pulumi.get(self, "refresh_interval_ms")


@pulumi.output_type
class GetTableRangePartitioningResult(dict):
    def __init__(__self__, *,
                 field: builtins.str,
                 ranges: Sequence['outputs.GetTableRangePartitioningRangeResult']):
        """
        :param builtins.str field: The field used to determine how to create a range-based partition.
        :param Sequence['GetTableRangePartitioningRangeArgs'] ranges: Information required to partition based on ranges. Structure is documented below.
        """
        pulumi.set(__self__, "field", field)
        pulumi.set(__self__, "ranges", ranges)

    @property
    @pulumi.getter
    def field(self) -> builtins.str:
        """
        The field used to determine how to create a range-based partition.
        """
        return pulumi.get(self, "field")

    @property
    @pulumi.getter
    def ranges(self) -> Sequence['outputs.GetTableRangePartitioningRangeResult']:
        """
        Information required to partition based on ranges. Structure is documented below.
        """
        return pulumi.get(self, "ranges")


@pulumi.output_type
class GetTableRangePartitioningRangeResult(dict):
    def __init__(__self__, *,
                 end: builtins.int,
                 interval: builtins.int,
                 start: builtins.int):
        """
        :param builtins.int end: End of the range partitioning, exclusive.
        :param builtins.int interval: The width of each range within the partition.
        :param builtins.int start: Start of the range partitioning, inclusive.
        """
        pulumi.set(__self__, "end", end)
        pulumi.set(__self__, "interval", interval)
        pulumi.set(__self__, "start", start)

    @property
    @pulumi.getter
    def end(self) -> builtins.int:
        """
        End of the range partitioning, exclusive.
        """
        return pulumi.get(self, "end")

    @property
    @pulumi.getter
    def interval(self) -> builtins.int:
        """
        The width of each range within the partition.
        """
        return pulumi.get(self, "interval")

    @property
    @pulumi.getter
    def start(self) -> builtins.int:
        """
        Start of the range partitioning, inclusive.
        """
        return pulumi.get(self, "start")


@pulumi.output_type
class GetTableSchemaForeignTypeInfoResult(dict):
    def __init__(__self__, *,
                 type_system: builtins.str):
        """
        :param builtins.str type_system: Specifies the system which defines the foreign data type.
        """
        pulumi.set(__self__, "type_system", type_system)

    @property
    @pulumi.getter(name="typeSystem")
    def type_system(self) -> builtins.str:
        """
        Specifies the system which defines the foreign data type.
        """
        return pulumi.get(self, "type_system")


@pulumi.output_type
class GetTableTableConstraintResult(dict):
    def __init__(__self__, *,
                 foreign_keys: Sequence['outputs.GetTableTableConstraintForeignKeyResult'],
                 primary_keys: Sequence['outputs.GetTableTableConstraintPrimaryKeyResult']):
        """
        :param Sequence['GetTableTableConstraintForeignKeyArgs'] foreign_keys: Present only if the table has a foreign key. The foreign key is not enforced.
        :param Sequence['GetTableTableConstraintPrimaryKeyArgs'] primary_keys: Represents a primary key constraint on a table's columns. Present only if the table has a primary key. The primary key is not enforced.
        """
        pulumi.set(__self__, "foreign_keys", foreign_keys)
        pulumi.set(__self__, "primary_keys", primary_keys)

    @property
    @pulumi.getter(name="foreignKeys")
    def foreign_keys(self) -> Sequence['outputs.GetTableTableConstraintForeignKeyResult']:
        """
        Present only if the table has a foreign key. The foreign key is not enforced.
        """
        return pulumi.get(self, "foreign_keys")

    @property
    @pulumi.getter(name="primaryKeys")
    def primary_keys(self) -> Sequence['outputs.GetTableTableConstraintPrimaryKeyResult']:
        """
        Represents a primary key constraint on a table's columns. Present only if the table has a primary key. The primary key is not enforced.
        """
        return pulumi.get(self, "primary_keys")


@pulumi.output_type
class GetTableTableConstraintForeignKeyResult(dict):
    def __init__(__self__, *,
                 column_references: Sequence['outputs.GetTableTableConstraintForeignKeyColumnReferenceResult'],
                 name: builtins.str,
                 referenced_tables: Sequence['outputs.GetTableTableConstraintForeignKeyReferencedTableResult']):
        """
        :param Sequence['GetTableTableConstraintForeignKeyColumnReferenceArgs'] column_references: The pair of the foreign key column and primary key column.
        :param builtins.str name: Set only if the foreign key constraint is named.
        :param Sequence['GetTableTableConstraintForeignKeyReferencedTableArgs'] referenced_tables: The table that holds the primary key and is referenced by this foreign key.
        """
        pulumi.set(__self__, "column_references", column_references)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "referenced_tables", referenced_tables)

    @property
    @pulumi.getter(name="columnReferences")
    def column_references(self) -> Sequence['outputs.GetTableTableConstraintForeignKeyColumnReferenceResult']:
        """
        The pair of the foreign key column and primary key column.
        """
        return pulumi.get(self, "column_references")

    @property
    @pulumi.getter
    def name(self) -> builtins.str:
        """
        Set only if the foreign key constraint is named.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="referencedTables")
    def referenced_tables(self) -> Sequence['outputs.GetTableTableConstraintForeignKeyReferencedTableResult']:
        """
        The table that holds the primary key and is referenced by this foreign key.
        """
        return pulumi.get(self, "referenced_tables")


@pulumi.output_type
class GetTableTableConstraintForeignKeyColumnReferenceResult(dict):
    def __init__(__self__, *,
                 referenced_column: builtins.str,
                 referencing_column: builtins.str):
        """
        :param builtins.str referenced_column: The column in the primary key that are referenced by the referencingColumn.
        :param builtins.str referencing_column: The column that composes the foreign key.
        """
        pulumi.set(__self__, "referenced_column", referenced_column)
        pulumi.set(__self__, "referencing_column", referencing_column)

    @property
    @pulumi.getter(name="referencedColumn")
    def referenced_column(self) -> builtins.str:
        """
        The column in the primary key that are referenced by the referencingColumn.
        """
        return pulumi.get(self, "referenced_column")

    @property
    @pulumi.getter(name="referencingColumn")
    def referencing_column(self) -> builtins.str:
        """
        The column that composes the foreign key.
        """
        return pulumi.get(self, "referencing_column")


@pulumi.output_type
class GetTableTableConstraintForeignKeyReferencedTableResult(dict):
    def __init__(__self__, *,
                 dataset_id: builtins.str,
                 project_id: builtins.str,
                 table_id: builtins.str):
        """
        :param builtins.str dataset_id: The dataset ID.
        :param builtins.str project_id: The ID of the project containing this table.
        :param builtins.str table_id: The table ID.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "project_id", project_id)
        pulumi.set(__self__, "table_id", table_id)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> builtins.str:
        """
        The dataset ID.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="projectId")
    def project_id(self) -> builtins.str:
        """
        The ID of the project containing this table.
        """
        return pulumi.get(self, "project_id")

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The table ID.
        """
        return pulumi.get(self, "table_id")


@pulumi.output_type
class GetTableTableConstraintPrimaryKeyResult(dict):
    def __init__(__self__, *,
                 columns: Sequence[builtins.str]):
        """
        :param Sequence[builtins.str] columns: The columns that are composed of the primary key constraint.
        """
        pulumi.set(__self__, "columns", columns)

    @property
    @pulumi.getter
    def columns(self) -> Sequence[builtins.str]:
        """
        The columns that are composed of the primary key constraint.
        """
        return pulumi.get(self, "columns")


@pulumi.output_type
class GetTableTableReplicationInfoResult(dict):
    def __init__(__self__, *,
                 replication_interval_ms: builtins.int,
                 source_dataset_id: builtins.str,
                 source_project_id: builtins.str,
                 source_table_id: builtins.str):
        """
        :param builtins.int replication_interval_ms: The interval at which the source materialized view is polled for updates. The default is 300000.
        :param builtins.str source_dataset_id: The ID of the source dataset.
        :param builtins.str source_project_id: The ID of the source project.
        :param builtins.str source_table_id: The ID of the source materialized view.
        """
        pulumi.set(__self__, "replication_interval_ms", replication_interval_ms)
        pulumi.set(__self__, "source_dataset_id", source_dataset_id)
        pulumi.set(__self__, "source_project_id", source_project_id)
        pulumi.set(__self__, "source_table_id", source_table_id)

    @property
    @pulumi.getter(name="replicationIntervalMs")
    def replication_interval_ms(self) -> builtins.int:
        """
        The interval at which the source materialized view is polled for updates. The default is 300000.
        """
        return pulumi.get(self, "replication_interval_ms")

    @property
    @pulumi.getter(name="sourceDatasetId")
    def source_dataset_id(self) -> builtins.str:
        """
        The ID of the source dataset.
        """
        return pulumi.get(self, "source_dataset_id")

    @property
    @pulumi.getter(name="sourceProjectId")
    def source_project_id(self) -> builtins.str:
        """
        The ID of the source project.
        """
        return pulumi.get(self, "source_project_id")

    @property
    @pulumi.getter(name="sourceTableId")
    def source_table_id(self) -> builtins.str:
        """
        The ID of the source materialized view.
        """
        return pulumi.get(self, "source_table_id")


@pulumi.output_type
class GetTableTimePartitioningResult(dict):
    def __init__(__self__, *,
                 expiration_ms: builtins.int,
                 field: builtins.str,
                 require_partition_filter: builtins.bool,
                 type: builtins.str):
        """
        :param builtins.int expiration_ms: Number of milliseconds for which to keep the storage for a partition.
        :param builtins.str field: The field used to determine how to create a time-based partition. If time-based partitioning is enabled without this value, the table is partitioned based on the load time.
        :param builtins.bool require_partition_filter: If set to true, queries over this table require a partition filter that can be used for partition elimination to be specified.
        :param builtins.str type: The supported types are DAY, HOUR, MONTH, and YEAR, which will generate one partition per day, hour, month, and year, respectively.
        """
        pulumi.set(__self__, "expiration_ms", expiration_ms)
        pulumi.set(__self__, "field", field)
        pulumi.set(__self__, "require_partition_filter", require_partition_filter)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="expirationMs")
    def expiration_ms(self) -> builtins.int:
        """
        Number of milliseconds for which to keep the storage for a partition.
        """
        return pulumi.get(self, "expiration_ms")

    @property
    @pulumi.getter
    def field(self) -> builtins.str:
        """
        The field used to determine how to create a time-based partition. If time-based partitioning is enabled without this value, the table is partitioned based on the load time.
        """
        return pulumi.get(self, "field")

    @property
    @pulumi.getter(name="requirePartitionFilter")
    def require_partition_filter(self) -> builtins.bool:
        """
        If set to true, queries over this table require a partition filter that can be used for partition elimination to be specified.
        """
        return pulumi.get(self, "require_partition_filter")

    @property
    @pulumi.getter
    def type(self) -> builtins.str:
        """
        The supported types are DAY, HOUR, MONTH, and YEAR, which will generate one partition per day, hour, month, and year, respectively.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class GetTableViewResult(dict):
    def __init__(__self__, *,
                 query: builtins.str,
                 use_legacy_sql: builtins.bool):
        """
        :param builtins.str query: A query that BigQuery executes when the view is referenced.
        :param builtins.bool use_legacy_sql: Specifies whether to use BigQuery's legacy SQL for this view. The default value is true. If set to false, the view will use BigQuery's standard SQL
        """
        pulumi.set(__self__, "query", query)
        pulumi.set(__self__, "use_legacy_sql", use_legacy_sql)

    @property
    @pulumi.getter
    def query(self) -> builtins.str:
        """
        A query that BigQuery executes when the view is referenced.
        """
        return pulumi.get(self, "query")

    @property
    @pulumi.getter(name="useLegacySql")
    def use_legacy_sql(self) -> builtins.bool:
        """
        Specifies whether to use BigQuery's legacy SQL for this view. The default value is true. If set to false, the view will use BigQuery's standard SQL
        """
        return pulumi.get(self, "use_legacy_sql")


@pulumi.output_type
class GetTablesTableResult(dict):
    def __init__(__self__, *,
                 labels: Mapping[str, builtins.str],
                 table_id: builtins.str):
        """
        :param Mapping[str, builtins.str] labels: User-provided table labels, in key/value pairs.
        :param builtins.str table_id: The name of the table.
        """
        pulumi.set(__self__, "labels", labels)
        pulumi.set(__self__, "table_id", table_id)

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, builtins.str]:
        """
        User-provided table labels, in key/value pairs.
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter(name="tableId")
    def table_id(self) -> builtins.str:
        """
        The name of the table.
        """
        return pulumi.get(self, "table_id")



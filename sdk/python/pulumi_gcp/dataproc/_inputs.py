# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from .. import _utilities

__all__ = [
    'AutoscalingPolicyBasicAlgorithmArgs',
    'AutoscalingPolicyBasicAlgorithmArgsDict',
    'AutoscalingPolicyBasicAlgorithmYarnConfigArgs',
    'AutoscalingPolicyBasicAlgorithmYarnConfigArgsDict',
    'AutoscalingPolicyIamBindingConditionArgs',
    'AutoscalingPolicyIamBindingConditionArgsDict',
    'AutoscalingPolicyIamMemberConditionArgs',
    'AutoscalingPolicyIamMemberConditionArgsDict',
    'AutoscalingPolicySecondaryWorkerConfigArgs',
    'AutoscalingPolicySecondaryWorkerConfigArgsDict',
    'AutoscalingPolicyWorkerConfigArgs',
    'AutoscalingPolicyWorkerConfigArgsDict',
    'ClusterClusterConfigArgs',
    'ClusterClusterConfigArgsDict',
    'ClusterClusterConfigAutoscalingConfigArgs',
    'ClusterClusterConfigAutoscalingConfigArgsDict',
    'ClusterClusterConfigAuxiliaryNodeGroupArgs',
    'ClusterClusterConfigAuxiliaryNodeGroupArgsDict',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgs',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgsDict',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgs',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgsDict',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgs',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgsDict',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgs',
    'ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgsDict',
    'ClusterClusterConfigDataprocMetricConfigArgs',
    'ClusterClusterConfigDataprocMetricConfigArgsDict',
    'ClusterClusterConfigDataprocMetricConfigMetricArgs',
    'ClusterClusterConfigDataprocMetricConfigMetricArgsDict',
    'ClusterClusterConfigEncryptionConfigArgs',
    'ClusterClusterConfigEncryptionConfigArgsDict',
    'ClusterClusterConfigEndpointConfigArgs',
    'ClusterClusterConfigEndpointConfigArgsDict',
    'ClusterClusterConfigGceClusterConfigArgs',
    'ClusterClusterConfigGceClusterConfigArgsDict',
    'ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgs',
    'ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgsDict',
    'ClusterClusterConfigGceClusterConfigReservationAffinityArgs',
    'ClusterClusterConfigGceClusterConfigReservationAffinityArgsDict',
    'ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgs',
    'ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict',
    'ClusterClusterConfigInitializationActionArgs',
    'ClusterClusterConfigInitializationActionArgsDict',
    'ClusterClusterConfigLifecycleConfigArgs',
    'ClusterClusterConfigLifecycleConfigArgsDict',
    'ClusterClusterConfigMasterConfigArgs',
    'ClusterClusterConfigMasterConfigArgsDict',
    'ClusterClusterConfigMasterConfigAcceleratorArgs',
    'ClusterClusterConfigMasterConfigAcceleratorArgsDict',
    'ClusterClusterConfigMasterConfigDiskConfigArgs',
    'ClusterClusterConfigMasterConfigDiskConfigArgsDict',
    'ClusterClusterConfigMetastoreConfigArgs',
    'ClusterClusterConfigMetastoreConfigArgsDict',
    'ClusterClusterConfigPreemptibleWorkerConfigArgs',
    'ClusterClusterConfigPreemptibleWorkerConfigArgsDict',
    'ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs',
    'ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgsDict',
    'ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgs',
    'ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgsDict',
    'ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgs',
    'ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgsDict',
    'ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgs',
    'ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgsDict',
    'ClusterClusterConfigSecurityConfigArgs',
    'ClusterClusterConfigSecurityConfigArgsDict',
    'ClusterClusterConfigSecurityConfigKerberosConfigArgs',
    'ClusterClusterConfigSecurityConfigKerberosConfigArgsDict',
    'ClusterClusterConfigSoftwareConfigArgs',
    'ClusterClusterConfigSoftwareConfigArgsDict',
    'ClusterClusterConfigWorkerConfigArgs',
    'ClusterClusterConfigWorkerConfigArgsDict',
    'ClusterClusterConfigWorkerConfigAcceleratorArgs',
    'ClusterClusterConfigWorkerConfigAcceleratorArgsDict',
    'ClusterClusterConfigWorkerConfigDiskConfigArgs',
    'ClusterClusterConfigWorkerConfigDiskConfigArgsDict',
    'ClusterIAMBindingConditionArgs',
    'ClusterIAMBindingConditionArgsDict',
    'ClusterIAMMemberConditionArgs',
    'ClusterIAMMemberConditionArgsDict',
    'ClusterVirtualClusterConfigArgs',
    'ClusterVirtualClusterConfigArgsDict',
    'ClusterVirtualClusterConfigAuxiliaryServicesConfigArgs',
    'ClusterVirtualClusterConfigAuxiliaryServicesConfigArgsDict',
    'ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgs',
    'ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgsDict',
    'ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgs',
    'ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgsDict',
    'ClusterVirtualClusterConfigKubernetesClusterConfigArgs',
    'ClusterVirtualClusterConfigKubernetesClusterConfigArgsDict',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgs',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgsDict',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgs',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgsDict',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgs',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgsDict',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgs',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgsDict',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgs',
    'ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgsDict',
    'ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgs',
    'ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgsDict',
    'JobHadoopConfigArgs',
    'JobHadoopConfigArgsDict',
    'JobHadoopConfigLoggingConfigArgs',
    'JobHadoopConfigLoggingConfigArgsDict',
    'JobHiveConfigArgs',
    'JobHiveConfigArgsDict',
    'JobIAMBindingConditionArgs',
    'JobIAMBindingConditionArgsDict',
    'JobIAMMemberConditionArgs',
    'JobIAMMemberConditionArgsDict',
    'JobPigConfigArgs',
    'JobPigConfigArgsDict',
    'JobPigConfigLoggingConfigArgs',
    'JobPigConfigLoggingConfigArgsDict',
    'JobPlacementArgs',
    'JobPlacementArgsDict',
    'JobPrestoConfigArgs',
    'JobPrestoConfigArgsDict',
    'JobPrestoConfigLoggingConfigArgs',
    'JobPrestoConfigLoggingConfigArgsDict',
    'JobPysparkConfigArgs',
    'JobPysparkConfigArgsDict',
    'JobPysparkConfigLoggingConfigArgs',
    'JobPysparkConfigLoggingConfigArgsDict',
    'JobReferenceArgs',
    'JobReferenceArgsDict',
    'JobSchedulingArgs',
    'JobSchedulingArgsDict',
    'JobSparkConfigArgs',
    'JobSparkConfigArgsDict',
    'JobSparkConfigLoggingConfigArgs',
    'JobSparkConfigLoggingConfigArgsDict',
    'JobSparksqlConfigArgs',
    'JobSparksqlConfigArgsDict',
    'JobSparksqlConfigLoggingConfigArgs',
    'JobSparksqlConfigLoggingConfigArgsDict',
    'JobStatusArgs',
    'JobStatusArgsDict',
    'MetastoreFederationBackendMetastoreArgs',
    'MetastoreFederationBackendMetastoreArgsDict',
    'MetastoreFederationIamBindingConditionArgs',
    'MetastoreFederationIamBindingConditionArgsDict',
    'MetastoreFederationIamMemberConditionArgs',
    'MetastoreFederationIamMemberConditionArgsDict',
    'MetastoreServiceEncryptionConfigArgs',
    'MetastoreServiceEncryptionConfigArgsDict',
    'MetastoreServiceHiveMetastoreConfigArgs',
    'MetastoreServiceHiveMetastoreConfigArgsDict',
    'MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgs',
    'MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgsDict',
    'MetastoreServiceHiveMetastoreConfigKerberosConfigArgs',
    'MetastoreServiceHiveMetastoreConfigKerberosConfigArgsDict',
    'MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgs',
    'MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgsDict',
    'MetastoreServiceIamBindingConditionArgs',
    'MetastoreServiceIamBindingConditionArgsDict',
    'MetastoreServiceIamMemberConditionArgs',
    'MetastoreServiceIamMemberConditionArgsDict',
    'MetastoreServiceMaintenanceWindowArgs',
    'MetastoreServiceMaintenanceWindowArgsDict',
    'MetastoreServiceMetadataIntegrationArgs',
    'MetastoreServiceMetadataIntegrationArgsDict',
    'MetastoreServiceMetadataIntegrationDataCatalogConfigArgs',
    'MetastoreServiceMetadataIntegrationDataCatalogConfigArgsDict',
    'MetastoreServiceNetworkConfigArgs',
    'MetastoreServiceNetworkConfigArgsDict',
    'MetastoreServiceNetworkConfigConsumerArgs',
    'MetastoreServiceNetworkConfigConsumerArgsDict',
    'MetastoreServiceScalingConfigArgs',
    'MetastoreServiceScalingConfigArgsDict',
    'MetastoreServiceScalingConfigAutoscalingConfigArgs',
    'MetastoreServiceScalingConfigAutoscalingConfigArgsDict',
    'MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgs',
    'MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgsDict',
    'MetastoreServiceScheduledBackupArgs',
    'MetastoreServiceScheduledBackupArgsDict',
    'MetastoreServiceTelemetryConfigArgs',
    'MetastoreServiceTelemetryConfigArgsDict',
    'WorkflowTemplateJobArgs',
    'WorkflowTemplateJobArgsDict',
    'WorkflowTemplateJobHadoopJobArgs',
    'WorkflowTemplateJobHadoopJobArgsDict',
    'WorkflowTemplateJobHadoopJobLoggingConfigArgs',
    'WorkflowTemplateJobHadoopJobLoggingConfigArgsDict',
    'WorkflowTemplateJobHiveJobArgs',
    'WorkflowTemplateJobHiveJobArgsDict',
    'WorkflowTemplateJobHiveJobQueryListArgs',
    'WorkflowTemplateJobHiveJobQueryListArgsDict',
    'WorkflowTemplateJobPigJobArgs',
    'WorkflowTemplateJobPigJobArgsDict',
    'WorkflowTemplateJobPigJobLoggingConfigArgs',
    'WorkflowTemplateJobPigJobLoggingConfigArgsDict',
    'WorkflowTemplateJobPigJobQueryListArgs',
    'WorkflowTemplateJobPigJobQueryListArgsDict',
    'WorkflowTemplateJobPrestoJobArgs',
    'WorkflowTemplateJobPrestoJobArgsDict',
    'WorkflowTemplateJobPrestoJobLoggingConfigArgs',
    'WorkflowTemplateJobPrestoJobLoggingConfigArgsDict',
    'WorkflowTemplateJobPrestoJobQueryListArgs',
    'WorkflowTemplateJobPrestoJobQueryListArgsDict',
    'WorkflowTemplateJobPysparkJobArgs',
    'WorkflowTemplateJobPysparkJobArgsDict',
    'WorkflowTemplateJobPysparkJobLoggingConfigArgs',
    'WorkflowTemplateJobPysparkJobLoggingConfigArgsDict',
    'WorkflowTemplateJobSchedulingArgs',
    'WorkflowTemplateJobSchedulingArgsDict',
    'WorkflowTemplateJobSparkJobArgs',
    'WorkflowTemplateJobSparkJobArgsDict',
    'WorkflowTemplateJobSparkJobLoggingConfigArgs',
    'WorkflowTemplateJobSparkJobLoggingConfigArgsDict',
    'WorkflowTemplateJobSparkRJobArgs',
    'WorkflowTemplateJobSparkRJobArgsDict',
    'WorkflowTemplateJobSparkRJobLoggingConfigArgs',
    'WorkflowTemplateJobSparkRJobLoggingConfigArgsDict',
    'WorkflowTemplateJobSparkSqlJobArgs',
    'WorkflowTemplateJobSparkSqlJobArgsDict',
    'WorkflowTemplateJobSparkSqlJobLoggingConfigArgs',
    'WorkflowTemplateJobSparkSqlJobLoggingConfigArgsDict',
    'WorkflowTemplateJobSparkSqlJobQueryListArgs',
    'WorkflowTemplateJobSparkSqlJobQueryListArgsDict',
    'WorkflowTemplateParameterArgs',
    'WorkflowTemplateParameterArgsDict',
    'WorkflowTemplateParameterValidationArgs',
    'WorkflowTemplateParameterValidationArgsDict',
    'WorkflowTemplateParameterValidationRegexArgs',
    'WorkflowTemplateParameterValidationRegexArgsDict',
    'WorkflowTemplateParameterValidationValuesArgs',
    'WorkflowTemplateParameterValidationValuesArgsDict',
    'WorkflowTemplatePlacementArgs',
    'WorkflowTemplatePlacementArgsDict',
    'WorkflowTemplatePlacementClusterSelectorArgs',
    'WorkflowTemplatePlacementClusterSelectorArgsDict',
    'WorkflowTemplatePlacementManagedClusterArgs',
    'WorkflowTemplatePlacementManagedClusterArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgs',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgs',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgs',
    'WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgs',
    'WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgs',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgs',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgs',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgsDict',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgs',
    'WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgsDict',
]

MYPY = False

if not MYPY:
    class AutoscalingPolicyBasicAlgorithmArgsDict(TypedDict):
        yarn_config: pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgsDict']
        """
        YARN autoscaling configuration.
        Structure is documented below.
        """
        cooldown_period: NotRequired[pulumi.Input[str]]
        """
        Duration between scaling events. A scaling period starts after the
        update operation from the previous event has completed.
        Bounds: [2m, 1d]. Default: 2m.
        """
elif False:
    AutoscalingPolicyBasicAlgorithmArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AutoscalingPolicyBasicAlgorithmArgs:
    def __init__(__self__, *,
                 yarn_config: pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgs'],
                 cooldown_period: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgs'] yarn_config: YARN autoscaling configuration.
               Structure is documented below.
        :param pulumi.Input[str] cooldown_period: Duration between scaling events. A scaling period starts after the
               update operation from the previous event has completed.
               Bounds: [2m, 1d]. Default: 2m.
        """
        pulumi.set(__self__, "yarn_config", yarn_config)
        if cooldown_period is not None:
            pulumi.set(__self__, "cooldown_period", cooldown_period)

    @property
    @pulumi.getter(name="yarnConfig")
    def yarn_config(self) -> pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgs']:
        """
        YARN autoscaling configuration.
        Structure is documented below.
        """
        return pulumi.get(self, "yarn_config")

    @yarn_config.setter
    def yarn_config(self, value: pulumi.Input['AutoscalingPolicyBasicAlgorithmYarnConfigArgs']):
        pulumi.set(self, "yarn_config", value)

    @property
    @pulumi.getter(name="cooldownPeriod")
    def cooldown_period(self) -> Optional[pulumi.Input[str]]:
        """
        Duration between scaling events. A scaling period starts after the
        update operation from the previous event has completed.
        Bounds: [2m, 1d]. Default: 2m.
        """
        return pulumi.get(self, "cooldown_period")

    @cooldown_period.setter
    def cooldown_period(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cooldown_period", value)


if not MYPY:
    class AutoscalingPolicyBasicAlgorithmYarnConfigArgsDict(TypedDict):
        graceful_decommission_timeout: pulumi.Input[str]
        """
        Timeout for YARN graceful decommissioning of Node Managers. Specifies the
        duration to wait for jobs to complete before forcefully removing workers
        (and potentially interrupting jobs). Only applicable to downscaling operations.
        Bounds: [0s, 1d].
        """
        scale_down_factor: pulumi.Input[float]
        """
        Fraction of average pending memory in the last cooldown period for which to
        remove workers. A scale-down factor of 1 will result in scaling down so that there
        is no available memory remaining after the update (more aggressive scaling).
        A scale-down factor of 0 disables removing workers, which can be beneficial for
        autoscaling a single job.
        Bounds: [0.0, 1.0].
        """
        scale_up_factor: pulumi.Input[float]
        """
        Fraction of average pending memory in the last cooldown period for which to
        add workers. A scale-up factor of 1.0 will result in scaling up so that there
        is no pending memory remaining after the update (more aggressive scaling).
        A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
        (less aggressive scaling).
        Bounds: [0.0, 1.0].
        """
        scale_down_min_worker_fraction: NotRequired[pulumi.Input[float]]
        """
        Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
        For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
        recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
        means the autoscaler will scale down on any recommended change.
        Bounds: [0.0, 1.0]. Default: 0.0.
        """
        scale_up_min_worker_fraction: NotRequired[pulumi.Input[float]]
        """
        Minimum scale-up threshold as a fraction of total cluster size before scaling
        occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
        must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
        0 means the autoscaler will scale up on any recommended change.
        Bounds: [0.0, 1.0]. Default: 0.0.
        """
elif False:
    AutoscalingPolicyBasicAlgorithmYarnConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AutoscalingPolicyBasicAlgorithmYarnConfigArgs:
    def __init__(__self__, *,
                 graceful_decommission_timeout: pulumi.Input[str],
                 scale_down_factor: pulumi.Input[float],
                 scale_up_factor: pulumi.Input[float],
                 scale_down_min_worker_fraction: Optional[pulumi.Input[float]] = None,
                 scale_up_min_worker_fraction: Optional[pulumi.Input[float]] = None):
        """
        :param pulumi.Input[str] graceful_decommission_timeout: Timeout for YARN graceful decommissioning of Node Managers. Specifies the
               duration to wait for jobs to complete before forcefully removing workers
               (and potentially interrupting jobs). Only applicable to downscaling operations.
               Bounds: [0s, 1d].
        :param pulumi.Input[float] scale_down_factor: Fraction of average pending memory in the last cooldown period for which to
               remove workers. A scale-down factor of 1 will result in scaling down so that there
               is no available memory remaining after the update (more aggressive scaling).
               A scale-down factor of 0 disables removing workers, which can be beneficial for
               autoscaling a single job.
               Bounds: [0.0, 1.0].
        :param pulumi.Input[float] scale_up_factor: Fraction of average pending memory in the last cooldown period for which to
               add workers. A scale-up factor of 1.0 will result in scaling up so that there
               is no pending memory remaining after the update (more aggressive scaling).
               A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
               (less aggressive scaling).
               Bounds: [0.0, 1.0].
        :param pulumi.Input[float] scale_down_min_worker_fraction: Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
               For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
               recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
               means the autoscaler will scale down on any recommended change.
               Bounds: [0.0, 1.0]. Default: 0.0.
        :param pulumi.Input[float] scale_up_min_worker_fraction: Minimum scale-up threshold as a fraction of total cluster size before scaling
               occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
               must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
               0 means the autoscaler will scale up on any recommended change.
               Bounds: [0.0, 1.0]. Default: 0.0.
        """
        pulumi.set(__self__, "graceful_decommission_timeout", graceful_decommission_timeout)
        pulumi.set(__self__, "scale_down_factor", scale_down_factor)
        pulumi.set(__self__, "scale_up_factor", scale_up_factor)
        if scale_down_min_worker_fraction is not None:
            pulumi.set(__self__, "scale_down_min_worker_fraction", scale_down_min_worker_fraction)
        if scale_up_min_worker_fraction is not None:
            pulumi.set(__self__, "scale_up_min_worker_fraction", scale_up_min_worker_fraction)

    @property
    @pulumi.getter(name="gracefulDecommissionTimeout")
    def graceful_decommission_timeout(self) -> pulumi.Input[str]:
        """
        Timeout for YARN graceful decommissioning of Node Managers. Specifies the
        duration to wait for jobs to complete before forcefully removing workers
        (and potentially interrupting jobs). Only applicable to downscaling operations.
        Bounds: [0s, 1d].
        """
        return pulumi.get(self, "graceful_decommission_timeout")

    @graceful_decommission_timeout.setter
    def graceful_decommission_timeout(self, value: pulumi.Input[str]):
        pulumi.set(self, "graceful_decommission_timeout", value)

    @property
    @pulumi.getter(name="scaleDownFactor")
    def scale_down_factor(self) -> pulumi.Input[float]:
        """
        Fraction of average pending memory in the last cooldown period for which to
        remove workers. A scale-down factor of 1 will result in scaling down so that there
        is no available memory remaining after the update (more aggressive scaling).
        A scale-down factor of 0 disables removing workers, which can be beneficial for
        autoscaling a single job.
        Bounds: [0.0, 1.0].
        """
        return pulumi.get(self, "scale_down_factor")

    @scale_down_factor.setter
    def scale_down_factor(self, value: pulumi.Input[float]):
        pulumi.set(self, "scale_down_factor", value)

    @property
    @pulumi.getter(name="scaleUpFactor")
    def scale_up_factor(self) -> pulumi.Input[float]:
        """
        Fraction of average pending memory in the last cooldown period for which to
        add workers. A scale-up factor of 1.0 will result in scaling up so that there
        is no pending memory remaining after the update (more aggressive scaling).
        A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
        (less aggressive scaling).
        Bounds: [0.0, 1.0].
        """
        return pulumi.get(self, "scale_up_factor")

    @scale_up_factor.setter
    def scale_up_factor(self, value: pulumi.Input[float]):
        pulumi.set(self, "scale_up_factor", value)

    @property
    @pulumi.getter(name="scaleDownMinWorkerFraction")
    def scale_down_min_worker_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
        For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
        recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
        means the autoscaler will scale down on any recommended change.
        Bounds: [0.0, 1.0]. Default: 0.0.
        """
        return pulumi.get(self, "scale_down_min_worker_fraction")

    @scale_down_min_worker_fraction.setter
    def scale_down_min_worker_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "scale_down_min_worker_fraction", value)

    @property
    @pulumi.getter(name="scaleUpMinWorkerFraction")
    def scale_up_min_worker_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        Minimum scale-up threshold as a fraction of total cluster size before scaling
        occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
        must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
        0 means the autoscaler will scale up on any recommended change.
        Bounds: [0.0, 1.0]. Default: 0.0.
        """
        return pulumi.get(self, "scale_up_min_worker_fraction")

    @scale_up_min_worker_fraction.setter
    def scale_up_min_worker_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "scale_up_min_worker_fraction", value)


if not MYPY:
    class AutoscalingPolicyIamBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AutoscalingPolicyIamBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AutoscalingPolicyIamBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AutoscalingPolicyIamMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    AutoscalingPolicyIamMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AutoscalingPolicyIamMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class AutoscalingPolicySecondaryWorkerConfigArgsDict(TypedDict):
        max_instances: NotRequired[pulumi.Input[int]]
        """
        Maximum number of instances for this group. Note that by default, clusters will not use
        secondary workers. Required for secondary workers if the minimum secondary instances is set.
        Bounds: [minInstances, ). Defaults to 0.
        """
        min_instances: NotRequired[pulumi.Input[int]]
        """
        Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
        """
        weight: NotRequired[pulumi.Input[int]]
        """
        Weight for the instance group, which is used to determine the fraction of total workers
        in the cluster from this instance group. For example, if primary workers have weight 2,
        and secondary workers have weight 1, the cluster will have approximately 2 primary workers
        for each secondary worker.
        The cluster may not reach the specified balance if constrained by min/max bounds or other
        autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
        primary workers will be added. The cluster can also be out of balance when created.
        If weight is not set on any instance group, the cluster will default to equal weight for
        all groups: the cluster will attempt to maintain an equal number of workers in each group
        within the configured size bounds for each group. If weight is set for one group only,
        the cluster will default to zero weight on the unset group. For example if weight is set
        only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
elif False:
    AutoscalingPolicySecondaryWorkerConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AutoscalingPolicySecondaryWorkerConfigArgs:
    def __init__(__self__, *,
                 max_instances: Optional[pulumi.Input[int]] = None,
                 min_instances: Optional[pulumi.Input[int]] = None,
                 weight: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] max_instances: Maximum number of instances for this group. Note that by default, clusters will not use
               secondary workers. Required for secondary workers if the minimum secondary instances is set.
               Bounds: [minInstances, ). Defaults to 0.
        :param pulumi.Input[int] min_instances: Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
        :param pulumi.Input[int] weight: Weight for the instance group, which is used to determine the fraction of total workers
               in the cluster from this instance group. For example, if primary workers have weight 2,
               and secondary workers have weight 1, the cluster will have approximately 2 primary workers
               for each secondary worker.
               The cluster may not reach the specified balance if constrained by min/max bounds or other
               autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
               primary workers will be added. The cluster can also be out of balance when created.
               If weight is not set on any instance group, the cluster will default to equal weight for
               all groups: the cluster will attempt to maintain an equal number of workers in each group
               within the configured size bounds for each group. If weight is set for one group only,
               the cluster will default to zero weight on the unset group. For example if weight is set
               only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
        if max_instances is not None:
            pulumi.set(__self__, "max_instances", max_instances)
        if min_instances is not None:
            pulumi.set(__self__, "min_instances", min_instances)
        if weight is not None:
            pulumi.set(__self__, "weight", weight)

    @property
    @pulumi.getter(name="maxInstances")
    def max_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum number of instances for this group. Note that by default, clusters will not use
        secondary workers. Required for secondary workers if the minimum secondary instances is set.
        Bounds: [minInstances, ). Defaults to 0.
        """
        return pulumi.get(self, "max_instances")

    @max_instances.setter
    def max_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_instances", value)

    @property
    @pulumi.getter(name="minInstances")
    def min_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
        """
        return pulumi.get(self, "min_instances")

    @min_instances.setter
    def min_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_instances", value)

    @property
    @pulumi.getter
    def weight(self) -> Optional[pulumi.Input[int]]:
        """
        Weight for the instance group, which is used to determine the fraction of total workers
        in the cluster from this instance group. For example, if primary workers have weight 2,
        and secondary workers have weight 1, the cluster will have approximately 2 primary workers
        for each secondary worker.
        The cluster may not reach the specified balance if constrained by min/max bounds or other
        autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
        primary workers will be added. The cluster can also be out of balance when created.
        If weight is not set on any instance group, the cluster will default to equal weight for
        all groups: the cluster will attempt to maintain an equal number of workers in each group
        within the configured size bounds for each group. If weight is set for one group only,
        the cluster will default to zero weight on the unset group. For example if weight is set
        only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
        return pulumi.get(self, "weight")

    @weight.setter
    def weight(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "weight", value)


if not MYPY:
    class AutoscalingPolicyWorkerConfigArgsDict(TypedDict):
        max_instances: pulumi.Input[int]
        """
        Maximum number of instances for this group.
        """
        min_instances: NotRequired[pulumi.Input[int]]
        """
        Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
        """
        weight: NotRequired[pulumi.Input[int]]
        """
        Weight for the instance group, which is used to determine the fraction of total workers
        in the cluster from this instance group. For example, if primary workers have weight 2,
        and secondary workers have weight 1, the cluster will have approximately 2 primary workers
        for each secondary worker.
        The cluster may not reach the specified balance if constrained by min/max bounds or other
        autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
        primary workers will be added. The cluster can also be out of balance when created.
        If weight is not set on any instance group, the cluster will default to equal weight for
        all groups: the cluster will attempt to maintain an equal number of workers in each group
        within the configured size bounds for each group. If weight is set for one group only,
        the cluster will default to zero weight on the unset group. For example if weight is set
        only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
elif False:
    AutoscalingPolicyWorkerConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class AutoscalingPolicyWorkerConfigArgs:
    def __init__(__self__, *,
                 max_instances: pulumi.Input[int],
                 min_instances: Optional[pulumi.Input[int]] = None,
                 weight: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] max_instances: Maximum number of instances for this group.
        :param pulumi.Input[int] min_instances: Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
        :param pulumi.Input[int] weight: Weight for the instance group, which is used to determine the fraction of total workers
               in the cluster from this instance group. For example, if primary workers have weight 2,
               and secondary workers have weight 1, the cluster will have approximately 2 primary workers
               for each secondary worker.
               The cluster may not reach the specified balance if constrained by min/max bounds or other
               autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
               primary workers will be added. The cluster can also be out of balance when created.
               If weight is not set on any instance group, the cluster will default to equal weight for
               all groups: the cluster will attempt to maintain an equal number of workers in each group
               within the configured size bounds for each group. If weight is set for one group only,
               the cluster will default to zero weight on the unset group. For example if weight is set
               only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
        pulumi.set(__self__, "max_instances", max_instances)
        if min_instances is not None:
            pulumi.set(__self__, "min_instances", min_instances)
        if weight is not None:
            pulumi.set(__self__, "weight", weight)

    @property
    @pulumi.getter(name="maxInstances")
    def max_instances(self) -> pulumi.Input[int]:
        """
        Maximum number of instances for this group.
        """
        return pulumi.get(self, "max_instances")

    @max_instances.setter
    def max_instances(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_instances", value)

    @property
    @pulumi.getter(name="minInstances")
    def min_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
        """
        return pulumi.get(self, "min_instances")

    @min_instances.setter
    def min_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_instances", value)

    @property
    @pulumi.getter
    def weight(self) -> Optional[pulumi.Input[int]]:
        """
        Weight for the instance group, which is used to determine the fraction of total workers
        in the cluster from this instance group. For example, if primary workers have weight 2,
        and secondary workers have weight 1, the cluster will have approximately 2 primary workers
        for each secondary worker.
        The cluster may not reach the specified balance if constrained by min/max bounds or other
        autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
        primary workers will be added. The cluster can also be out of balance when created.
        If weight is not set on any instance group, the cluster will default to equal weight for
        all groups: the cluster will attempt to maintain an equal number of workers in each group
        within the configured size bounds for each group. If weight is set for one group only,
        the cluster will default to zero weight on the unset group. For example if weight is set
        only on primary workers, the cluster will use primary workers only and no secondary workers.
        """
        return pulumi.get(self, "weight")

    @weight.setter
    def weight(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "weight", value)


if not MYPY:
    class ClusterClusterConfigArgsDict(TypedDict):
        autoscaling_config: NotRequired[pulumi.Input['ClusterClusterConfigAutoscalingConfigArgsDict']]
        """
        The autoscaling policy config associated with the cluster.
        Note that once set, if `autoscaling_config` is the only field set in `cluster_config`, it can
        only be removed by setting `policy_uri = ""`, rather than removing the whole block.
        Structure defined below.
        """
        auxiliary_node_groups: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupArgsDict']]]]
        """
        A Dataproc NodeGroup resource is a group of Dataproc cluster nodes that execute an assigned role. 
        Structure defined below.
        """
        bucket: NotRequired[pulumi.Input[str]]
        """
        The name of the cloud storage bucket ultimately used to house the staging data
        for the cluster. If `staging_bucket` is specified, it will contain this value, otherwise
        it will be the auto generated name.
        """
        dataproc_metric_config: NotRequired[pulumi.Input['ClusterClusterConfigDataprocMetricConfigArgsDict']]
        """
        The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
        Structure defined below.
        """
        encryption_config: NotRequired[pulumi.Input['ClusterClusterConfigEncryptionConfigArgsDict']]
        """
        The Customer managed encryption keys settings for the cluster.
        Structure defined below.
        """
        endpoint_config: NotRequired[pulumi.Input['ClusterClusterConfigEndpointConfigArgsDict']]
        """
        The config settings for port access on the cluster.
        Structure defined below.
        """
        gce_cluster_config: NotRequired[pulumi.Input['ClusterClusterConfigGceClusterConfigArgsDict']]
        """
        Common config settings for resources of Google Compute Engine cluster
        instances, applicable to all instances in the cluster. Structure defined below.
        """
        initialization_actions: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigInitializationActionArgsDict']]]]
        """
        Commands to execute on each node after config is completed.
        You can specify multiple versions of these. Structure defined below.
        """
        lifecycle_config: NotRequired[pulumi.Input['ClusterClusterConfigLifecycleConfigArgsDict']]
        """
        The settings for auto deletion cluster schedule.
        Structure defined below.
        """
        master_config: NotRequired[pulumi.Input['ClusterClusterConfigMasterConfigArgsDict']]
        """
        The Google Compute Engine config settings for the master instances
        in a cluster. Structure defined below.
        """
        metastore_config: NotRequired[pulumi.Input['ClusterClusterConfigMetastoreConfigArgsDict']]
        """
        The config setting for metastore service with the cluster.
        Structure defined below.
        - - -
        """
        preemptible_worker_config: NotRequired[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgsDict']]
        """
        The Google Compute Engine config settings for the additional
        instances in a cluster. Structure defined below.
        * **NOTE** : `preemptible_worker_config` is
        an alias for the api's [secondaryWorkerConfig](https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig#InstanceGroupConfig). The name doesn't necessarily mean it is preemptible and is named as
        such for legacy/compatibility reasons.
        """
        security_config: NotRequired[pulumi.Input['ClusterClusterConfigSecurityConfigArgsDict']]
        """
        Security related configuration. Structure defined below.
        """
        software_config: NotRequired[pulumi.Input['ClusterClusterConfigSoftwareConfigArgsDict']]
        """
        The config settings for software inside the cluster.
        Structure defined below.
        """
        staging_bucket: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage staging bucket used to stage files,
        such as Hadoop jars, between client machines and the cluster.
        Note: If you don't explicitly specify a `staging_bucket`
        then GCP will auto create / assign one for you. However, you are not guaranteed
        an auto generated bucket which is solely dedicated to your cluster; it may be shared
        with other clusters in the same region/zone also choosing to use the auto generation
        option.
        """
        temp_bucket: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage temp bucket used to store ephemeral cluster
        and jobs data, such as Spark and MapReduce history files.
        Note: If you don't explicitly specify a `temp_bucket` then GCP will auto create / assign one for you.
        """
        worker_config: NotRequired[pulumi.Input['ClusterClusterConfigWorkerConfigArgsDict']]
        """
        The Google Compute Engine config settings for the worker instances
        in a cluster. Structure defined below.
        """
elif False:
    ClusterClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigArgs:
    def __init__(__self__, *,
                 autoscaling_config: Optional[pulumi.Input['ClusterClusterConfigAutoscalingConfigArgs']] = None,
                 auxiliary_node_groups: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupArgs']]]] = None,
                 bucket: Optional[pulumi.Input[str]] = None,
                 dataproc_metric_config: Optional[pulumi.Input['ClusterClusterConfigDataprocMetricConfigArgs']] = None,
                 encryption_config: Optional[pulumi.Input['ClusterClusterConfigEncryptionConfigArgs']] = None,
                 endpoint_config: Optional[pulumi.Input['ClusterClusterConfigEndpointConfigArgs']] = None,
                 gce_cluster_config: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigArgs']] = None,
                 initialization_actions: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigInitializationActionArgs']]]] = None,
                 lifecycle_config: Optional[pulumi.Input['ClusterClusterConfigLifecycleConfigArgs']] = None,
                 master_config: Optional[pulumi.Input['ClusterClusterConfigMasterConfigArgs']] = None,
                 metastore_config: Optional[pulumi.Input['ClusterClusterConfigMetastoreConfigArgs']] = None,
                 preemptible_worker_config: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgs']] = None,
                 security_config: Optional[pulumi.Input['ClusterClusterConfigSecurityConfigArgs']] = None,
                 software_config: Optional[pulumi.Input['ClusterClusterConfigSoftwareConfigArgs']] = None,
                 staging_bucket: Optional[pulumi.Input[str]] = None,
                 temp_bucket: Optional[pulumi.Input[str]] = None,
                 worker_config: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigArgs']] = None):
        """
        :param pulumi.Input['ClusterClusterConfigAutoscalingConfigArgs'] autoscaling_config: The autoscaling policy config associated with the cluster.
               Note that once set, if `autoscaling_config` is the only field set in `cluster_config`, it can
               only be removed by setting `policy_uri = ""`, rather than removing the whole block.
               Structure defined below.
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupArgs']]] auxiliary_node_groups: A Dataproc NodeGroup resource is a group of Dataproc cluster nodes that execute an assigned role. 
               Structure defined below.
        :param pulumi.Input[str] bucket: The name of the cloud storage bucket ultimately used to house the staging data
               for the cluster. If `staging_bucket` is specified, it will contain this value, otherwise
               it will be the auto generated name.
        :param pulumi.Input['ClusterClusterConfigDataprocMetricConfigArgs'] dataproc_metric_config: The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
               Structure defined below.
        :param pulumi.Input['ClusterClusterConfigEncryptionConfigArgs'] encryption_config: The Customer managed encryption keys settings for the cluster.
               Structure defined below.
        :param pulumi.Input['ClusterClusterConfigEndpointConfigArgs'] endpoint_config: The config settings for port access on the cluster.
               Structure defined below.
        :param pulumi.Input['ClusterClusterConfigGceClusterConfigArgs'] gce_cluster_config: Common config settings for resources of Google Compute Engine cluster
               instances, applicable to all instances in the cluster. Structure defined below.
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigInitializationActionArgs']]] initialization_actions: Commands to execute on each node after config is completed.
               You can specify multiple versions of these. Structure defined below.
        :param pulumi.Input['ClusterClusterConfigLifecycleConfigArgs'] lifecycle_config: The settings for auto deletion cluster schedule.
               Structure defined below.
        :param pulumi.Input['ClusterClusterConfigMasterConfigArgs'] master_config: The Google Compute Engine config settings for the master instances
               in a cluster. Structure defined below.
        :param pulumi.Input['ClusterClusterConfigMetastoreConfigArgs'] metastore_config: The config setting for metastore service with the cluster.
               Structure defined below.
               - - -
        :param pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgs'] preemptible_worker_config: The Google Compute Engine config settings for the additional
               instances in a cluster. Structure defined below.
               * **NOTE** : `preemptible_worker_config` is
               an alias for the api's [secondaryWorkerConfig](https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig#InstanceGroupConfig). The name doesn't necessarily mean it is preemptible and is named as
               such for legacy/compatibility reasons.
        :param pulumi.Input['ClusterClusterConfigSecurityConfigArgs'] security_config: Security related configuration. Structure defined below.
        :param pulumi.Input['ClusterClusterConfigSoftwareConfigArgs'] software_config: The config settings for software inside the cluster.
               Structure defined below.
        :param pulumi.Input[str] staging_bucket: The Cloud Storage staging bucket used to stage files,
               such as Hadoop jars, between client machines and the cluster.
               Note: If you don't explicitly specify a `staging_bucket`
               then GCP will auto create / assign one for you. However, you are not guaranteed
               an auto generated bucket which is solely dedicated to your cluster; it may be shared
               with other clusters in the same region/zone also choosing to use the auto generation
               option.
        :param pulumi.Input[str] temp_bucket: The Cloud Storage temp bucket used to store ephemeral cluster
               and jobs data, such as Spark and MapReduce history files.
               Note: If you don't explicitly specify a `temp_bucket` then GCP will auto create / assign one for you.
        :param pulumi.Input['ClusterClusterConfigWorkerConfigArgs'] worker_config: The Google Compute Engine config settings for the worker instances
               in a cluster. Structure defined below.
        """
        if autoscaling_config is not None:
            pulumi.set(__self__, "autoscaling_config", autoscaling_config)
        if auxiliary_node_groups is not None:
            pulumi.set(__self__, "auxiliary_node_groups", auxiliary_node_groups)
        if bucket is not None:
            pulumi.set(__self__, "bucket", bucket)
        if dataproc_metric_config is not None:
            pulumi.set(__self__, "dataproc_metric_config", dataproc_metric_config)
        if encryption_config is not None:
            pulumi.set(__self__, "encryption_config", encryption_config)
        if endpoint_config is not None:
            pulumi.set(__self__, "endpoint_config", endpoint_config)
        if gce_cluster_config is not None:
            pulumi.set(__self__, "gce_cluster_config", gce_cluster_config)
        if initialization_actions is not None:
            pulumi.set(__self__, "initialization_actions", initialization_actions)
        if lifecycle_config is not None:
            pulumi.set(__self__, "lifecycle_config", lifecycle_config)
        if master_config is not None:
            pulumi.set(__self__, "master_config", master_config)
        if metastore_config is not None:
            pulumi.set(__self__, "metastore_config", metastore_config)
        if preemptible_worker_config is not None:
            pulumi.set(__self__, "preemptible_worker_config", preemptible_worker_config)
        if security_config is not None:
            pulumi.set(__self__, "security_config", security_config)
        if software_config is not None:
            pulumi.set(__self__, "software_config", software_config)
        if staging_bucket is not None:
            pulumi.set(__self__, "staging_bucket", staging_bucket)
        if temp_bucket is not None:
            pulumi.set(__self__, "temp_bucket", temp_bucket)
        if worker_config is not None:
            pulumi.set(__self__, "worker_config", worker_config)

    @property
    @pulumi.getter(name="autoscalingConfig")
    def autoscaling_config(self) -> Optional[pulumi.Input['ClusterClusterConfigAutoscalingConfigArgs']]:
        """
        The autoscaling policy config associated with the cluster.
        Note that once set, if `autoscaling_config` is the only field set in `cluster_config`, it can
        only be removed by setting `policy_uri = ""`, rather than removing the whole block.
        Structure defined below.
        """
        return pulumi.get(self, "autoscaling_config")

    @autoscaling_config.setter
    def autoscaling_config(self, value: Optional[pulumi.Input['ClusterClusterConfigAutoscalingConfigArgs']]):
        pulumi.set(self, "autoscaling_config", value)

    @property
    @pulumi.getter(name="auxiliaryNodeGroups")
    def auxiliary_node_groups(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupArgs']]]]:
        """
        A Dataproc NodeGroup resource is a group of Dataproc cluster nodes that execute an assigned role. 
        Structure defined below.
        """
        return pulumi.get(self, "auxiliary_node_groups")

    @auxiliary_node_groups.setter
    def auxiliary_node_groups(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupArgs']]]]):
        pulumi.set(self, "auxiliary_node_groups", value)

    @property
    @pulumi.getter
    def bucket(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the cloud storage bucket ultimately used to house the staging data
        for the cluster. If `staging_bucket` is specified, it will contain this value, otherwise
        it will be the auto generated name.
        """
        return pulumi.get(self, "bucket")

    @bucket.setter
    def bucket(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "bucket", value)

    @property
    @pulumi.getter(name="dataprocMetricConfig")
    def dataproc_metric_config(self) -> Optional[pulumi.Input['ClusterClusterConfigDataprocMetricConfigArgs']]:
        """
        The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
        Structure defined below.
        """
        return pulumi.get(self, "dataproc_metric_config")

    @dataproc_metric_config.setter
    def dataproc_metric_config(self, value: Optional[pulumi.Input['ClusterClusterConfigDataprocMetricConfigArgs']]):
        pulumi.set(self, "dataproc_metric_config", value)

    @property
    @pulumi.getter(name="encryptionConfig")
    def encryption_config(self) -> Optional[pulumi.Input['ClusterClusterConfigEncryptionConfigArgs']]:
        """
        The Customer managed encryption keys settings for the cluster.
        Structure defined below.
        """
        return pulumi.get(self, "encryption_config")

    @encryption_config.setter
    def encryption_config(self, value: Optional[pulumi.Input['ClusterClusterConfigEncryptionConfigArgs']]):
        pulumi.set(self, "encryption_config", value)

    @property
    @pulumi.getter(name="endpointConfig")
    def endpoint_config(self) -> Optional[pulumi.Input['ClusterClusterConfigEndpointConfigArgs']]:
        """
        The config settings for port access on the cluster.
        Structure defined below.
        """
        return pulumi.get(self, "endpoint_config")

    @endpoint_config.setter
    def endpoint_config(self, value: Optional[pulumi.Input['ClusterClusterConfigEndpointConfigArgs']]):
        pulumi.set(self, "endpoint_config", value)

    @property
    @pulumi.getter(name="gceClusterConfig")
    def gce_cluster_config(self) -> Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigArgs']]:
        """
        Common config settings for resources of Google Compute Engine cluster
        instances, applicable to all instances in the cluster. Structure defined below.
        """
        return pulumi.get(self, "gce_cluster_config")

    @gce_cluster_config.setter
    def gce_cluster_config(self, value: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigArgs']]):
        pulumi.set(self, "gce_cluster_config", value)

    @property
    @pulumi.getter(name="initializationActions")
    def initialization_actions(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigInitializationActionArgs']]]]:
        """
        Commands to execute on each node after config is completed.
        You can specify multiple versions of these. Structure defined below.
        """
        return pulumi.get(self, "initialization_actions")

    @initialization_actions.setter
    def initialization_actions(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigInitializationActionArgs']]]]):
        pulumi.set(self, "initialization_actions", value)

    @property
    @pulumi.getter(name="lifecycleConfig")
    def lifecycle_config(self) -> Optional[pulumi.Input['ClusterClusterConfigLifecycleConfigArgs']]:
        """
        The settings for auto deletion cluster schedule.
        Structure defined below.
        """
        return pulumi.get(self, "lifecycle_config")

    @lifecycle_config.setter
    def lifecycle_config(self, value: Optional[pulumi.Input['ClusterClusterConfigLifecycleConfigArgs']]):
        pulumi.set(self, "lifecycle_config", value)

    @property
    @pulumi.getter(name="masterConfig")
    def master_config(self) -> Optional[pulumi.Input['ClusterClusterConfigMasterConfigArgs']]:
        """
        The Google Compute Engine config settings for the master instances
        in a cluster. Structure defined below.
        """
        return pulumi.get(self, "master_config")

    @master_config.setter
    def master_config(self, value: Optional[pulumi.Input['ClusterClusterConfigMasterConfigArgs']]):
        pulumi.set(self, "master_config", value)

    @property
    @pulumi.getter(name="metastoreConfig")
    def metastore_config(self) -> Optional[pulumi.Input['ClusterClusterConfigMetastoreConfigArgs']]:
        """
        The config setting for metastore service with the cluster.
        Structure defined below.
        - - -
        """
        return pulumi.get(self, "metastore_config")

    @metastore_config.setter
    def metastore_config(self, value: Optional[pulumi.Input['ClusterClusterConfigMetastoreConfigArgs']]):
        pulumi.set(self, "metastore_config", value)

    @property
    @pulumi.getter(name="preemptibleWorkerConfig")
    def preemptible_worker_config(self) -> Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgs']]:
        """
        The Google Compute Engine config settings for the additional
        instances in a cluster. Structure defined below.
        * **NOTE** : `preemptible_worker_config` is
        an alias for the api's [secondaryWorkerConfig](https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig#InstanceGroupConfig). The name doesn't necessarily mean it is preemptible and is named as
        such for legacy/compatibility reasons.
        """
        return pulumi.get(self, "preemptible_worker_config")

    @preemptible_worker_config.setter
    def preemptible_worker_config(self, value: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigArgs']]):
        pulumi.set(self, "preemptible_worker_config", value)

    @property
    @pulumi.getter(name="securityConfig")
    def security_config(self) -> Optional[pulumi.Input['ClusterClusterConfigSecurityConfigArgs']]:
        """
        Security related configuration. Structure defined below.
        """
        return pulumi.get(self, "security_config")

    @security_config.setter
    def security_config(self, value: Optional[pulumi.Input['ClusterClusterConfigSecurityConfigArgs']]):
        pulumi.set(self, "security_config", value)

    @property
    @pulumi.getter(name="softwareConfig")
    def software_config(self) -> Optional[pulumi.Input['ClusterClusterConfigSoftwareConfigArgs']]:
        """
        The config settings for software inside the cluster.
        Structure defined below.
        """
        return pulumi.get(self, "software_config")

    @software_config.setter
    def software_config(self, value: Optional[pulumi.Input['ClusterClusterConfigSoftwareConfigArgs']]):
        pulumi.set(self, "software_config", value)

    @property
    @pulumi.getter(name="stagingBucket")
    def staging_bucket(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage staging bucket used to stage files,
        such as Hadoop jars, between client machines and the cluster.
        Note: If you don't explicitly specify a `staging_bucket`
        then GCP will auto create / assign one for you. However, you are not guaranteed
        an auto generated bucket which is solely dedicated to your cluster; it may be shared
        with other clusters in the same region/zone also choosing to use the auto generation
        option.
        """
        return pulumi.get(self, "staging_bucket")

    @staging_bucket.setter
    def staging_bucket(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "staging_bucket", value)

    @property
    @pulumi.getter(name="tempBucket")
    def temp_bucket(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage temp bucket used to store ephemeral cluster
        and jobs data, such as Spark and MapReduce history files.
        Note: If you don't explicitly specify a `temp_bucket` then GCP will auto create / assign one for you.
        """
        return pulumi.get(self, "temp_bucket")

    @temp_bucket.setter
    def temp_bucket(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "temp_bucket", value)

    @property
    @pulumi.getter(name="workerConfig")
    def worker_config(self) -> Optional[pulumi.Input['ClusterClusterConfigWorkerConfigArgs']]:
        """
        The Google Compute Engine config settings for the worker instances
        in a cluster. Structure defined below.
        """
        return pulumi.get(self, "worker_config")

    @worker_config.setter
    def worker_config(self, value: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigArgs']]):
        pulumi.set(self, "worker_config", value)


if not MYPY:
    class ClusterClusterConfigAutoscalingConfigArgsDict(TypedDict):
        policy_uri: pulumi.Input[str]
        """
        The autoscaling policy used by the cluster.

        Only resource names including projectid and location (region) are valid. Examples:

        `https://www.googleapis.com/compute/v1/projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
        `projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
        Note that the policy must be in the same project and Cloud Dataproc region.

        - - -
        """
elif False:
    ClusterClusterConfigAutoscalingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigAutoscalingConfigArgs:
    def __init__(__self__, *,
                 policy_uri: pulumi.Input[str]):
        """
        :param pulumi.Input[str] policy_uri: The autoscaling policy used by the cluster.
               
               Only resource names including projectid and location (region) are valid. Examples:
               
               `https://www.googleapis.com/compute/v1/projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
               `projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
               Note that the policy must be in the same project and Cloud Dataproc region.
               
               - - -
        """
        pulumi.set(__self__, "policy_uri", policy_uri)

    @property
    @pulumi.getter(name="policyUri")
    def policy_uri(self) -> pulumi.Input[str]:
        """
        The autoscaling policy used by the cluster.

        Only resource names including projectid and location (region) are valid. Examples:

        `https://www.googleapis.com/compute/v1/projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
        `projects/[projectId]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
        Note that the policy must be in the same project and Cloud Dataproc region.

        - - -
        """
        return pulumi.get(self, "policy_uri")

    @policy_uri.setter
    def policy_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "policy_uri", value)


if not MYPY:
    class ClusterClusterConfigAuxiliaryNodeGroupArgsDict(TypedDict):
        node_groups: pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgsDict']]]
        """
        Node group configuration.
        """
        node_group_id: NotRequired[pulumi.Input[str]]
        """
        A node group ID. Generated if not specified. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
        """
elif False:
    ClusterClusterConfigAuxiliaryNodeGroupArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigAuxiliaryNodeGroupArgs:
    def __init__(__self__, *,
                 node_groups: pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgs']]],
                 node_group_id: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgs']]] node_groups: Node group configuration.
        :param pulumi.Input[str] node_group_id: A node group ID. Generated if not specified. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
        """
        pulumi.set(__self__, "node_groups", node_groups)
        if node_group_id is not None:
            pulumi.set(__self__, "node_group_id", node_group_id)

    @property
    @pulumi.getter(name="nodeGroups")
    def node_groups(self) -> pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgs']]]:
        """
        Node group configuration.
        """
        return pulumi.get(self, "node_groups")

    @node_groups.setter
    def node_groups(self, value: pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgs']]]):
        pulumi.set(self, "node_groups", value)

    @property
    @pulumi.getter(name="nodeGroupId")
    def node_group_id(self) -> Optional[pulumi.Input[str]]:
        """
        A node group ID. Generated if not specified. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
        """
        return pulumi.get(self, "node_group_id")

    @node_group_id.setter
    def node_group_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "node_group_id", value)


if not MYPY:
    class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgsDict(TypedDict):
        roles: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Node group roles. 
        One of `"DRIVER"`.
        """
        name: NotRequired[pulumi.Input[str]]
        """
        The Node group resource name.
        """
        node_group_config: NotRequired[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgsDict']]
        """
        The node group instance group configuration.
        """
elif False:
    ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupArgs:
    def __init__(__self__, *,
                 roles: pulumi.Input[Sequence[pulumi.Input[str]]],
                 name: Optional[pulumi.Input[str]] = None,
                 node_group_config: Optional[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgs']] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] roles: Node group roles. 
               One of `"DRIVER"`.
        :param pulumi.Input[str] name: The Node group resource name.
        :param pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgs'] node_group_config: The node group instance group configuration.
        """
        pulumi.set(__self__, "roles", roles)
        if name is not None:
            pulumi.set(__self__, "name", name)
        if node_group_config is not None:
            pulumi.set(__self__, "node_group_config", node_group_config)

    @property
    @pulumi.getter
    def roles(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Node group roles. 
        One of `"DRIVER"`.
        """
        return pulumi.get(self, "roles")

    @roles.setter
    def roles(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "roles", value)

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        The Node group resource name.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter(name="nodeGroupConfig")
    def node_group_config(self) -> Optional[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgs']]:
        """
        The node group instance group configuration.
        """
        return pulumi.get(self, "node_group_config")

    @node_group_config.setter
    def node_group_config(self, value: Optional[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgs']]):
        pulumi.set(self, "node_group_config", value)


if not MYPY:
    class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgsDict(TypedDict):
        accelerators: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgsDict']]]]
        """
        The Compute Engine accelerator (GPU) configuration for these instances. Can be specified 
        multiple times.
        """
        disk_config: NotRequired[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgsDict']]
        """
        Disk Config
        """
        instance_names: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        List of auxiliary node group instance names which have been assigned to the cluster.
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        The name of a Google Compute Engine machine type
        to create for the node group. If not specified, GCP will default to a predetermined
        computed value (currently `n1-standard-4`).
        """
        min_cpu_platform: NotRequired[pulumi.Input[str]]
        """
        The name of a minimum generation of CPU family
        for the node group. If not specified, GCP will default to a predetermined computed value
        for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for details about which CPU families are available (and defaulted) for each zone.
        """
        num_instances: NotRequired[pulumi.Input[int]]
        """
        Specifies the number of master nodes to create.
        Please set a number greater than 0. Node Group must have at least 1 instance.
        """
elif False:
    ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigArgs:
    def __init__(__self__, *,
                 accelerators: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgs']]]] = None,
                 disk_config: Optional[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgs']] = None,
                 instance_names: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 num_instances: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgs']]] accelerators: The Compute Engine accelerator (GPU) configuration for these instances. Can be specified 
               multiple times.
        :param pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgs'] disk_config: Disk Config
        :param pulumi.Input[Sequence[pulumi.Input[str]]] instance_names: List of auxiliary node group instance names which have been assigned to the cluster.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type
               to create for the node group. If not specified, GCP will default to a predetermined
               computed value (currently `n1-standard-4`).
        :param pulumi.Input[str] min_cpu_platform: The name of a minimum generation of CPU family
               for the node group. If not specified, GCP will default to a predetermined computed value
               for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for details about which CPU families are available (and defaulted) for each zone.
        :param pulumi.Input[int] num_instances: Specifies the number of master nodes to create.
               Please set a number greater than 0. Node Group must have at least 1 instance.
        """
        if accelerators is not None:
            pulumi.set(__self__, "accelerators", accelerators)
        if disk_config is not None:
            pulumi.set(__self__, "disk_config", disk_config)
        if instance_names is not None:
            pulumi.set(__self__, "instance_names", instance_names)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if num_instances is not None:
            pulumi.set(__self__, "num_instances", num_instances)

    @property
    @pulumi.getter
    def accelerators(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgs']]]]:
        """
        The Compute Engine accelerator (GPU) configuration for these instances. Can be specified 
        multiple times.
        """
        return pulumi.get(self, "accelerators")

    @accelerators.setter
    def accelerators(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgs']]]]):
        pulumi.set(self, "accelerators", value)

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> Optional[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgs']]:
        """
        Disk Config
        """
        return pulumi.get(self, "disk_config")

    @disk_config.setter
    def disk_config(self, value: Optional[pulumi.Input['ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgs']]):
        pulumi.set(self, "disk_config", value)

    @property
    @pulumi.getter(name="instanceNames")
    def instance_names(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        List of auxiliary node group instance names which have been assigned to the cluster.
        """
        return pulumi.get(self, "instance_names")

    @instance_names.setter
    def instance_names(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "instance_names", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a Google Compute Engine machine type
        to create for the node group. If not specified, GCP will default to a predetermined
        computed value (currently `n1-standard-4`).
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a minimum generation of CPU family
        for the node group. If not specified, GCP will default to a predetermined computed value
        for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for details about which CPU families are available (and defaulted) for each zone.
        """
        return pulumi.get(self, "min_cpu_platform")

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_cpu_platform", value)

    @property
    @pulumi.getter(name="numInstances")
    def num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Specifies the number of master nodes to create.
        Please set a number greater than 0. Node Group must have at least 1 instance.
        """
        return pulumi.get(self, "num_instances")

    @num_instances.setter
    def num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_instances", value)


if not MYPY:
    class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgsDict(TypedDict):
        accelerator_count: pulumi.Input[int]
        """
        The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.


        - - -
        """
        accelerator_type: pulumi.Input[str]
        """
        The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
elif False:
    ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAcceleratorArgs:
    def __init__(__self__, *,
                 accelerator_count: pulumi.Input[int],
                 accelerator_type: pulumi.Input[str]):
        """
        :param pulumi.Input[int] accelerator_count: The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
               
               
               - - -
        :param pulumi.Input[str] accelerator_type: The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        pulumi.set(__self__, "accelerator_count", accelerator_count)
        pulumi.set(__self__, "accelerator_type", accelerator_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> pulumi.Input[int]:
        """
        The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.


        - - -
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> pulumi.Input[str]:
        """
        The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: pulumi.Input[str]):
        pulumi.set(self, "accelerator_type", value)


if not MYPY:
    class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgsDict(TypedDict):
        boot_disk_size_gb: NotRequired[pulumi.Input[int]]
        """
        Size of the primary disk attached to each node, specified
        in GB. The primary disk contains the boot volume and system libraries, and the
        smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        boot_disk_type: NotRequired[pulumi.Input[str]]
        """
        The disk type of the primary disk attached to each node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        local_ssd_interface: NotRequired[pulumi.Input[str]]
        """
        Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        """
        num_local_ssds: NotRequired[pulumi.Input[int]]
        """
        The amount of local SSD disks that will be attached to each master cluster node. 
        Defaults to 0.
        """
elif False:
    ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfigArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None,
                 local_ssd_interface: Optional[pulumi.Input[str]] = None,
                 num_local_ssds: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] boot_disk_size_gb: Size of the primary disk attached to each node, specified
               in GB. The primary disk contains the boot volume and system libraries, and the
               smallest allowed disk size is 10GB. GCP will default to a predetermined
               computed value if not set (currently 500GB). Note: If SSDs are not
               attached, it also contains the HDFS data blocks and Hadoop working directories.
        :param pulumi.Input[str] boot_disk_type: The disk type of the primary disk attached to each node.
               One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        :param pulumi.Input[str] local_ssd_interface: Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        :param pulumi.Input[int] num_local_ssds: The amount of local SSD disks that will be attached to each master cluster node. 
               Defaults to 0.
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)
        if local_ssd_interface is not None:
            pulumi.set(__self__, "local_ssd_interface", local_ssd_interface)
        if num_local_ssds is not None:
            pulumi.set(__self__, "num_local_ssds", num_local_ssds)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size of the primary disk attached to each node, specified
        in GB. The primary disk contains the boot volume and system libraries, and the
        smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        The disk type of the primary disk attached to each node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)

    @property
    @pulumi.getter(name="localSsdInterface")
    def local_ssd_interface(self) -> Optional[pulumi.Input[str]]:
        """
        Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        """
        return pulumi.get(self, "local_ssd_interface")

    @local_ssd_interface.setter
    def local_ssd_interface(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "local_ssd_interface", value)

    @property
    @pulumi.getter(name="numLocalSsds")
    def num_local_ssds(self) -> Optional[pulumi.Input[int]]:
        """
        The amount of local SSD disks that will be attached to each master cluster node. 
        Defaults to 0.
        """
        return pulumi.get(self, "num_local_ssds")

    @num_local_ssds.setter
    def num_local_ssds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_local_ssds", value)


if not MYPY:
    class ClusterClusterConfigDataprocMetricConfigArgsDict(TypedDict):
        metrics: pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigDataprocMetricConfigMetricArgsDict']]]
        """
        Metrics sources to enable.
        """
elif False:
    ClusterClusterConfigDataprocMetricConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigDataprocMetricConfigArgs:
    def __init__(__self__, *,
                 metrics: pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigDataprocMetricConfigMetricArgs']]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigDataprocMetricConfigMetricArgs']]] metrics: Metrics sources to enable.
        """
        pulumi.set(__self__, "metrics", metrics)

    @property
    @pulumi.getter
    def metrics(self) -> pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigDataprocMetricConfigMetricArgs']]]:
        """
        Metrics sources to enable.
        """
        return pulumi.get(self, "metrics")

    @metrics.setter
    def metrics(self, value: pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigDataprocMetricConfigMetricArgs']]]):
        pulumi.set(self, "metrics", value)


if not MYPY:
    class ClusterClusterConfigDataprocMetricConfigMetricArgsDict(TypedDict):
        metric_source: pulumi.Input[str]
        """
        A source for the collection of Dataproc OSS metrics (see [available OSS metrics](https://cloud.google.com//dataproc/docs/guides/monitoring#available_oss_metrics)).
        """
        metric_overrides: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.

        - - -
        """
elif False:
    ClusterClusterConfigDataprocMetricConfigMetricArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigDataprocMetricConfigMetricArgs:
    def __init__(__self__, *,
                 metric_source: pulumi.Input[str],
                 metric_overrides: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] metric_source: A source for the collection of Dataproc OSS metrics (see [available OSS metrics](https://cloud.google.com//dataproc/docs/guides/monitoring#available_oss_metrics)).
        :param pulumi.Input[Sequence[pulumi.Input[str]]] metric_overrides: One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
               
               - - -
        """
        pulumi.set(__self__, "metric_source", metric_source)
        if metric_overrides is not None:
            pulumi.set(__self__, "metric_overrides", metric_overrides)

    @property
    @pulumi.getter(name="metricSource")
    def metric_source(self) -> pulumi.Input[str]:
        """
        A source for the collection of Dataproc OSS metrics (see [available OSS metrics](https://cloud.google.com//dataproc/docs/guides/monitoring#available_oss_metrics)).
        """
        return pulumi.get(self, "metric_source")

    @metric_source.setter
    def metric_source(self, value: pulumi.Input[str]):
        pulumi.set(self, "metric_source", value)

    @property
    @pulumi.getter(name="metricOverrides")
    def metric_overrides(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.

        - - -
        """
        return pulumi.get(self, "metric_overrides")

    @metric_overrides.setter
    def metric_overrides(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "metric_overrides", value)


if not MYPY:
    class ClusterClusterConfigEncryptionConfigArgsDict(TypedDict):
        kms_key_name: pulumi.Input[str]
        """
        The Cloud KMS key name to use for PD disk encryption for
        all instances in the cluster.

        - - -
        """
elif False:
    ClusterClusterConfigEncryptionConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigEncryptionConfigArgs:
    def __init__(__self__, *,
                 kms_key_name: pulumi.Input[str]):
        """
        :param pulumi.Input[str] kms_key_name: The Cloud KMS key name to use for PD disk encryption for
               all instances in the cluster.
               
               - - -
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> pulumi.Input[str]:
        """
        The Cloud KMS key name to use for PD disk encryption for
        all instances in the cluster.

        - - -
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "kms_key_name", value)


if not MYPY:
    class ClusterClusterConfigEndpointConfigArgsDict(TypedDict):
        enable_http_port_access: pulumi.Input[bool]
        """
        The flag to enable http access to specific ports
        on the cluster from external sources (aka Component Gateway). Defaults to false.
        """
        http_ports: NotRequired[pulumi.Input[Mapping[str, Any]]]
        """
        The map of port descriptions to URLs. Will only be populated if
        `enable_http_port_access` is true.
        """
elif False:
    ClusterClusterConfigEndpointConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigEndpointConfigArgs:
    def __init__(__self__, *,
                 enable_http_port_access: pulumi.Input[bool],
                 http_ports: Optional[pulumi.Input[Mapping[str, Any]]] = None):
        """
        :param pulumi.Input[bool] enable_http_port_access: The flag to enable http access to specific ports
               on the cluster from external sources (aka Component Gateway). Defaults to false.
        :param pulumi.Input[Mapping[str, Any]] http_ports: The map of port descriptions to URLs. Will only be populated if
               `enable_http_port_access` is true.
        """
        pulumi.set(__self__, "enable_http_port_access", enable_http_port_access)
        if http_ports is not None:
            pulumi.set(__self__, "http_ports", http_ports)

    @property
    @pulumi.getter(name="enableHttpPortAccess")
    def enable_http_port_access(self) -> pulumi.Input[bool]:
        """
        The flag to enable http access to specific ports
        on the cluster from external sources (aka Component Gateway). Defaults to false.
        """
        return pulumi.get(self, "enable_http_port_access")

    @enable_http_port_access.setter
    def enable_http_port_access(self, value: pulumi.Input[bool]):
        pulumi.set(self, "enable_http_port_access", value)

    @property
    @pulumi.getter(name="httpPorts")
    def http_ports(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        The map of port descriptions to URLs. Will only be populated if
        `enable_http_port_access` is true.
        """
        return pulumi.get(self, "http_ports")

    @http_ports.setter
    def http_ports(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "http_ports", value)


if not MYPY:
    class ClusterClusterConfigGceClusterConfigArgsDict(TypedDict):
        internal_ip_only: NotRequired[pulumi.Input[bool]]
        """
        By default, clusters are not restricted to internal IP addresses,
        and will have ephemeral external IP addresses assigned to each instance. If set to true, all
        instances in the cluster will only have internal IP addresses. Note: Private Google Access
        (also known as `privateIpGoogleAccess`) must be enabled on the subnetwork that the cluster
        will be launched in.
        """
        metadata: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A map of the Compute Engine metadata entries to add to all instances
        (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        """
        network: NotRequired[pulumi.Input[str]]
        """
        The name or self_link of the Google Compute Engine
        network to the cluster will be part of. Conflicts with `subnetwork`.
        If neither is specified, this defaults to the "default" network.
        """
        node_group_affinity: NotRequired[pulumi.Input['ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgsDict']]
        """
        Node Group Affinity for sole-tenant clusters.
        """
        reservation_affinity: NotRequired[pulumi.Input['ClusterClusterConfigGceClusterConfigReservationAffinityArgsDict']]
        """
        Reservation Affinity for consuming zonal reservation.
        """
        service_account: NotRequired[pulumi.Input[str]]
        """
        The service account to be used by the Node VMs.
        If not specified, the "default" service account is used.
        """
        service_account_scopes: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The set of Google API scopes
        to be made available on all of the node VMs under the `service_account`
        specified. Both OAuth2 URLs and gcloud
        short names are supported. To allow full access to all Cloud APIs, use the
        `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
        """
        shielded_instance_config: NotRequired[pulumi.Input['ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict']]
        """
        Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).

        - - -
        """
        subnetwork: NotRequired[pulumi.Input[str]]
        """
        The name or self_link of the Google Compute Engine
        subnetwork the cluster will be part of. Conflicts with `network`.
        """
        tags: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The list of instance tags applied to instances in the cluster.
        Tags are used to identify valid sources or targets for network firewalls.
        """
        zone: NotRequired[pulumi.Input[str]]
        """
        The GCP zone where your data is stored and used (i.e. where
        the master and the worker nodes will be created in). If `region` is set to 'global' (default)
        then `zone` is mandatory, otherwise GCP is able to make use of [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/auto-zone)
        to determine this automatically for you.
        Note: This setting additionally determines and restricts
        which computing resources are available for use with other configs such as
        `cluster_config.master_config.machine_type` and `cluster_config.worker_config.machine_type`.
        """
elif False:
    ClusterClusterConfigGceClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigGceClusterConfigArgs:
    def __init__(__self__, *,
                 internal_ip_only: Optional[pulumi.Input[bool]] = None,
                 metadata: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 node_group_affinity: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgs']] = None,
                 reservation_affinity: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigReservationAffinityArgs']] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 service_account_scopes: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 shielded_instance_config: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgs']] = None,
                 subnetwork: Optional[pulumi.Input[str]] = None,
                 tags: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 zone: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[bool] internal_ip_only: By default, clusters are not restricted to internal IP addresses,
               and will have ephemeral external IP addresses assigned to each instance. If set to true, all
               instances in the cluster will only have internal IP addresses. Note: Private Google Access
               (also known as `privateIpGoogleAccess`) must be enabled on the subnetwork that the cluster
               will be launched in.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] metadata: A map of the Compute Engine metadata entries to add to all instances
               (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        :param pulumi.Input[str] network: The name or self_link of the Google Compute Engine
               network to the cluster will be part of. Conflicts with `subnetwork`.
               If neither is specified, this defaults to the "default" network.
        :param pulumi.Input['ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgs'] node_group_affinity: Node Group Affinity for sole-tenant clusters.
        :param pulumi.Input['ClusterClusterConfigGceClusterConfigReservationAffinityArgs'] reservation_affinity: Reservation Affinity for consuming zonal reservation.
        :param pulumi.Input[str] service_account: The service account to be used by the Node VMs.
               If not specified, the "default" service account is used.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] service_account_scopes: The set of Google API scopes
               to be made available on all of the node VMs under the `service_account`
               specified. Both OAuth2 URLs and gcloud
               short names are supported. To allow full access to all Cloud APIs, use the
               `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
        :param pulumi.Input['ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgs'] shielded_instance_config: Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).
               
               - - -
        :param pulumi.Input[str] subnetwork: The name or self_link of the Google Compute Engine
               subnetwork the cluster will be part of. Conflicts with `network`.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] tags: The list of instance tags applied to instances in the cluster.
               Tags are used to identify valid sources or targets for network firewalls.
        :param pulumi.Input[str] zone: The GCP zone where your data is stored and used (i.e. where
               the master and the worker nodes will be created in). If `region` is set to 'global' (default)
               then `zone` is mandatory, otherwise GCP is able to make use of [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/auto-zone)
               to determine this automatically for you.
               Note: This setting additionally determines and restricts
               which computing resources are available for use with other configs such as
               `cluster_config.master_config.machine_type` and `cluster_config.worker_config.machine_type`.
        """
        if internal_ip_only is not None:
            pulumi.set(__self__, "internal_ip_only", internal_ip_only)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if network is not None:
            pulumi.set(__self__, "network", network)
        if node_group_affinity is not None:
            pulumi.set(__self__, "node_group_affinity", node_group_affinity)
        if reservation_affinity is not None:
            pulumi.set(__self__, "reservation_affinity", reservation_affinity)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if service_account_scopes is not None:
            pulumi.set(__self__, "service_account_scopes", service_account_scopes)
        if shielded_instance_config is not None:
            pulumi.set(__self__, "shielded_instance_config", shielded_instance_config)
        if subnetwork is not None:
            pulumi.set(__self__, "subnetwork", subnetwork)
        if tags is not None:
            pulumi.set(__self__, "tags", tags)
        if zone is not None:
            pulumi.set(__self__, "zone", zone)

    @property
    @pulumi.getter(name="internalIpOnly")
    def internal_ip_only(self) -> Optional[pulumi.Input[bool]]:
        """
        By default, clusters are not restricted to internal IP addresses,
        and will have ephemeral external IP addresses assigned to each instance. If set to true, all
        instances in the cluster will only have internal IP addresses. Note: Private Google Access
        (also known as `privateIpGoogleAccess`) must be enabled on the subnetwork that the cluster
        will be launched in.
        """
        return pulumi.get(self, "internal_ip_only")

    @internal_ip_only.setter
    def internal_ip_only(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "internal_ip_only", value)

    @property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A map of the Compute Engine metadata entries to add to all instances
        (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        """
        return pulumi.get(self, "metadata")

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "metadata", value)

    @property
    @pulumi.getter
    def network(self) -> Optional[pulumi.Input[str]]:
        """
        The name or self_link of the Google Compute Engine
        network to the cluster will be part of. Conflicts with `subnetwork`.
        If neither is specified, this defaults to the "default" network.
        """
        return pulumi.get(self, "network")

    @network.setter
    def network(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "network", value)

    @property
    @pulumi.getter(name="nodeGroupAffinity")
    def node_group_affinity(self) -> Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgs']]:
        """
        Node Group Affinity for sole-tenant clusters.
        """
        return pulumi.get(self, "node_group_affinity")

    @node_group_affinity.setter
    def node_group_affinity(self, value: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgs']]):
        pulumi.set(self, "node_group_affinity", value)

    @property
    @pulumi.getter(name="reservationAffinity")
    def reservation_affinity(self) -> Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigReservationAffinityArgs']]:
        """
        Reservation Affinity for consuming zonal reservation.
        """
        return pulumi.get(self, "reservation_affinity")

    @reservation_affinity.setter
    def reservation_affinity(self, value: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigReservationAffinityArgs']]):
        pulumi.set(self, "reservation_affinity", value)

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        The service account to be used by the Node VMs.
        If not specified, the "default" service account is used.
        """
        return pulumi.get(self, "service_account")

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_account", value)

    @property
    @pulumi.getter(name="serviceAccountScopes")
    def service_account_scopes(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The set of Google API scopes
        to be made available on all of the node VMs under the `service_account`
        specified. Both OAuth2 URLs and gcloud
        short names are supported. To allow full access to all Cloud APIs, use the
        `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
        """
        return pulumi.get(self, "service_account_scopes")

    @service_account_scopes.setter
    def service_account_scopes(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "service_account_scopes", value)

    @property
    @pulumi.getter(name="shieldedInstanceConfig")
    def shielded_instance_config(self) -> Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgs']]:
        """
        Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).

        - - -
        """
        return pulumi.get(self, "shielded_instance_config")

    @shielded_instance_config.setter
    def shielded_instance_config(self, value: Optional[pulumi.Input['ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgs']]):
        pulumi.set(self, "shielded_instance_config", value)

    @property
    @pulumi.getter
    def subnetwork(self) -> Optional[pulumi.Input[str]]:
        """
        The name or self_link of the Google Compute Engine
        subnetwork the cluster will be part of. Conflicts with `network`.
        """
        return pulumi.get(self, "subnetwork")

    @subnetwork.setter
    def subnetwork(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "subnetwork", value)

    @property
    @pulumi.getter
    def tags(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The list of instance tags applied to instances in the cluster.
        Tags are used to identify valid sources or targets for network firewalls.
        """
        return pulumi.get(self, "tags")

    @tags.setter
    def tags(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "tags", value)

    @property
    @pulumi.getter
    def zone(self) -> Optional[pulumi.Input[str]]:
        """
        The GCP zone where your data is stored and used (i.e. where
        the master and the worker nodes will be created in). If `region` is set to 'global' (default)
        then `zone` is mandatory, otherwise GCP is able to make use of [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/auto-zone)
        to determine this automatically for you.
        Note: This setting additionally determines and restricts
        which computing resources are available for use with other configs such as
        `cluster_config.master_config.machine_type` and `cluster_config.worker_config.machine_type`.
        """
        return pulumi.get(self, "zone")

    @zone.setter
    def zone(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "zone", value)


if not MYPY:
    class ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgsDict(TypedDict):
        node_group_uri: pulumi.Input[str]
        """
        The URI of a sole-tenant node group resource that the cluster will be created on.
        """
elif False:
    ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigGceClusterConfigNodeGroupAffinityArgs:
    def __init__(__self__, *,
                 node_group_uri: pulumi.Input[str]):
        """
        :param pulumi.Input[str] node_group_uri: The URI of a sole-tenant node group resource that the cluster will be created on.
        """
        pulumi.set(__self__, "node_group_uri", node_group_uri)

    @property
    @pulumi.getter(name="nodeGroupUri")
    def node_group_uri(self) -> pulumi.Input[str]:
        """
        The URI of a sole-tenant node group resource that the cluster will be created on.
        """
        return pulumi.get(self, "node_group_uri")

    @node_group_uri.setter
    def node_group_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "node_group_uri", value)


if not MYPY:
    class ClusterClusterConfigGceClusterConfigReservationAffinityArgsDict(TypedDict):
        consume_reservation_type: NotRequired[pulumi.Input[str]]
        """
        Corresponds to the type of reservation consumption.
        """
        key: NotRequired[pulumi.Input[str]]
        """
        Corresponds to the label key of reservation resource.
        """
        values: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Corresponds to the label values of reservation resource.
        """
elif False:
    ClusterClusterConfigGceClusterConfigReservationAffinityArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigGceClusterConfigReservationAffinityArgs:
    def __init__(__self__, *,
                 consume_reservation_type: Optional[pulumi.Input[str]] = None,
                 key: Optional[pulumi.Input[str]] = None,
                 values: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] consume_reservation_type: Corresponds to the type of reservation consumption.
        :param pulumi.Input[str] key: Corresponds to the label key of reservation resource.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] values: Corresponds to the label values of reservation resource.
        """
        if consume_reservation_type is not None:
            pulumi.set(__self__, "consume_reservation_type", consume_reservation_type)
        if key is not None:
            pulumi.set(__self__, "key", key)
        if values is not None:
            pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter(name="consumeReservationType")
    def consume_reservation_type(self) -> Optional[pulumi.Input[str]]:
        """
        Corresponds to the type of reservation consumption.
        """
        return pulumi.get(self, "consume_reservation_type")

    @consume_reservation_type.setter
    def consume_reservation_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "consume_reservation_type", value)

    @property
    @pulumi.getter
    def key(self) -> Optional[pulumi.Input[str]]:
        """
        Corresponds to the label key of reservation resource.
        """
        return pulumi.get(self, "key")

    @key.setter
    def key(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "key", value)

    @property
    @pulumi.getter
    def values(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Corresponds to the label values of reservation resource.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "values", value)


if not MYPY:
    class ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict(TypedDict):
        enable_integrity_monitoring: NotRequired[pulumi.Input[bool]]
        """
        Defines whether instances have integrity monitoring enabled.

        - - -
        """
        enable_secure_boot: NotRequired[pulumi.Input[bool]]
        """
        Defines whether instances have Secure Boot enabled.
        """
        enable_vtpm: NotRequired[pulumi.Input[bool]]
        """
        Defines whether instances have the [vTPM](https://cloud.google.com/security/shielded-cloud/shielded-vm#vtpm) enabled.
        """
elif False:
    ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigGceClusterConfigShieldedInstanceConfigArgs:
    def __init__(__self__, *,
                 enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None,
                 enable_secure_boot: Optional[pulumi.Input[bool]] = None,
                 enable_vtpm: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[bool] enable_integrity_monitoring: Defines whether instances have integrity monitoring enabled.
               
               - - -
        :param pulumi.Input[bool] enable_secure_boot: Defines whether instances have Secure Boot enabled.
        :param pulumi.Input[bool] enable_vtpm: Defines whether instances have the [vTPM](https://cloud.google.com/security/shielded-cloud/shielded-vm#vtpm) enabled.
        """
        if enable_integrity_monitoring is not None:
            pulumi.set(__self__, "enable_integrity_monitoring", enable_integrity_monitoring)
        if enable_secure_boot is not None:
            pulumi.set(__self__, "enable_secure_boot", enable_secure_boot)
        if enable_vtpm is not None:
            pulumi.set(__self__, "enable_vtpm", enable_vtpm)

    @property
    @pulumi.getter(name="enableIntegrityMonitoring")
    def enable_integrity_monitoring(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether instances have integrity monitoring enabled.

        - - -
        """
        return pulumi.get(self, "enable_integrity_monitoring")

    @enable_integrity_monitoring.setter
    def enable_integrity_monitoring(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_integrity_monitoring", value)

    @property
    @pulumi.getter(name="enableSecureBoot")
    def enable_secure_boot(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether instances have Secure Boot enabled.
        """
        return pulumi.get(self, "enable_secure_boot")

    @enable_secure_boot.setter
    def enable_secure_boot(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_secure_boot", value)

    @property
    @pulumi.getter(name="enableVtpm")
    def enable_vtpm(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether instances have the [vTPM](https://cloud.google.com/security/shielded-cloud/shielded-vm#vtpm) enabled.
        """
        return pulumi.get(self, "enable_vtpm")

    @enable_vtpm.setter
    def enable_vtpm(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_vtpm", value)


if not MYPY:
    class ClusterClusterConfigInitializationActionArgsDict(TypedDict):
        script: pulumi.Input[str]
        """
        The script to be executed during initialization of the cluster.
        The script must be a GCS file with a gs:// prefix.
        """
        timeout_sec: NotRequired[pulumi.Input[int]]
        """
        The maximum duration (in seconds) which `script` is
        allowed to take to execute its action. GCP will default to a predetermined
        computed value if not set (currently 300).

        - - -
        """
elif False:
    ClusterClusterConfigInitializationActionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigInitializationActionArgs:
    def __init__(__self__, *,
                 script: pulumi.Input[str],
                 timeout_sec: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[str] script: The script to be executed during initialization of the cluster.
               The script must be a GCS file with a gs:// prefix.
        :param pulumi.Input[int] timeout_sec: The maximum duration (in seconds) which `script` is
               allowed to take to execute its action. GCP will default to a predetermined
               computed value if not set (currently 300).
               
               - - -
        """
        pulumi.set(__self__, "script", script)
        if timeout_sec is not None:
            pulumi.set(__self__, "timeout_sec", timeout_sec)

    @property
    @pulumi.getter
    def script(self) -> pulumi.Input[str]:
        """
        The script to be executed during initialization of the cluster.
        The script must be a GCS file with a gs:// prefix.
        """
        return pulumi.get(self, "script")

    @script.setter
    def script(self, value: pulumi.Input[str]):
        pulumi.set(self, "script", value)

    @property
    @pulumi.getter(name="timeoutSec")
    def timeout_sec(self) -> Optional[pulumi.Input[int]]:
        """
        The maximum duration (in seconds) which `script` is
        allowed to take to execute its action. GCP will default to a predetermined
        computed value if not set (currently 300).

        - - -
        """
        return pulumi.get(self, "timeout_sec")

    @timeout_sec.setter
    def timeout_sec(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "timeout_sec", value)


if not MYPY:
    class ClusterClusterConfigLifecycleConfigArgsDict(TypedDict):
        auto_delete_time: NotRequired[pulumi.Input[str]]
        """
        The time when cluster will be auto-deleted.
        A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
        Example: "2014-10-02T15:01:23.045123456Z".

        - - -
        """
        idle_delete_ttl: NotRequired[pulumi.Input[str]]
        """
        The duration to keep the cluster alive while idling
        (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
        """
        idle_start_time: NotRequired[pulumi.Input[str]]
        """
        Time when the cluster became idle
        (most recent job finished) and became eligible for deletion due to idleness.
        """
elif False:
    ClusterClusterConfigLifecycleConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigLifecycleConfigArgs:
    def __init__(__self__, *,
                 auto_delete_time: Optional[pulumi.Input[str]] = None,
                 idle_delete_ttl: Optional[pulumi.Input[str]] = None,
                 idle_start_time: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] auto_delete_time: The time when cluster will be auto-deleted.
               A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
               Example: "2014-10-02T15:01:23.045123456Z".
               
               - - -
        :param pulumi.Input[str] idle_delete_ttl: The duration to keep the cluster alive while idling
               (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
        :param pulumi.Input[str] idle_start_time: Time when the cluster became idle
               (most recent job finished) and became eligible for deletion due to idleness.
        """
        if auto_delete_time is not None:
            pulumi.set(__self__, "auto_delete_time", auto_delete_time)
        if idle_delete_ttl is not None:
            pulumi.set(__self__, "idle_delete_ttl", idle_delete_ttl)
        if idle_start_time is not None:
            pulumi.set(__self__, "idle_start_time", idle_start_time)

    @property
    @pulumi.getter(name="autoDeleteTime")
    def auto_delete_time(self) -> Optional[pulumi.Input[str]]:
        """
        The time when cluster will be auto-deleted.
        A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
        Example: "2014-10-02T15:01:23.045123456Z".

        - - -
        """
        return pulumi.get(self, "auto_delete_time")

    @auto_delete_time.setter
    def auto_delete_time(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "auto_delete_time", value)

    @property
    @pulumi.getter(name="idleDeleteTtl")
    def idle_delete_ttl(self) -> Optional[pulumi.Input[str]]:
        """
        The duration to keep the cluster alive while idling
        (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
        """
        return pulumi.get(self, "idle_delete_ttl")

    @idle_delete_ttl.setter
    def idle_delete_ttl(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "idle_delete_ttl", value)

    @property
    @pulumi.getter(name="idleStartTime")
    def idle_start_time(self) -> Optional[pulumi.Input[str]]:
        """
        Time when the cluster became idle
        (most recent job finished) and became eligible for deletion due to idleness.
        """
        return pulumi.get(self, "idle_start_time")

    @idle_start_time.setter
    def idle_start_time(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "idle_start_time", value)


if not MYPY:
    class ClusterClusterConfigMasterConfigArgsDict(TypedDict):
        accelerators: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgsDict']]]]
        """
        The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
        """
        disk_config: NotRequired[pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgsDict']]
        """
        Disk Config
        """
        image_uri: NotRequired[pulumi.Input[str]]
        """
        The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
        for more information.
        """
        instance_names: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        List of master instance names which
        have been assigned to the cluster.
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        The name of a Google Compute Engine machine type
        to create for the master. If not specified, GCP will default to a predetermined
        computed value (currently `n1-standard-4`).
        """
        min_cpu_platform: NotRequired[pulumi.Input[str]]
        """
        The name of a minimum generation of CPU family
        for the master. If not specified, GCP will default to a predetermined computed value
        for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for details about which CPU families are available (and defaulted) for each zone.
        """
        num_instances: NotRequired[pulumi.Input[int]]
        """
        Specifies the number of master nodes to create.
        If not specified, GCP will default to a predetermined computed value (currently 1).
        """
elif False:
    ClusterClusterConfigMasterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigMasterConfigArgs:
    def __init__(__self__, *,
                 accelerators: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgs']]]] = None,
                 disk_config: Optional[pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgs']] = None,
                 image_uri: Optional[pulumi.Input[str]] = None,
                 instance_names: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 num_instances: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgs']]] accelerators: The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
        :param pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgs'] disk_config: Disk Config
        :param pulumi.Input[str] image_uri: The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
               for more information.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] instance_names: List of master instance names which
               have been assigned to the cluster.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type
               to create for the master. If not specified, GCP will default to a predetermined
               computed value (currently `n1-standard-4`).
        :param pulumi.Input[str] min_cpu_platform: The name of a minimum generation of CPU family
               for the master. If not specified, GCP will default to a predetermined computed value
               for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for details about which CPU families are available (and defaulted) for each zone.
        :param pulumi.Input[int] num_instances: Specifies the number of master nodes to create.
               If not specified, GCP will default to a predetermined computed value (currently 1).
        """
        if accelerators is not None:
            pulumi.set(__self__, "accelerators", accelerators)
        if disk_config is not None:
            pulumi.set(__self__, "disk_config", disk_config)
        if image_uri is not None:
            pulumi.set(__self__, "image_uri", image_uri)
        if instance_names is not None:
            pulumi.set(__self__, "instance_names", instance_names)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if num_instances is not None:
            pulumi.set(__self__, "num_instances", num_instances)

    @property
    @pulumi.getter
    def accelerators(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgs']]]]:
        """
        The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
        """
        return pulumi.get(self, "accelerators")

    @accelerators.setter
    def accelerators(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigMasterConfigAcceleratorArgs']]]]):
        pulumi.set(self, "accelerators", value)

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> Optional[pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgs']]:
        """
        Disk Config
        """
        return pulumi.get(self, "disk_config")

    @disk_config.setter
    def disk_config(self, value: Optional[pulumi.Input['ClusterClusterConfigMasterConfigDiskConfigArgs']]):
        pulumi.set(self, "disk_config", value)

    @property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
        for more information.
        """
        return pulumi.get(self, "image_uri")

    @image_uri.setter
    def image_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "image_uri", value)

    @property
    @pulumi.getter(name="instanceNames")
    def instance_names(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        List of master instance names which
        have been assigned to the cluster.
        """
        return pulumi.get(self, "instance_names")

    @instance_names.setter
    def instance_names(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "instance_names", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a Google Compute Engine machine type
        to create for the master. If not specified, GCP will default to a predetermined
        computed value (currently `n1-standard-4`).
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a minimum generation of CPU family
        for the master. If not specified, GCP will default to a predetermined computed value
        for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for details about which CPU families are available (and defaulted) for each zone.
        """
        return pulumi.get(self, "min_cpu_platform")

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_cpu_platform", value)

    @property
    @pulumi.getter(name="numInstances")
    def num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Specifies the number of master nodes to create.
        If not specified, GCP will default to a predetermined computed value (currently 1).
        """
        return pulumi.get(self, "num_instances")

    @num_instances.setter
    def num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_instances", value)


if not MYPY:
    class ClusterClusterConfigMasterConfigAcceleratorArgsDict(TypedDict):
        accelerator_count: pulumi.Input[int]
        """
        The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.

        > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
        zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
        if you are trying to use accelerators in a given zone.

        - - -
        """
        accelerator_type: pulumi.Input[str]
        """
        The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
elif False:
    ClusterClusterConfigMasterConfigAcceleratorArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigMasterConfigAcceleratorArgs:
    def __init__(__self__, *,
                 accelerator_count: pulumi.Input[int],
                 accelerator_type: pulumi.Input[str]):
        """
        :param pulumi.Input[int] accelerator_count: The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
               
               > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
               zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
               if you are trying to use accelerators in a given zone.
               
               - - -
        :param pulumi.Input[str] accelerator_type: The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        pulumi.set(__self__, "accelerator_count", accelerator_count)
        pulumi.set(__self__, "accelerator_type", accelerator_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> pulumi.Input[int]:
        """
        The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.

        > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
        zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
        if you are trying to use accelerators in a given zone.

        - - -
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> pulumi.Input[str]:
        """
        The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: pulumi.Input[str]):
        pulumi.set(self, "accelerator_type", value)


if not MYPY:
    class ClusterClusterConfigMasterConfigDiskConfigArgsDict(TypedDict):
        boot_disk_size_gb: NotRequired[pulumi.Input[int]]
        """
        Size of the primary disk attached to each node, specified
        in GB. The primary disk contains the boot volume and system libraries, and the
        smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        boot_disk_type: NotRequired[pulumi.Input[str]]
        """
        The disk type of the primary disk attached to each node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        local_ssd_interface: NotRequired[pulumi.Input[str]]
        """
        Optional. Interface type of local SSDs (default is "scsi").
        Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
        Memory Express). See
        [local SSD performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).
        """
        num_local_ssds: NotRequired[pulumi.Input[int]]
        """
        The amount of local SSD disks that will be
        attached to each master cluster node. Defaults to 0.
        """
elif False:
    ClusterClusterConfigMasterConfigDiskConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigMasterConfigDiskConfigArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None,
                 local_ssd_interface: Optional[pulumi.Input[str]] = None,
                 num_local_ssds: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] boot_disk_size_gb: Size of the primary disk attached to each node, specified
               in GB. The primary disk contains the boot volume and system libraries, and the
               smallest allowed disk size is 10GB. GCP will default to a predetermined
               computed value if not set (currently 500GB). Note: If SSDs are not
               attached, it also contains the HDFS data blocks and Hadoop working directories.
        :param pulumi.Input[str] boot_disk_type: The disk type of the primary disk attached to each node.
               One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        :param pulumi.Input[str] local_ssd_interface: Optional. Interface type of local SSDs (default is "scsi").
               Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
               Memory Express). See
               [local SSD performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).
        :param pulumi.Input[int] num_local_ssds: The amount of local SSD disks that will be
               attached to each master cluster node. Defaults to 0.
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)
        if local_ssd_interface is not None:
            pulumi.set(__self__, "local_ssd_interface", local_ssd_interface)
        if num_local_ssds is not None:
            pulumi.set(__self__, "num_local_ssds", num_local_ssds)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size of the primary disk attached to each node, specified
        in GB. The primary disk contains the boot volume and system libraries, and the
        smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        The disk type of the primary disk attached to each node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)

    @property
    @pulumi.getter(name="localSsdInterface")
    def local_ssd_interface(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. Interface type of local SSDs (default is "scsi").
        Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
        Memory Express). See
        [local SSD performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).
        """
        return pulumi.get(self, "local_ssd_interface")

    @local_ssd_interface.setter
    def local_ssd_interface(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "local_ssd_interface", value)

    @property
    @pulumi.getter(name="numLocalSsds")
    def num_local_ssds(self) -> Optional[pulumi.Input[int]]:
        """
        The amount of local SSD disks that will be
        attached to each master cluster node. Defaults to 0.
        """
        return pulumi.get(self, "num_local_ssds")

    @num_local_ssds.setter
    def num_local_ssds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_local_ssds", value)


if not MYPY:
    class ClusterClusterConfigMetastoreConfigArgsDict(TypedDict):
        dataproc_metastore_service: pulumi.Input[str]
        """
        Resource name of an existing Dataproc Metastore service.

        Only resource names including projectid and location (region) are valid. Examples:

        `projects/[projectId]/locations/[dataproc_region]/services/[service-name]`
        """
elif False:
    ClusterClusterConfigMetastoreConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigMetastoreConfigArgs:
    def __init__(__self__, *,
                 dataproc_metastore_service: pulumi.Input[str]):
        """
        :param pulumi.Input[str] dataproc_metastore_service: Resource name of an existing Dataproc Metastore service.
               
               Only resource names including projectid and location (region) are valid. Examples:
               
               `projects/[projectId]/locations/[dataproc_region]/services/[service-name]`
        """
        pulumi.set(__self__, "dataproc_metastore_service", dataproc_metastore_service)

    @property
    @pulumi.getter(name="dataprocMetastoreService")
    def dataproc_metastore_service(self) -> pulumi.Input[str]:
        """
        Resource name of an existing Dataproc Metastore service.

        Only resource names including projectid and location (region) are valid. Examples:

        `projects/[projectId]/locations/[dataproc_region]/services/[service-name]`
        """
        return pulumi.get(self, "dataproc_metastore_service")

    @dataproc_metastore_service.setter
    def dataproc_metastore_service(self, value: pulumi.Input[str]):
        pulumi.set(self, "dataproc_metastore_service", value)


if not MYPY:
    class ClusterClusterConfigPreemptibleWorkerConfigArgsDict(TypedDict):
        disk_config: NotRequired[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgsDict']]
        """
        Disk Config
        """
        instance_flexibility_policy: NotRequired[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgsDict']]
        """
        Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
        """
        instance_names: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        List of preemptible instance names which have been assigned
        to the cluster.
        """
        num_instances: NotRequired[pulumi.Input[int]]
        """
        Specifies the number of preemptible nodes to create.
        Defaults to 0.
        """
        preemptibility: NotRequired[pulumi.Input[str]]
        """
        Specifies the preemptibility of the secondary workers. The default value is `PREEMPTIBLE`
        Accepted values are:
        * PREEMPTIBILITY_UNSPECIFIED
        * NON_PREEMPTIBLE
        * PREEMPTIBLE
        """
elif False:
    ClusterClusterConfigPreemptibleWorkerConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigPreemptibleWorkerConfigArgs:
    def __init__(__self__, *,
                 disk_config: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs']] = None,
                 instance_flexibility_policy: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgs']] = None,
                 instance_names: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 num_instances: Optional[pulumi.Input[int]] = None,
                 preemptibility: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs'] disk_config: Disk Config
        :param pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgs'] instance_flexibility_policy: Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] instance_names: List of preemptible instance names which have been assigned
               to the cluster.
        :param pulumi.Input[int] num_instances: Specifies the number of preemptible nodes to create.
               Defaults to 0.
        :param pulumi.Input[str] preemptibility: Specifies the preemptibility of the secondary workers. The default value is `PREEMPTIBLE`
               Accepted values are:
               * PREEMPTIBILITY_UNSPECIFIED
               * NON_PREEMPTIBLE
               * PREEMPTIBLE
        """
        if disk_config is not None:
            pulumi.set(__self__, "disk_config", disk_config)
        if instance_flexibility_policy is not None:
            pulumi.set(__self__, "instance_flexibility_policy", instance_flexibility_policy)
        if instance_names is not None:
            pulumi.set(__self__, "instance_names", instance_names)
        if num_instances is not None:
            pulumi.set(__self__, "num_instances", num_instances)
        if preemptibility is not None:
            pulumi.set(__self__, "preemptibility", preemptibility)

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs']]:
        """
        Disk Config
        """
        return pulumi.get(self, "disk_config")

    @disk_config.setter
    def disk_config(self, value: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs']]):
        pulumi.set(self, "disk_config", value)

    @property
    @pulumi.getter(name="instanceFlexibilityPolicy")
    def instance_flexibility_policy(self) -> Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgs']]:
        """
        Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
        """
        return pulumi.get(self, "instance_flexibility_policy")

    @instance_flexibility_policy.setter
    def instance_flexibility_policy(self, value: Optional[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgs']]):
        pulumi.set(self, "instance_flexibility_policy", value)

    @property
    @pulumi.getter(name="instanceNames")
    def instance_names(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        List of preemptible instance names which have been assigned
        to the cluster.
        """
        return pulumi.get(self, "instance_names")

    @instance_names.setter
    def instance_names(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "instance_names", value)

    @property
    @pulumi.getter(name="numInstances")
    def num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Specifies the number of preemptible nodes to create.
        Defaults to 0.
        """
        return pulumi.get(self, "num_instances")

    @num_instances.setter
    def num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_instances", value)

    @property
    @pulumi.getter
    def preemptibility(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the preemptibility of the secondary workers. The default value is `PREEMPTIBLE`
        Accepted values are:
        * PREEMPTIBILITY_UNSPECIFIED
        * NON_PREEMPTIBLE
        * PREEMPTIBLE
        """
        return pulumi.get(self, "preemptibility")

    @preemptibility.setter
    def preemptibility(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "preemptibility", value)


if not MYPY:
    class ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgsDict(TypedDict):
        boot_disk_size_gb: NotRequired[pulumi.Input[int]]
        """
        Size of the primary disk attached to each preemptible worker node, specified
        in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        boot_disk_type: NotRequired[pulumi.Input[str]]
        """
        The disk type of the primary disk attached to each preemptible worker node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        local_ssd_interface: NotRequired[pulumi.Input[str]]
        """
        Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        """
        num_local_ssds: NotRequired[pulumi.Input[int]]
        """
        The amount of local SSD disks that will be
        attached to each preemptible worker node. Defaults to 0.
        """
elif False:
    ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigPreemptibleWorkerConfigDiskConfigArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None,
                 local_ssd_interface: Optional[pulumi.Input[str]] = None,
                 num_local_ssds: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] boot_disk_size_gb: Size of the primary disk attached to each preemptible worker node, specified
               in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
               computed value if not set (currently 500GB). Note: If SSDs are not
               attached, it also contains the HDFS data blocks and Hadoop working directories.
        :param pulumi.Input[str] boot_disk_type: The disk type of the primary disk attached to each preemptible worker node.
               One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        :param pulumi.Input[str] local_ssd_interface: Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        :param pulumi.Input[int] num_local_ssds: The amount of local SSD disks that will be
               attached to each preemptible worker node. Defaults to 0.
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)
        if local_ssd_interface is not None:
            pulumi.set(__self__, "local_ssd_interface", local_ssd_interface)
        if num_local_ssds is not None:
            pulumi.set(__self__, "num_local_ssds", num_local_ssds)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size of the primary disk attached to each preemptible worker node, specified
        in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        The disk type of the primary disk attached to each preemptible worker node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)

    @property
    @pulumi.getter(name="localSsdInterface")
    def local_ssd_interface(self) -> Optional[pulumi.Input[str]]:
        """
        Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        """
        return pulumi.get(self, "local_ssd_interface")

    @local_ssd_interface.setter
    def local_ssd_interface(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "local_ssd_interface", value)

    @property
    @pulumi.getter(name="numLocalSsds")
    def num_local_ssds(self) -> Optional[pulumi.Input[int]]:
        """
        The amount of local SSD disks that will be
        attached to each preemptible worker node. Defaults to 0.
        """
        return pulumi.get(self, "num_local_ssds")

    @num_local_ssds.setter
    def num_local_ssds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_local_ssds", value)


if not MYPY:
    class ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgsDict(TypedDict):
        instance_selection_lists: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgsDict']]]]
        """
        List of instance selection options that the group will use when creating new VMs.
        """
        instance_selection_results: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgsDict']]]]
        """
        A list of instance selection results in the group.
        """
elif False:
    ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyArgs:
    def __init__(__self__, *,
                 instance_selection_lists: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgs']]]] = None,
                 instance_selection_results: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgs']]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgs']]] instance_selection_lists: List of instance selection options that the group will use when creating new VMs.
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgs']]] instance_selection_results: A list of instance selection results in the group.
        """
        if instance_selection_lists is not None:
            pulumi.set(__self__, "instance_selection_lists", instance_selection_lists)
        if instance_selection_results is not None:
            pulumi.set(__self__, "instance_selection_results", instance_selection_results)

    @property
    @pulumi.getter(name="instanceSelectionLists")
    def instance_selection_lists(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgs']]]]:
        """
        List of instance selection options that the group will use when creating new VMs.
        """
        return pulumi.get(self, "instance_selection_lists")

    @instance_selection_lists.setter
    def instance_selection_lists(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgs']]]]):
        pulumi.set(self, "instance_selection_lists", value)

    @property
    @pulumi.getter(name="instanceSelectionResults")
    def instance_selection_results(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgs']]]]:
        """
        A list of instance selection results in the group.
        """
        return pulumi.get(self, "instance_selection_results")

    @instance_selection_results.setter
    def instance_selection_results(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgs']]]]):
        pulumi.set(self, "instance_selection_results", value)


if not MYPY:
    class ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgsDict(TypedDict):
        machine_types: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Full machine-type names, e.g. `"n1-standard-16"`.
        """
        rank: NotRequired[pulumi.Input[int]]
        """
        Preference of this instance selection. A lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.

        - - -
        """
elif False:
    ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionListArgs:
    def __init__(__self__, *,
                 machine_types: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 rank: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] machine_types: Full machine-type names, e.g. `"n1-standard-16"`.
        :param pulumi.Input[int] rank: Preference of this instance selection. A lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.
               
               - - -
        """
        if machine_types is not None:
            pulumi.set(__self__, "machine_types", machine_types)
        if rank is not None:
            pulumi.set(__self__, "rank", rank)

    @property
    @pulumi.getter(name="machineTypes")
    def machine_types(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Full machine-type names, e.g. `"n1-standard-16"`.
        """
        return pulumi.get(self, "machine_types")

    @machine_types.setter
    def machine_types(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "machine_types", value)

    @property
    @pulumi.getter
    def rank(self) -> Optional[pulumi.Input[int]]:
        """
        Preference of this instance selection. A lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.

        - - -
        """
        return pulumi.get(self, "rank")

    @rank.setter
    def rank(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "rank", value)


if not MYPY:
    class ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgsDict(TypedDict):
        machine_type: NotRequired[pulumi.Input[str]]
        """
        Full machine-type names, e.g. "n1-standard-16".
        """
        vm_count: NotRequired[pulumi.Input[int]]
        """
        Number of VM provisioned with the machine_type.
        """
elif False:
    ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResultArgs:
    def __init__(__self__, *,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 vm_count: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[str] machine_type: Full machine-type names, e.g. "n1-standard-16".
        :param pulumi.Input[int] vm_count: Number of VM provisioned with the machine_type.
        """
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if vm_count is not None:
            pulumi.set(__self__, "vm_count", vm_count)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        Full machine-type names, e.g. "n1-standard-16".
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="vmCount")
    def vm_count(self) -> Optional[pulumi.Input[int]]:
        """
        Number of VM provisioned with the machine_type.
        """
        return pulumi.get(self, "vm_count")

    @vm_count.setter
    def vm_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "vm_count", value)


if not MYPY:
    class ClusterClusterConfigSecurityConfigArgsDict(TypedDict):
        kerberos_config: pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgsDict']
        """
        Kerberos Configuration
        """
elif False:
    ClusterClusterConfigSecurityConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigSecurityConfigArgs:
    def __init__(__self__, *,
                 kerberos_config: pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgs']):
        """
        :param pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgs'] kerberos_config: Kerberos Configuration
        """
        pulumi.set(__self__, "kerberos_config", kerberos_config)

    @property
    @pulumi.getter(name="kerberosConfig")
    def kerberos_config(self) -> pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgs']:
        """
        Kerberos Configuration
        """
        return pulumi.get(self, "kerberos_config")

    @kerberos_config.setter
    def kerberos_config(self, value: pulumi.Input['ClusterClusterConfigSecurityConfigKerberosConfigArgs']):
        pulumi.set(self, "kerberos_config", value)


if not MYPY:
    class ClusterClusterConfigSecurityConfigKerberosConfigArgsDict(TypedDict):
        kms_key_uri: pulumi.Input[str]
        """
        The URI of the KMS key used to encrypt various sensitive files.
        """
        root_principal_password_uri: pulumi.Input[str]
        """
        The Cloud Storage URI of a KMS encrypted file
        containing the root principal password.
        """
        cross_realm_trust_admin_server: NotRequired[pulumi.Input[str]]
        """
        The admin server (IP or hostname) for the
        remote trusted realm in a cross realm trust relationship.
        """
        cross_realm_trust_kdc: NotRequired[pulumi.Input[str]]
        """
        The KDC (IP or hostname) for the
        remote trusted realm in a cross realm trust relationship.
        """
        cross_realm_trust_realm: NotRequired[pulumi.Input[str]]
        """
        The remote realm the Dataproc on-cluster KDC will
        trust, should the user enable cross realm trust.
        """
        cross_realm_trust_shared_password_uri: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS
        encrypted file containing the shared password between the on-cluster Kerberos realm
        and the remote trusted realm, in a cross realm trust relationship.
        """
        enable_kerberos: NotRequired[pulumi.Input[bool]]
        """
        Flag to indicate whether to Kerberize the cluster.
        """
        kdc_db_key_uri: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing
        the master key of the KDC database.
        """
        key_password_uri: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing
        the password to the user provided key. For the self-signed certificate, this password
        is generated by Dataproc.
        """
        keystore_password_uri: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing
        the password to the user provided keystore. For the self-signed certificated, the password
        is generated by Dataproc.
        """
        keystore_uri: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of the keystore file used for SSL encryption.
        If not provided, Dataproc will provide a self-signed certificate.
        """
        realm: NotRequired[pulumi.Input[str]]
        """
        The name of the on-cluster Kerberos realm. If not specified, the
        uppercased domain of hostnames will be the realm.
        """
        tgt_lifetime_hours: NotRequired[pulumi.Input[int]]
        """
        The lifetime of the ticket granting ticket, in hours.
        """
        truststore_password_uri: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file
        containing the password to the user provided truststore. For the self-signed
        certificate, this password is generated by Dataproc.
        """
        truststore_uri: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of the truststore file used for
        SSL encryption. If not provided, Dataproc will provide a self-signed certificate.

        - - -
        """
elif False:
    ClusterClusterConfigSecurityConfigKerberosConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigSecurityConfigKerberosConfigArgs:
    def __init__(__self__, *,
                 kms_key_uri: pulumi.Input[str],
                 root_principal_password_uri: pulumi.Input[str],
                 cross_realm_trust_admin_server: Optional[pulumi.Input[str]] = None,
                 cross_realm_trust_kdc: Optional[pulumi.Input[str]] = None,
                 cross_realm_trust_realm: Optional[pulumi.Input[str]] = None,
                 cross_realm_trust_shared_password_uri: Optional[pulumi.Input[str]] = None,
                 enable_kerberos: Optional[pulumi.Input[bool]] = None,
                 kdc_db_key_uri: Optional[pulumi.Input[str]] = None,
                 key_password_uri: Optional[pulumi.Input[str]] = None,
                 keystore_password_uri: Optional[pulumi.Input[str]] = None,
                 keystore_uri: Optional[pulumi.Input[str]] = None,
                 realm: Optional[pulumi.Input[str]] = None,
                 tgt_lifetime_hours: Optional[pulumi.Input[int]] = None,
                 truststore_password_uri: Optional[pulumi.Input[str]] = None,
                 truststore_uri: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] kms_key_uri: The URI of the KMS key used to encrypt various sensitive files.
        :param pulumi.Input[str] root_principal_password_uri: The Cloud Storage URI of a KMS encrypted file
               containing the root principal password.
        :param pulumi.Input[str] cross_realm_trust_admin_server: The admin server (IP or hostname) for the
               remote trusted realm in a cross realm trust relationship.
        :param pulumi.Input[str] cross_realm_trust_kdc: The KDC (IP or hostname) for the
               remote trusted realm in a cross realm trust relationship.
        :param pulumi.Input[str] cross_realm_trust_realm: The remote realm the Dataproc on-cluster KDC will
               trust, should the user enable cross realm trust.
        :param pulumi.Input[str] cross_realm_trust_shared_password_uri: The Cloud Storage URI of a KMS
               encrypted file containing the shared password between the on-cluster Kerberos realm
               and the remote trusted realm, in a cross realm trust relationship.
        :param pulumi.Input[bool] enable_kerberos: Flag to indicate whether to Kerberize the cluster.
        :param pulumi.Input[str] kdc_db_key_uri: The Cloud Storage URI of a KMS encrypted file containing
               the master key of the KDC database.
        :param pulumi.Input[str] key_password_uri: The Cloud Storage URI of a KMS encrypted file containing
               the password to the user provided key. For the self-signed certificate, this password
               is generated by Dataproc.
        :param pulumi.Input[str] keystore_password_uri: The Cloud Storage URI of a KMS encrypted file containing
               the password to the user provided keystore. For the self-signed certificated, the password
               is generated by Dataproc.
        :param pulumi.Input[str] keystore_uri: The Cloud Storage URI of the keystore file used for SSL encryption.
               If not provided, Dataproc will provide a self-signed certificate.
        :param pulumi.Input[str] realm: The name of the on-cluster Kerberos realm. If not specified, the
               uppercased domain of hostnames will be the realm.
        :param pulumi.Input[int] tgt_lifetime_hours: The lifetime of the ticket granting ticket, in hours.
        :param pulumi.Input[str] truststore_password_uri: The Cloud Storage URI of a KMS encrypted file
               containing the password to the user provided truststore. For the self-signed
               certificate, this password is generated by Dataproc.
        :param pulumi.Input[str] truststore_uri: The Cloud Storage URI of the truststore file used for
               SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
               
               - - -
        """
        pulumi.set(__self__, "kms_key_uri", kms_key_uri)
        pulumi.set(__self__, "root_principal_password_uri", root_principal_password_uri)
        if cross_realm_trust_admin_server is not None:
            pulumi.set(__self__, "cross_realm_trust_admin_server", cross_realm_trust_admin_server)
        if cross_realm_trust_kdc is not None:
            pulumi.set(__self__, "cross_realm_trust_kdc", cross_realm_trust_kdc)
        if cross_realm_trust_realm is not None:
            pulumi.set(__self__, "cross_realm_trust_realm", cross_realm_trust_realm)
        if cross_realm_trust_shared_password_uri is not None:
            pulumi.set(__self__, "cross_realm_trust_shared_password_uri", cross_realm_trust_shared_password_uri)
        if enable_kerberos is not None:
            pulumi.set(__self__, "enable_kerberos", enable_kerberos)
        if kdc_db_key_uri is not None:
            pulumi.set(__self__, "kdc_db_key_uri", kdc_db_key_uri)
        if key_password_uri is not None:
            pulumi.set(__self__, "key_password_uri", key_password_uri)
        if keystore_password_uri is not None:
            pulumi.set(__self__, "keystore_password_uri", keystore_password_uri)
        if keystore_uri is not None:
            pulumi.set(__self__, "keystore_uri", keystore_uri)
        if realm is not None:
            pulumi.set(__self__, "realm", realm)
        if tgt_lifetime_hours is not None:
            pulumi.set(__self__, "tgt_lifetime_hours", tgt_lifetime_hours)
        if truststore_password_uri is not None:
            pulumi.set(__self__, "truststore_password_uri", truststore_password_uri)
        if truststore_uri is not None:
            pulumi.set(__self__, "truststore_uri", truststore_uri)

    @property
    @pulumi.getter(name="kmsKeyUri")
    def kms_key_uri(self) -> pulumi.Input[str]:
        """
        The URI of the KMS key used to encrypt various sensitive files.
        """
        return pulumi.get(self, "kms_key_uri")

    @kms_key_uri.setter
    def kms_key_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "kms_key_uri", value)

    @property
    @pulumi.getter(name="rootPrincipalPasswordUri")
    def root_principal_password_uri(self) -> pulumi.Input[str]:
        """
        The Cloud Storage URI of a KMS encrypted file
        containing the root principal password.
        """
        return pulumi.get(self, "root_principal_password_uri")

    @root_principal_password_uri.setter
    def root_principal_password_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "root_principal_password_uri", value)

    @property
    @pulumi.getter(name="crossRealmTrustAdminServer")
    def cross_realm_trust_admin_server(self) -> Optional[pulumi.Input[str]]:
        """
        The admin server (IP or hostname) for the
        remote trusted realm in a cross realm trust relationship.
        """
        return pulumi.get(self, "cross_realm_trust_admin_server")

    @cross_realm_trust_admin_server.setter
    def cross_realm_trust_admin_server(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_admin_server", value)

    @property
    @pulumi.getter(name="crossRealmTrustKdc")
    def cross_realm_trust_kdc(self) -> Optional[pulumi.Input[str]]:
        """
        The KDC (IP or hostname) for the
        remote trusted realm in a cross realm trust relationship.
        """
        return pulumi.get(self, "cross_realm_trust_kdc")

    @cross_realm_trust_kdc.setter
    def cross_realm_trust_kdc(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_kdc", value)

    @property
    @pulumi.getter(name="crossRealmTrustRealm")
    def cross_realm_trust_realm(self) -> Optional[pulumi.Input[str]]:
        """
        The remote realm the Dataproc on-cluster KDC will
        trust, should the user enable cross realm trust.
        """
        return pulumi.get(self, "cross_realm_trust_realm")

    @cross_realm_trust_realm.setter
    def cross_realm_trust_realm(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_realm", value)

    @property
    @pulumi.getter(name="crossRealmTrustSharedPasswordUri")
    def cross_realm_trust_shared_password_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS
        encrypted file containing the shared password between the on-cluster Kerberos realm
        and the remote trusted realm, in a cross realm trust relationship.
        """
        return pulumi.get(self, "cross_realm_trust_shared_password_uri")

    @cross_realm_trust_shared_password_uri.setter
    def cross_realm_trust_shared_password_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_shared_password_uri", value)

    @property
    @pulumi.getter(name="enableKerberos")
    def enable_kerberos(self) -> Optional[pulumi.Input[bool]]:
        """
        Flag to indicate whether to Kerberize the cluster.
        """
        return pulumi.get(self, "enable_kerberos")

    @enable_kerberos.setter
    def enable_kerberos(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_kerberos", value)

    @property
    @pulumi.getter(name="kdcDbKeyUri")
    def kdc_db_key_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing
        the master key of the KDC database.
        """
        return pulumi.get(self, "kdc_db_key_uri")

    @kdc_db_key_uri.setter
    def kdc_db_key_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kdc_db_key_uri", value)

    @property
    @pulumi.getter(name="keyPasswordUri")
    def key_password_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing
        the password to the user provided key. For the self-signed certificate, this password
        is generated by Dataproc.
        """
        return pulumi.get(self, "key_password_uri")

    @key_password_uri.setter
    def key_password_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "key_password_uri", value)

    @property
    @pulumi.getter(name="keystorePasswordUri")
    def keystore_password_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing
        the password to the user provided keystore. For the self-signed certificated, the password
        is generated by Dataproc.
        """
        return pulumi.get(self, "keystore_password_uri")

    @keystore_password_uri.setter
    def keystore_password_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "keystore_password_uri", value)

    @property
    @pulumi.getter(name="keystoreUri")
    def keystore_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of the keystore file used for SSL encryption.
        If not provided, Dataproc will provide a self-signed certificate.
        """
        return pulumi.get(self, "keystore_uri")

    @keystore_uri.setter
    def keystore_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "keystore_uri", value)

    @property
    @pulumi.getter
    def realm(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the on-cluster Kerberos realm. If not specified, the
        uppercased domain of hostnames will be the realm.
        """
        return pulumi.get(self, "realm")

    @realm.setter
    def realm(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "realm", value)

    @property
    @pulumi.getter(name="tgtLifetimeHours")
    def tgt_lifetime_hours(self) -> Optional[pulumi.Input[int]]:
        """
        The lifetime of the ticket granting ticket, in hours.
        """
        return pulumi.get(self, "tgt_lifetime_hours")

    @tgt_lifetime_hours.setter
    def tgt_lifetime_hours(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "tgt_lifetime_hours", value)

    @property
    @pulumi.getter(name="truststorePasswordUri")
    def truststore_password_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file
        containing the password to the user provided truststore. For the self-signed
        certificate, this password is generated by Dataproc.
        """
        return pulumi.get(self, "truststore_password_uri")

    @truststore_password_uri.setter
    def truststore_password_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "truststore_password_uri", value)

    @property
    @pulumi.getter(name="truststoreUri")
    def truststore_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of the truststore file used for
        SSL encryption. If not provided, Dataproc will provide a self-signed certificate.

        - - -
        """
        return pulumi.get(self, "truststore_uri")

    @truststore_uri.setter
    def truststore_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "truststore_uri", value)


if not MYPY:
    class ClusterClusterConfigSoftwareConfigArgsDict(TypedDict):
        image_version: NotRequired[pulumi.Input[str]]
        """
        The Cloud Dataproc image version to use
        for the cluster - this controls the sets of software versions
        installed onto the nodes when you create clusters. If not specified, defaults to the
        latest version. For a list of valid versions see
        [Cloud Dataproc versions](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions)
        """
        optional_components: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The set of optional components to activate on the cluster. See [Available Optional Components](https://cloud.google.com/dataproc/docs/concepts/components/overview#available_optional_components).

        - - -
        """
        override_properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A list of override and additional properties (key/value pairs)
        used to modify various aspects of the common configuration files used when creating
        a cluster. For a list of valid properties please see
        [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties)
        """
        properties: NotRequired[pulumi.Input[Mapping[str, Any]]]
        """
        A list of the properties used to set the daemon config files.
        This will include any values supplied by the user via `cluster_config.software_config.override_properties`
        """
elif False:
    ClusterClusterConfigSoftwareConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigSoftwareConfigArgs:
    def __init__(__self__, *,
                 image_version: Optional[pulumi.Input[str]] = None,
                 optional_components: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 override_properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, Any]]] = None):
        """
        :param pulumi.Input[str] image_version: The Cloud Dataproc image version to use
               for the cluster - this controls the sets of software versions
               installed onto the nodes when you create clusters. If not specified, defaults to the
               latest version. For a list of valid versions see
               [Cloud Dataproc versions](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions)
        :param pulumi.Input[Sequence[pulumi.Input[str]]] optional_components: The set of optional components to activate on the cluster. See [Available Optional Components](https://cloud.google.com/dataproc/docs/concepts/components/overview#available_optional_components).
               
               - - -
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] override_properties: A list of override and additional properties (key/value pairs)
               used to modify various aspects of the common configuration files used when creating
               a cluster. For a list of valid properties please see
               [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties)
        :param pulumi.Input[Mapping[str, Any]] properties: A list of the properties used to set the daemon config files.
               This will include any values supplied by the user via `cluster_config.software_config.override_properties`
        """
        if image_version is not None:
            pulumi.set(__self__, "image_version", image_version)
        if optional_components is not None:
            pulumi.set(__self__, "optional_components", optional_components)
        if override_properties is not None:
            pulumi.set(__self__, "override_properties", override_properties)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="imageVersion")
    def image_version(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Dataproc image version to use
        for the cluster - this controls the sets of software versions
        installed onto the nodes when you create clusters. If not specified, defaults to the
        latest version. For a list of valid versions see
        [Cloud Dataproc versions](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions)
        """
        return pulumi.get(self, "image_version")

    @image_version.setter
    def image_version(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "image_version", value)

    @property
    @pulumi.getter(name="optionalComponents")
    def optional_components(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The set of optional components to activate on the cluster. See [Available Optional Components](https://cloud.google.com/dataproc/docs/concepts/components/overview#available_optional_components).

        - - -
        """
        return pulumi.get(self, "optional_components")

    @optional_components.setter
    def optional_components(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "optional_components", value)

    @property
    @pulumi.getter(name="overrideProperties")
    def override_properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A list of override and additional properties (key/value pairs)
        used to modify various aspects of the common configuration files used when creating
        a cluster. For a list of valid properties please see
        [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties)
        """
        return pulumi.get(self, "override_properties")

    @override_properties.setter
    def override_properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "override_properties", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        A list of the properties used to set the daemon config files.
        This will include any values supplied by the user via `cluster_config.software_config.override_properties`
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class ClusterClusterConfigWorkerConfigArgsDict(TypedDict):
        accelerators: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgsDict']]]]
        """
        The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
        """
        disk_config: NotRequired[pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgsDict']]
        """
        Disk Config
        """
        image_uri: NotRequired[pulumi.Input[str]]
        """
        The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
        for more information.
        """
        instance_names: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        List of worker instance names which have been assigned
        to the cluster.
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        The name of a Google Compute Engine machine type
        to create for the worker nodes. If not specified, GCP will default to a predetermined
        computed value (currently `n1-standard-4`).
        """
        min_cpu_platform: NotRequired[pulumi.Input[str]]
        """
        The name of a minimum generation of CPU family
        for the master. If not specified, GCP will default to a predetermined computed value
        for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for details about which CPU families are available (and defaulted) for each zone.
        """
        min_num_instances: NotRequired[pulumi.Input[int]]
        """
        The minimum number of primary worker instances to create.  If `min_num_instances` is set, cluster creation will succeed if the number of primary workers created is at least equal to the `min_num_instances` number.
        """
        num_instances: NotRequired[pulumi.Input[int]]
        """
        Specifies the number of worker nodes to create.
        If not specified, GCP will default to a predetermined computed value (currently 2).
        There is currently a beta feature which allows you to run a
        [Single Node Cluster](https://cloud.google.com/dataproc/docs/concepts/single-node-clusters).
        In order to take advantage of this you need to set
        `"dataproc:dataproc.allow.zero.workers" = "true"` in
        `cluster_config.software_config.properties`
        """
elif False:
    ClusterClusterConfigWorkerConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigWorkerConfigArgs:
    def __init__(__self__, *,
                 accelerators: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgs']]]] = None,
                 disk_config: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgs']] = None,
                 image_uri: Optional[pulumi.Input[str]] = None,
                 instance_names: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 min_num_instances: Optional[pulumi.Input[int]] = None,
                 num_instances: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgs']]] accelerators: The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
        :param pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgs'] disk_config: Disk Config
        :param pulumi.Input[str] image_uri: The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
               for more information.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] instance_names: List of worker instance names which have been assigned
               to the cluster.
        :param pulumi.Input[str] machine_type: The name of a Google Compute Engine machine type
               to create for the worker nodes. If not specified, GCP will default to a predetermined
               computed value (currently `n1-standard-4`).
        :param pulumi.Input[str] min_cpu_platform: The name of a minimum generation of CPU family
               for the master. If not specified, GCP will default to a predetermined computed value
               for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
               for details about which CPU families are available (and defaulted) for each zone.
        :param pulumi.Input[int] min_num_instances: The minimum number of primary worker instances to create.  If `min_num_instances` is set, cluster creation will succeed if the number of primary workers created is at least equal to the `min_num_instances` number.
        :param pulumi.Input[int] num_instances: Specifies the number of worker nodes to create.
               If not specified, GCP will default to a predetermined computed value (currently 2).
               There is currently a beta feature which allows you to run a
               [Single Node Cluster](https://cloud.google.com/dataproc/docs/concepts/single-node-clusters).
               In order to take advantage of this you need to set
               `"dataproc:dataproc.allow.zero.workers" = "true"` in
               `cluster_config.software_config.properties`
        """
        if accelerators is not None:
            pulumi.set(__self__, "accelerators", accelerators)
        if disk_config is not None:
            pulumi.set(__self__, "disk_config", disk_config)
        if image_uri is not None:
            pulumi.set(__self__, "image_uri", image_uri)
        if instance_names is not None:
            pulumi.set(__self__, "instance_names", instance_names)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if min_num_instances is not None:
            pulumi.set(__self__, "min_num_instances", min_num_instances)
        if num_instances is not None:
            pulumi.set(__self__, "num_instances", num_instances)

    @property
    @pulumi.getter
    def accelerators(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgs']]]]:
        """
        The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
        """
        return pulumi.get(self, "accelerators")

    @accelerators.setter
    def accelerators(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterConfigWorkerConfigAcceleratorArgs']]]]):
        pulumi.set(self, "accelerators", value)

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> Optional[pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgs']]:
        """
        Disk Config
        """
        return pulumi.get(self, "disk_config")

    @disk_config.setter
    def disk_config(self, value: Optional[pulumi.Input['ClusterClusterConfigWorkerConfigDiskConfigArgs']]):
        pulumi.set(self, "disk_config", value)

    @property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
        for more information.
        """
        return pulumi.get(self, "image_uri")

    @image_uri.setter
    def image_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "image_uri", value)

    @property
    @pulumi.getter(name="instanceNames")
    def instance_names(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        List of worker instance names which have been assigned
        to the cluster.
        """
        return pulumi.get(self, "instance_names")

    @instance_names.setter
    def instance_names(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "instance_names", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a Google Compute Engine machine type
        to create for the worker nodes. If not specified, GCP will default to a predetermined
        computed value (currently `n1-standard-4`).
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a minimum generation of CPU family
        for the master. If not specified, GCP will default to a predetermined computed value
        for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
        for details about which CPU families are available (and defaulted) for each zone.
        """
        return pulumi.get(self, "min_cpu_platform")

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_cpu_platform", value)

    @property
    @pulumi.getter(name="minNumInstances")
    def min_num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        The minimum number of primary worker instances to create.  If `min_num_instances` is set, cluster creation will succeed if the number of primary workers created is at least equal to the `min_num_instances` number.
        """
        return pulumi.get(self, "min_num_instances")

    @min_num_instances.setter
    def min_num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_num_instances", value)

    @property
    @pulumi.getter(name="numInstances")
    def num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Specifies the number of worker nodes to create.
        If not specified, GCP will default to a predetermined computed value (currently 2).
        There is currently a beta feature which allows you to run a
        [Single Node Cluster](https://cloud.google.com/dataproc/docs/concepts/single-node-clusters).
        In order to take advantage of this you need to set
        `"dataproc:dataproc.allow.zero.workers" = "true"` in
        `cluster_config.software_config.properties`
        """
        return pulumi.get(self, "num_instances")

    @num_instances.setter
    def num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_instances", value)


if not MYPY:
    class ClusterClusterConfigWorkerConfigAcceleratorArgsDict(TypedDict):
        accelerator_count: pulumi.Input[int]
        """
        The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.

        > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
        zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
        if you are trying to use accelerators in a given zone.

        - - -
        """
        accelerator_type: pulumi.Input[str]
        """
        The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
elif False:
    ClusterClusterConfigWorkerConfigAcceleratorArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigWorkerConfigAcceleratorArgs:
    def __init__(__self__, *,
                 accelerator_count: pulumi.Input[int],
                 accelerator_type: pulumi.Input[str]):
        """
        :param pulumi.Input[int] accelerator_count: The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
               
               > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
               zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
               if you are trying to use accelerators in a given zone.
               
               - - -
        :param pulumi.Input[str] accelerator_type: The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        pulumi.set(__self__, "accelerator_count", accelerator_count)
        pulumi.set(__self__, "accelerator_type", accelerator_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> pulumi.Input[int]:
        """
        The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.

        > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
        zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
        if you are trying to use accelerators in a given zone.

        - - -
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> pulumi.Input[str]:
        """
        The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: pulumi.Input[str]):
        pulumi.set(self, "accelerator_type", value)


if not MYPY:
    class ClusterClusterConfigWorkerConfigDiskConfigArgsDict(TypedDict):
        boot_disk_size_gb: NotRequired[pulumi.Input[int]]
        """
        Size of the primary disk attached to each worker node, specified
        in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        boot_disk_type: NotRequired[pulumi.Input[str]]
        """
        The disk type of the primary disk attached to each node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        local_ssd_interface: NotRequired[pulumi.Input[str]]
        """
        Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        """
        num_local_ssds: NotRequired[pulumi.Input[int]]
        """
        The amount of local SSD disks that will be
        attached to each worker cluster node. Defaults to 0.
        """
elif False:
    ClusterClusterConfigWorkerConfigDiskConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterClusterConfigWorkerConfigDiskConfigArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None,
                 local_ssd_interface: Optional[pulumi.Input[str]] = None,
                 num_local_ssds: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] boot_disk_size_gb: Size of the primary disk attached to each worker node, specified
               in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
               computed value if not set (currently 500GB). Note: If SSDs are not
               attached, it also contains the HDFS data blocks and Hadoop working directories.
        :param pulumi.Input[str] boot_disk_type: The disk type of the primary disk attached to each node.
               One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        :param pulumi.Input[str] local_ssd_interface: Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        :param pulumi.Input[int] num_local_ssds: The amount of local SSD disks that will be
               attached to each worker cluster node. Defaults to 0.
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)
        if local_ssd_interface is not None:
            pulumi.set(__self__, "local_ssd_interface", local_ssd_interface)
        if num_local_ssds is not None:
            pulumi.set(__self__, "num_local_ssds", num_local_ssds)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size of the primary disk attached to each worker node, specified
        in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
        computed value if not set (currently 500GB). Note: If SSDs are not
        attached, it also contains the HDFS data blocks and Hadoop working directories.
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        The disk type of the primary disk attached to each node.
        One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)

    @property
    @pulumi.getter(name="localSsdInterface")
    def local_ssd_interface(self) -> Optional[pulumi.Input[str]]:
        """
        Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
        """
        return pulumi.get(self, "local_ssd_interface")

    @local_ssd_interface.setter
    def local_ssd_interface(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "local_ssd_interface", value)

    @property
    @pulumi.getter(name="numLocalSsds")
    def num_local_ssds(self) -> Optional[pulumi.Input[int]]:
        """
        The amount of local SSD disks that will be
        attached to each worker cluster node. Defaults to 0.
        """
        return pulumi.get(self, "num_local_ssds")

    @num_local_ssds.setter
    def num_local_ssds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_local_ssds", value)


if not MYPY:
    class ClusterIAMBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    ClusterIAMBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterIAMBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class ClusterIAMMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    ClusterIAMMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterIAMMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class ClusterVirtualClusterConfigArgsDict(TypedDict):
        auxiliary_services_config: NotRequired[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigArgsDict']]
        """
        Configuration of auxiliary services used by this cluster. 
        Structure defined below.
        """
        kubernetes_cluster_config: NotRequired[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigArgsDict']]
        """
        The configuration for running the Dataproc cluster on Kubernetes.
        Structure defined below.
        - - -
        """
        staging_bucket: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage staging bucket used to stage files,
        such as Hadoop jars, between client machines and the cluster.
        Note: If you don't explicitly specify a `staging_bucket`
        then GCP will auto create / assign one for you. However, you are not guaranteed
        an auto generated bucket which is solely dedicated to your cluster; it may be shared
        with other clusters in the same region/zone also choosing to use the auto generation
        option.
        """
elif False:
    ClusterVirtualClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigArgs:
    def __init__(__self__, *,
                 auxiliary_services_config: Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigArgs']] = None,
                 kubernetes_cluster_config: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigArgs']] = None,
                 staging_bucket: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigArgs'] auxiliary_services_config: Configuration of auxiliary services used by this cluster. 
               Structure defined below.
        :param pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigArgs'] kubernetes_cluster_config: The configuration for running the Dataproc cluster on Kubernetes.
               Structure defined below.
               - - -
        :param pulumi.Input[str] staging_bucket: The Cloud Storage staging bucket used to stage files,
               such as Hadoop jars, between client machines and the cluster.
               Note: If you don't explicitly specify a `staging_bucket`
               then GCP will auto create / assign one for you. However, you are not guaranteed
               an auto generated bucket which is solely dedicated to your cluster; it may be shared
               with other clusters in the same region/zone also choosing to use the auto generation
               option.
        """
        if auxiliary_services_config is not None:
            pulumi.set(__self__, "auxiliary_services_config", auxiliary_services_config)
        if kubernetes_cluster_config is not None:
            pulumi.set(__self__, "kubernetes_cluster_config", kubernetes_cluster_config)
        if staging_bucket is not None:
            pulumi.set(__self__, "staging_bucket", staging_bucket)

    @property
    @pulumi.getter(name="auxiliaryServicesConfig")
    def auxiliary_services_config(self) -> Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigArgs']]:
        """
        Configuration of auxiliary services used by this cluster. 
        Structure defined below.
        """
        return pulumi.get(self, "auxiliary_services_config")

    @auxiliary_services_config.setter
    def auxiliary_services_config(self, value: Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigArgs']]):
        pulumi.set(self, "auxiliary_services_config", value)

    @property
    @pulumi.getter(name="kubernetesClusterConfig")
    def kubernetes_cluster_config(self) -> Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigArgs']]:
        """
        The configuration for running the Dataproc cluster on Kubernetes.
        Structure defined below.
        - - -
        """
        return pulumi.get(self, "kubernetes_cluster_config")

    @kubernetes_cluster_config.setter
    def kubernetes_cluster_config(self, value: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigArgs']]):
        pulumi.set(self, "kubernetes_cluster_config", value)

    @property
    @pulumi.getter(name="stagingBucket")
    def staging_bucket(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage staging bucket used to stage files,
        such as Hadoop jars, between client machines and the cluster.
        Note: If you don't explicitly specify a `staging_bucket`
        then GCP will auto create / assign one for you. However, you are not guaranteed
        an auto generated bucket which is solely dedicated to your cluster; it may be shared
        with other clusters in the same region/zone also choosing to use the auto generation
        option.
        """
        return pulumi.get(self, "staging_bucket")

    @staging_bucket.setter
    def staging_bucket(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "staging_bucket", value)


if not MYPY:
    class ClusterVirtualClusterConfigAuxiliaryServicesConfigArgsDict(TypedDict):
        metastore_config: NotRequired[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgsDict']]
        """
        The Hive Metastore configuration for this workload.
        """
        spark_history_server_config: NotRequired[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgsDict']]
        """
        The Spark History Server configuration for the workload.
        """
elif False:
    ClusterVirtualClusterConfigAuxiliaryServicesConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigAuxiliaryServicesConfigArgs:
    def __init__(__self__, *,
                 metastore_config: Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgs']] = None,
                 spark_history_server_config: Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgs']] = None):
        """
        :param pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgs'] metastore_config: The Hive Metastore configuration for this workload.
        :param pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgs'] spark_history_server_config: The Spark History Server configuration for the workload.
        """
        if metastore_config is not None:
            pulumi.set(__self__, "metastore_config", metastore_config)
        if spark_history_server_config is not None:
            pulumi.set(__self__, "spark_history_server_config", spark_history_server_config)

    @property
    @pulumi.getter(name="metastoreConfig")
    def metastore_config(self) -> Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgs']]:
        """
        The Hive Metastore configuration for this workload.
        """
        return pulumi.get(self, "metastore_config")

    @metastore_config.setter
    def metastore_config(self, value: Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgs']]):
        pulumi.set(self, "metastore_config", value)

    @property
    @pulumi.getter(name="sparkHistoryServerConfig")
    def spark_history_server_config(self) -> Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgs']]:
        """
        The Spark History Server configuration for the workload.
        """
        return pulumi.get(self, "spark_history_server_config")

    @spark_history_server_config.setter
    def spark_history_server_config(self, value: Optional[pulumi.Input['ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgs']]):
        pulumi.set(self, "spark_history_server_config", value)


if not MYPY:
    class ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgsDict(TypedDict):
        dataproc_metastore_service: NotRequired[pulumi.Input[str]]
        """
        Resource name of an existing Dataproc Metastore service.
        """
elif False:
    ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfigArgs:
    def __init__(__self__, *,
                 dataproc_metastore_service: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] dataproc_metastore_service: Resource name of an existing Dataproc Metastore service.
        """
        if dataproc_metastore_service is not None:
            pulumi.set(__self__, "dataproc_metastore_service", dataproc_metastore_service)

    @property
    @pulumi.getter(name="dataprocMetastoreService")
    def dataproc_metastore_service(self) -> Optional[pulumi.Input[str]]:
        """
        Resource name of an existing Dataproc Metastore service.
        """
        return pulumi.get(self, "dataproc_metastore_service")

    @dataproc_metastore_service.setter
    def dataproc_metastore_service(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "dataproc_metastore_service", value)


if not MYPY:
    class ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgsDict(TypedDict):
        dataproc_cluster: NotRequired[pulumi.Input[str]]
        """
        Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
        - - -
        """
elif False:
    ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfigArgs:
    def __init__(__self__, *,
                 dataproc_cluster: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] dataproc_cluster: Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
               - - -
        """
        if dataproc_cluster is not None:
            pulumi.set(__self__, "dataproc_cluster", dataproc_cluster)

    @property
    @pulumi.getter(name="dataprocCluster")
    def dataproc_cluster(self) -> Optional[pulumi.Input[str]]:
        """
        Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
        - - -
        """
        return pulumi.get(self, "dataproc_cluster")

    @dataproc_cluster.setter
    def dataproc_cluster(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "dataproc_cluster", value)


if not MYPY:
    class ClusterVirtualClusterConfigKubernetesClusterConfigArgsDict(TypedDict):
        gke_cluster_config: pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgsDict']
        """
        The configuration for running the Dataproc cluster on GKE.
        """
        kubernetes_software_config: pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgsDict']
        """
        The software configuration for this Dataproc cluster running on Kubernetes.
        """
        kubernetes_namespace: NotRequired[pulumi.Input[str]]
        """
        A namespace within the Kubernetes cluster to deploy into. 
        If this namespace does not exist, it is created.
        If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
        If not specified, the name of the Dataproc Cluster is used.
        """
elif False:
    ClusterVirtualClusterConfigKubernetesClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigKubernetesClusterConfigArgs:
    def __init__(__self__, *,
                 gke_cluster_config: pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgs'],
                 kubernetes_software_config: pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgs'],
                 kubernetes_namespace: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgs'] gke_cluster_config: The configuration for running the Dataproc cluster on GKE.
        :param pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgs'] kubernetes_software_config: The software configuration for this Dataproc cluster running on Kubernetes.
        :param pulumi.Input[str] kubernetes_namespace: A namespace within the Kubernetes cluster to deploy into. 
               If this namespace does not exist, it is created.
               If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
               If not specified, the name of the Dataproc Cluster is used.
        """
        pulumi.set(__self__, "gke_cluster_config", gke_cluster_config)
        pulumi.set(__self__, "kubernetes_software_config", kubernetes_software_config)
        if kubernetes_namespace is not None:
            pulumi.set(__self__, "kubernetes_namespace", kubernetes_namespace)

    @property
    @pulumi.getter(name="gkeClusterConfig")
    def gke_cluster_config(self) -> pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgs']:
        """
        The configuration for running the Dataproc cluster on GKE.
        """
        return pulumi.get(self, "gke_cluster_config")

    @gke_cluster_config.setter
    def gke_cluster_config(self, value: pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgs']):
        pulumi.set(self, "gke_cluster_config", value)

    @property
    @pulumi.getter(name="kubernetesSoftwareConfig")
    def kubernetes_software_config(self) -> pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgs']:
        """
        The software configuration for this Dataproc cluster running on Kubernetes.
        """
        return pulumi.get(self, "kubernetes_software_config")

    @kubernetes_software_config.setter
    def kubernetes_software_config(self, value: pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgs']):
        pulumi.set(self, "kubernetes_software_config", value)

    @property
    @pulumi.getter(name="kubernetesNamespace")
    def kubernetes_namespace(self) -> Optional[pulumi.Input[str]]:
        """
        A namespace within the Kubernetes cluster to deploy into. 
        If this namespace does not exist, it is created.
        If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
        If not specified, the name of the Dataproc Cluster is used.
        """
        return pulumi.get(self, "kubernetes_namespace")

    @kubernetes_namespace.setter
    def kubernetes_namespace(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kubernetes_namespace", value)


if not MYPY:
    class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgsDict(TypedDict):
        gke_cluster_target: NotRequired[pulumi.Input[str]]
        """
        A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster 
        (the GKE cluster can be zonal or regional)
        """
        node_pool_targets: NotRequired[pulumi.Input[Sequence[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgsDict']]]]
        """
        GKE node pools where workloads will be scheduled. At least one node pool must be assigned the `DEFAULT` 
        GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a `DEFAULT` GkeNodePoolTarget.
        Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
        """
elif False:
    ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigArgs:
    def __init__(__self__, *,
                 gke_cluster_target: Optional[pulumi.Input[str]] = None,
                 node_pool_targets: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgs']]]] = None):
        """
        :param pulumi.Input[str] gke_cluster_target: A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster 
               (the GKE cluster can be zonal or regional)
        :param pulumi.Input[Sequence[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgs']]] node_pool_targets: GKE node pools where workloads will be scheduled. At least one node pool must be assigned the `DEFAULT` 
               GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a `DEFAULT` GkeNodePoolTarget.
               Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
        """
        if gke_cluster_target is not None:
            pulumi.set(__self__, "gke_cluster_target", gke_cluster_target)
        if node_pool_targets is not None:
            pulumi.set(__self__, "node_pool_targets", node_pool_targets)

    @property
    @pulumi.getter(name="gkeClusterTarget")
    def gke_cluster_target(self) -> Optional[pulumi.Input[str]]:
        """
        A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster 
        (the GKE cluster can be zonal or regional)
        """
        return pulumi.get(self, "gke_cluster_target")

    @gke_cluster_target.setter
    def gke_cluster_target(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "gke_cluster_target", value)

    @property
    @pulumi.getter(name="nodePoolTargets")
    def node_pool_targets(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgs']]]]:
        """
        GKE node pools where workloads will be scheduled. At least one node pool must be assigned the `DEFAULT` 
        GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a `DEFAULT` GkeNodePoolTarget.
        Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
        """
        return pulumi.get(self, "node_pool_targets")

    @node_pool_targets.setter
    def node_pool_targets(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgs']]]]):
        pulumi.set(self, "node_pool_targets", value)


if not MYPY:
    class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgsDict(TypedDict):
        node_pool: pulumi.Input[str]
        """
        The target GKE node pool.
        """
        roles: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        The roles associated with the GKE node pool. 
        One of `"DEFAULT"`, `"CONTROLLER"`, `"SPARK_DRIVER"` or `"SPARK_EXECUTOR"`.
        """
        node_pool_config: NotRequired[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgsDict']]
        """
        The configuration for the GKE node pool. 
        If specified, Dataproc attempts to create a node pool with the specified shape.
        If one with the same name already exists, it is verified against all specified fields.
        If a field differs, the virtual cluster creation will fail.
        """
elif False:
    ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetArgs:
    def __init__(__self__, *,
                 node_pool: pulumi.Input[str],
                 roles: pulumi.Input[Sequence[pulumi.Input[str]]],
                 node_pool_config: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgs']] = None):
        """
        :param pulumi.Input[str] node_pool: The target GKE node pool.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] roles: The roles associated with the GKE node pool. 
               One of `"DEFAULT"`, `"CONTROLLER"`, `"SPARK_DRIVER"` or `"SPARK_EXECUTOR"`.
        :param pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgs'] node_pool_config: The configuration for the GKE node pool. 
               If specified, Dataproc attempts to create a node pool with the specified shape.
               If one with the same name already exists, it is verified against all specified fields.
               If a field differs, the virtual cluster creation will fail.
        """
        pulumi.set(__self__, "node_pool", node_pool)
        pulumi.set(__self__, "roles", roles)
        if node_pool_config is not None:
            pulumi.set(__self__, "node_pool_config", node_pool_config)

    @property
    @pulumi.getter(name="nodePool")
    def node_pool(self) -> pulumi.Input[str]:
        """
        The target GKE node pool.
        """
        return pulumi.get(self, "node_pool")

    @node_pool.setter
    def node_pool(self, value: pulumi.Input[str]):
        pulumi.set(self, "node_pool", value)

    @property
    @pulumi.getter
    def roles(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        The roles associated with the GKE node pool. 
        One of `"DEFAULT"`, `"CONTROLLER"`, `"SPARK_DRIVER"` or `"SPARK_EXECUTOR"`.
        """
        return pulumi.get(self, "roles")

    @roles.setter
    def roles(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "roles", value)

    @property
    @pulumi.getter(name="nodePoolConfig")
    def node_pool_config(self) -> Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgs']]:
        """
        The configuration for the GKE node pool. 
        If specified, Dataproc attempts to create a node pool with the specified shape.
        If one with the same name already exists, it is verified against all specified fields.
        If a field differs, the virtual cluster creation will fail.
        """
        return pulumi.get(self, "node_pool_config")

    @node_pool_config.setter
    def node_pool_config(self, value: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgs']]):
        pulumi.set(self, "node_pool_config", value)


if not MYPY:
    class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgsDict(TypedDict):
        locations: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        The list of Compute Engine zones where node pool nodes associated 
        with a Dataproc on GKE virtual cluster will be located.
        - - -
        """
        autoscaling: NotRequired[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgsDict']]
        """
        The autoscaler configuration for this node pool. 
        The autoscaler is enabled only when a valid configuration is present.
        """
        config: NotRequired[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgsDict']]
        """
        The node pool configuration.
        """
elif False:
    ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigArgs:
    def __init__(__self__, *,
                 locations: pulumi.Input[Sequence[pulumi.Input[str]]],
                 autoscaling: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgs']] = None,
                 config: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgs']] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] locations: The list of Compute Engine zones where node pool nodes associated 
               with a Dataproc on GKE virtual cluster will be located.
               - - -
        :param pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgs'] autoscaling: The autoscaler configuration for this node pool. 
               The autoscaler is enabled only when a valid configuration is present.
        :param pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgs'] config: The node pool configuration.
        """
        pulumi.set(__self__, "locations", locations)
        if autoscaling is not None:
            pulumi.set(__self__, "autoscaling", autoscaling)
        if config is not None:
            pulumi.set(__self__, "config", config)

    @property
    @pulumi.getter
    def locations(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        The list of Compute Engine zones where node pool nodes associated 
        with a Dataproc on GKE virtual cluster will be located.
        - - -
        """
        return pulumi.get(self, "locations")

    @locations.setter
    def locations(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "locations", value)

    @property
    @pulumi.getter
    def autoscaling(self) -> Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgs']]:
        """
        The autoscaler configuration for this node pool. 
        The autoscaler is enabled only when a valid configuration is present.
        """
        return pulumi.get(self, "autoscaling")

    @autoscaling.setter
    def autoscaling(self, value: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgs']]):
        pulumi.set(self, "autoscaling", value)

    @property
    @pulumi.getter
    def config(self) -> Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgs']]:
        """
        The node pool configuration.
        """
        return pulumi.get(self, "config")

    @config.setter
    def config(self, value: Optional[pulumi.Input['ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgs']]):
        pulumi.set(self, "config", value)


if not MYPY:
    class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgsDict(TypedDict):
        max_node_count: NotRequired[pulumi.Input[int]]
        """
        The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
        """
        min_node_count: NotRequired[pulumi.Input[int]]
        """
        The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
        """
elif False:
    ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscalingArgs:
    def __init__(__self__, *,
                 max_node_count: Optional[pulumi.Input[int]] = None,
                 min_node_count: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] max_node_count: The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
        :param pulumi.Input[int] min_node_count: The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
        """
        if max_node_count is not None:
            pulumi.set(__self__, "max_node_count", max_node_count)
        if min_node_count is not None:
            pulumi.set(__self__, "min_node_count", min_node_count)

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> Optional[pulumi.Input[int]]:
        """
        The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
        """
        return pulumi.get(self, "max_node_count")

    @max_node_count.setter
    def max_node_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_node_count", value)

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> Optional[pulumi.Input[int]]:
        """
        The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
        """
        return pulumi.get(self, "min_node_count")

    @min_node_count.setter
    def min_node_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_node_count", value)


if not MYPY:
    class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgsDict(TypedDict):
        local_ssd_count: NotRequired[pulumi.Input[int]]
        """
        The number of local SSD disks to attach to the node, 
        which is limited by the maximum number of disks allowable per zone.
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        The name of a Compute Engine machine type.
        """
        min_cpu_platform: NotRequired[pulumi.Input[str]]
        """
        Minimum CPU platform to be used by this instance. 
        The instance may be scheduled on the specified or a newer CPU platform.
        Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
        """
        preemptible: NotRequired[pulumi.Input[bool]]
        """
        Whether the nodes are created as preemptible VM instances. 
        Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
        CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
        """
        spot: NotRequired[pulumi.Input[bool]]
        """
        Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
        """
elif False:
    ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfigArgs:
    def __init__(__self__, *,
                 local_ssd_count: Optional[pulumi.Input[int]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 preemptible: Optional[pulumi.Input[bool]] = None,
                 spot: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[int] local_ssd_count: The number of local SSD disks to attach to the node, 
               which is limited by the maximum number of disks allowable per zone.
        :param pulumi.Input[str] machine_type: The name of a Compute Engine machine type.
        :param pulumi.Input[str] min_cpu_platform: Minimum CPU platform to be used by this instance. 
               The instance may be scheduled on the specified or a newer CPU platform.
               Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
        :param pulumi.Input[bool] preemptible: Whether the nodes are created as preemptible VM instances. 
               Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
               CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
        :param pulumi.Input[bool] spot: Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
        """
        if local_ssd_count is not None:
            pulumi.set(__self__, "local_ssd_count", local_ssd_count)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if preemptible is not None:
            pulumi.set(__self__, "preemptible", preemptible)
        if spot is not None:
            pulumi.set(__self__, "spot", spot)

    @property
    @pulumi.getter(name="localSsdCount")
    def local_ssd_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of local SSD disks to attach to the node, 
        which is limited by the maximum number of disks allowable per zone.
        """
        return pulumi.get(self, "local_ssd_count")

    @local_ssd_count.setter
    def local_ssd_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "local_ssd_count", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The name of a Compute Engine machine type.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        Minimum CPU platform to be used by this instance. 
        The instance may be scheduled on the specified or a newer CPU platform.
        Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
        """
        return pulumi.get(self, "min_cpu_platform")

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_cpu_platform", value)

    @property
    @pulumi.getter
    def preemptible(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether the nodes are created as preemptible VM instances. 
        Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
        CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
        """
        return pulumi.get(self, "preemptible")

    @preemptible.setter
    def preemptible(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "preemptible", value)

    @property
    @pulumi.getter
    def spot(self) -> Optional[pulumi.Input[bool]]:
        """
        Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
        """
        return pulumi.get(self, "spot")

    @spot.setter
    def spot(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "spot", value)


if not MYPY:
    class ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgsDict(TypedDict):
        component_version: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        The components that should be installed in this Dataproc cluster. The key must be a string from the   
        KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
        * **NOTE** : `component_version[SPARK]` is mandatory to set, or the creation of the cluster will fail.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The properties to set on daemon config files. Property keys are specified in prefix:property format, 
        for example spark:spark.kubernetes.container.image.
        """
elif False:
    ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfigArgs:
    def __init__(__self__, *,
                 component_version: pulumi.Input[Mapping[str, pulumi.Input[str]]],
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] component_version: The components that should be installed in this Dataproc cluster. The key must be a string from the   
               KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
               * **NOTE** : `component_version[SPARK]` is mandatory to set, or the creation of the cluster will fail.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: The properties to set on daemon config files. Property keys are specified in prefix:property format, 
               for example spark:spark.kubernetes.container.image.
        """
        pulumi.set(__self__, "component_version", component_version)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="componentVersion")
    def component_version(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        The components that should be installed in this Dataproc cluster. The key must be a string from the   
        KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
        * **NOTE** : `component_version[SPARK]` is mandatory to set, or the creation of the cluster will fail.
        """
        return pulumi.get(self, "component_version")

    @component_version.setter
    def component_version(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "component_version", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The properties to set on daemon config files. Property keys are specified in prefix:property format, 
        for example spark:spark.kubernetes.container.image.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class JobHadoopConfigArgsDict(TypedDict):
        archive_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        args: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        """
        logging_config: NotRequired[pulumi.Input['JobHadoopConfigLoggingConfigArgsDict']]
        """
        The runtime logging config of the job
        """
        main_class: NotRequired[pulumi.Input[str]]
        """
        The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
        """
        main_jar_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site` and classes in user code..

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    JobHadoopConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobHadoopConfigArgs:
    def __init__(__self__, *,
                 archive_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['JobHadoopConfigLoggingConfigArgs']] = None,
                 main_class: Optional[pulumi.Input[str]] = None,
                 main_jar_file_uri: Optional[pulumi.Input[str]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] file_uris: HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        :param pulumi.Input['JobHadoopConfigLoggingConfigArgs'] logging_config: The runtime logging config of the job
        :param pulumi.Input[str] main_class: The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
        :param pulumi.Input[str] main_jar_file_uri: The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site` and classes in user code..
               
               * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if main_class is not None:
            pulumi.set(__self__, "main_class", main_class)
        if main_jar_file_uri is not None:
            pulumi.set(__self__, "main_jar_file_uri", main_jar_file_uri)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        return pulumi.get(self, "archive_uris")

    @archive_uris.setter
    def archive_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "archive_uris", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        """
        return pulumi.get(self, "file_uris")

    @file_uris.setter
    def file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "file_uris", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['JobHadoopConfigLoggingConfigArgs']]:
        """
        The runtime logging config of the job
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['JobHadoopConfigLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter(name="mainClass")
    def main_class(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`. Conflicts with `main_jar_file_uri`
        """
        return pulumi.get(self, "main_class")

    @main_class.setter
    def main_class(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_class", value)

    @property
    @pulumi.getter(name="mainJarFileUri")
    def main_jar_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `main_class`
        """
        return pulumi.get(self, "main_jar_file_uri")

    @main_jar_file_uri.setter
    def main_jar_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_jar_file_uri", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site` and classes in user code..

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class JobHadoopConfigLoggingConfigArgsDict(TypedDict):
        driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
elif False:
    JobHadoopConfigLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobHadoopConfigLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class JobHiveConfigArgsDict(TypedDict):
        continue_on_failure: NotRequired[pulumi.Input[bool]]
        """
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/hive/conf/hive-site.xml`, and classes in user code..
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with `query_list`
        """
        query_lists: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
        script_variables: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
        """
elif False:
    JobHiveConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobHiveConfigArgs:
    def __init__(__self__, *,
                 continue_on_failure: Optional[pulumi.Input[bool]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_lists: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 script_variables: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/hive/conf/hive-site.xml`, and classes in user code..
        :param pulumi.Input[str] query_file_uri: HCFS URI of file containing Hive script to execute as the job.
               Conflicts with `query_list`
        :param pulumi.Input[Sequence[pulumi.Input[str]]] query_lists: The list of Hive queries or statements to execute as part of the job.
               Conflicts with `query_file_uri`
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
        """
        if continue_on_failure is not None:
            pulumi.set(__self__, "continue_on_failure", continue_on_failure)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_lists is not None:
            pulumi.set(__self__, "query_lists", query_lists)
        if script_variables is not None:
            pulumi.set(__self__, "script_variables", script_variables)

    @property
    @pulumi.getter(name="continueOnFailure")
    def continue_on_failure(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        """
        return pulumi.get(self, "continue_on_failure")

    @continue_on_failure.setter
    def continue_on_failure(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "continue_on_failure", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/hive/conf/hive-site.xml`, and classes in user code..
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with `query_list`
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryLists")
    def query_lists(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
        return pulumi.get(self, "query_lists")

    @query_lists.setter
    def query_lists(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "query_lists", value)

    @property
    @pulumi.getter(name="scriptVariables")
    def script_variables(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
        """
        return pulumi.get(self, "script_variables")

    @script_variables.setter
    def script_variables(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "script_variables", value)


if not MYPY:
    class JobIAMBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    JobIAMBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobIAMBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class JobIAMMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    JobIAMMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobIAMMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class JobPigConfigArgsDict(TypedDict):
        continue_on_failure: NotRequired[pulumi.Input[bool]]
        """
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        logging_config: NotRequired[pulumi.Input['JobPigConfigLoggingConfigArgsDict']]
        """
        The runtime logging config of the job
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/pig/conf/pig.properties`, and classes in user code.
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with `query_list`
        """
        query_lists: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
        script_variables: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        Mapping of query variable names to values (equivalent to the Pig command: `name=[value]`).
        """
elif False:
    JobPigConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobPigConfigArgs:
    def __init__(__self__, *,
                 continue_on_failure: Optional[pulumi.Input[bool]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['JobPigConfigLoggingConfigArgs']] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_lists: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 script_variables: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
               
               * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        :param pulumi.Input['JobPigConfigLoggingConfigArgs'] logging_config: The runtime logging config of the job
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/pig/conf/pig.properties`, and classes in user code.
        :param pulumi.Input[str] query_file_uri: HCFS URI of file containing Hive script to execute as the job.
               Conflicts with `query_list`
        :param pulumi.Input[Sequence[pulumi.Input[str]]] query_lists: The list of Hive queries or statements to execute as part of the job.
               Conflicts with `query_file_uri`
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Pig command: `name=[value]`).
        """
        if continue_on_failure is not None:
            pulumi.set(__self__, "continue_on_failure", continue_on_failure)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_lists is not None:
            pulumi.set(__self__, "query_lists", query_lists)
        if script_variables is not None:
            pulumi.set(__self__, "script_variables", script_variables)

    @property
    @pulumi.getter(name="continueOnFailure")
    def continue_on_failure(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        """
        return pulumi.get(self, "continue_on_failure")

    @continue_on_failure.setter
    def continue_on_failure(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "continue_on_failure", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['JobPigConfigLoggingConfigArgs']]:
        """
        The runtime logging config of the job
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['JobPigConfigLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/pig/conf/pig.properties`, and classes in user code.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with `query_list`
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryLists")
    def query_lists(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
        return pulumi.get(self, "query_lists")

    @query_lists.setter
    def query_lists(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "query_lists", value)

    @property
    @pulumi.getter(name="scriptVariables")
    def script_variables(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Mapping of query variable names to values (equivalent to the Pig command: `name=[value]`).
        """
        return pulumi.get(self, "script_variables")

    @script_variables.setter
    def script_variables(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "script_variables", value)


if not MYPY:
    class JobPigConfigLoggingConfigArgsDict(TypedDict):
        driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
elif False:
    JobPigConfigLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobPigConfigLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class JobPlacementArgsDict(TypedDict):
        cluster_name: pulumi.Input[str]
        """
        The name of the cluster where the job will be submitted
        """
        cluster_uuid: NotRequired[pulumi.Input[str]]
        """
        Output-only. A cluster UUID generated by the Cloud Dataproc service when the job is submitted
        """
elif False:
    JobPlacementArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobPlacementArgs:
    def __init__(__self__, *,
                 cluster_name: pulumi.Input[str],
                 cluster_uuid: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] cluster_name: The name of the cluster where the job will be submitted
        :param pulumi.Input[str] cluster_uuid: Output-only. A cluster UUID generated by the Cloud Dataproc service when the job is submitted
        """
        pulumi.set(__self__, "cluster_name", cluster_name)
        if cluster_uuid is not None:
            pulumi.set(__self__, "cluster_uuid", cluster_uuid)

    @property
    @pulumi.getter(name="clusterName")
    def cluster_name(self) -> pulumi.Input[str]:
        """
        The name of the cluster where the job will be submitted
        """
        return pulumi.get(self, "cluster_name")

    @cluster_name.setter
    def cluster_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "cluster_name", value)

    @property
    @pulumi.getter(name="clusterUuid")
    def cluster_uuid(self) -> Optional[pulumi.Input[str]]:
        """
        Output-only. A cluster UUID generated by the Cloud Dataproc service when the job is submitted
        """
        return pulumi.get(self, "cluster_uuid")

    @cluster_uuid.setter
    def cluster_uuid(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cluster_uuid", value)


if not MYPY:
    class JobPrestoConfigArgsDict(TypedDict):
        client_tags: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Presto client tags to attach to this query.
        """
        continue_on_failure: NotRequired[pulumi.Input[bool]]
        """
        Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        """
        logging_config: NotRequired[pulumi.Input['JobPrestoConfigLoggingConfigArgsDict']]
        """
        The runtime logging config of the job
        """
        output_format: NotRequired[pulumi.Input[str]]
        """
        The format in which query output will be displayed. See the Presto documentation for supported output formats.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the script that contains SQL queries.
        Conflicts with `query_list`
        """
        query_lists: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
elif False:
    JobPrestoConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobPrestoConfigArgs:
    def __init__(__self__, *,
                 client_tags: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 continue_on_failure: Optional[pulumi.Input[bool]] = None,
                 logging_config: Optional[pulumi.Input['JobPrestoConfigLoggingConfigArgs']] = None,
                 output_format: Optional[pulumi.Input[str]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_lists: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] client_tags: Presto client tags to attach to this query.
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        :param pulumi.Input['JobPrestoConfigLoggingConfigArgs'] logging_config: The runtime logging config of the job
        :param pulumi.Input[str] output_format: The format in which query output will be displayed. See the Presto documentation for supported output formats.
               
               * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains SQL queries.
               Conflicts with `query_list`
        :param pulumi.Input[Sequence[pulumi.Input[str]]] query_lists: The list of SQL queries or statements to execute as part of the job.
               Conflicts with `query_file_uri`
        """
        if client_tags is not None:
            pulumi.set(__self__, "client_tags", client_tags)
        if continue_on_failure is not None:
            pulumi.set(__self__, "continue_on_failure", continue_on_failure)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if output_format is not None:
            pulumi.set(__self__, "output_format", output_format)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_lists is not None:
            pulumi.set(__self__, "query_lists", query_lists)

    @property
    @pulumi.getter(name="clientTags")
    def client_tags(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Presto client tags to attach to this query.
        """
        return pulumi.get(self, "client_tags")

    @client_tags.setter
    def client_tags(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "client_tags", value)

    @property
    @pulumi.getter(name="continueOnFailure")
    def continue_on_failure(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
        """
        return pulumi.get(self, "continue_on_failure")

    @continue_on_failure.setter
    def continue_on_failure(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "continue_on_failure", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['JobPrestoConfigLoggingConfigArgs']]:
        """
        The runtime logging config of the job
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['JobPrestoConfigLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter(name="outputFormat")
    def output_format(self) -> Optional[pulumi.Input[str]]:
        """
        The format in which query output will be displayed. See the Presto documentation for supported output formats.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "output_format")

    @output_format.setter
    def output_format(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "output_format", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the script that contains SQL queries.
        Conflicts with `query_list`
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryLists")
    def query_lists(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
        return pulumi.get(self, "query_lists")

    @query_lists.setter
    def query_lists(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "query_lists", value)


if not MYPY:
    class JobPrestoConfigLoggingConfigArgsDict(TypedDict):
        driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
elif False:
    JobPrestoConfigLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobPrestoConfigLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class JobPysparkConfigArgsDict(TypedDict):
        main_python_file_uri: pulumi.Input[str]
        """
        The HCFS URI of the main Python file to use as the driver. Must be a .py file.
        """
        archive_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        args: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The arguments to pass to the driver.
        """
        file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
        """
        logging_config: NotRequired[pulumi.Input['JobPysparkConfigLoggingConfigArgsDict']]
        """
        The runtime logging config of the job
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        python_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        """
elif False:
    JobPysparkConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobPysparkConfigArgs:
    def __init__(__self__, *,
                 main_python_file_uri: pulumi.Input[str],
                 archive_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['JobPysparkConfigLoggingConfigArgs']] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 python_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] main_python_file_uri: The HCFS URI of the main Python file to use as the driver. Must be a .py file.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to pass to the driver.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] file_uris: HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
        :param pulumi.Input['JobPysparkConfigLoggingConfigArgs'] logging_config: The runtime logging config of the job
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.
               
               * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        :param pulumi.Input[Sequence[pulumi.Input[str]]] python_file_uris: HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        """
        pulumi.set(__self__, "main_python_file_uri", main_python_file_uri)
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if python_file_uris is not None:
            pulumi.set(__self__, "python_file_uris", python_file_uris)

    @property
    @pulumi.getter(name="mainPythonFileUri")
    def main_python_file_uri(self) -> pulumi.Input[str]:
        """
        The HCFS URI of the main Python file to use as the driver. Must be a .py file.
        """
        return pulumi.get(self, "main_python_file_uri")

    @main_python_file_uri.setter
    def main_python_file_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "main_python_file_uri", value)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        return pulumi.get(self, "archive_uris")

    @archive_uris.setter
    def archive_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "archive_uris", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to pass to the driver.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
        """
        return pulumi.get(self, "file_uris")

    @file_uris.setter
    def file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "file_uris", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['JobPysparkConfigLoggingConfigArgs']]:
        """
        The runtime logging config of the job
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['JobPysparkConfigLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="pythonFileUris")
    def python_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        """
        return pulumi.get(self, "python_file_uris")

    @python_file_uris.setter
    def python_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "python_file_uris", value)


if not MYPY:
    class JobPysparkConfigLoggingConfigArgsDict(TypedDict):
        driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
elif False:
    JobPysparkConfigLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobPysparkConfigLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class JobReferenceArgsDict(TypedDict):
        job_id: NotRequired[pulumi.Input[str]]
        """
        The job ID, which must be unique within the project. The job ID is generated by the server upon job submission or provided by the user as a means to perform retries without creating duplicate jobs
        """
elif False:
    JobReferenceArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobReferenceArgs:
    def __init__(__self__, *,
                 job_id: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] job_id: The job ID, which must be unique within the project. The job ID is generated by the server upon job submission or provided by the user as a means to perform retries without creating duplicate jobs
        """
        if job_id is not None:
            pulumi.set(__self__, "job_id", job_id)

    @property
    @pulumi.getter(name="jobId")
    def job_id(self) -> Optional[pulumi.Input[str]]:
        """
        The job ID, which must be unique within the project. The job ID is generated by the server upon job submission or provided by the user as a means to perform retries without creating duplicate jobs
        """
        return pulumi.get(self, "job_id")

    @job_id.setter
    def job_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "job_id", value)


if not MYPY:
    class JobSchedulingArgsDict(TypedDict):
        max_failures_per_hour: pulumi.Input[int]
        """
        Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
        """
        max_failures_total: pulumi.Input[int]
        """
        Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
        """
elif False:
    JobSchedulingArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobSchedulingArgs:
    def __init__(__self__, *,
                 max_failures_per_hour: pulumi.Input[int],
                 max_failures_total: pulumi.Input[int]):
        """
        :param pulumi.Input[int] max_failures_per_hour: Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
        :param pulumi.Input[int] max_failures_total: Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
        """
        pulumi.set(__self__, "max_failures_per_hour", max_failures_per_hour)
        pulumi.set(__self__, "max_failures_total", max_failures_total)

    @property
    @pulumi.getter(name="maxFailuresPerHour")
    def max_failures_per_hour(self) -> pulumi.Input[int]:
        """
        Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
        """
        return pulumi.get(self, "max_failures_per_hour")

    @max_failures_per_hour.setter
    def max_failures_per_hour(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_failures_per_hour", value)

    @property
    @pulumi.getter(name="maxFailuresTotal")
    def max_failures_total(self) -> pulumi.Input[int]:
        """
        Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
        """
        return pulumi.get(self, "max_failures_total")

    @max_failures_total.setter
    def max_failures_total(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_failures_total", value)


if not MYPY:
    class JobSparkConfigArgsDict(TypedDict):
        archive_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        args: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The arguments to pass to the driver.
        """
        file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        """
        logging_config: NotRequired[pulumi.Input['JobSparkConfigLoggingConfigArgsDict']]
        """
        The runtime logging config of the job
        """
        main_class: NotRequired[pulumi.Input[str]]
        """
        The class containing the main method of the driver. Must be in a
        provided jar or jar that is already on the classpath. Conflicts with `main_jar_file_uri`
        """
        main_jar_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of jar file containing
        the driver jar. Conflicts with `main_class`
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    JobSparkConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobSparkConfigArgs:
    def __init__(__self__, *,
                 archive_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['JobSparkConfigLoggingConfigArgs']] = None,
                 main_class: Optional[pulumi.Input[str]] = None,
                 main_jar_file_uri: Optional[pulumi.Input[str]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to pass to the driver.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] file_uris: HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        :param pulumi.Input['JobSparkConfigLoggingConfigArgs'] logging_config: The runtime logging config of the job
        :param pulumi.Input[str] main_class: The class containing the main method of the driver. Must be in a
               provided jar or jar that is already on the classpath. Conflicts with `main_jar_file_uri`
        :param pulumi.Input[str] main_jar_file_uri: The HCFS URI of jar file containing
               the driver jar. Conflicts with `main_class`
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.
               
               * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if main_class is not None:
            pulumi.set(__self__, "main_class", main_class)
        if main_jar_file_uri is not None:
            pulumi.set(__self__, "main_jar_file_uri", main_jar_file_uri)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        return pulumi.get(self, "archive_uris")

    @archive_uris.setter
    def archive_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "archive_uris", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to pass to the driver.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
        """
        return pulumi.get(self, "file_uris")

    @file_uris.setter
    def file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "file_uris", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['JobSparkConfigLoggingConfigArgs']]:
        """
        The runtime logging config of the job
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['JobSparkConfigLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter(name="mainClass")
    def main_class(self) -> Optional[pulumi.Input[str]]:
        """
        The class containing the main method of the driver. Must be in a
        provided jar or jar that is already on the classpath. Conflicts with `main_jar_file_uri`
        """
        return pulumi.get(self, "main_class")

    @main_class.setter
    def main_class(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_class", value)

    @property
    @pulumi.getter(name="mainJarFileUri")
    def main_jar_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of jar file containing
        the driver jar. Conflicts with `main_class`
        """
        return pulumi.get(self, "main_jar_file_uri")

    @main_jar_file_uri.setter
    def main_jar_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_jar_file_uri", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class JobSparkConfigLoggingConfigArgsDict(TypedDict):
        driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
elif False:
    JobSparkConfigLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobSparkConfigLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class JobSparksqlConfigArgsDict(TypedDict):
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to be added to the Spark CLASSPATH.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        logging_config: NotRequired[pulumi.Input['JobSparksqlConfigLoggingConfigArgsDict']]
        """
        The runtime logging config of the job
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the script that contains SQL queries.
        Conflicts with `query_list`
        """
        query_lists: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
        script_variables: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
        """
elif False:
    JobSparksqlConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobSparksqlConfigArgs:
    def __init__(__self__, *,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['JobSparksqlConfigLoggingConfigArgs']] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_lists: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 script_variables: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
               
               * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        :param pulumi.Input['JobSparksqlConfigLoggingConfigArgs'] logging_config: The runtime logging config of the job
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains SQL queries.
               Conflicts with `query_list`
        :param pulumi.Input[Sequence[pulumi.Input[str]]] query_lists: The list of SQL queries or statements to execute as part of the job.
               Conflicts with `query_file_uri`
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
        """
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_lists is not None:
            pulumi.set(__self__, "query_lists", query_lists)
        if script_variables is not None:
            pulumi.set(__self__, "script_variables", script_variables)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to be added to the Spark CLASSPATH.

        * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['JobSparksqlConfigLoggingConfigArgs']]:
        """
        The runtime logging config of the job
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['JobSparksqlConfigLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the script that contains SQL queries.
        Conflicts with `query_list`
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryLists")
    def query_lists(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with `query_file_uri`
        """
        return pulumi.get(self, "query_lists")

    @query_lists.setter
    def query_lists(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "query_lists", value)

    @property
    @pulumi.getter(name="scriptVariables")
    def script_variables(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
        """
        return pulumi.get(self, "script_variables")

    @script_variables.setter
    def script_variables(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "script_variables", value)


if not MYPY:
    class JobSparksqlConfigLoggingConfigArgsDict(TypedDict):
        driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
elif False:
    JobSparksqlConfigLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobSparksqlConfigLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class JobStatusArgsDict(TypedDict):
        details: NotRequired[pulumi.Input[str]]
        """
        Optional job state details, such as an error description if the state is ERROR.
        """
        state: NotRequired[pulumi.Input[str]]
        """
        A state message specifying the overall job state.
        """
        state_start_time: NotRequired[pulumi.Input[str]]
        """
        The time when this state was entered.
        """
        substate: NotRequired[pulumi.Input[str]]
        """
        Additional state information, which includes status reported by the agent.
        """
elif False:
    JobStatusArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class JobStatusArgs:
    def __init__(__self__, *,
                 details: Optional[pulumi.Input[str]] = None,
                 state: Optional[pulumi.Input[str]] = None,
                 state_start_time: Optional[pulumi.Input[str]] = None,
                 substate: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] details: Optional job state details, such as an error description if the state is ERROR.
        :param pulumi.Input[str] state: A state message specifying the overall job state.
        :param pulumi.Input[str] state_start_time: The time when this state was entered.
        :param pulumi.Input[str] substate: Additional state information, which includes status reported by the agent.
        """
        if details is not None:
            pulumi.set(__self__, "details", details)
        if state is not None:
            pulumi.set(__self__, "state", state)
        if state_start_time is not None:
            pulumi.set(__self__, "state_start_time", state_start_time)
        if substate is not None:
            pulumi.set(__self__, "substate", substate)

    @property
    @pulumi.getter
    def details(self) -> Optional[pulumi.Input[str]]:
        """
        Optional job state details, such as an error description if the state is ERROR.
        """
        return pulumi.get(self, "details")

    @details.setter
    def details(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "details", value)

    @property
    @pulumi.getter
    def state(self) -> Optional[pulumi.Input[str]]:
        """
        A state message specifying the overall job state.
        """
        return pulumi.get(self, "state")

    @state.setter
    def state(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "state", value)

    @property
    @pulumi.getter(name="stateStartTime")
    def state_start_time(self) -> Optional[pulumi.Input[str]]:
        """
        The time when this state was entered.
        """
        return pulumi.get(self, "state_start_time")

    @state_start_time.setter
    def state_start_time(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "state_start_time", value)

    @property
    @pulumi.getter
    def substate(self) -> Optional[pulumi.Input[str]]:
        """
        Additional state information, which includes status reported by the agent.
        """
        return pulumi.get(self, "substate")

    @substate.setter
    def substate(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "substate", value)


if not MYPY:
    class MetastoreFederationBackendMetastoreArgsDict(TypedDict):
        metastore_type: pulumi.Input[str]
        """
        The type of the backend metastore.
        Possible values are: `METASTORE_TYPE_UNSPECIFIED`, `DATAPROC_METASTORE`, `BIGQUERY`.

        - - -
        """
        name: pulumi.Input[str]
        """
        The relative resource name of the metastore that is being federated. The formats of the relative resource names for the currently supported metastores are listed below: Dataplex: projects/{projectId}/locations/{location}/lakes/{lake_id} BigQuery: projects/{projectId} Dataproc Metastore: projects/{projectId}/locations/{location}/services/{serviceId}
        """
        rank: pulumi.Input[str]
        """
        The identifier for this object. Format specified above.
        """
elif False:
    MetastoreFederationBackendMetastoreArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreFederationBackendMetastoreArgs:
    def __init__(__self__, *,
                 metastore_type: pulumi.Input[str],
                 name: pulumi.Input[str],
                 rank: pulumi.Input[str]):
        """
        :param pulumi.Input[str] metastore_type: The type of the backend metastore.
               Possible values are: `METASTORE_TYPE_UNSPECIFIED`, `DATAPROC_METASTORE`, `BIGQUERY`.
               
               - - -
        :param pulumi.Input[str] name: The relative resource name of the metastore that is being federated. The formats of the relative resource names for the currently supported metastores are listed below: Dataplex: projects/{projectId}/locations/{location}/lakes/{lake_id} BigQuery: projects/{projectId} Dataproc Metastore: projects/{projectId}/locations/{location}/services/{serviceId}
        :param pulumi.Input[str] rank: The identifier for this object. Format specified above.
        """
        pulumi.set(__self__, "metastore_type", metastore_type)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "rank", rank)

    @property
    @pulumi.getter(name="metastoreType")
    def metastore_type(self) -> pulumi.Input[str]:
        """
        The type of the backend metastore.
        Possible values are: `METASTORE_TYPE_UNSPECIFIED`, `DATAPROC_METASTORE`, `BIGQUERY`.

        - - -
        """
        return pulumi.get(self, "metastore_type")

    @metastore_type.setter
    def metastore_type(self, value: pulumi.Input[str]):
        pulumi.set(self, "metastore_type", value)

    @property
    @pulumi.getter
    def name(self) -> pulumi.Input[str]:
        """
        The relative resource name of the metastore that is being federated. The formats of the relative resource names for the currently supported metastores are listed below: Dataplex: projects/{projectId}/locations/{location}/lakes/{lake_id} BigQuery: projects/{projectId} Dataproc Metastore: projects/{projectId}/locations/{location}/services/{serviceId}
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[str]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def rank(self) -> pulumi.Input[str]:
        """
        The identifier for this object. Format specified above.
        """
        return pulumi.get(self, "rank")

    @rank.setter
    def rank(self, value: pulumi.Input[str]):
        pulumi.set(self, "rank", value)


if not MYPY:
    class MetastoreFederationIamBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    MetastoreFederationIamBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreFederationIamBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class MetastoreFederationIamMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    MetastoreFederationIamMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreFederationIamMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class MetastoreServiceEncryptionConfigArgsDict(TypedDict):
        kms_key: pulumi.Input[str]
        """
        The fully qualified customer provided Cloud KMS key name to use for customer data encryption.
        Use the following format: `projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)`
        """
elif False:
    MetastoreServiceEncryptionConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceEncryptionConfigArgs:
    def __init__(__self__, *,
                 kms_key: pulumi.Input[str]):
        """
        :param pulumi.Input[str] kms_key: The fully qualified customer provided Cloud KMS key name to use for customer data encryption.
               Use the following format: `projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)`
        """
        pulumi.set(__self__, "kms_key", kms_key)

    @property
    @pulumi.getter(name="kmsKey")
    def kms_key(self) -> pulumi.Input[str]:
        """
        The fully qualified customer provided Cloud KMS key name to use for customer data encryption.
        Use the following format: `projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)`
        """
        return pulumi.get(self, "kms_key")

    @kms_key.setter
    def kms_key(self, value: pulumi.Input[str]):
        pulumi.set(self, "kms_key", value)


if not MYPY:
    class MetastoreServiceHiveMetastoreConfigArgsDict(TypedDict):
        version: pulumi.Input[str]
        """
        The Hive metastore schema version.
        """
        auxiliary_versions: NotRequired[pulumi.Input[Sequence[pulumi.Input['MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgsDict']]]]
        """
        A mapping of Hive metastore version to the auxiliary version configuration.
        When specified, a secondary Hive metastore service is created along with the primary service.
        All auxiliary versions must be less than the service's primary version.
        The key is the auxiliary service name and it must match the regular expression a-z?.
        This means that the first character must be a lowercase letter, and all the following characters must be hyphens, lowercase letters, or digits, except the last character, which cannot be a hyphen.
        Structure is documented below.
        """
        config_overrides: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml).
        The mappings override system defaults (some keys cannot be overridden)
        """
        endpoint_protocol: NotRequired[pulumi.Input[str]]
        """
        The protocol to use for the metastore service endpoint. If unspecified, defaults to `THRIFT`.
        Default value is `THRIFT`.
        Possible values are: `THRIFT`, `GRPC`.
        """
        kerberos_config: NotRequired[pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigArgsDict']]
        """
        Information used to configure the Hive metastore service as a service principal in a Kerberos realm.
        Structure is documented below.
        """
elif False:
    MetastoreServiceHiveMetastoreConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceHiveMetastoreConfigArgs:
    def __init__(__self__, *,
                 version: pulumi.Input[str],
                 auxiliary_versions: Optional[pulumi.Input[Sequence[pulumi.Input['MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgs']]]] = None,
                 config_overrides: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 endpoint_protocol: Optional[pulumi.Input[str]] = None,
                 kerberos_config: Optional[pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigArgs']] = None):
        """
        :param pulumi.Input[str] version: The Hive metastore schema version.
        :param pulumi.Input[Sequence[pulumi.Input['MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgs']]] auxiliary_versions: A mapping of Hive metastore version to the auxiliary version configuration.
               When specified, a secondary Hive metastore service is created along with the primary service.
               All auxiliary versions must be less than the service's primary version.
               The key is the auxiliary service name and it must match the regular expression a-z?.
               This means that the first character must be a lowercase letter, and all the following characters must be hyphens, lowercase letters, or digits, except the last character, which cannot be a hyphen.
               Structure is documented below.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_overrides: A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml).
               The mappings override system defaults (some keys cannot be overridden)
        :param pulumi.Input[str] endpoint_protocol: The protocol to use for the metastore service endpoint. If unspecified, defaults to `THRIFT`.
               Default value is `THRIFT`.
               Possible values are: `THRIFT`, `GRPC`.
        :param pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigArgs'] kerberos_config: Information used to configure the Hive metastore service as a service principal in a Kerberos realm.
               Structure is documented below.
        """
        pulumi.set(__self__, "version", version)
        if auxiliary_versions is not None:
            pulumi.set(__self__, "auxiliary_versions", auxiliary_versions)
        if config_overrides is not None:
            pulumi.set(__self__, "config_overrides", config_overrides)
        if endpoint_protocol is not None:
            pulumi.set(__self__, "endpoint_protocol", endpoint_protocol)
        if kerberos_config is not None:
            pulumi.set(__self__, "kerberos_config", kerberos_config)

    @property
    @pulumi.getter
    def version(self) -> pulumi.Input[str]:
        """
        The Hive metastore schema version.
        """
        return pulumi.get(self, "version")

    @version.setter
    def version(self, value: pulumi.Input[str]):
        pulumi.set(self, "version", value)

    @property
    @pulumi.getter(name="auxiliaryVersions")
    def auxiliary_versions(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgs']]]]:
        """
        A mapping of Hive metastore version to the auxiliary version configuration.
        When specified, a secondary Hive metastore service is created along with the primary service.
        All auxiliary versions must be less than the service's primary version.
        The key is the auxiliary service name and it must match the regular expression a-z?.
        This means that the first character must be a lowercase letter, and all the following characters must be hyphens, lowercase letters, or digits, except the last character, which cannot be a hyphen.
        Structure is documented below.
        """
        return pulumi.get(self, "auxiliary_versions")

    @auxiliary_versions.setter
    def auxiliary_versions(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgs']]]]):
        pulumi.set(self, "auxiliary_versions", value)

    @property
    @pulumi.getter(name="configOverrides")
    def config_overrides(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml).
        The mappings override system defaults (some keys cannot be overridden)
        """
        return pulumi.get(self, "config_overrides")

    @config_overrides.setter
    def config_overrides(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "config_overrides", value)

    @property
    @pulumi.getter(name="endpointProtocol")
    def endpoint_protocol(self) -> Optional[pulumi.Input[str]]:
        """
        The protocol to use for the metastore service endpoint. If unspecified, defaults to `THRIFT`.
        Default value is `THRIFT`.
        Possible values are: `THRIFT`, `GRPC`.
        """
        return pulumi.get(self, "endpoint_protocol")

    @endpoint_protocol.setter
    def endpoint_protocol(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "endpoint_protocol", value)

    @property
    @pulumi.getter(name="kerberosConfig")
    def kerberos_config(self) -> Optional[pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigArgs']]:
        """
        Information used to configure the Hive metastore service as a service principal in a Kerberos realm.
        Structure is documented below.
        """
        return pulumi.get(self, "kerberos_config")

    @kerberos_config.setter
    def kerberos_config(self, value: Optional[pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigArgs']]):
        pulumi.set(self, "kerberos_config", value)


if not MYPY:
    class MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgsDict(TypedDict):
        key: pulumi.Input[str]
        """
        The identifier for this object. Format specified above.
        """
        version: pulumi.Input[str]
        """
        The Hive metastore version of the auxiliary service. It must be less than the primary Hive metastore service's version.
        """
        config_overrides: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of Hive metastore configuration key-value pairs to apply to the auxiliary Hive metastore (configured in hive-site.xml) in addition to the primary version's overrides.
        If keys are present in both the auxiliary version's overrides and the primary version's overrides, the value from the auxiliary version's overrides takes precedence.
        """
elif False:
    MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceHiveMetastoreConfigAuxiliaryVersionArgs:
    def __init__(__self__, *,
                 key: pulumi.Input[str],
                 version: pulumi.Input[str],
                 config_overrides: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] key: The identifier for this object. Format specified above.
        :param pulumi.Input[str] version: The Hive metastore version of the auxiliary service. It must be less than the primary Hive metastore service's version.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_overrides: A mapping of Hive metastore configuration key-value pairs to apply to the auxiliary Hive metastore (configured in hive-site.xml) in addition to the primary version's overrides.
               If keys are present in both the auxiliary version's overrides and the primary version's overrides, the value from the auxiliary version's overrides takes precedence.
        """
        pulumi.set(__self__, "key", key)
        pulumi.set(__self__, "version", version)
        if config_overrides is not None:
            pulumi.set(__self__, "config_overrides", config_overrides)

    @property
    @pulumi.getter
    def key(self) -> pulumi.Input[str]:
        """
        The identifier for this object. Format specified above.
        """
        return pulumi.get(self, "key")

    @key.setter
    def key(self, value: pulumi.Input[str]):
        pulumi.set(self, "key", value)

    @property
    @pulumi.getter
    def version(self) -> pulumi.Input[str]:
        """
        The Hive metastore version of the auxiliary service. It must be less than the primary Hive metastore service's version.
        """
        return pulumi.get(self, "version")

    @version.setter
    def version(self, value: pulumi.Input[str]):
        pulumi.set(self, "version", value)

    @property
    @pulumi.getter(name="configOverrides")
    def config_overrides(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of Hive metastore configuration key-value pairs to apply to the auxiliary Hive metastore (configured in hive-site.xml) in addition to the primary version's overrides.
        If keys are present in both the auxiliary version's overrides and the primary version's overrides, the value from the auxiliary version's overrides takes precedence.
        """
        return pulumi.get(self, "config_overrides")

    @config_overrides.setter
    def config_overrides(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "config_overrides", value)


if not MYPY:
    class MetastoreServiceHiveMetastoreConfigKerberosConfigArgsDict(TypedDict):
        keytab: pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgsDict']
        """
        A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC).
        Structure is documented below.
        """
        krb5_config_gcs_uri: pulumi.Input[str]
        """
        A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
        """
        principal: pulumi.Input[str]
        """
        A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
        """
elif False:
    MetastoreServiceHiveMetastoreConfigKerberosConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceHiveMetastoreConfigKerberosConfigArgs:
    def __init__(__self__, *,
                 keytab: pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgs'],
                 krb5_config_gcs_uri: pulumi.Input[str],
                 principal: pulumi.Input[str]):
        """
        :param pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgs'] keytab: A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC).
               Structure is documented below.
        :param pulumi.Input[str] krb5_config_gcs_uri: A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
        :param pulumi.Input[str] principal: A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
        """
        pulumi.set(__self__, "keytab", keytab)
        pulumi.set(__self__, "krb5_config_gcs_uri", krb5_config_gcs_uri)
        pulumi.set(__self__, "principal", principal)

    @property
    @pulumi.getter
    def keytab(self) -> pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgs']:
        """
        A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC).
        Structure is documented below.
        """
        return pulumi.get(self, "keytab")

    @keytab.setter
    def keytab(self, value: pulumi.Input['MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgs']):
        pulumi.set(self, "keytab", value)

    @property
    @pulumi.getter(name="krb5ConfigGcsUri")
    def krb5_config_gcs_uri(self) -> pulumi.Input[str]:
        """
        A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
        """
        return pulumi.get(self, "krb5_config_gcs_uri")

    @krb5_config_gcs_uri.setter
    def krb5_config_gcs_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "krb5_config_gcs_uri", value)

    @property
    @pulumi.getter
    def principal(self) -> pulumi.Input[str]:
        """
        A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
        """
        return pulumi.get(self, "principal")

    @principal.setter
    def principal(self, value: pulumi.Input[str]):
        pulumi.set(self, "principal", value)


if not MYPY:
    class MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgsDict(TypedDict):
        cloud_secret: pulumi.Input[str]
        """
        The relative resource name of a Secret Manager secret version, in the following form:
        "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
        """
elif False:
    MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceHiveMetastoreConfigKerberosConfigKeytabArgs:
    def __init__(__self__, *,
                 cloud_secret: pulumi.Input[str]):
        """
        :param pulumi.Input[str] cloud_secret: The relative resource name of a Secret Manager secret version, in the following form:
               "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
        """
        pulumi.set(__self__, "cloud_secret", cloud_secret)

    @property
    @pulumi.getter(name="cloudSecret")
    def cloud_secret(self) -> pulumi.Input[str]:
        """
        The relative resource name of a Secret Manager secret version, in the following form:
        "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
        """
        return pulumi.get(self, "cloud_secret")

    @cloud_secret.setter
    def cloud_secret(self, value: pulumi.Input[str]):
        pulumi.set(self, "cloud_secret", value)


if not MYPY:
    class MetastoreServiceIamBindingConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    MetastoreServiceIamBindingConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceIamBindingConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class MetastoreServiceIamMemberConditionArgsDict(TypedDict):
        expression: pulumi.Input[str]
        title: pulumi.Input[str]
        description: NotRequired[pulumi.Input[str]]
elif False:
    MetastoreServiceIamMemberConditionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceIamMemberConditionArgs:
    def __init__(__self__, *,
                 expression: pulumi.Input[str],
                 title: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None):
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "title", title)
        if description is not None:
            pulumi.set(__self__, "description", description)

    @property
    @pulumi.getter
    def expression(self) -> pulumi.Input[str]:
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: pulumi.Input[str]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def title(self) -> pulumi.Input[str]:
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: pulumi.Input[str]):
        pulumi.set(self, "title", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)


if not MYPY:
    class MetastoreServiceMaintenanceWindowArgsDict(TypedDict):
        day_of_week: pulumi.Input[str]
        """
        The day of week, when the window starts.
        Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
        """
        hour_of_day: pulumi.Input[int]
        """
        The hour of day (0-23) when the window starts.
        """
elif False:
    MetastoreServiceMaintenanceWindowArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceMaintenanceWindowArgs:
    def __init__(__self__, *,
                 day_of_week: pulumi.Input[str],
                 hour_of_day: pulumi.Input[int]):
        """
        :param pulumi.Input[str] day_of_week: The day of week, when the window starts.
               Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
        :param pulumi.Input[int] hour_of_day: The hour of day (0-23) when the window starts.
        """
        pulumi.set(__self__, "day_of_week", day_of_week)
        pulumi.set(__self__, "hour_of_day", hour_of_day)

    @property
    @pulumi.getter(name="dayOfWeek")
    def day_of_week(self) -> pulumi.Input[str]:
        """
        The day of week, when the window starts.
        Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
        """
        return pulumi.get(self, "day_of_week")

    @day_of_week.setter
    def day_of_week(self, value: pulumi.Input[str]):
        pulumi.set(self, "day_of_week", value)

    @property
    @pulumi.getter(name="hourOfDay")
    def hour_of_day(self) -> pulumi.Input[int]:
        """
        The hour of day (0-23) when the window starts.
        """
        return pulumi.get(self, "hour_of_day")

    @hour_of_day.setter
    def hour_of_day(self, value: pulumi.Input[int]):
        pulumi.set(self, "hour_of_day", value)


if not MYPY:
    class MetastoreServiceMetadataIntegrationArgsDict(TypedDict):
        data_catalog_config: pulumi.Input['MetastoreServiceMetadataIntegrationDataCatalogConfigArgsDict']
        """
        The integration config for the Data Catalog service.
        Structure is documented below.
        """
elif False:
    MetastoreServiceMetadataIntegrationArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceMetadataIntegrationArgs:
    def __init__(__self__, *,
                 data_catalog_config: pulumi.Input['MetastoreServiceMetadataIntegrationDataCatalogConfigArgs']):
        """
        :param pulumi.Input['MetastoreServiceMetadataIntegrationDataCatalogConfigArgs'] data_catalog_config: The integration config for the Data Catalog service.
               Structure is documented below.
        """
        pulumi.set(__self__, "data_catalog_config", data_catalog_config)

    @property
    @pulumi.getter(name="dataCatalogConfig")
    def data_catalog_config(self) -> pulumi.Input['MetastoreServiceMetadataIntegrationDataCatalogConfigArgs']:
        """
        The integration config for the Data Catalog service.
        Structure is documented below.
        """
        return pulumi.get(self, "data_catalog_config")

    @data_catalog_config.setter
    def data_catalog_config(self, value: pulumi.Input['MetastoreServiceMetadataIntegrationDataCatalogConfigArgs']):
        pulumi.set(self, "data_catalog_config", value)


if not MYPY:
    class MetastoreServiceMetadataIntegrationDataCatalogConfigArgsDict(TypedDict):
        enabled: pulumi.Input[bool]
        """
        Defines whether the metastore metadata should be synced to Data Catalog. The default value is to disable syncing metastore metadata to Data Catalog.
        """
elif False:
    MetastoreServiceMetadataIntegrationDataCatalogConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceMetadataIntegrationDataCatalogConfigArgs:
    def __init__(__self__, *,
                 enabled: pulumi.Input[bool]):
        """
        :param pulumi.Input[bool] enabled: Defines whether the metastore metadata should be synced to Data Catalog. The default value is to disable syncing metastore metadata to Data Catalog.
        """
        pulumi.set(__self__, "enabled", enabled)

    @property
    @pulumi.getter
    def enabled(self) -> pulumi.Input[bool]:
        """
        Defines whether the metastore metadata should be synced to Data Catalog. The default value is to disable syncing metastore metadata to Data Catalog.
        """
        return pulumi.get(self, "enabled")

    @enabled.setter
    def enabled(self, value: pulumi.Input[bool]):
        pulumi.set(self, "enabled", value)


if not MYPY:
    class MetastoreServiceNetworkConfigArgsDict(TypedDict):
        consumers: pulumi.Input[Sequence[pulumi.Input['MetastoreServiceNetworkConfigConsumerArgsDict']]]
        """
        The consumer-side network configuration for the Dataproc Metastore instance.
        Structure is documented below.
        """
        custom_routes_enabled: NotRequired[pulumi.Input[bool]]
        """
        Enables custom routes to be imported and exported for the Dataproc Metastore service's peered VPC network.
        """
elif False:
    MetastoreServiceNetworkConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceNetworkConfigArgs:
    def __init__(__self__, *,
                 consumers: pulumi.Input[Sequence[pulumi.Input['MetastoreServiceNetworkConfigConsumerArgs']]],
                 custom_routes_enabled: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['MetastoreServiceNetworkConfigConsumerArgs']]] consumers: The consumer-side network configuration for the Dataproc Metastore instance.
               Structure is documented below.
        :param pulumi.Input[bool] custom_routes_enabled: Enables custom routes to be imported and exported for the Dataproc Metastore service's peered VPC network.
        """
        pulumi.set(__self__, "consumers", consumers)
        if custom_routes_enabled is not None:
            pulumi.set(__self__, "custom_routes_enabled", custom_routes_enabled)

    @property
    @pulumi.getter
    def consumers(self) -> pulumi.Input[Sequence[pulumi.Input['MetastoreServiceNetworkConfigConsumerArgs']]]:
        """
        The consumer-side network configuration for the Dataproc Metastore instance.
        Structure is documented below.
        """
        return pulumi.get(self, "consumers")

    @consumers.setter
    def consumers(self, value: pulumi.Input[Sequence[pulumi.Input['MetastoreServiceNetworkConfigConsumerArgs']]]):
        pulumi.set(self, "consumers", value)

    @property
    @pulumi.getter(name="customRoutesEnabled")
    def custom_routes_enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Enables custom routes to be imported and exported for the Dataproc Metastore service's peered VPC network.
        """
        return pulumi.get(self, "custom_routes_enabled")

    @custom_routes_enabled.setter
    def custom_routes_enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "custom_routes_enabled", value)


if not MYPY:
    class MetastoreServiceNetworkConfigConsumerArgsDict(TypedDict):
        subnetwork: pulumi.Input[str]
        """
        The subnetwork of the customer project from which an IP address is reserved and used as the Dataproc Metastore service's endpoint.
        It is accessible to hosts in the subnet and to all hosts in a subnet in the same region and same network.
        There must be at least one IP address available in the subnet's primary range. The subnet is specified in the following form:
        `projects/{projectNumber}/regions/{region_id}/subnetworks/{subnetwork_id}
        """
        endpoint_uri: NotRequired[pulumi.Input[str]]
        """
        (Output)
        The URI of the endpoint used to access the metastore service.
        """
elif False:
    MetastoreServiceNetworkConfigConsumerArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceNetworkConfigConsumerArgs:
    def __init__(__self__, *,
                 subnetwork: pulumi.Input[str],
                 endpoint_uri: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] subnetwork: The subnetwork of the customer project from which an IP address is reserved and used as the Dataproc Metastore service's endpoint.
               It is accessible to hosts in the subnet and to all hosts in a subnet in the same region and same network.
               There must be at least one IP address available in the subnet's primary range. The subnet is specified in the following form:
               `projects/{projectNumber}/regions/{region_id}/subnetworks/{subnetwork_id}
        :param pulumi.Input[str] endpoint_uri: (Output)
               The URI of the endpoint used to access the metastore service.
        """
        pulumi.set(__self__, "subnetwork", subnetwork)
        if endpoint_uri is not None:
            pulumi.set(__self__, "endpoint_uri", endpoint_uri)

    @property
    @pulumi.getter
    def subnetwork(self) -> pulumi.Input[str]:
        """
        The subnetwork of the customer project from which an IP address is reserved and used as the Dataproc Metastore service's endpoint.
        It is accessible to hosts in the subnet and to all hosts in a subnet in the same region and same network.
        There must be at least one IP address available in the subnet's primary range. The subnet is specified in the following form:
        `projects/{projectNumber}/regions/{region_id}/subnetworks/{subnetwork_id}
        """
        return pulumi.get(self, "subnetwork")

    @subnetwork.setter
    def subnetwork(self, value: pulumi.Input[str]):
        pulumi.set(self, "subnetwork", value)

    @property
    @pulumi.getter(name="endpointUri")
    def endpoint_uri(self) -> Optional[pulumi.Input[str]]:
        """
        (Output)
        The URI of the endpoint used to access the metastore service.
        """
        return pulumi.get(self, "endpoint_uri")

    @endpoint_uri.setter
    def endpoint_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "endpoint_uri", value)


if not MYPY:
    class MetastoreServiceScalingConfigArgsDict(TypedDict):
        autoscaling_config: NotRequired[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigArgsDict']]
        """
        Represents the autoscaling configuration of a metastore service.
        Structure is documented below.
        """
        instance_size: NotRequired[pulumi.Input[str]]
        """
        Metastore instance sizes.
        Possible values are: `EXTRA_SMALL`, `SMALL`, `MEDIUM`, `LARGE`, `EXTRA_LARGE`.
        """
        scaling_factor: NotRequired[pulumi.Input[float]]
        """
        Scaling factor, in increments of 0.1 for values less than 1.0, and increments of 1.0 for values greater than 1.0.
        """
elif False:
    MetastoreServiceScalingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceScalingConfigArgs:
    def __init__(__self__, *,
                 autoscaling_config: Optional[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigArgs']] = None,
                 instance_size: Optional[pulumi.Input[str]] = None,
                 scaling_factor: Optional[pulumi.Input[float]] = None):
        """
        :param pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigArgs'] autoscaling_config: Represents the autoscaling configuration of a metastore service.
               Structure is documented below.
        :param pulumi.Input[str] instance_size: Metastore instance sizes.
               Possible values are: `EXTRA_SMALL`, `SMALL`, `MEDIUM`, `LARGE`, `EXTRA_LARGE`.
        :param pulumi.Input[float] scaling_factor: Scaling factor, in increments of 0.1 for values less than 1.0, and increments of 1.0 for values greater than 1.0.
        """
        if autoscaling_config is not None:
            pulumi.set(__self__, "autoscaling_config", autoscaling_config)
        if instance_size is not None:
            pulumi.set(__self__, "instance_size", instance_size)
        if scaling_factor is not None:
            pulumi.set(__self__, "scaling_factor", scaling_factor)

    @property
    @pulumi.getter(name="autoscalingConfig")
    def autoscaling_config(self) -> Optional[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigArgs']]:
        """
        Represents the autoscaling configuration of a metastore service.
        Structure is documented below.
        """
        return pulumi.get(self, "autoscaling_config")

    @autoscaling_config.setter
    def autoscaling_config(self, value: Optional[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigArgs']]):
        pulumi.set(self, "autoscaling_config", value)

    @property
    @pulumi.getter(name="instanceSize")
    def instance_size(self) -> Optional[pulumi.Input[str]]:
        """
        Metastore instance sizes.
        Possible values are: `EXTRA_SMALL`, `SMALL`, `MEDIUM`, `LARGE`, `EXTRA_LARGE`.
        """
        return pulumi.get(self, "instance_size")

    @instance_size.setter
    def instance_size(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_size", value)

    @property
    @pulumi.getter(name="scalingFactor")
    def scaling_factor(self) -> Optional[pulumi.Input[float]]:
        """
        Scaling factor, in increments of 0.1 for values less than 1.0, and increments of 1.0 for values greater than 1.0.
        """
        return pulumi.get(self, "scaling_factor")

    @scaling_factor.setter
    def scaling_factor(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "scaling_factor", value)


if not MYPY:
    class MetastoreServiceScalingConfigAutoscalingConfigArgsDict(TypedDict):
        autoscaling_enabled: NotRequired[pulumi.Input[bool]]
        """
        Defines whether autoscaling is enabled. The default value is false.
        """
        limit_config: NotRequired[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgsDict']]
        """
        Represents the limit configuration of a metastore service.
        Structure is documented below.
        """
elif False:
    MetastoreServiceScalingConfigAutoscalingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceScalingConfigAutoscalingConfigArgs:
    def __init__(__self__, *,
                 autoscaling_enabled: Optional[pulumi.Input[bool]] = None,
                 limit_config: Optional[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgs']] = None):
        """
        :param pulumi.Input[bool] autoscaling_enabled: Defines whether autoscaling is enabled. The default value is false.
        :param pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgs'] limit_config: Represents the limit configuration of a metastore service.
               Structure is documented below.
        """
        if autoscaling_enabled is not None:
            pulumi.set(__self__, "autoscaling_enabled", autoscaling_enabled)
        if limit_config is not None:
            pulumi.set(__self__, "limit_config", limit_config)

    @property
    @pulumi.getter(name="autoscalingEnabled")
    def autoscaling_enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether autoscaling is enabled. The default value is false.
        """
        return pulumi.get(self, "autoscaling_enabled")

    @autoscaling_enabled.setter
    def autoscaling_enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "autoscaling_enabled", value)

    @property
    @pulumi.getter(name="limitConfig")
    def limit_config(self) -> Optional[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgs']]:
        """
        Represents the limit configuration of a metastore service.
        Structure is documented below.
        """
        return pulumi.get(self, "limit_config")

    @limit_config.setter
    def limit_config(self, value: Optional[pulumi.Input['MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgs']]):
        pulumi.set(self, "limit_config", value)


if not MYPY:
    class MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgsDict(TypedDict):
        max_scaling_factor: NotRequired[pulumi.Input[float]]
        """
        The maximum scaling factor that the service will autoscale to. The default value is 6.0.
        """
        min_scaling_factor: NotRequired[pulumi.Input[float]]
        """
        The minimum scaling factor that the service will autoscale to. The default value is 0.1.
        """
elif False:
    MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceScalingConfigAutoscalingConfigLimitConfigArgs:
    def __init__(__self__, *,
                 max_scaling_factor: Optional[pulumi.Input[float]] = None,
                 min_scaling_factor: Optional[pulumi.Input[float]] = None):
        """
        :param pulumi.Input[float] max_scaling_factor: The maximum scaling factor that the service will autoscale to. The default value is 6.0.
        :param pulumi.Input[float] min_scaling_factor: The minimum scaling factor that the service will autoscale to. The default value is 0.1.
        """
        if max_scaling_factor is not None:
            pulumi.set(__self__, "max_scaling_factor", max_scaling_factor)
        if min_scaling_factor is not None:
            pulumi.set(__self__, "min_scaling_factor", min_scaling_factor)

    @property
    @pulumi.getter(name="maxScalingFactor")
    def max_scaling_factor(self) -> Optional[pulumi.Input[float]]:
        """
        The maximum scaling factor that the service will autoscale to. The default value is 6.0.
        """
        return pulumi.get(self, "max_scaling_factor")

    @max_scaling_factor.setter
    def max_scaling_factor(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "max_scaling_factor", value)

    @property
    @pulumi.getter(name="minScalingFactor")
    def min_scaling_factor(self) -> Optional[pulumi.Input[float]]:
        """
        The minimum scaling factor that the service will autoscale to. The default value is 0.1.
        """
        return pulumi.get(self, "min_scaling_factor")

    @min_scaling_factor.setter
    def min_scaling_factor(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "min_scaling_factor", value)


if not MYPY:
    class MetastoreServiceScheduledBackupArgsDict(TypedDict):
        backup_location: pulumi.Input[str]
        """
        A Cloud Storage URI of a folder, in the format gs://<bucket_name>/<path_inside_bucket>. A sub-folder <backup_folder> containing backup files will be stored below it.
        """
        cron_schedule: NotRequired[pulumi.Input[str]]
        """
        The scheduled interval in Cron format, see https://en.wikipedia.org/wiki/Cron The default is empty: scheduled backup is not enabled. Must be specified to enable scheduled backups.
        """
        enabled: NotRequired[pulumi.Input[bool]]
        """
        Defines whether the scheduled backup is enabled. The default value is false.
        """
        time_zone: NotRequired[pulumi.Input[str]]
        """
        Specifies the time zone to be used when interpreting cronSchedule. Must be a time zone name from the time zone database (https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), e.g. America/Los_Angeles or Africa/Abidjan. If left unspecified, the default is UTC.
        """
elif False:
    MetastoreServiceScheduledBackupArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceScheduledBackupArgs:
    def __init__(__self__, *,
                 backup_location: pulumi.Input[str],
                 cron_schedule: Optional[pulumi.Input[str]] = None,
                 enabled: Optional[pulumi.Input[bool]] = None,
                 time_zone: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] backup_location: A Cloud Storage URI of a folder, in the format gs://<bucket_name>/<path_inside_bucket>. A sub-folder <backup_folder> containing backup files will be stored below it.
        :param pulumi.Input[str] cron_schedule: The scheduled interval in Cron format, see https://en.wikipedia.org/wiki/Cron The default is empty: scheduled backup is not enabled. Must be specified to enable scheduled backups.
        :param pulumi.Input[bool] enabled: Defines whether the scheduled backup is enabled. The default value is false.
        :param pulumi.Input[str] time_zone: Specifies the time zone to be used when interpreting cronSchedule. Must be a time zone name from the time zone database (https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), e.g. America/Los_Angeles or Africa/Abidjan. If left unspecified, the default is UTC.
        """
        pulumi.set(__self__, "backup_location", backup_location)
        if cron_schedule is not None:
            pulumi.set(__self__, "cron_schedule", cron_schedule)
        if enabled is not None:
            pulumi.set(__self__, "enabled", enabled)
        if time_zone is not None:
            pulumi.set(__self__, "time_zone", time_zone)

    @property
    @pulumi.getter(name="backupLocation")
    def backup_location(self) -> pulumi.Input[str]:
        """
        A Cloud Storage URI of a folder, in the format gs://<bucket_name>/<path_inside_bucket>. A sub-folder <backup_folder> containing backup files will be stored below it.
        """
        return pulumi.get(self, "backup_location")

    @backup_location.setter
    def backup_location(self, value: pulumi.Input[str]):
        pulumi.set(self, "backup_location", value)

    @property
    @pulumi.getter(name="cronSchedule")
    def cron_schedule(self) -> Optional[pulumi.Input[str]]:
        """
        The scheduled interval in Cron format, see https://en.wikipedia.org/wiki/Cron The default is empty: scheduled backup is not enabled. Must be specified to enable scheduled backups.
        """
        return pulumi.get(self, "cron_schedule")

    @cron_schedule.setter
    def cron_schedule(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cron_schedule", value)

    @property
    @pulumi.getter
    def enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether the scheduled backup is enabled. The default value is false.
        """
        return pulumi.get(self, "enabled")

    @enabled.setter
    def enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enabled", value)

    @property
    @pulumi.getter(name="timeZone")
    def time_zone(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the time zone to be used when interpreting cronSchedule. Must be a time zone name from the time zone database (https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), e.g. America/Los_Angeles or Africa/Abidjan. If left unspecified, the default is UTC.
        """
        return pulumi.get(self, "time_zone")

    @time_zone.setter
    def time_zone(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "time_zone", value)


if not MYPY:
    class MetastoreServiceTelemetryConfigArgsDict(TypedDict):
        log_format: NotRequired[pulumi.Input[str]]
        """
        The output format of the Dataproc Metastore service's logs.
        Default value is `JSON`.
        Possible values are: `LEGACY`, `JSON`.
        """
elif False:
    MetastoreServiceTelemetryConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class MetastoreServiceTelemetryConfigArgs:
    def __init__(__self__, *,
                 log_format: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] log_format: The output format of the Dataproc Metastore service's logs.
               Default value is `JSON`.
               Possible values are: `LEGACY`, `JSON`.
        """
        if log_format is not None:
            pulumi.set(__self__, "log_format", log_format)

    @property
    @pulumi.getter(name="logFormat")
    def log_format(self) -> Optional[pulumi.Input[str]]:
        """
        The output format of the Dataproc Metastore service's logs.
        Default value is `JSON`.
        Possible values are: `LEGACY`, `JSON`.
        """
        return pulumi.get(self, "log_format")

    @log_format.setter
    def log_format(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "log_format", value)


if not MYPY:
    class WorkflowTemplateJobArgsDict(TypedDict):
        step_id: pulumi.Input[str]
        """
        Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job `goog-dataproc-workflow-step-id` label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
        """
        hadoop_job: NotRequired[pulumi.Input['WorkflowTemplateJobHadoopJobArgsDict']]
        """
        Job is a Hadoop job.
        """
        hive_job: NotRequired[pulumi.Input['WorkflowTemplateJobHiveJobArgsDict']]
        """
        Job is a Hive job.
        """
        labels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
        """
        pig_job: NotRequired[pulumi.Input['WorkflowTemplateJobPigJobArgsDict']]
        """
        Job is a Pig job.
        """
        prerequisite_step_ids: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
        """
        presto_job: NotRequired[pulumi.Input['WorkflowTemplateJobPrestoJobArgsDict']]
        """
        Job is a Presto job.
        """
        pyspark_job: NotRequired[pulumi.Input['WorkflowTemplateJobPysparkJobArgsDict']]
        """
        Job is a PySpark job.
        """
        scheduling: NotRequired[pulumi.Input['WorkflowTemplateJobSchedulingArgsDict']]
        """
        Job scheduling configuration.
        """
        spark_job: NotRequired[pulumi.Input['WorkflowTemplateJobSparkJobArgsDict']]
        """
        Job is a Spark job.
        """
        spark_r_job: NotRequired[pulumi.Input['WorkflowTemplateJobSparkRJobArgsDict']]
        """
        Job is a SparkR job.
        """
        spark_sql_job: NotRequired[pulumi.Input['WorkflowTemplateJobSparkSqlJobArgsDict']]
        """
        Job is a SparkSql job.
        """
elif False:
    WorkflowTemplateJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobArgs:
    def __init__(__self__, *,
                 step_id: pulumi.Input[str],
                 hadoop_job: Optional[pulumi.Input['WorkflowTemplateJobHadoopJobArgs']] = None,
                 hive_job: Optional[pulumi.Input['WorkflowTemplateJobHiveJobArgs']] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 pig_job: Optional[pulumi.Input['WorkflowTemplateJobPigJobArgs']] = None,
                 prerequisite_step_ids: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 presto_job: Optional[pulumi.Input['WorkflowTemplateJobPrestoJobArgs']] = None,
                 pyspark_job: Optional[pulumi.Input['WorkflowTemplateJobPysparkJobArgs']] = None,
                 scheduling: Optional[pulumi.Input['WorkflowTemplateJobSchedulingArgs']] = None,
                 spark_job: Optional[pulumi.Input['WorkflowTemplateJobSparkJobArgs']] = None,
                 spark_r_job: Optional[pulumi.Input['WorkflowTemplateJobSparkRJobArgs']] = None,
                 spark_sql_job: Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobArgs']] = None):
        """
        :param pulumi.Input[str] step_id: Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job `goog-dataproc-workflow-step-id` label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
        :param pulumi.Input['WorkflowTemplateJobHadoopJobArgs'] hadoop_job: Job is a Hadoop job.
        :param pulumi.Input['WorkflowTemplateJobHiveJobArgs'] hive_job: Job is a Hive job.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
        :param pulumi.Input['WorkflowTemplateJobPigJobArgs'] pig_job: Job is a Pig job.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] prerequisite_step_ids: The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
        :param pulumi.Input['WorkflowTemplateJobPrestoJobArgs'] presto_job: Job is a Presto job.
        :param pulumi.Input['WorkflowTemplateJobPysparkJobArgs'] pyspark_job: Job is a PySpark job.
        :param pulumi.Input['WorkflowTemplateJobSchedulingArgs'] scheduling: Job scheduling configuration.
        :param pulumi.Input['WorkflowTemplateJobSparkJobArgs'] spark_job: Job is a Spark job.
        :param pulumi.Input['WorkflowTemplateJobSparkRJobArgs'] spark_r_job: Job is a SparkR job.
        :param pulumi.Input['WorkflowTemplateJobSparkSqlJobArgs'] spark_sql_job: Job is a SparkSql job.
        """
        pulumi.set(__self__, "step_id", step_id)
        if hadoop_job is not None:
            pulumi.set(__self__, "hadoop_job", hadoop_job)
        if hive_job is not None:
            pulumi.set(__self__, "hive_job", hive_job)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)
        if pig_job is not None:
            pulumi.set(__self__, "pig_job", pig_job)
        if prerequisite_step_ids is not None:
            pulumi.set(__self__, "prerequisite_step_ids", prerequisite_step_ids)
        if presto_job is not None:
            pulumi.set(__self__, "presto_job", presto_job)
        if pyspark_job is not None:
            pulumi.set(__self__, "pyspark_job", pyspark_job)
        if scheduling is not None:
            pulumi.set(__self__, "scheduling", scheduling)
        if spark_job is not None:
            pulumi.set(__self__, "spark_job", spark_job)
        if spark_r_job is not None:
            pulumi.set(__self__, "spark_r_job", spark_r_job)
        if spark_sql_job is not None:
            pulumi.set(__self__, "spark_sql_job", spark_sql_job)

    @property
    @pulumi.getter(name="stepId")
    def step_id(self) -> pulumi.Input[str]:
        """
        Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job `goog-dataproc-workflow-step-id` label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
        """
        return pulumi.get(self, "step_id")

    @step_id.setter
    def step_id(self, value: pulumi.Input[str]):
        pulumi.set(self, "step_id", value)

    @property
    @pulumi.getter(name="hadoopJob")
    def hadoop_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobHadoopJobArgs']]:
        """
        Job is a Hadoop job.
        """
        return pulumi.get(self, "hadoop_job")

    @hadoop_job.setter
    def hadoop_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobHadoopJobArgs']]):
        pulumi.set(self, "hadoop_job", value)

    @property
    @pulumi.getter(name="hiveJob")
    def hive_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobHiveJobArgs']]:
        """
        Job is a Hive job.
        """
        return pulumi.get(self, "hive_job")

    @hive_job.setter
    def hive_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobHiveJobArgs']]):
        pulumi.set(self, "hive_job", value)

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
        """
        return pulumi.get(self, "labels")

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "labels", value)

    @property
    @pulumi.getter(name="pigJob")
    def pig_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobPigJobArgs']]:
        """
        Job is a Pig job.
        """
        return pulumi.get(self, "pig_job")

    @pig_job.setter
    def pig_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobPigJobArgs']]):
        pulumi.set(self, "pig_job", value)

    @property
    @pulumi.getter(name="prerequisiteStepIds")
    def prerequisite_step_ids(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
        """
        return pulumi.get(self, "prerequisite_step_ids")

    @prerequisite_step_ids.setter
    def prerequisite_step_ids(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "prerequisite_step_ids", value)

    @property
    @pulumi.getter(name="prestoJob")
    def presto_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobPrestoJobArgs']]:
        """
        Job is a Presto job.
        """
        return pulumi.get(self, "presto_job")

    @presto_job.setter
    def presto_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobPrestoJobArgs']]):
        pulumi.set(self, "presto_job", value)

    @property
    @pulumi.getter(name="pysparkJob")
    def pyspark_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobPysparkJobArgs']]:
        """
        Job is a PySpark job.
        """
        return pulumi.get(self, "pyspark_job")

    @pyspark_job.setter
    def pyspark_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobPysparkJobArgs']]):
        pulumi.set(self, "pyspark_job", value)

    @property
    @pulumi.getter
    def scheduling(self) -> Optional[pulumi.Input['WorkflowTemplateJobSchedulingArgs']]:
        """
        Job scheduling configuration.
        """
        return pulumi.get(self, "scheduling")

    @scheduling.setter
    def scheduling(self, value: Optional[pulumi.Input['WorkflowTemplateJobSchedulingArgs']]):
        pulumi.set(self, "scheduling", value)

    @property
    @pulumi.getter(name="sparkJob")
    def spark_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobSparkJobArgs']]:
        """
        Job is a Spark job.
        """
        return pulumi.get(self, "spark_job")

    @spark_job.setter
    def spark_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobSparkJobArgs']]):
        pulumi.set(self, "spark_job", value)

    @property
    @pulumi.getter(name="sparkRJob")
    def spark_r_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobSparkRJobArgs']]:
        """
        Job is a SparkR job.
        """
        return pulumi.get(self, "spark_r_job")

    @spark_r_job.setter
    def spark_r_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobSparkRJobArgs']]):
        pulumi.set(self, "spark_r_job", value)

    @property
    @pulumi.getter(name="sparkSqlJob")
    def spark_sql_job(self) -> Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobArgs']]:
        """
        Job is a SparkSql job.
        """
        return pulumi.get(self, "spark_sql_job")

    @spark_sql_job.setter
    def spark_sql_job(self, value: Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobArgs']]):
        pulumi.set(self, "spark_sql_job", value)


if not MYPY:
    class WorkflowTemplateJobHadoopJobArgsDict(TypedDict):
        archive_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
        """
        args: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The arguments to pass to the driver. Do not include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
        """
        logging_config: NotRequired[pulumi.Input['WorkflowTemplateJobHadoopJobLoggingConfigArgsDict']]
        """
        The runtime log config for job execution.
        """
        main_class: NotRequired[pulumi.Input[str]]
        """
        The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`.
        """
        main_jar_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
        """
elif False:
    WorkflowTemplateJobHadoopJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobHadoopJobArgs:
    def __init__(__self__, *,
                 archive_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['WorkflowTemplateJobHadoopJobLoggingConfigArgs']] = None,
                 main_class: Optional[pulumi.Input[str]] = None,
                 main_jar_file_uri: Optional[pulumi.Input[str]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] file_uris: HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
        :param pulumi.Input['WorkflowTemplateJobHadoopJobLoggingConfigArgs'] logging_config: The runtime log config for job execution.
        :param pulumi.Input[str] main_class: The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`.
        :param pulumi.Input[str] main_jar_file_uri: The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
        """
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if main_class is not None:
            pulumi.set(__self__, "main_class", main_class)
        if main_jar_file_uri is not None:
            pulumi.set(__self__, "main_jar_file_uri", main_jar_file_uri)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
        """
        return pulumi.get(self, "archive_uris")

    @archive_uris.setter
    def archive_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "archive_uris", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to pass to the driver. Do not include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
        """
        return pulumi.get(self, "file_uris")

    @file_uris.setter
    def file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "file_uris", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['WorkflowTemplateJobHadoopJobLoggingConfigArgs']]:
        """
        The runtime log config for job execution.
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['WorkflowTemplateJobHadoopJobLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter(name="mainClass")
    def main_class(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`.
        """
        return pulumi.get(self, "main_class")

    @main_class.setter
    def main_class(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_class", value)

    @property
    @pulumi.getter(name="mainJarFileUri")
    def main_jar_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
        """
        return pulumi.get(self, "main_jar_file_uri")

    @main_jar_file_uri.setter
    def main_jar_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_jar_file_uri", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class WorkflowTemplateJobHadoopJobLoggingConfigArgsDict(TypedDict):
        driver_log_levels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    WorkflowTemplateJobHadoopJobLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobHadoopJobLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if driver_log_levels is not None:
            pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class WorkflowTemplateJobHiveJobArgsDict(TypedDict):
        continue_on_failure: NotRequired[pulumi.Input[bool]]
        """
        Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the script that contains Hive queries.
        """
        query_list: NotRequired[pulumi.Input['WorkflowTemplateJobHiveJobQueryListArgsDict']]
        """
        A list of queries.
        """
        script_variables: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
        """
elif False:
    WorkflowTemplateJobHiveJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobHiveJobArgs:
    def __init__(__self__, *,
                 continue_on_failure: Optional[pulumi.Input[bool]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_list: Optional[pulumi.Input['WorkflowTemplateJobHiveJobQueryListArgs']] = None,
                 script_variables: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains Hive queries.
        :param pulumi.Input['WorkflowTemplateJobHiveJobQueryListArgs'] query_list: A list of queries.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
        """
        if continue_on_failure is not None:
            pulumi.set(__self__, "continue_on_failure", continue_on_failure)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_list is not None:
            pulumi.set(__self__, "query_list", query_list)
        if script_variables is not None:
            pulumi.set(__self__, "script_variables", script_variables)

    @property
    @pulumi.getter(name="continueOnFailure")
    def continue_on_failure(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        """
        return pulumi.get(self, "continue_on_failure")

    @continue_on_failure.setter
    def continue_on_failure(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "continue_on_failure", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the script that contains Hive queries.
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryList")
    def query_list(self) -> Optional[pulumi.Input['WorkflowTemplateJobHiveJobQueryListArgs']]:
        """
        A list of queries.
        """
        return pulumi.get(self, "query_list")

    @query_list.setter
    def query_list(self, value: Optional[pulumi.Input['WorkflowTemplateJobHiveJobQueryListArgs']]):
        pulumi.set(self, "query_list", value)

    @property
    @pulumi.getter(name="scriptVariables")
    def script_variables(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
        """
        return pulumi.get(self, "script_variables")

    @script_variables.setter
    def script_variables(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "script_variables", value)


if not MYPY:
    class WorkflowTemplateJobHiveJobQueryListArgsDict(TypedDict):
        queries: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
elif False:
    WorkflowTemplateJobHiveJobQueryListArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobHiveJobQueryListArgs:
    def __init__(__self__, *,
                 queries: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] queries: Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        pulumi.set(__self__, "queries", queries)

    @property
    @pulumi.getter
    def queries(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        return pulumi.get(self, "queries")

    @queries.setter
    def queries(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "queries", value)


if not MYPY:
    class WorkflowTemplateJobPigJobArgsDict(TypedDict):
        continue_on_failure: NotRequired[pulumi.Input[bool]]
        """
        Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
        """
        logging_config: NotRequired[pulumi.Input['WorkflowTemplateJobPigJobLoggingConfigArgsDict']]
        """
        The runtime log config for job execution.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the script that contains the Pig queries.
        """
        query_list: NotRequired[pulumi.Input['WorkflowTemplateJobPigJobQueryListArgsDict']]
        """
        A list of queries.
        """
        script_variables: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        Mapping of query variable names to values (equivalent to the Pig command: `name=`).
        """
elif False:
    WorkflowTemplateJobPigJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPigJobArgs:
    def __init__(__self__, *,
                 continue_on_failure: Optional[pulumi.Input[bool]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['WorkflowTemplateJobPigJobLoggingConfigArgs']] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_list: Optional[pulumi.Input['WorkflowTemplateJobPigJobQueryListArgs']] = None,
                 script_variables: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
        :param pulumi.Input['WorkflowTemplateJobPigJobLoggingConfigArgs'] logging_config: The runtime log config for job execution.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains the Pig queries.
        :param pulumi.Input['WorkflowTemplateJobPigJobQueryListArgs'] query_list: A list of queries.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Pig command: `name=`).
        """
        if continue_on_failure is not None:
            pulumi.set(__self__, "continue_on_failure", continue_on_failure)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_list is not None:
            pulumi.set(__self__, "query_list", query_list)
        if script_variables is not None:
            pulumi.set(__self__, "script_variables", script_variables)

    @property
    @pulumi.getter(name="continueOnFailure")
    def continue_on_failure(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        """
        return pulumi.get(self, "continue_on_failure")

    @continue_on_failure.setter
    def continue_on_failure(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "continue_on_failure", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['WorkflowTemplateJobPigJobLoggingConfigArgs']]:
        """
        The runtime log config for job execution.
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['WorkflowTemplateJobPigJobLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the script that contains the Pig queries.
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryList")
    def query_list(self) -> Optional[pulumi.Input['WorkflowTemplateJobPigJobQueryListArgs']]:
        """
        A list of queries.
        """
        return pulumi.get(self, "query_list")

    @query_list.setter
    def query_list(self, value: Optional[pulumi.Input['WorkflowTemplateJobPigJobQueryListArgs']]):
        pulumi.set(self, "query_list", value)

    @property
    @pulumi.getter(name="scriptVariables")
    def script_variables(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Mapping of query variable names to values (equivalent to the Pig command: `name=`).
        """
        return pulumi.get(self, "script_variables")

    @script_variables.setter
    def script_variables(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "script_variables", value)


if not MYPY:
    class WorkflowTemplateJobPigJobLoggingConfigArgsDict(TypedDict):
        driver_log_levels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    WorkflowTemplateJobPigJobLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPigJobLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if driver_log_levels is not None:
            pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class WorkflowTemplateJobPigJobQueryListArgsDict(TypedDict):
        queries: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
elif False:
    WorkflowTemplateJobPigJobQueryListArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPigJobQueryListArgs:
    def __init__(__self__, *,
                 queries: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] queries: Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        pulumi.set(__self__, "queries", queries)

    @property
    @pulumi.getter
    def queries(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        return pulumi.get(self, "queries")

    @queries.setter
    def queries(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "queries", value)


if not MYPY:
    class WorkflowTemplateJobPrestoJobArgsDict(TypedDict):
        client_tags: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Presto client tags to attach to this query
        """
        continue_on_failure: NotRequired[pulumi.Input[bool]]
        """
        Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        """
        logging_config: NotRequired[pulumi.Input['WorkflowTemplateJobPrestoJobLoggingConfigArgsDict']]
        """
        The runtime log config for job execution.
        """
        output_format: NotRequired[pulumi.Input[str]]
        """
        The format in which query output will be displayed. See the Presto documentation for supported output formats
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values. Used to set Presto (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the script that contains SQL queries.
        """
        query_list: NotRequired[pulumi.Input['WorkflowTemplateJobPrestoJobQueryListArgsDict']]
        """
        A list of queries.
        """
elif False:
    WorkflowTemplateJobPrestoJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPrestoJobArgs:
    def __init__(__self__, *,
                 client_tags: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 continue_on_failure: Optional[pulumi.Input[bool]] = None,
                 logging_config: Optional[pulumi.Input['WorkflowTemplateJobPrestoJobLoggingConfigArgs']] = None,
                 output_format: Optional[pulumi.Input[str]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_list: Optional[pulumi.Input['WorkflowTemplateJobPrestoJobQueryListArgs']] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] client_tags: Presto client tags to attach to this query
        :param pulumi.Input[bool] continue_on_failure: Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        :param pulumi.Input['WorkflowTemplateJobPrestoJobLoggingConfigArgs'] logging_config: The runtime log config for job execution.
        :param pulumi.Input[str] output_format: The format in which query output will be displayed. See the Presto documentation for supported output formats
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values. Used to set Presto (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains SQL queries.
        :param pulumi.Input['WorkflowTemplateJobPrestoJobQueryListArgs'] query_list: A list of queries.
        """
        if client_tags is not None:
            pulumi.set(__self__, "client_tags", client_tags)
        if continue_on_failure is not None:
            pulumi.set(__self__, "continue_on_failure", continue_on_failure)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if output_format is not None:
            pulumi.set(__self__, "output_format", output_format)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_list is not None:
            pulumi.set(__self__, "query_list", query_list)

    @property
    @pulumi.getter(name="clientTags")
    def client_tags(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Presto client tags to attach to this query
        """
        return pulumi.get(self, "client_tags")

    @client_tags.setter
    def client_tags(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "client_tags", value)

    @property
    @pulumi.getter(name="continueOnFailure")
    def continue_on_failure(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
        """
        return pulumi.get(self, "continue_on_failure")

    @continue_on_failure.setter
    def continue_on_failure(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "continue_on_failure", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['WorkflowTemplateJobPrestoJobLoggingConfigArgs']]:
        """
        The runtime log config for job execution.
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['WorkflowTemplateJobPrestoJobLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter(name="outputFormat")
    def output_format(self) -> Optional[pulumi.Input[str]]:
        """
        The format in which query output will be displayed. See the Presto documentation for supported output formats
        """
        return pulumi.get(self, "output_format")

    @output_format.setter
    def output_format(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "output_format", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values. Used to set Presto (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the script that contains SQL queries.
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryList")
    def query_list(self) -> Optional[pulumi.Input['WorkflowTemplateJobPrestoJobQueryListArgs']]:
        """
        A list of queries.
        """
        return pulumi.get(self, "query_list")

    @query_list.setter
    def query_list(self, value: Optional[pulumi.Input['WorkflowTemplateJobPrestoJobQueryListArgs']]):
        pulumi.set(self, "query_list", value)


if not MYPY:
    class WorkflowTemplateJobPrestoJobLoggingConfigArgsDict(TypedDict):
        driver_log_levels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    WorkflowTemplateJobPrestoJobLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPrestoJobLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if driver_log_levels is not None:
            pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class WorkflowTemplateJobPrestoJobQueryListArgsDict(TypedDict):
        queries: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
elif False:
    WorkflowTemplateJobPrestoJobQueryListArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPrestoJobQueryListArgs:
    def __init__(__self__, *,
                 queries: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] queries: Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        pulumi.set(__self__, "queries", queries)

    @property
    @pulumi.getter
    def queries(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        return pulumi.get(self, "queries")

    @queries.setter
    def queries(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "queries", value)


if not MYPY:
    class WorkflowTemplateJobPysparkJobArgsDict(TypedDict):
        main_python_file_uri: pulumi.Input[str]
        """
        Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
        """
        archive_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        args: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
        """
        logging_config: NotRequired[pulumi.Input['WorkflowTemplateJobPysparkJobLoggingConfigArgsDict']]
        """
        The runtime log config for job execution.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        python_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        """
elif False:
    WorkflowTemplateJobPysparkJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPysparkJobArgs:
    def __init__(__self__, *,
                 main_python_file_uri: pulumi.Input[str],
                 archive_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['WorkflowTemplateJobPysparkJobLoggingConfigArgs']] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 python_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] main_python_file_uri: Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] file_uris: HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
        :param pulumi.Input['WorkflowTemplateJobPysparkJobLoggingConfigArgs'] logging_config: The runtime log config for job execution.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] python_file_uris: HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        """
        pulumi.set(__self__, "main_python_file_uri", main_python_file_uri)
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if python_file_uris is not None:
            pulumi.set(__self__, "python_file_uris", python_file_uris)

    @property
    @pulumi.getter(name="mainPythonFileUri")
    def main_python_file_uri(self) -> pulumi.Input[str]:
        """
        Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
        """
        return pulumi.get(self, "main_python_file_uri")

    @main_python_file_uri.setter
    def main_python_file_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "main_python_file_uri", value)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        return pulumi.get(self, "archive_uris")

    @archive_uris.setter
    def archive_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "archive_uris", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        """
        return pulumi.get(self, "file_uris")

    @file_uris.setter
    def file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "file_uris", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['WorkflowTemplateJobPysparkJobLoggingConfigArgs']]:
        """
        The runtime log config for job execution.
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['WorkflowTemplateJobPysparkJobLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="pythonFileUris")
    def python_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
        """
        return pulumi.get(self, "python_file_uris")

    @python_file_uris.setter
    def python_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "python_file_uris", value)


if not MYPY:
    class WorkflowTemplateJobPysparkJobLoggingConfigArgsDict(TypedDict):
        driver_log_levels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    WorkflowTemplateJobPysparkJobLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobPysparkJobLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if driver_log_levels is not None:
            pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class WorkflowTemplateJobSchedulingArgsDict(TypedDict):
        max_failures_per_hour: NotRequired[pulumi.Input[int]]
        """
        Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
        """
        max_failures_total: NotRequired[pulumi.Input[int]]
        """
        Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
        """
elif False:
    WorkflowTemplateJobSchedulingArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSchedulingArgs:
    def __init__(__self__, *,
                 max_failures_per_hour: Optional[pulumi.Input[int]] = None,
                 max_failures_total: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] max_failures_per_hour: Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
        :param pulumi.Input[int] max_failures_total: Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
        """
        if max_failures_per_hour is not None:
            pulumi.set(__self__, "max_failures_per_hour", max_failures_per_hour)
        if max_failures_total is not None:
            pulumi.set(__self__, "max_failures_total", max_failures_total)

    @property
    @pulumi.getter(name="maxFailuresPerHour")
    def max_failures_per_hour(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
        """
        return pulumi.get(self, "max_failures_per_hour")

    @max_failures_per_hour.setter
    def max_failures_per_hour(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_failures_per_hour", value)

    @property
    @pulumi.getter(name="maxFailuresTotal")
    def max_failures_total(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
        """
        return pulumi.get(self, "max_failures_total")

    @max_failures_total.setter
    def max_failures_total(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_failures_total", value)


if not MYPY:
    class WorkflowTemplateJobSparkJobArgsDict(TypedDict):
        archive_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        args: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        """
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        """
        logging_config: NotRequired[pulumi.Input['WorkflowTemplateJobSparkJobLoggingConfigArgsDict']]
        """
        The runtime log config for job execution.
        """
        main_class: NotRequired[pulumi.Input[str]]
        """
        The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in `jar_file_uris`.
        """
        main_jar_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the jar file that contains the main class.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
elif False:
    WorkflowTemplateJobSparkJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSparkJobArgs:
    def __init__(__self__, *,
                 archive_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['WorkflowTemplateJobSparkJobLoggingConfigArgs']] = None,
                 main_class: Optional[pulumi.Input[str]] = None,
                 main_jar_file_uri: Optional[pulumi.Input[str]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] file_uris: HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        :param pulumi.Input['WorkflowTemplateJobSparkJobLoggingConfigArgs'] logging_config: The runtime log config for job execution.
        :param pulumi.Input[str] main_class: The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in `jar_file_uris`.
        :param pulumi.Input[str] main_jar_file_uri: The HCFS URI of the jar file that contains the main class.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if main_class is not None:
            pulumi.set(__self__, "main_class", main_class)
        if main_jar_file_uri is not None:
            pulumi.set(__self__, "main_jar_file_uri", main_jar_file_uri)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        return pulumi.get(self, "archive_uris")

    @archive_uris.setter
    def archive_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "archive_uris", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        """
        return pulumi.get(self, "file_uris")

    @file_uris.setter
    def file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "file_uris", value)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['WorkflowTemplateJobSparkJobLoggingConfigArgs']]:
        """
        The runtime log config for job execution.
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['WorkflowTemplateJobSparkJobLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter(name="mainClass")
    def main_class(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in `jar_file_uris`.
        """
        return pulumi.get(self, "main_class")

    @main_class.setter
    def main_class(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_class", value)

    @property
    @pulumi.getter(name="mainJarFileUri")
    def main_jar_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the jar file that contains the main class.
        """
        return pulumi.get(self, "main_jar_file_uri")

    @main_jar_file_uri.setter
    def main_jar_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "main_jar_file_uri", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class WorkflowTemplateJobSparkJobLoggingConfigArgsDict(TypedDict):
        driver_log_levels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    WorkflowTemplateJobSparkJobLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSparkJobLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if driver_log_levels is not None:
            pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class WorkflowTemplateJobSparkRJobArgsDict(TypedDict):
        main_r_file_uri: pulumi.Input[str]
        """
        Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
        """
        archive_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        args: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        """
        logging_config: NotRequired[pulumi.Input['WorkflowTemplateJobSparkRJobLoggingConfigArgsDict']]
        """
        The runtime log config for job execution.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
elif False:
    WorkflowTemplateJobSparkRJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSparkRJobArgs:
    def __init__(__self__, *,
                 main_r_file_uri: pulumi.Input[str],
                 archive_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['WorkflowTemplateJobSparkRJobLoggingConfigArgs']] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] main_r_file_uri: Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] archive_uris: HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] file_uris: HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        :param pulumi.Input['WorkflowTemplateJobSparkRJobLoggingConfigArgs'] logging_config: The runtime log config for job execution.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        pulumi.set(__self__, "main_r_file_uri", main_r_file_uri)
        if archive_uris is not None:
            pulumi.set(__self__, "archive_uris", archive_uris)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if file_uris is not None:
            pulumi.set(__self__, "file_uris", file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="mainRFileUri")
    def main_r_file_uri(self) -> pulumi.Input[str]:
        """
        Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
        """
        return pulumi.get(self, "main_r_file_uri")

    @main_r_file_uri.setter
    def main_r_file_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "main_r_file_uri", value)

    @property
    @pulumi.getter(name="archiveUris")
    def archive_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        return pulumi.get(self, "archive_uris")

    @archive_uris.setter
    def archive_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "archive_uris", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter(name="fileUris")
    def file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
        """
        return pulumi.get(self, "file_uris")

    @file_uris.setter
    def file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['WorkflowTemplateJobSparkRJobLoggingConfigArgs']]:
        """
        The runtime log config for job execution.
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['WorkflowTemplateJobSparkRJobLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class WorkflowTemplateJobSparkRJobLoggingConfigArgsDict(TypedDict):
        driver_log_levels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    WorkflowTemplateJobSparkRJobLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSparkRJobLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if driver_log_levels is not None:
            pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class WorkflowTemplateJobSparkSqlJobArgsDict(TypedDict):
        jar_file_uris: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        HCFS URIs of jar files to be added to the Spark CLASSPATH.
        """
        logging_config: NotRequired[pulumi.Input['WorkflowTemplateJobSparkSqlJobLoggingConfigArgsDict']]
        """
        The runtime log config for job execution.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
        """
        query_file_uri: NotRequired[pulumi.Input[str]]
        """
        The HCFS URI of the script that contains SQL queries.
        """
        query_list: NotRequired[pulumi.Input['WorkflowTemplateJobSparkSqlJobQueryListArgsDict']]
        """
        A list of queries.
        """
        script_variables: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        Mapping of query variable names to values (equivalent to the Spark SQL command: SET `name="value";`).
        """
elif False:
    WorkflowTemplateJobSparkSqlJobArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSparkSqlJobArgs:
    def __init__(__self__, *,
                 jar_file_uris: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 logging_config: Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobLoggingConfigArgs']] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 query_file_uri: Optional[pulumi.Input[str]] = None,
                 query_list: Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobQueryListArgs']] = None,
                 script_variables: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] jar_file_uris: HCFS URIs of jar files to be added to the Spark CLASSPATH.
        :param pulumi.Input['WorkflowTemplateJobSparkSqlJobLoggingConfigArgs'] logging_config: The runtime log config for job execution.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
        :param pulumi.Input[str] query_file_uri: The HCFS URI of the script that contains SQL queries.
        :param pulumi.Input['WorkflowTemplateJobSparkSqlJobQueryListArgs'] query_list: A list of queries.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] script_variables: Mapping of query variable names to values (equivalent to the Spark SQL command: SET `name="value";`).
        """
        if jar_file_uris is not None:
            pulumi.set(__self__, "jar_file_uris", jar_file_uris)
        if logging_config is not None:
            pulumi.set(__self__, "logging_config", logging_config)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)
        if query_file_uri is not None:
            pulumi.set(__self__, "query_file_uri", query_file_uri)
        if query_list is not None:
            pulumi.set(__self__, "query_list", query_list)
        if script_variables is not None:
            pulumi.set(__self__, "script_variables", script_variables)

    @property
    @pulumi.getter(name="jarFileUris")
    def jar_file_uris(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        HCFS URIs of jar files to be added to the Spark CLASSPATH.
        """
        return pulumi.get(self, "jar_file_uris")

    @jar_file_uris.setter
    def jar_file_uris(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "jar_file_uris", value)

    @property
    @pulumi.getter(name="loggingConfig")
    def logging_config(self) -> Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobLoggingConfigArgs']]:
        """
        The runtime log config for job execution.
        """
        return pulumi.get(self, "logging_config")

    @logging_config.setter
    def logging_config(self, value: Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobLoggingConfigArgs']]):
        pulumi.set(self, "logging_config", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)

    @property
    @pulumi.getter(name="queryFileUri")
    def query_file_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The HCFS URI of the script that contains SQL queries.
        """
        return pulumi.get(self, "query_file_uri")

    @query_file_uri.setter
    def query_file_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "query_file_uri", value)

    @property
    @pulumi.getter(name="queryList")
    def query_list(self) -> Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobQueryListArgs']]:
        """
        A list of queries.
        """
        return pulumi.get(self, "query_list")

    @query_list.setter
    def query_list(self, value: Optional[pulumi.Input['WorkflowTemplateJobSparkSqlJobQueryListArgs']]):
        pulumi.set(self, "query_list", value)

    @property
    @pulumi.getter(name="scriptVariables")
    def script_variables(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Mapping of query variable names to values (equivalent to the Spark SQL command: SET `name="value";`).
        """
        return pulumi.get(self, "script_variables")

    @script_variables.setter
    def script_variables(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "script_variables", value)


if not MYPY:
    class WorkflowTemplateJobSparkSqlJobLoggingConfigArgsDict(TypedDict):
        driver_log_levels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
elif False:
    WorkflowTemplateJobSparkSqlJobLoggingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSparkSqlJobLoggingConfigArgs:
    def __init__(__self__, *,
                 driver_log_levels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] driver_log_levels: The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        if driver_log_levels is not None:
            pulumi.set(__self__, "driver_log_levels", driver_log_levels)

    @property
    @pulumi.getter(name="driverLogLevels")
    def driver_log_levels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        return pulumi.get(self, "driver_log_levels")

    @driver_log_levels.setter
    def driver_log_levels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "driver_log_levels", value)


if not MYPY:
    class WorkflowTemplateJobSparkSqlJobQueryListArgsDict(TypedDict):
        queries: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
elif False:
    WorkflowTemplateJobSparkSqlJobQueryListArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateJobSparkSqlJobQueryListArgs:
    def __init__(__self__, *,
                 queries: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] queries: Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        pulumi.set(__self__, "queries", queries)

    @property
    @pulumi.getter
    def queries(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
        """
        return pulumi.get(self, "queries")

    @queries.setter
    def queries(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "queries", value)


if not MYPY:
    class WorkflowTemplateParameterArgsDict(TypedDict):
        fields: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
        """
        name: pulumi.Input[str]
        """
        Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
        """
        description: NotRequired[pulumi.Input[str]]
        """
        Brief description of the parameter. Must not exceed 1024 characters.
        """
        validation: NotRequired[pulumi.Input['WorkflowTemplateParameterValidationArgsDict']]
        """
        Validation rules to be applied to this parameter's value.
        """
elif False:
    WorkflowTemplateParameterArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateParameterArgs:
    def __init__(__self__, *,
                 fields: pulumi.Input[Sequence[pulumi.Input[str]]],
                 name: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None,
                 validation: Optional[pulumi.Input['WorkflowTemplateParameterValidationArgs']] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] fields: Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
        :param pulumi.Input[str] name: Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
        :param pulumi.Input[str] description: Brief description of the parameter. Must not exceed 1024 characters.
        :param pulumi.Input['WorkflowTemplateParameterValidationArgs'] validation: Validation rules to be applied to this parameter's value.
        """
        pulumi.set(__self__, "fields", fields)
        pulumi.set(__self__, "name", name)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if validation is not None:
            pulumi.set(__self__, "validation", validation)

    @property
    @pulumi.getter
    def fields(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
        """
        return pulumi.get(self, "fields")

    @fields.setter
    def fields(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "fields", value)

    @property
    @pulumi.getter
    def name(self) -> pulumi.Input[str]:
        """
        Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[str]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        """
        Brief description of the parameter. Must not exceed 1024 characters.
        """
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)

    @property
    @pulumi.getter
    def validation(self) -> Optional[pulumi.Input['WorkflowTemplateParameterValidationArgs']]:
        """
        Validation rules to be applied to this parameter's value.
        """
        return pulumi.get(self, "validation")

    @validation.setter
    def validation(self, value: Optional[pulumi.Input['WorkflowTemplateParameterValidationArgs']]):
        pulumi.set(self, "validation", value)


if not MYPY:
    class WorkflowTemplateParameterValidationArgsDict(TypedDict):
        regex: NotRequired[pulumi.Input['WorkflowTemplateParameterValidationRegexArgsDict']]
        """
        Validation based on regular expressions.
        """
        values: NotRequired[pulumi.Input['WorkflowTemplateParameterValidationValuesArgsDict']]
        """
        Validation based on a list of allowed values.
        """
elif False:
    WorkflowTemplateParameterValidationArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateParameterValidationArgs:
    def __init__(__self__, *,
                 regex: Optional[pulumi.Input['WorkflowTemplateParameterValidationRegexArgs']] = None,
                 values: Optional[pulumi.Input['WorkflowTemplateParameterValidationValuesArgs']] = None):
        """
        :param pulumi.Input['WorkflowTemplateParameterValidationRegexArgs'] regex: Validation based on regular expressions.
        :param pulumi.Input['WorkflowTemplateParameterValidationValuesArgs'] values: Validation based on a list of allowed values.
        """
        if regex is not None:
            pulumi.set(__self__, "regex", regex)
        if values is not None:
            pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def regex(self) -> Optional[pulumi.Input['WorkflowTemplateParameterValidationRegexArgs']]:
        """
        Validation based on regular expressions.
        """
        return pulumi.get(self, "regex")

    @regex.setter
    def regex(self, value: Optional[pulumi.Input['WorkflowTemplateParameterValidationRegexArgs']]):
        pulumi.set(self, "regex", value)

    @property
    @pulumi.getter
    def values(self) -> Optional[pulumi.Input['WorkflowTemplateParameterValidationValuesArgs']]:
        """
        Validation based on a list of allowed values.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: Optional[pulumi.Input['WorkflowTemplateParameterValidationValuesArgs']]):
        pulumi.set(self, "values", value)


if not MYPY:
    class WorkflowTemplateParameterValidationRegexArgsDict(TypedDict):
        regexes: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
        """
elif False:
    WorkflowTemplateParameterValidationRegexArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateParameterValidationRegexArgs:
    def __init__(__self__, *,
                 regexes: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] regexes: Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
        """
        pulumi.set(__self__, "regexes", regexes)

    @property
    @pulumi.getter
    def regexes(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
        """
        return pulumi.get(self, "regexes")

    @regexes.setter
    def regexes(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "regexes", value)


if not MYPY:
    class WorkflowTemplateParameterValidationValuesArgsDict(TypedDict):
        values: pulumi.Input[Sequence[pulumi.Input[str]]]
        """
        Required. List of allowed values for the parameter.
        """
elif False:
    WorkflowTemplateParameterValidationValuesArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplateParameterValidationValuesArgs:
    def __init__(__self__, *,
                 values: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] values: Required. List of allowed values for the parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Required. List of allowed values for the parameter.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "values", value)


if not MYPY:
    class WorkflowTemplatePlacementArgsDict(TypedDict):
        cluster_selector: NotRequired[pulumi.Input['WorkflowTemplatePlacementClusterSelectorArgsDict']]
        """
        A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
        """
        managed_cluster: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterArgsDict']]
        """
        A cluster that is managed by the workflow.
        """
elif False:
    WorkflowTemplatePlacementArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementArgs:
    def __init__(__self__, *,
                 cluster_selector: Optional[pulumi.Input['WorkflowTemplatePlacementClusterSelectorArgs']] = None,
                 managed_cluster: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterArgs']] = None):
        """
        :param pulumi.Input['WorkflowTemplatePlacementClusterSelectorArgs'] cluster_selector: A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterArgs'] managed_cluster: A cluster that is managed by the workflow.
        """
        if cluster_selector is not None:
            pulumi.set(__self__, "cluster_selector", cluster_selector)
        if managed_cluster is not None:
            pulumi.set(__self__, "managed_cluster", managed_cluster)

    @property
    @pulumi.getter(name="clusterSelector")
    def cluster_selector(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementClusterSelectorArgs']]:
        """
        A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
        """
        return pulumi.get(self, "cluster_selector")

    @cluster_selector.setter
    def cluster_selector(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementClusterSelectorArgs']]):
        pulumi.set(self, "cluster_selector", value)

    @property
    @pulumi.getter(name="managedCluster")
    def managed_cluster(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterArgs']]:
        """
        A cluster that is managed by the workflow.
        """
        return pulumi.get(self, "managed_cluster")

    @managed_cluster.setter
    def managed_cluster(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterArgs']]):
        pulumi.set(self, "managed_cluster", value)


if not MYPY:
    class WorkflowTemplatePlacementClusterSelectorArgsDict(TypedDict):
        cluster_labels: pulumi.Input[Mapping[str, pulumi.Input[str]]]
        """
        Required. The cluster labels. Cluster must have all labels to match.
        """
        zone: NotRequired[pulumi.Input[str]]
        """
        The zone where workflow process executes. This parameter does not affect the selection of the cluster. If unspecified, the zone of the first cluster matching the selector is used.
        """
elif False:
    WorkflowTemplatePlacementClusterSelectorArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementClusterSelectorArgs:
    def __init__(__self__, *,
                 cluster_labels: pulumi.Input[Mapping[str, pulumi.Input[str]]],
                 zone: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] cluster_labels: Required. The cluster labels. Cluster must have all labels to match.
        :param pulumi.Input[str] zone: The zone where workflow process executes. This parameter does not affect the selection of the cluster. If unspecified, the zone of the first cluster matching the selector is used.
        """
        pulumi.set(__self__, "cluster_labels", cluster_labels)
        if zone is not None:
            pulumi.set(__self__, "zone", zone)

    @property
    @pulumi.getter(name="clusterLabels")
    def cluster_labels(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Required. The cluster labels. Cluster must have all labels to match.
        """
        return pulumi.get(self, "cluster_labels")

    @cluster_labels.setter
    def cluster_labels(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "cluster_labels", value)

    @property
    @pulumi.getter
    def zone(self) -> Optional[pulumi.Input[str]]:
        """
        The zone where workflow process executes. This parameter does not affect the selection of the cluster. If unspecified, the zone of the first cluster matching the selector is used.
        """
        return pulumi.get(self, "zone")

    @zone.setter
    def zone(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "zone", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterArgsDict(TypedDict):
        cluster_name: pulumi.Input[str]
        """
        Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
        """
        config: pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigArgsDict']
        """
        Required. The cluster configuration.
        """
        labels: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterArgs:
    def __init__(__self__, *,
                 cluster_name: pulumi.Input[str],
                 config: pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigArgs'],
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] cluster_name: Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigArgs'] config: Required. The cluster configuration.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
        """
        pulumi.set(__self__, "cluster_name", cluster_name)
        pulumi.set(__self__, "config", config)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)

    @property
    @pulumi.getter(name="clusterName")
    def cluster_name(self) -> pulumi.Input[str]:
        """
        Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
        """
        return pulumi.get(self, "cluster_name")

    @cluster_name.setter
    def cluster_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "cluster_name", value)

    @property
    @pulumi.getter
    def config(self) -> pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigArgs']:
        """
        Required. The cluster configuration.
        """
        return pulumi.get(self, "config")

    @config.setter
    def config(self, value: pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigArgs']):
        pulumi.set(self, "config", value)

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
        """
        return pulumi.get(self, "labels")

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "labels", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigArgsDict(TypedDict):
        autoscaling_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgsDict']]
        """
        Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
        """
        encryption_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgsDict']]
        """
        Encryption settings for the cluster.
        """
        endpoint_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgsDict']]
        """
        Port/endpoint configuration for this cluster
        """
        gce_cluster_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgsDict']]
        """
        The shared Compute Engine config settings for all instances in a cluster.
        """
        gke_cluster_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgsDict']]
        """
        The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as `gce_cluster_config`, `master_config`, `worker_config`, `secondary_worker_config`, and `autoscaling_config`.
        """
        initialization_actions: NotRequired[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgsDict']]]]
        """
        Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's `role` metadata to run an executable on a master or worker node, as shown below using `curl` (you can also use `wget`): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
        """
        lifecycle_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgsDict']]
        """
        Lifecycle setting for the cluster.
        """
        master_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgsDict']]
        """
        The Compute Engine config settings for additional worker instances in a cluster.
        """
        metastore_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgsDict']]
        """
        Metastore configuration.
        """
        secondary_worker_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgsDict']]
        """
        The Compute Engine config settings for additional worker instances in a cluster.
        """
        security_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgsDict']]
        """
        Security settings for the cluster.
        """
        software_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgsDict']]
        """
        The config settings for software inside the cluster.
        """
        staging_bucket: NotRequired[pulumi.Input[str]]
        """
        A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
        """
        temp_bucket: NotRequired[pulumi.Input[str]]
        """
        A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
        """
        worker_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgsDict']]
        """
        The Compute Engine config settings for additional worker instances in a cluster.

        - - -
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigArgs:
    def __init__(__self__, *,
                 autoscaling_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgs']] = None,
                 encryption_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgs']] = None,
                 endpoint_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgs']] = None,
                 gce_cluster_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgs']] = None,
                 gke_cluster_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgs']] = None,
                 initialization_actions: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgs']]]] = None,
                 lifecycle_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgs']] = None,
                 master_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgs']] = None,
                 metastore_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgs']] = None,
                 secondary_worker_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgs']] = None,
                 security_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgs']] = None,
                 software_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgs']] = None,
                 staging_bucket: Optional[pulumi.Input[str]] = None,
                 temp_bucket: Optional[pulumi.Input[str]] = None,
                 worker_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgs']] = None):
        """
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgs'] autoscaling_config: Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgs'] encryption_config: Encryption settings for the cluster.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgs'] endpoint_config: Port/endpoint configuration for this cluster
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgs'] gce_cluster_config: The shared Compute Engine config settings for all instances in a cluster.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgs'] gke_cluster_config: The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as `gce_cluster_config`, `master_config`, `worker_config`, `secondary_worker_config`, and `autoscaling_config`.
        :param pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgs']]] initialization_actions: Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's `role` metadata to run an executable on a master or worker node, as shown below using `curl` (you can also use `wget`): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgs'] lifecycle_config: Lifecycle setting for the cluster.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgs'] master_config: The Compute Engine config settings for additional worker instances in a cluster.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgs'] metastore_config: Metastore configuration.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgs'] secondary_worker_config: The Compute Engine config settings for additional worker instances in a cluster.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgs'] security_config: Security settings for the cluster.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgs'] software_config: The config settings for software inside the cluster.
        :param pulumi.Input[str] staging_bucket: A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
        :param pulumi.Input[str] temp_bucket: A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgs'] worker_config: The Compute Engine config settings for additional worker instances in a cluster.
               
               - - -
        """
        if autoscaling_config is not None:
            pulumi.set(__self__, "autoscaling_config", autoscaling_config)
        if encryption_config is not None:
            pulumi.set(__self__, "encryption_config", encryption_config)
        if endpoint_config is not None:
            pulumi.set(__self__, "endpoint_config", endpoint_config)
        if gce_cluster_config is not None:
            pulumi.set(__self__, "gce_cluster_config", gce_cluster_config)
        if gke_cluster_config is not None:
            pulumi.set(__self__, "gke_cluster_config", gke_cluster_config)
        if initialization_actions is not None:
            pulumi.set(__self__, "initialization_actions", initialization_actions)
        if lifecycle_config is not None:
            pulumi.set(__self__, "lifecycle_config", lifecycle_config)
        if master_config is not None:
            pulumi.set(__self__, "master_config", master_config)
        if metastore_config is not None:
            pulumi.set(__self__, "metastore_config", metastore_config)
        if secondary_worker_config is not None:
            pulumi.set(__self__, "secondary_worker_config", secondary_worker_config)
        if security_config is not None:
            pulumi.set(__self__, "security_config", security_config)
        if software_config is not None:
            pulumi.set(__self__, "software_config", software_config)
        if staging_bucket is not None:
            pulumi.set(__self__, "staging_bucket", staging_bucket)
        if temp_bucket is not None:
            pulumi.set(__self__, "temp_bucket", temp_bucket)
        if worker_config is not None:
            pulumi.set(__self__, "worker_config", worker_config)

    @property
    @pulumi.getter(name="autoscalingConfig")
    def autoscaling_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgs']]:
        """
        Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
        """
        return pulumi.get(self, "autoscaling_config")

    @autoscaling_config.setter
    def autoscaling_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgs']]):
        pulumi.set(self, "autoscaling_config", value)

    @property
    @pulumi.getter(name="encryptionConfig")
    def encryption_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgs']]:
        """
        Encryption settings for the cluster.
        """
        return pulumi.get(self, "encryption_config")

    @encryption_config.setter
    def encryption_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgs']]):
        pulumi.set(self, "encryption_config", value)

    @property
    @pulumi.getter(name="endpointConfig")
    def endpoint_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgs']]:
        """
        Port/endpoint configuration for this cluster
        """
        return pulumi.get(self, "endpoint_config")

    @endpoint_config.setter
    def endpoint_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgs']]):
        pulumi.set(self, "endpoint_config", value)

    @property
    @pulumi.getter(name="gceClusterConfig")
    def gce_cluster_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgs']]:
        """
        The shared Compute Engine config settings for all instances in a cluster.
        """
        return pulumi.get(self, "gce_cluster_config")

    @gce_cluster_config.setter
    def gce_cluster_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgs']]):
        pulumi.set(self, "gce_cluster_config", value)

    @property
    @pulumi.getter(name="gkeClusterConfig")
    def gke_cluster_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgs']]:
        """
        The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as `gce_cluster_config`, `master_config`, `worker_config`, `secondary_worker_config`, and `autoscaling_config`.
        """
        return pulumi.get(self, "gke_cluster_config")

    @gke_cluster_config.setter
    def gke_cluster_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgs']]):
        pulumi.set(self, "gke_cluster_config", value)

    @property
    @pulumi.getter(name="initializationActions")
    def initialization_actions(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgs']]]]:
        """
        Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's `role` metadata to run an executable on a master or worker node, as shown below using `curl` (you can also use `wget`): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
        """
        return pulumi.get(self, "initialization_actions")

    @initialization_actions.setter
    def initialization_actions(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgs']]]]):
        pulumi.set(self, "initialization_actions", value)

    @property
    @pulumi.getter(name="lifecycleConfig")
    def lifecycle_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgs']]:
        """
        Lifecycle setting for the cluster.
        """
        return pulumi.get(self, "lifecycle_config")

    @lifecycle_config.setter
    def lifecycle_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgs']]):
        pulumi.set(self, "lifecycle_config", value)

    @property
    @pulumi.getter(name="masterConfig")
    def master_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgs']]:
        """
        The Compute Engine config settings for additional worker instances in a cluster.
        """
        return pulumi.get(self, "master_config")

    @master_config.setter
    def master_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgs']]):
        pulumi.set(self, "master_config", value)

    @property
    @pulumi.getter(name="metastoreConfig")
    def metastore_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgs']]:
        """
        Metastore configuration.
        """
        return pulumi.get(self, "metastore_config")

    @metastore_config.setter
    def metastore_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgs']]):
        pulumi.set(self, "metastore_config", value)

    @property
    @pulumi.getter(name="secondaryWorkerConfig")
    def secondary_worker_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgs']]:
        """
        The Compute Engine config settings for additional worker instances in a cluster.
        """
        return pulumi.get(self, "secondary_worker_config")

    @secondary_worker_config.setter
    def secondary_worker_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgs']]):
        pulumi.set(self, "secondary_worker_config", value)

    @property
    @pulumi.getter(name="securityConfig")
    def security_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgs']]:
        """
        Security settings for the cluster.
        """
        return pulumi.get(self, "security_config")

    @security_config.setter
    def security_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgs']]):
        pulumi.set(self, "security_config", value)

    @property
    @pulumi.getter(name="softwareConfig")
    def software_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgs']]:
        """
        The config settings for software inside the cluster.
        """
        return pulumi.get(self, "software_config")

    @software_config.setter
    def software_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgs']]):
        pulumi.set(self, "software_config", value)

    @property
    @pulumi.getter(name="stagingBucket")
    def staging_bucket(self) -> Optional[pulumi.Input[str]]:
        """
        A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
        """
        return pulumi.get(self, "staging_bucket")

    @staging_bucket.setter
    def staging_bucket(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "staging_bucket", value)

    @property
    @pulumi.getter(name="tempBucket")
    def temp_bucket(self) -> Optional[pulumi.Input[str]]:
        """
        A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
        """
        return pulumi.get(self, "temp_bucket")

    @temp_bucket.setter
    def temp_bucket(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "temp_bucket", value)

    @property
    @pulumi.getter(name="workerConfig")
    def worker_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgs']]:
        """
        The Compute Engine config settings for additional worker instances in a cluster.

        - - -
        """
        return pulumi.get(self, "worker_config")

    @worker_config.setter
    def worker_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgs']]):
        pulumi.set(self, "worker_config", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgsDict(TypedDict):
        policy: NotRequired[pulumi.Input[str]]
        """
        The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` Note that the policy must be in the same project and Dataproc region.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigArgs:
    def __init__(__self__, *,
                 policy: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] policy: The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` Note that the policy must be in the same project and Dataproc region.
        """
        if policy is not None:
            pulumi.set(__self__, "policy", policy)

    @property
    @pulumi.getter
    def policy(self) -> Optional[pulumi.Input[str]]:
        """
        The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` Note that the policy must be in the same project and Dataproc region.
        """
        return pulumi.get(self, "policy")

    @policy.setter
    def policy(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "policy", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgsDict(TypedDict):
        gce_pd_kms_key_name: NotRequired[pulumi.Input[str]]
        """
        The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigArgs:
    def __init__(__self__, *,
                 gce_pd_kms_key_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] gce_pd_kms_key_name: The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
        """
        if gce_pd_kms_key_name is not None:
            pulumi.set(__self__, "gce_pd_kms_key_name", gce_pd_kms_key_name)

    @property
    @pulumi.getter(name="gcePdKmsKeyName")
    def gce_pd_kms_key_name(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
        """
        return pulumi.get(self, "gce_pd_kms_key_name")

    @gce_pd_kms_key_name.setter
    def gce_pd_kms_key_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "gce_pd_kms_key_name", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgsDict(TypedDict):
        enable_http_port_access: NotRequired[pulumi.Input[bool]]
        """
        If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
        """
        http_ports: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        Output only. The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigEndpointConfigArgs:
    def __init__(__self__, *,
                 enable_http_port_access: Optional[pulumi.Input[bool]] = None,
                 http_ports: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[bool] enable_http_port_access: If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] http_ports: Output only. The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
        """
        if enable_http_port_access is not None:
            pulumi.set(__self__, "enable_http_port_access", enable_http_port_access)
        if http_ports is not None:
            pulumi.set(__self__, "http_ports", http_ports)

    @property
    @pulumi.getter(name="enableHttpPortAccess")
    def enable_http_port_access(self) -> Optional[pulumi.Input[bool]]:
        """
        If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
        """
        return pulumi.get(self, "enable_http_port_access")

    @enable_http_port_access.setter
    def enable_http_port_access(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_http_port_access", value)

    @property
    @pulumi.getter(name="httpPorts")
    def http_ports(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Output only. The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
        """
        return pulumi.get(self, "http_ports")

    @http_ports.setter
    def http_ports(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "http_ports", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgsDict(TypedDict):
        internal_ip_only: NotRequired[pulumi.Input[bool]]
        """
        If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This `internal_ip_only` restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
        """
        metadata: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        """
        network: NotRequired[pulumi.Input[str]]
        """
        The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither `network_uri` nor `subnetwork_uri` is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default` * `default`
        """
        node_group_affinity: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgsDict']]
        """
        Node Group Affinity for sole-tenant clusters.
        """
        private_ipv6_google_access: NotRequired[pulumi.Input[str]]
        """
        The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
        """
        reservation_affinity: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgsDict']]
        """
        Reservation Affinity for consuming Zonal reservation.
        """
        service_account: NotRequired[pulumi.Input[str]]
        """
        The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
        """
        service_account_scopes: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
        """
        shielded_instance_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict']]
        """
        Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm). Structure defined below.
        """
        subnetwork: NotRequired[pulumi.Input[str]]
        """
        The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0` * `sub0`
        """
        tags: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
        """
        zone: NotRequired[pulumi.Input[str]]
        """
        The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` * `us-central1-f`
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigArgs:
    def __init__(__self__, *,
                 internal_ip_only: Optional[pulumi.Input[bool]] = None,
                 metadata: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 node_group_affinity: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgs']] = None,
                 private_ipv6_google_access: Optional[pulumi.Input[str]] = None,
                 reservation_affinity: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgs']] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 service_account_scopes: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 shielded_instance_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgs']] = None,
                 subnetwork: Optional[pulumi.Input[str]] = None,
                 tags: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 zone: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[bool] internal_ip_only: If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This `internal_ip_only` restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] metadata: The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        :param pulumi.Input[str] network: The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither `network_uri` nor `subnetwork_uri` is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default` * `default`
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgs'] node_group_affinity: Node Group Affinity for sole-tenant clusters.
        :param pulumi.Input[str] private_ipv6_google_access: The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgs'] reservation_affinity: Reservation Affinity for consuming Zonal reservation.
        :param pulumi.Input[str] service_account: The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] service_account_scopes: The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgs'] shielded_instance_config: Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm). Structure defined below.
        :param pulumi.Input[str] subnetwork: The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0` * `sub0`
        :param pulumi.Input[Sequence[pulumi.Input[str]]] tags: The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
        :param pulumi.Input[str] zone: The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` * `us-central1-f`
        """
        if internal_ip_only is not None:
            pulumi.set(__self__, "internal_ip_only", internal_ip_only)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if network is not None:
            pulumi.set(__self__, "network", network)
        if node_group_affinity is not None:
            pulumi.set(__self__, "node_group_affinity", node_group_affinity)
        if private_ipv6_google_access is not None:
            pulumi.set(__self__, "private_ipv6_google_access", private_ipv6_google_access)
        if reservation_affinity is not None:
            pulumi.set(__self__, "reservation_affinity", reservation_affinity)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if service_account_scopes is not None:
            pulumi.set(__self__, "service_account_scopes", service_account_scopes)
        if shielded_instance_config is not None:
            pulumi.set(__self__, "shielded_instance_config", shielded_instance_config)
        if subnetwork is not None:
            pulumi.set(__self__, "subnetwork", subnetwork)
        if tags is not None:
            pulumi.set(__self__, "tags", tags)
        if zone is not None:
            pulumi.set(__self__, "zone", zone)

    @property
    @pulumi.getter(name="internalIpOnly")
    def internal_ip_only(self) -> Optional[pulumi.Input[bool]]:
        """
        If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This `internal_ip_only` restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
        """
        return pulumi.get(self, "internal_ip_only")

    @internal_ip_only.setter
    def internal_ip_only(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "internal_ip_only", value)

    @property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        """
        return pulumi.get(self, "metadata")

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "metadata", value)

    @property
    @pulumi.getter
    def network(self) -> Optional[pulumi.Input[str]]:
        """
        The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither `network_uri` nor `subnetwork_uri` is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default` * `default`
        """
        return pulumi.get(self, "network")

    @network.setter
    def network(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "network", value)

    @property
    @pulumi.getter(name="nodeGroupAffinity")
    def node_group_affinity(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgs']]:
        """
        Node Group Affinity for sole-tenant clusters.
        """
        return pulumi.get(self, "node_group_affinity")

    @node_group_affinity.setter
    def node_group_affinity(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgs']]):
        pulumi.set(self, "node_group_affinity", value)

    @property
    @pulumi.getter(name="privateIpv6GoogleAccess")
    def private_ipv6_google_access(self) -> Optional[pulumi.Input[str]]:
        """
        The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
        """
        return pulumi.get(self, "private_ipv6_google_access")

    @private_ipv6_google_access.setter
    def private_ipv6_google_access(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "private_ipv6_google_access", value)

    @property
    @pulumi.getter(name="reservationAffinity")
    def reservation_affinity(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgs']]:
        """
        Reservation Affinity for consuming Zonal reservation.
        """
        return pulumi.get(self, "reservation_affinity")

    @reservation_affinity.setter
    def reservation_affinity(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgs']]):
        pulumi.set(self, "reservation_affinity", value)

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
        """
        return pulumi.get(self, "service_account")

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_account", value)

    @property
    @pulumi.getter(name="serviceAccountScopes")
    def service_account_scopes(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
        """
        return pulumi.get(self, "service_account_scopes")

    @service_account_scopes.setter
    def service_account_scopes(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "service_account_scopes", value)

    @property
    @pulumi.getter(name="shieldedInstanceConfig")
    def shielded_instance_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgs']]:
        """
        Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm). Structure defined below.
        """
        return pulumi.get(self, "shielded_instance_config")

    @shielded_instance_config.setter
    def shielded_instance_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgs']]):
        pulumi.set(self, "shielded_instance_config", value)

    @property
    @pulumi.getter
    def subnetwork(self) -> Optional[pulumi.Input[str]]:
        """
        The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0` * `sub0`
        """
        return pulumi.get(self, "subnetwork")

    @subnetwork.setter
    def subnetwork(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "subnetwork", value)

    @property
    @pulumi.getter
    def tags(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
        """
        return pulumi.get(self, "tags")

    @tags.setter
    def tags(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "tags", value)

    @property
    @pulumi.getter
    def zone(self) -> Optional[pulumi.Input[str]]:
        """
        The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` * `us-central1-f`
        """
        return pulumi.get(self, "zone")

    @zone.setter
    def zone(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "zone", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgsDict(TypedDict):
        node_group: pulumi.Input[str]
        """
        Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1` * `node-group-1`
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinityArgs:
    def __init__(__self__, *,
                 node_group: pulumi.Input[str]):
        """
        :param pulumi.Input[str] node_group: Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1` * `node-group-1`
        """
        pulumi.set(__self__, "node_group", node_group)

    @property
    @pulumi.getter(name="nodeGroup")
    def node_group(self) -> pulumi.Input[str]:
        """
        Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1` * `node-group-1`
        """
        return pulumi.get(self, "node_group")

    @node_group.setter
    def node_group(self, value: pulumi.Input[str]):
        pulumi.set(self, "node_group", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgsDict(TypedDict):
        consume_reservation_type: NotRequired[pulumi.Input[str]]
        """
        Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
        """
        key: NotRequired[pulumi.Input[str]]
        """
        Corresponds to the label key of reservation resource.
        """
        values: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Corresponds to the label values of reservation resource.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinityArgs:
    def __init__(__self__, *,
                 consume_reservation_type: Optional[pulumi.Input[str]] = None,
                 key: Optional[pulumi.Input[str]] = None,
                 values: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] consume_reservation_type: Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
        :param pulumi.Input[str] key: Corresponds to the label key of reservation resource.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] values: Corresponds to the label values of reservation resource.
        """
        if consume_reservation_type is not None:
            pulumi.set(__self__, "consume_reservation_type", consume_reservation_type)
        if key is not None:
            pulumi.set(__self__, "key", key)
        if values is not None:
            pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter(name="consumeReservationType")
    def consume_reservation_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
        """
        return pulumi.get(self, "consume_reservation_type")

    @consume_reservation_type.setter
    def consume_reservation_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "consume_reservation_type", value)

    @property
    @pulumi.getter
    def key(self) -> Optional[pulumi.Input[str]]:
        """
        Corresponds to the label key of reservation resource.
        """
        return pulumi.get(self, "key")

    @key.setter
    def key(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "key", value)

    @property
    @pulumi.getter
    def values(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Corresponds to the label values of reservation resource.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "values", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict(TypedDict):
        enable_integrity_monitoring: NotRequired[pulumi.Input[bool]]
        """
        Defines whether instances have [Integrity Monitoring](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#integrity-monitoring) enabled.
        """
        enable_secure_boot: NotRequired[pulumi.Input[bool]]
        """
        Defines whether instances have [Secure Boot](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot) enabled.
        """
        enable_vtpm: NotRequired[pulumi.Input[bool]]
        """
        Defines whether instances have the [vTPM](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#vtpm) enabled.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfigArgs:
    def __init__(__self__, *,
                 enable_integrity_monitoring: Optional[pulumi.Input[bool]] = None,
                 enable_secure_boot: Optional[pulumi.Input[bool]] = None,
                 enable_vtpm: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[bool] enable_integrity_monitoring: Defines whether instances have [Integrity Monitoring](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#integrity-monitoring) enabled.
        :param pulumi.Input[bool] enable_secure_boot: Defines whether instances have [Secure Boot](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot) enabled.
        :param pulumi.Input[bool] enable_vtpm: Defines whether instances have the [vTPM](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#vtpm) enabled.
        """
        if enable_integrity_monitoring is not None:
            pulumi.set(__self__, "enable_integrity_monitoring", enable_integrity_monitoring)
        if enable_secure_boot is not None:
            pulumi.set(__self__, "enable_secure_boot", enable_secure_boot)
        if enable_vtpm is not None:
            pulumi.set(__self__, "enable_vtpm", enable_vtpm)

    @property
    @pulumi.getter(name="enableIntegrityMonitoring")
    def enable_integrity_monitoring(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether instances have [Integrity Monitoring](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#integrity-monitoring) enabled.
        """
        return pulumi.get(self, "enable_integrity_monitoring")

    @enable_integrity_monitoring.setter
    def enable_integrity_monitoring(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_integrity_monitoring", value)

    @property
    @pulumi.getter(name="enableSecureBoot")
    def enable_secure_boot(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether instances have [Secure Boot](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot) enabled.
        """
        return pulumi.get(self, "enable_secure_boot")

    @enable_secure_boot.setter
    def enable_secure_boot(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_secure_boot", value)

    @property
    @pulumi.getter(name="enableVtpm")
    def enable_vtpm(self) -> Optional[pulumi.Input[bool]]:
        """
        Defines whether instances have the [vTPM](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#vtpm) enabled.
        """
        return pulumi.get(self, "enable_vtpm")

    @enable_vtpm.setter
    def enable_vtpm(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_vtpm", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgsDict(TypedDict):
        namespaced_gke_deployment_target: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgsDict']]
        """
        A target for the deployment.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigArgs:
    def __init__(__self__, *,
                 namespaced_gke_deployment_target: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgs']] = None):
        """
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgs'] namespaced_gke_deployment_target: A target for the deployment.
        """
        if namespaced_gke_deployment_target is not None:
            pulumi.set(__self__, "namespaced_gke_deployment_target", namespaced_gke_deployment_target)

    @property
    @pulumi.getter(name="namespacedGkeDeploymentTarget")
    def namespaced_gke_deployment_target(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgs']]:
        """
        A target for the deployment.
        """
        return pulumi.get(self, "namespaced_gke_deployment_target")

    @namespaced_gke_deployment_target.setter
    def namespaced_gke_deployment_target(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgs']]):
        pulumi.set(self, "namespaced_gke_deployment_target", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgsDict(TypedDict):
        cluster_namespace: NotRequired[pulumi.Input[str]]
        """
        A namespace within the GKE cluster to deploy into.
        """
        target_gke_cluster: NotRequired[pulumi.Input[str]]
        """
        The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTargetArgs:
    def __init__(__self__, *,
                 cluster_namespace: Optional[pulumi.Input[str]] = None,
                 target_gke_cluster: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] cluster_namespace: A namespace within the GKE cluster to deploy into.
        :param pulumi.Input[str] target_gke_cluster: The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
        """
        if cluster_namespace is not None:
            pulumi.set(__self__, "cluster_namespace", cluster_namespace)
        if target_gke_cluster is not None:
            pulumi.set(__self__, "target_gke_cluster", target_gke_cluster)

    @property
    @pulumi.getter(name="clusterNamespace")
    def cluster_namespace(self) -> Optional[pulumi.Input[str]]:
        """
        A namespace within the GKE cluster to deploy into.
        """
        return pulumi.get(self, "cluster_namespace")

    @cluster_namespace.setter
    def cluster_namespace(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cluster_namespace", value)

    @property
    @pulumi.getter(name="targetGkeCluster")
    def target_gke_cluster(self) -> Optional[pulumi.Input[str]]:
        """
        The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
        """
        return pulumi.get(self, "target_gke_cluster")

    @target_gke_cluster.setter
    def target_gke_cluster(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "target_gke_cluster", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgsDict(TypedDict):
        executable_file: NotRequired[pulumi.Input[str]]
        """
        Required. Cloud Storage URI of executable file.
        """
        execution_timeout: NotRequired[pulumi.Input[str]]
        """
        Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigInitializationActionArgs:
    def __init__(__self__, *,
                 executable_file: Optional[pulumi.Input[str]] = None,
                 execution_timeout: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] executable_file: Required. Cloud Storage URI of executable file.
        :param pulumi.Input[str] execution_timeout: Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
        """
        if executable_file is not None:
            pulumi.set(__self__, "executable_file", executable_file)
        if execution_timeout is not None:
            pulumi.set(__self__, "execution_timeout", execution_timeout)

    @property
    @pulumi.getter(name="executableFile")
    def executable_file(self) -> Optional[pulumi.Input[str]]:
        """
        Required. Cloud Storage URI of executable file.
        """
        return pulumi.get(self, "executable_file")

    @executable_file.setter
    def executable_file(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "executable_file", value)

    @property
    @pulumi.getter(name="executionTimeout")
    def execution_timeout(self) -> Optional[pulumi.Input[str]]:
        """
        Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
        """
        return pulumi.get(self, "execution_timeout")

    @execution_timeout.setter
    def execution_timeout(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "execution_timeout", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgsDict(TypedDict):
        auto_delete_time: NotRequired[pulumi.Input[str]]
        """
        The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        auto_delete_ttl: NotRequired[pulumi.Input[str]]
        """
        The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        idle_delete_ttl: NotRequired[pulumi.Input[str]]
        """
        The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
        """
        idle_start_time: NotRequired[pulumi.Input[str]]
        """
        Output only. The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigArgs:
    def __init__(__self__, *,
                 auto_delete_time: Optional[pulumi.Input[str]] = None,
                 auto_delete_ttl: Optional[pulumi.Input[str]] = None,
                 idle_delete_ttl: Optional[pulumi.Input[str]] = None,
                 idle_start_time: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] auto_delete_time: The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        :param pulumi.Input[str] auto_delete_ttl: The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        :param pulumi.Input[str] idle_delete_ttl: The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
        :param pulumi.Input[str] idle_start_time: Output only. The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        if auto_delete_time is not None:
            pulumi.set(__self__, "auto_delete_time", auto_delete_time)
        if auto_delete_ttl is not None:
            pulumi.set(__self__, "auto_delete_ttl", auto_delete_ttl)
        if idle_delete_ttl is not None:
            pulumi.set(__self__, "idle_delete_ttl", idle_delete_ttl)
        if idle_start_time is not None:
            pulumi.set(__self__, "idle_start_time", idle_start_time)

    @property
    @pulumi.getter(name="autoDeleteTime")
    def auto_delete_time(self) -> Optional[pulumi.Input[str]]:
        """
        The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        return pulumi.get(self, "auto_delete_time")

    @auto_delete_time.setter
    def auto_delete_time(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "auto_delete_time", value)

    @property
    @pulumi.getter(name="autoDeleteTtl")
    def auto_delete_ttl(self) -> Optional[pulumi.Input[str]]:
        """
        The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        return pulumi.get(self, "auto_delete_ttl")

    @auto_delete_ttl.setter
    def auto_delete_ttl(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "auto_delete_ttl", value)

    @property
    @pulumi.getter(name="idleDeleteTtl")
    def idle_delete_ttl(self) -> Optional[pulumi.Input[str]]:
        """
        The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
        """
        return pulumi.get(self, "idle_delete_ttl")

    @idle_delete_ttl.setter
    def idle_delete_ttl(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "idle_delete_ttl", value)

    @property
    @pulumi.getter(name="idleStartTime")
    def idle_start_time(self) -> Optional[pulumi.Input[str]]:
        """
        Output only. The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        return pulumi.get(self, "idle_start_time")

    @idle_start_time.setter
    def idle_start_time(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "idle_start_time", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgsDict(TypedDict):
        accelerators: NotRequired[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgsDict']]]]
        """
        The Compute Engine accelerator configuration for these instances.
        """
        disk_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgsDict']]
        """
        Disk option config settings.
        """
        image: NotRequired[pulumi.Input[str]]
        """
        The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        """
        instance_names: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        """
        is_preemptible: NotRequired[pulumi.Input[bool]]
        """
        Output only. Specifies that this instance group contains preemptible instances.
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        """
        managed_group_configs: NotRequired[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgsDict']]]]
        """
        Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        """
        min_cpu_platform: NotRequired[pulumi.Input[str]]
        """
        Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        """
        num_instances: NotRequired[pulumi.Input[int]]
        """
        The number of VM instances in the instance group. For master instance groups, must be set to 1.
        """
        preemptibility: NotRequired[pulumi.Input[str]]
        """
        Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigMasterConfigArgs:
    def __init__(__self__, *,
                 accelerators: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgs']]]] = None,
                 disk_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgs']] = None,
                 image: Optional[pulumi.Input[str]] = None,
                 instance_names: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 is_preemptible: Optional[pulumi.Input[bool]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 managed_group_configs: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgs']]]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 num_instances: Optional[pulumi.Input[int]] = None,
                 preemptibility: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgs']]] accelerators: The Compute Engine accelerator configuration for these instances.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgs'] disk_config: Disk option config settings.
        :param pulumi.Input[str] image: The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] instance_names: Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        :param pulumi.Input[bool] is_preemptible: Output only. Specifies that this instance group contains preemptible instances.
        :param pulumi.Input[str] machine_type: The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        :param pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgs']]] managed_group_configs: Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        :param pulumi.Input[str] min_cpu_platform: Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        :param pulumi.Input[int] num_instances: The number of VM instances in the instance group. For master instance groups, must be set to 1.
        :param pulumi.Input[str] preemptibility: Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
        if accelerators is not None:
            pulumi.set(__self__, "accelerators", accelerators)
        if disk_config is not None:
            pulumi.set(__self__, "disk_config", disk_config)
        if image is not None:
            pulumi.set(__self__, "image", image)
        if instance_names is not None:
            pulumi.set(__self__, "instance_names", instance_names)
        if is_preemptible is not None:
            pulumi.set(__self__, "is_preemptible", is_preemptible)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if managed_group_configs is not None:
            pulumi.set(__self__, "managed_group_configs", managed_group_configs)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if num_instances is not None:
            pulumi.set(__self__, "num_instances", num_instances)
        if preemptibility is not None:
            pulumi.set(__self__, "preemptibility", preemptibility)

    @property
    @pulumi.getter
    def accelerators(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgs']]]]:
        """
        The Compute Engine accelerator configuration for these instances.
        """
        return pulumi.get(self, "accelerators")

    @accelerators.setter
    def accelerators(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgs']]]]):
        pulumi.set(self, "accelerators", value)

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgs']]:
        """
        Disk option config settings.
        """
        return pulumi.get(self, "disk_config")

    @disk_config.setter
    def disk_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgs']]):
        pulumi.set(self, "disk_config", value)

    @property
    @pulumi.getter
    def image(self) -> Optional[pulumi.Input[str]]:
        """
        The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        """
        return pulumi.get(self, "image")

    @image.setter
    def image(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "image", value)

    @property
    @pulumi.getter(name="instanceNames")
    def instance_names(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        """
        return pulumi.get(self, "instance_names")

    @instance_names.setter
    def instance_names(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "instance_names", value)

    @property
    @pulumi.getter(name="isPreemptible")
    def is_preemptible(self) -> Optional[pulumi.Input[bool]]:
        """
        Output only. Specifies that this instance group contains preemptible instances.
        """
        return pulumi.get(self, "is_preemptible")

    @is_preemptible.setter
    def is_preemptible(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "is_preemptible", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="managedGroupConfigs")
    def managed_group_configs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgs']]]]:
        """
        Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        """
        return pulumi.get(self, "managed_group_configs")

    @managed_group_configs.setter
    def managed_group_configs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgs']]]]):
        pulumi.set(self, "managed_group_configs", value)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        """
        return pulumi.get(self, "min_cpu_platform")

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_cpu_platform", value)

    @property
    @pulumi.getter(name="numInstances")
    def num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        The number of VM instances in the instance group. For master instance groups, must be set to 1.
        """
        return pulumi.get(self, "num_instances")

    @num_instances.setter
    def num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_instances", value)

    @property
    @pulumi.getter
    def preemptibility(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
        return pulumi.get(self, "preemptibility")

    @preemptibility.setter
    def preemptibility(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "preemptibility", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgsDict(TypedDict):
        accelerator_count: NotRequired[pulumi.Input[int]]
        """
        The number of the accelerator cards of this type exposed to this instance.
        """
        accelerator_type: NotRequired[pulumi.Input[str]]
        """
        Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigMasterConfigAcceleratorArgs:
    def __init__(__self__, *,
                 accelerator_count: Optional[pulumi.Input[int]] = None,
                 accelerator_type: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] accelerator_count: The number of the accelerator cards of this type exposed to this instance.
        :param pulumi.Input[str] accelerator_type: Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of the accelerator cards of this type exposed to this instance.
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[pulumi.Input[str]]:
        """
        Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "accelerator_type", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgsDict(TypedDict):
        boot_disk_size_gb: NotRequired[pulumi.Input[int]]
        """
        Size in GB of the boot disk (default is 500GB).
        """
        boot_disk_type: NotRequired[pulumi.Input[str]]
        """
        Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        num_local_ssds: NotRequired[pulumi.Input[int]]
        """
        Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfigArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None,
                 num_local_ssds: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] boot_disk_size_gb: Size in GB of the boot disk (default is 500GB).
        :param pulumi.Input[str] boot_disk_type: Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        :param pulumi.Input[int] num_local_ssds: Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)
        if num_local_ssds is not None:
            pulumi.set(__self__, "num_local_ssds", num_local_ssds)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size in GB of the boot disk (default is 500GB).
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)

    @property
    @pulumi.getter(name="numLocalSsds")
    def num_local_ssds(self) -> Optional[pulumi.Input[int]]:
        """
        Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
        return pulumi.get(self, "num_local_ssds")

    @num_local_ssds.setter
    def num_local_ssds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_local_ssds", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgsDict(TypedDict):
        instance_group_manager_name: NotRequired[pulumi.Input[str]]
        """
        Output only. The name of the Instance Group Manager for this group.
        """
        instance_template_name: NotRequired[pulumi.Input[str]]
        """
        Output only. The name of the Instance Template used for the Managed Instance Group.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfigArgs:
    def __init__(__self__, *,
                 instance_group_manager_name: Optional[pulumi.Input[str]] = None,
                 instance_template_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] instance_group_manager_name: Output only. The name of the Instance Group Manager for this group.
        :param pulumi.Input[str] instance_template_name: Output only. The name of the Instance Template used for the Managed Instance Group.
        """
        if instance_group_manager_name is not None:
            pulumi.set(__self__, "instance_group_manager_name", instance_group_manager_name)
        if instance_template_name is not None:
            pulumi.set(__self__, "instance_template_name", instance_template_name)

    @property
    @pulumi.getter(name="instanceGroupManagerName")
    def instance_group_manager_name(self) -> Optional[pulumi.Input[str]]:
        """
        Output only. The name of the Instance Group Manager for this group.
        """
        return pulumi.get(self, "instance_group_manager_name")

    @instance_group_manager_name.setter
    def instance_group_manager_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_group_manager_name", value)

    @property
    @pulumi.getter(name="instanceTemplateName")
    def instance_template_name(self) -> Optional[pulumi.Input[str]]:
        """
        Output only. The name of the Instance Template used for the Managed Instance Group.
        """
        return pulumi.get(self, "instance_template_name")

    @instance_template_name.setter
    def instance_template_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_template_name", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgsDict(TypedDict):
        dataproc_metastore_service: pulumi.Input[str]
        """
        Required. Resource name of an existing Dataproc Metastore service. Example: * `projects/`
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigArgs:
    def __init__(__self__, *,
                 dataproc_metastore_service: pulumi.Input[str]):
        """
        :param pulumi.Input[str] dataproc_metastore_service: Required. Resource name of an existing Dataproc Metastore service. Example: * `projects/`
        """
        pulumi.set(__self__, "dataproc_metastore_service", dataproc_metastore_service)

    @property
    @pulumi.getter(name="dataprocMetastoreService")
    def dataproc_metastore_service(self) -> pulumi.Input[str]:
        """
        Required. Resource name of an existing Dataproc Metastore service. Example: * `projects/`
        """
        return pulumi.get(self, "dataproc_metastore_service")

    @dataproc_metastore_service.setter
    def dataproc_metastore_service(self, value: pulumi.Input[str]):
        pulumi.set(self, "dataproc_metastore_service", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgsDict(TypedDict):
        accelerators: NotRequired[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgsDict']]]]
        """
        Optional. The Compute Engine accelerator configuration for these instances.
        """
        disk_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgsDict']]
        """
        Optional. Disk option config settings.
        """
        image: NotRequired[pulumi.Input[str]]
        """
        Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        """
        instance_names: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        """
        is_preemptible: NotRequired[pulumi.Input[bool]]
        """
        Output only. Specifies that this instance group contains preemptible instances.
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        """
        managed_group_configs: NotRequired[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgsDict']]]]
        """
        Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        """
        min_cpu_platform: NotRequired[pulumi.Input[str]]
        """
        Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        """
        num_instances: NotRequired[pulumi.Input[int]]
        """
        Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
        """
        preemptibility: NotRequired[pulumi.Input[str]]
        """
        Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigArgs:
    def __init__(__self__, *,
                 accelerators: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgs']]]] = None,
                 disk_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgs']] = None,
                 image: Optional[pulumi.Input[str]] = None,
                 instance_names: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 is_preemptible: Optional[pulumi.Input[bool]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 managed_group_configs: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgs']]]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 num_instances: Optional[pulumi.Input[int]] = None,
                 preemptibility: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgs']]] accelerators: Optional. The Compute Engine accelerator configuration for these instances.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgs'] disk_config: Optional. Disk option config settings.
        :param pulumi.Input[str] image: Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] instance_names: Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        :param pulumi.Input[bool] is_preemptible: Output only. Specifies that this instance group contains preemptible instances.
        :param pulumi.Input[str] machine_type: Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        :param pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgs']]] managed_group_configs: Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        :param pulumi.Input[str] min_cpu_platform: Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        :param pulumi.Input[int] num_instances: Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
        :param pulumi.Input[str] preemptibility: Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
        if accelerators is not None:
            pulumi.set(__self__, "accelerators", accelerators)
        if disk_config is not None:
            pulumi.set(__self__, "disk_config", disk_config)
        if image is not None:
            pulumi.set(__self__, "image", image)
        if instance_names is not None:
            pulumi.set(__self__, "instance_names", instance_names)
        if is_preemptible is not None:
            pulumi.set(__self__, "is_preemptible", is_preemptible)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if managed_group_configs is not None:
            pulumi.set(__self__, "managed_group_configs", managed_group_configs)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if num_instances is not None:
            pulumi.set(__self__, "num_instances", num_instances)
        if preemptibility is not None:
            pulumi.set(__self__, "preemptibility", preemptibility)

    @property
    @pulumi.getter
    def accelerators(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgs']]]]:
        """
        Optional. The Compute Engine accelerator configuration for these instances.
        """
        return pulumi.get(self, "accelerators")

    @accelerators.setter
    def accelerators(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgs']]]]):
        pulumi.set(self, "accelerators", value)

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgs']]:
        """
        Optional. Disk option config settings.
        """
        return pulumi.get(self, "disk_config")

    @disk_config.setter
    def disk_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgs']]):
        pulumi.set(self, "disk_config", value)

    @property
    @pulumi.getter
    def image(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        """
        return pulumi.get(self, "image")

    @image.setter
    def image(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "image", value)

    @property
    @pulumi.getter(name="instanceNames")
    def instance_names(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        """
        return pulumi.get(self, "instance_names")

    @instance_names.setter
    def instance_names(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "instance_names", value)

    @property
    @pulumi.getter(name="isPreemptible")
    def is_preemptible(self) -> Optional[pulumi.Input[bool]]:
        """
        Output only. Specifies that this instance group contains preemptible instances.
        """
        return pulumi.get(self, "is_preemptible")

    @is_preemptible.setter
    def is_preemptible(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "is_preemptible", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="managedGroupConfigs")
    def managed_group_configs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgs']]]]:
        """
        Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        """
        return pulumi.get(self, "managed_group_configs")

    @managed_group_configs.setter
    def managed_group_configs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgs']]]]):
        pulumi.set(self, "managed_group_configs", value)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        """
        return pulumi.get(self, "min_cpu_platform")

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_cpu_platform", value)

    @property
    @pulumi.getter(name="numInstances")
    def num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
        """
        return pulumi.get(self, "num_instances")

    @num_instances.setter
    def num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_instances", value)

    @property
    @pulumi.getter
    def preemptibility(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
        return pulumi.get(self, "preemptibility")

    @preemptibility.setter
    def preemptibility(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "preemptibility", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgsDict(TypedDict):
        accelerator_count: NotRequired[pulumi.Input[int]]
        """
        The number of the accelerator cards of this type exposed to this instance.
        """
        accelerator_type: NotRequired[pulumi.Input[str]]
        """
        Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAcceleratorArgs:
    def __init__(__self__, *,
                 accelerator_count: Optional[pulumi.Input[int]] = None,
                 accelerator_type: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] accelerator_count: The number of the accelerator cards of this type exposed to this instance.
        :param pulumi.Input[str] accelerator_type: Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of the accelerator cards of this type exposed to this instance.
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[pulumi.Input[str]]:
        """
        Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "accelerator_type", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgsDict(TypedDict):
        boot_disk_size_gb: NotRequired[pulumi.Input[int]]
        """
        Size in GB of the boot disk (default is 500GB).
        """
        boot_disk_type: NotRequired[pulumi.Input[str]]
        """
        Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        num_local_ssds: NotRequired[pulumi.Input[int]]
        """
        Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfigArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None,
                 num_local_ssds: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] boot_disk_size_gb: Size in GB of the boot disk (default is 500GB).
        :param pulumi.Input[str] boot_disk_type: Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        :param pulumi.Input[int] num_local_ssds: Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)
        if num_local_ssds is not None:
            pulumi.set(__self__, "num_local_ssds", num_local_ssds)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size in GB of the boot disk (default is 500GB).
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)

    @property
    @pulumi.getter(name="numLocalSsds")
    def num_local_ssds(self) -> Optional[pulumi.Input[int]]:
        """
        Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
        return pulumi.get(self, "num_local_ssds")

    @num_local_ssds.setter
    def num_local_ssds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_local_ssds", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgsDict(TypedDict):
        instance_group_manager_name: NotRequired[pulumi.Input[str]]
        """
        Output only. The name of the Instance Group Manager for this group.
        """
        instance_template_name: NotRequired[pulumi.Input[str]]
        """
        Output only. The name of the Instance Template used for the Managed Instance Group.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfigArgs:
    def __init__(__self__, *,
                 instance_group_manager_name: Optional[pulumi.Input[str]] = None,
                 instance_template_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] instance_group_manager_name: Output only. The name of the Instance Group Manager for this group.
        :param pulumi.Input[str] instance_template_name: Output only. The name of the Instance Template used for the Managed Instance Group.
        """
        if instance_group_manager_name is not None:
            pulumi.set(__self__, "instance_group_manager_name", instance_group_manager_name)
        if instance_template_name is not None:
            pulumi.set(__self__, "instance_template_name", instance_template_name)

    @property
    @pulumi.getter(name="instanceGroupManagerName")
    def instance_group_manager_name(self) -> Optional[pulumi.Input[str]]:
        """
        Output only. The name of the Instance Group Manager for this group.
        """
        return pulumi.get(self, "instance_group_manager_name")

    @instance_group_manager_name.setter
    def instance_group_manager_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_group_manager_name", value)

    @property
    @pulumi.getter(name="instanceTemplateName")
    def instance_template_name(self) -> Optional[pulumi.Input[str]]:
        """
        Output only. The name of the Instance Template used for the Managed Instance Group.
        """
        return pulumi.get(self, "instance_template_name")

    @instance_template_name.setter
    def instance_template_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_template_name", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgsDict(TypedDict):
        kerberos_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgsDict']]
        """
        Kerberos related configuration.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigSecurityConfigArgs:
    def __init__(__self__, *,
                 kerberos_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgs']] = None):
        """
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgs'] kerberos_config: Kerberos related configuration.
        """
        if kerberos_config is not None:
            pulumi.set(__self__, "kerberos_config", kerberos_config)

    @property
    @pulumi.getter(name="kerberosConfig")
    def kerberos_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgs']]:
        """
        Kerberos related configuration.
        """
        return pulumi.get(self, "kerberos_config")

    @kerberos_config.setter
    def kerberos_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgs']]):
        pulumi.set(self, "kerberos_config", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgsDict(TypedDict):
        cross_realm_trust_admin_server: NotRequired[pulumi.Input[str]]
        """
        The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
        """
        cross_realm_trust_kdc: NotRequired[pulumi.Input[str]]
        """
        The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
        """
        cross_realm_trust_realm: NotRequired[pulumi.Input[str]]
        """
        The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
        """
        cross_realm_trust_shared_password: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
        """
        enable_kerberos: NotRequired[pulumi.Input[bool]]
        """
        Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
        """
        kdc_db_key: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
        """
        key_password: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
        """
        keystore: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
        """
        keystore_password: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
        """
        kms_key: NotRequired[pulumi.Input[str]]
        """
        The uri of the KMS key used to encrypt various sensitive files.
        """
        realm: NotRequired[pulumi.Input[str]]
        """
        The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
        """
        root_principal_password: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing the root principal password.
        """
        tgt_lifetime_hours: NotRequired[pulumi.Input[int]]
        """
        The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
        """
        truststore: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
        """
        truststore_password: NotRequired[pulumi.Input[str]]
        """
        The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfigArgs:
    def __init__(__self__, *,
                 cross_realm_trust_admin_server: Optional[pulumi.Input[str]] = None,
                 cross_realm_trust_kdc: Optional[pulumi.Input[str]] = None,
                 cross_realm_trust_realm: Optional[pulumi.Input[str]] = None,
                 cross_realm_trust_shared_password: Optional[pulumi.Input[str]] = None,
                 enable_kerberos: Optional[pulumi.Input[bool]] = None,
                 kdc_db_key: Optional[pulumi.Input[str]] = None,
                 key_password: Optional[pulumi.Input[str]] = None,
                 keystore: Optional[pulumi.Input[str]] = None,
                 keystore_password: Optional[pulumi.Input[str]] = None,
                 kms_key: Optional[pulumi.Input[str]] = None,
                 realm: Optional[pulumi.Input[str]] = None,
                 root_principal_password: Optional[pulumi.Input[str]] = None,
                 tgt_lifetime_hours: Optional[pulumi.Input[int]] = None,
                 truststore: Optional[pulumi.Input[str]] = None,
                 truststore_password: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] cross_realm_trust_admin_server: The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
        :param pulumi.Input[str] cross_realm_trust_kdc: The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
        :param pulumi.Input[str] cross_realm_trust_realm: The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
        :param pulumi.Input[str] cross_realm_trust_shared_password: The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
        :param pulumi.Input[bool] enable_kerberos: Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
        :param pulumi.Input[str] kdc_db_key: The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
        :param pulumi.Input[str] key_password: The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
        :param pulumi.Input[str] keystore: The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
        :param pulumi.Input[str] keystore_password: The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
        :param pulumi.Input[str] kms_key: The uri of the KMS key used to encrypt various sensitive files.
        :param pulumi.Input[str] realm: The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
        :param pulumi.Input[str] root_principal_password: The Cloud Storage URI of a KMS encrypted file containing the root principal password.
        :param pulumi.Input[int] tgt_lifetime_hours: The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
        :param pulumi.Input[str] truststore: The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
        :param pulumi.Input[str] truststore_password: The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
        """
        if cross_realm_trust_admin_server is not None:
            pulumi.set(__self__, "cross_realm_trust_admin_server", cross_realm_trust_admin_server)
        if cross_realm_trust_kdc is not None:
            pulumi.set(__self__, "cross_realm_trust_kdc", cross_realm_trust_kdc)
        if cross_realm_trust_realm is not None:
            pulumi.set(__self__, "cross_realm_trust_realm", cross_realm_trust_realm)
        if cross_realm_trust_shared_password is not None:
            pulumi.set(__self__, "cross_realm_trust_shared_password", cross_realm_trust_shared_password)
        if enable_kerberos is not None:
            pulumi.set(__self__, "enable_kerberos", enable_kerberos)
        if kdc_db_key is not None:
            pulumi.set(__self__, "kdc_db_key", kdc_db_key)
        if key_password is not None:
            pulumi.set(__self__, "key_password", key_password)
        if keystore is not None:
            pulumi.set(__self__, "keystore", keystore)
        if keystore_password is not None:
            pulumi.set(__self__, "keystore_password", keystore_password)
        if kms_key is not None:
            pulumi.set(__self__, "kms_key", kms_key)
        if realm is not None:
            pulumi.set(__self__, "realm", realm)
        if root_principal_password is not None:
            pulumi.set(__self__, "root_principal_password", root_principal_password)
        if tgt_lifetime_hours is not None:
            pulumi.set(__self__, "tgt_lifetime_hours", tgt_lifetime_hours)
        if truststore is not None:
            pulumi.set(__self__, "truststore", truststore)
        if truststore_password is not None:
            pulumi.set(__self__, "truststore_password", truststore_password)

    @property
    @pulumi.getter(name="crossRealmTrustAdminServer")
    def cross_realm_trust_admin_server(self) -> Optional[pulumi.Input[str]]:
        """
        The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
        """
        return pulumi.get(self, "cross_realm_trust_admin_server")

    @cross_realm_trust_admin_server.setter
    def cross_realm_trust_admin_server(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_admin_server", value)

    @property
    @pulumi.getter(name="crossRealmTrustKdc")
    def cross_realm_trust_kdc(self) -> Optional[pulumi.Input[str]]:
        """
        The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
        """
        return pulumi.get(self, "cross_realm_trust_kdc")

    @cross_realm_trust_kdc.setter
    def cross_realm_trust_kdc(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_kdc", value)

    @property
    @pulumi.getter(name="crossRealmTrustRealm")
    def cross_realm_trust_realm(self) -> Optional[pulumi.Input[str]]:
        """
        The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
        """
        return pulumi.get(self, "cross_realm_trust_realm")

    @cross_realm_trust_realm.setter
    def cross_realm_trust_realm(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_realm", value)

    @property
    @pulumi.getter(name="crossRealmTrustSharedPassword")
    def cross_realm_trust_shared_password(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
        """
        return pulumi.get(self, "cross_realm_trust_shared_password")

    @cross_realm_trust_shared_password.setter
    def cross_realm_trust_shared_password(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cross_realm_trust_shared_password", value)

    @property
    @pulumi.getter(name="enableKerberos")
    def enable_kerberos(self) -> Optional[pulumi.Input[bool]]:
        """
        Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
        """
        return pulumi.get(self, "enable_kerberos")

    @enable_kerberos.setter
    def enable_kerberos(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_kerberos", value)

    @property
    @pulumi.getter(name="kdcDbKey")
    def kdc_db_key(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
        """
        return pulumi.get(self, "kdc_db_key")

    @kdc_db_key.setter
    def kdc_db_key(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kdc_db_key", value)

    @property
    @pulumi.getter(name="keyPassword")
    def key_password(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
        """
        return pulumi.get(self, "key_password")

    @key_password.setter
    def key_password(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "key_password", value)

    @property
    @pulumi.getter
    def keystore(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
        """
        return pulumi.get(self, "keystore")

    @keystore.setter
    def keystore(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "keystore", value)

    @property
    @pulumi.getter(name="keystorePassword")
    def keystore_password(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
        """
        return pulumi.get(self, "keystore_password")

    @keystore_password.setter
    def keystore_password(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "keystore_password", value)

    @property
    @pulumi.getter(name="kmsKey")
    def kms_key(self) -> Optional[pulumi.Input[str]]:
        """
        The uri of the KMS key used to encrypt various sensitive files.
        """
        return pulumi.get(self, "kms_key")

    @kms_key.setter
    def kms_key(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kms_key", value)

    @property
    @pulumi.getter
    def realm(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
        """
        return pulumi.get(self, "realm")

    @realm.setter
    def realm(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "realm", value)

    @property
    @pulumi.getter(name="rootPrincipalPassword")
    def root_principal_password(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing the root principal password.
        """
        return pulumi.get(self, "root_principal_password")

    @root_principal_password.setter
    def root_principal_password(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "root_principal_password", value)

    @property
    @pulumi.getter(name="tgtLifetimeHours")
    def tgt_lifetime_hours(self) -> Optional[pulumi.Input[int]]:
        """
        The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
        """
        return pulumi.get(self, "tgt_lifetime_hours")

    @tgt_lifetime_hours.setter
    def tgt_lifetime_hours(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "tgt_lifetime_hours", value)

    @property
    @pulumi.getter
    def truststore(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
        """
        return pulumi.get(self, "truststore")

    @truststore.setter
    def truststore(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "truststore", value)

    @property
    @pulumi.getter(name="truststorePassword")
    def truststore_password(self) -> Optional[pulumi.Input[str]]:
        """
        The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
        """
        return pulumi.get(self, "truststore_password")

    @truststore_password.setter
    def truststore_password(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "truststore_password", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgsDict(TypedDict):
        image_version: NotRequired[pulumi.Input[str]]
        """
        The version of software inside the cluster. It must be one of the supported [Dataproc Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the ["preview" version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
        """
        optional_components: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        The set of components to activate on the cluster.
        """
        properties: NotRequired[pulumi.Input[Mapping[str, pulumi.Input[str]]]]
        """
        The properties to set on daemon config files.

        Property keys are specified in `prefix:property` format, for example `core:hadoop.tmp.dir`. The following are supported prefixes and their mappings:

        * capacity-scheduler: `capacity-scheduler.xml`
        * core: `core-site.xml`
        * distcp: `distcp-default.xml`
        * hdfs: `hdfs-site.xml`
        * hive: `hive-site.xml`
        * mapred: `mapred-site.xml`
        * pig: `pig.properties`
        * spark: `spark-defaults.conf`
        * yarn: `yarn-site.xml`


        For more information, see [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigArgs:
    def __init__(__self__, *,
                 image_version: Optional[pulumi.Input[str]] = None,
                 optional_components: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 properties: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input[str] image_version: The version of software inside the cluster. It must be one of the supported [Dataproc Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the ["preview" version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] optional_components: The set of components to activate on the cluster.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] properties: The properties to set on daemon config files.
               
               Property keys are specified in `prefix:property` format, for example `core:hadoop.tmp.dir`. The following are supported prefixes and their mappings:
               
               * capacity-scheduler: `capacity-scheduler.xml`
               * core: `core-site.xml`
               * distcp: `distcp-default.xml`
               * hdfs: `hdfs-site.xml`
               * hive: `hive-site.xml`
               * mapred: `mapred-site.xml`
               * pig: `pig.properties`
               * spark: `spark-defaults.conf`
               * yarn: `yarn-site.xml`
               
               
               For more information, see [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
        """
        if image_version is not None:
            pulumi.set(__self__, "image_version", image_version)
        if optional_components is not None:
            pulumi.set(__self__, "optional_components", optional_components)
        if properties is not None:
            pulumi.set(__self__, "properties", properties)

    @property
    @pulumi.getter(name="imageVersion")
    def image_version(self) -> Optional[pulumi.Input[str]]:
        """
        The version of software inside the cluster. It must be one of the supported [Dataproc Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the ["preview" version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
        """
        return pulumi.get(self, "image_version")

    @image_version.setter
    def image_version(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "image_version", value)

    @property
    @pulumi.getter(name="optionalComponents")
    def optional_components(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The set of components to activate on the cluster.
        """
        return pulumi.get(self, "optional_components")

    @optional_components.setter
    def optional_components(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "optional_components", value)

    @property
    @pulumi.getter
    def properties(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The properties to set on daemon config files.

        Property keys are specified in `prefix:property` format, for example `core:hadoop.tmp.dir`. The following are supported prefixes and their mappings:

        * capacity-scheduler: `capacity-scheduler.xml`
        * core: `core-site.xml`
        * distcp: `distcp-default.xml`
        * hdfs: `hdfs-site.xml`
        * hive: `hive-site.xml`
        * mapred: `mapred-site.xml`
        * pig: `pig.properties`
        * spark: `spark-defaults.conf`
        * yarn: `yarn-site.xml`


        For more information, see [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
        """
        return pulumi.get(self, "properties")

    @properties.setter
    def properties(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "properties", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgsDict(TypedDict):
        accelerators: NotRequired[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgsDict']]]]
        """
        Optional. The Compute Engine accelerator configuration for these instances.
        """
        disk_config: NotRequired[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgsDict']]
        """
        Optional. Disk option config settings.
        """
        image: NotRequired[pulumi.Input[str]]
        """
        Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        """
        instance_names: NotRequired[pulumi.Input[Sequence[pulumi.Input[str]]]]
        """
        Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        """
        is_preemptible: NotRequired[pulumi.Input[bool]]
        """
        Output only. Specifies that this instance group contains preemptible instances.
        """
        machine_type: NotRequired[pulumi.Input[str]]
        """
        Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        """
        managed_group_configs: NotRequired[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgsDict']]]]
        """
        Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        """
        min_cpu_platform: NotRequired[pulumi.Input[str]]
        """
        Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        """
        num_instances: NotRequired[pulumi.Input[int]]
        """
        Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
        """
        preemptibility: NotRequired[pulumi.Input[str]]
        """
        Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigArgs:
    def __init__(__self__, *,
                 accelerators: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgs']]]] = None,
                 disk_config: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgs']] = None,
                 image: Optional[pulumi.Input[str]] = None,
                 instance_names: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 is_preemptible: Optional[pulumi.Input[bool]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 managed_group_configs: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgs']]]] = None,
                 min_cpu_platform: Optional[pulumi.Input[str]] = None,
                 num_instances: Optional[pulumi.Input[int]] = None,
                 preemptibility: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgs']]] accelerators: Optional. The Compute Engine accelerator configuration for these instances.
        :param pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgs'] disk_config: Optional. Disk option config settings.
        :param pulumi.Input[str] image: Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] instance_names: Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        :param pulumi.Input[bool] is_preemptible: Output only. Specifies that this instance group contains preemptible instances.
        :param pulumi.Input[str] machine_type: Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        :param pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgs']]] managed_group_configs: Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        :param pulumi.Input[str] min_cpu_platform: Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        :param pulumi.Input[int] num_instances: Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
        :param pulumi.Input[str] preemptibility: Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
        if accelerators is not None:
            pulumi.set(__self__, "accelerators", accelerators)
        if disk_config is not None:
            pulumi.set(__self__, "disk_config", disk_config)
        if image is not None:
            pulumi.set(__self__, "image", image)
        if instance_names is not None:
            pulumi.set(__self__, "instance_names", instance_names)
        if is_preemptible is not None:
            pulumi.set(__self__, "is_preemptible", is_preemptible)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if managed_group_configs is not None:
            pulumi.set(__self__, "managed_group_configs", managed_group_configs)
        if min_cpu_platform is not None:
            pulumi.set(__self__, "min_cpu_platform", min_cpu_platform)
        if num_instances is not None:
            pulumi.set(__self__, "num_instances", num_instances)
        if preemptibility is not None:
            pulumi.set(__self__, "preemptibility", preemptibility)

    @property
    @pulumi.getter
    def accelerators(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgs']]]]:
        """
        Optional. The Compute Engine accelerator configuration for these instances.
        """
        return pulumi.get(self, "accelerators")

    @accelerators.setter
    def accelerators(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgs']]]]):
        pulumi.set(self, "accelerators", value)

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgs']]:
        """
        Optional. Disk option config settings.
        """
        return pulumi.get(self, "disk_config")

    @disk_config.setter
    def disk_config(self, value: Optional[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgs']]):
        pulumi.set(self, "disk_config", value)

    @property
    @pulumi.getter
    def image(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
        """
        return pulumi.get(self, "image")

    @image.setter
    def image(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "image", value)

    @property
    @pulumi.getter(name="instanceNames")
    def instance_names(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
        """
        return pulumi.get(self, "instance_names")

    @instance_names.setter
    def instance_names(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "instance_names", value)

    @property
    @pulumi.getter(name="isPreemptible")
    def is_preemptible(self) -> Optional[pulumi.Input[bool]]:
        """
        Output only. Specifies that this instance group contains preemptible instances.
        """
        return pulumi.get(self, "is_preemptible")

    @is_preemptible.setter
    def is_preemptible(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "is_preemptible", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="managedGroupConfigs")
    def managed_group_configs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgs']]]]:
        """
        Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
        """
        return pulumi.get(self, "managed_group_configs")

    @managed_group_configs.setter
    def managed_group_configs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgs']]]]):
        pulumi.set(self, "managed_group_configs", value)

    @property
    @pulumi.getter(name="minCpuPlatform")
    def min_cpu_platform(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
        """
        return pulumi.get(self, "min_cpu_platform")

    @min_cpu_platform.setter
    def min_cpu_platform(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_cpu_platform", value)

    @property
    @pulumi.getter(name="numInstances")
    def num_instances(self) -> Optional[pulumi.Input[int]]:
        """
        Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
        """
        return pulumi.get(self, "num_instances")

    @num_instances.setter
    def num_instances(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_instances", value)

    @property
    @pulumi.getter
    def preemptibility(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
        """
        return pulumi.get(self, "preemptibility")

    @preemptibility.setter
    def preemptibility(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "preemptibility", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgsDict(TypedDict):
        accelerator_count: NotRequired[pulumi.Input[int]]
        """
        The number of the accelerator cards of this type exposed to this instance.
        """
        accelerator_type: NotRequired[pulumi.Input[str]]
        """
        Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAcceleratorArgs:
    def __init__(__self__, *,
                 accelerator_count: Optional[pulumi.Input[int]] = None,
                 accelerator_type: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] accelerator_count: The number of the accelerator cards of this type exposed to this instance.
        :param pulumi.Input[str] accelerator_type: Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of the accelerator cards of this type exposed to this instance.
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[pulumi.Input[str]]:
        """
        Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "accelerator_type", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgsDict(TypedDict):
        boot_disk_size_gb: NotRequired[pulumi.Input[int]]
        """
        Size in GB of the boot disk (default is 500GB).
        """
        boot_disk_type: NotRequired[pulumi.Input[str]]
        """
        Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        num_local_ssds: NotRequired[pulumi.Input[int]]
        """
        Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfigArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None,
                 num_local_ssds: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] boot_disk_size_gb: Size in GB of the boot disk (default is 500GB).
        :param pulumi.Input[str] boot_disk_type: Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        :param pulumi.Input[int] num_local_ssds: Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)
        if num_local_ssds is not None:
            pulumi.set(__self__, "num_local_ssds", num_local_ssds)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size in GB of the boot disk (default is 500GB).
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)

    @property
    @pulumi.getter(name="numLocalSsds")
    def num_local_ssds(self) -> Optional[pulumi.Input[int]]:
        """
        Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
        """
        return pulumi.get(self, "num_local_ssds")

    @num_local_ssds.setter
    def num_local_ssds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "num_local_ssds", value)


if not MYPY:
    class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgsDict(TypedDict):
        instance_group_manager_name: NotRequired[pulumi.Input[str]]
        """
        Output only. The name of the Instance Group Manager for this group.
        """
        instance_template_name: NotRequired[pulumi.Input[str]]
        """
        Output only. The name of the Instance Template used for the Managed Instance Group.
        """
elif False:
    WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgsDict: TypeAlias = Mapping[str, Any]

@pulumi.input_type
class WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfigArgs:
    def __init__(__self__, *,
                 instance_group_manager_name: Optional[pulumi.Input[str]] = None,
                 instance_template_name: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] instance_group_manager_name: Output only. The name of the Instance Group Manager for this group.
        :param pulumi.Input[str] instance_template_name: Output only. The name of the Instance Template used for the Managed Instance Group.
        """
        if instance_group_manager_name is not None:
            pulumi.set(__self__, "instance_group_manager_name", instance_group_manager_name)
        if instance_template_name is not None:
            pulumi.set(__self__, "instance_template_name", instance_template_name)

    @property
    @pulumi.getter(name="instanceGroupManagerName")
    def instance_group_manager_name(self) -> Optional[pulumi.Input[str]]:
        """
        Output only. The name of the Instance Group Manager for this group.
        """
        return pulumi.get(self, "instance_group_manager_name")

    @instance_group_manager_name.setter
    def instance_group_manager_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_group_manager_name", value)

    @property
    @pulumi.getter(name="instanceTemplateName")
    def instance_template_name(self) -> Optional[pulumi.Input[str]]:
        """
        Output only. The name of the Instance Template used for the Managed Instance Group.
        """
        return pulumi.get(self, "instance_template_name")

    @instance_template_name.setter
    def instance_template_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_template_name", value)



# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Callable, Mapping, Optional, Sequence, Union, overload
from .. import _utilities

__all__ = ['JobArgs', 'Job']

@pulumi.input_type
class JobArgs:
    def __init__(__self__, *,
                 temp_gcs_location: pulumi.Input[str],
                 template_gcs_path: pulumi.Input[str],
                 additional_experiments: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 enable_streaming_engine: Optional[pulumi.Input[bool]] = None,
                 ip_configuration: Optional[pulumi.Input[str]] = None,
                 kms_key_name: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 max_workers: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 on_delete: Optional[pulumi.Input[str]] = None,
                 parameters: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 project: Optional[pulumi.Input[str]] = None,
                 region: Optional[pulumi.Input[str]] = None,
                 service_account_email: Optional[pulumi.Input[str]] = None,
                 skip_wait_on_job_termination: Optional[pulumi.Input[bool]] = None,
                 subnetwork: Optional[pulumi.Input[str]] = None,
                 transform_name_mapping: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 zone: Optional[pulumi.Input[str]] = None):
        """
        The set of arguments for constructing a Job resource.
        :param pulumi.Input[str] temp_gcs_location: A writeable location on GCS for the Dataflow job to dump its temporary data.
               
               - - -
        :param pulumi.Input[str] template_gcs_path: The GCS path to the Dataflow job template.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] additional_experiments: List of experiments that should be used by the job. An example value is `["enable_stackdriver_agent_metrics"]`.
        :param pulumi.Input[bool] enable_streaming_engine: Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
        :param pulumi.Input[str] ip_configuration: The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
        :param pulumi.Input[str] kms_key_name: The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
        :param pulumi.Input[Mapping[str, Any]] labels: User labels to be specified for the job. Keys and values should follow the restrictions
               specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
               **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
               Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
        :param pulumi.Input[str] machine_type: The machine type to use for the job.
        :param pulumi.Input[int] max_workers: The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
        :param pulumi.Input[str] name: A unique name for the resource, required by Dataflow.
        :param pulumi.Input[str] network: The network to which VMs will be assigned. If it is not provided, "default" will be used.
        :param pulumi.Input[str] on_delete: One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
        :param pulumi.Input[Mapping[str, Any]] parameters: Key/Value pairs to be passed to the Dataflow job (as used in the template).
        :param pulumi.Input[str] project: The project in which the resource belongs. If it is not provided, the provider project is used.
        :param pulumi.Input[str] region: The region in which the created job should run.
        :param pulumi.Input[str] service_account_email: The Service Account email used to create the job.
        :param pulumi.Input[bool] skip_wait_on_job_termination: If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
        :param pulumi.Input[str] subnetwork: The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
        :param pulumi.Input[Mapping[str, Any]] transform_name_mapping: Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
        :param pulumi.Input[str] zone: The zone in which the created job should run. If it is not provided, the provider zone is used.
        """
        JobArgs._configure(
            lambda key, value: pulumi.set(__self__, key, value),
            temp_gcs_location=temp_gcs_location,
            template_gcs_path=template_gcs_path,
            additional_experiments=additional_experiments,
            enable_streaming_engine=enable_streaming_engine,
            ip_configuration=ip_configuration,
            kms_key_name=kms_key_name,
            labels=labels,
            machine_type=machine_type,
            max_workers=max_workers,
            name=name,
            network=network,
            on_delete=on_delete,
            parameters=parameters,
            project=project,
            region=region,
            service_account_email=service_account_email,
            skip_wait_on_job_termination=skip_wait_on_job_termination,
            subnetwork=subnetwork,
            transform_name_mapping=transform_name_mapping,
            zone=zone,
        )
    @staticmethod
    def _configure(
             _setter: Callable[[Any, Any], None],
             temp_gcs_location: pulumi.Input[str],
             template_gcs_path: pulumi.Input[str],
             additional_experiments: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
             enable_streaming_engine: Optional[pulumi.Input[bool]] = None,
             ip_configuration: Optional[pulumi.Input[str]] = None,
             kms_key_name: Optional[pulumi.Input[str]] = None,
             labels: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             machine_type: Optional[pulumi.Input[str]] = None,
             max_workers: Optional[pulumi.Input[int]] = None,
             name: Optional[pulumi.Input[str]] = None,
             network: Optional[pulumi.Input[str]] = None,
             on_delete: Optional[pulumi.Input[str]] = None,
             parameters: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             project: Optional[pulumi.Input[str]] = None,
             region: Optional[pulumi.Input[str]] = None,
             service_account_email: Optional[pulumi.Input[str]] = None,
             skip_wait_on_job_termination: Optional[pulumi.Input[bool]] = None,
             subnetwork: Optional[pulumi.Input[str]] = None,
             transform_name_mapping: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             zone: Optional[pulumi.Input[str]] = None,
             opts: Optional[pulumi.ResourceOptions]=None):
        _setter("temp_gcs_location", temp_gcs_location)
        _setter("template_gcs_path", template_gcs_path)
        if additional_experiments is not None:
            _setter("additional_experiments", additional_experiments)
        if enable_streaming_engine is not None:
            _setter("enable_streaming_engine", enable_streaming_engine)
        if ip_configuration is not None:
            _setter("ip_configuration", ip_configuration)
        if kms_key_name is not None:
            _setter("kms_key_name", kms_key_name)
        if labels is not None:
            _setter("labels", labels)
        if machine_type is not None:
            _setter("machine_type", machine_type)
        if max_workers is not None:
            _setter("max_workers", max_workers)
        if name is not None:
            _setter("name", name)
        if network is not None:
            _setter("network", network)
        if on_delete is not None:
            _setter("on_delete", on_delete)
        if parameters is not None:
            _setter("parameters", parameters)
        if project is not None:
            _setter("project", project)
        if region is not None:
            _setter("region", region)
        if service_account_email is not None:
            _setter("service_account_email", service_account_email)
        if skip_wait_on_job_termination is not None:
            _setter("skip_wait_on_job_termination", skip_wait_on_job_termination)
        if subnetwork is not None:
            _setter("subnetwork", subnetwork)
        if transform_name_mapping is not None:
            _setter("transform_name_mapping", transform_name_mapping)
        if zone is not None:
            _setter("zone", zone)

    @property
    @pulumi.getter(name="tempGcsLocation")
    def temp_gcs_location(self) -> pulumi.Input[str]:
        """
        A writeable location on GCS for the Dataflow job to dump its temporary data.

        - - -
        """
        return pulumi.get(self, "temp_gcs_location")

    @temp_gcs_location.setter
    def temp_gcs_location(self, value: pulumi.Input[str]):
        pulumi.set(self, "temp_gcs_location", value)

    @property
    @pulumi.getter(name="templateGcsPath")
    def template_gcs_path(self) -> pulumi.Input[str]:
        """
        The GCS path to the Dataflow job template.
        """
        return pulumi.get(self, "template_gcs_path")

    @template_gcs_path.setter
    def template_gcs_path(self, value: pulumi.Input[str]):
        pulumi.set(self, "template_gcs_path", value)

    @property
    @pulumi.getter(name="additionalExperiments")
    def additional_experiments(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        List of experiments that should be used by the job. An example value is `["enable_stackdriver_agent_metrics"]`.
        """
        return pulumi.get(self, "additional_experiments")

    @additional_experiments.setter
    def additional_experiments(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "additional_experiments", value)

    @property
    @pulumi.getter(name="enableStreamingEngine")
    def enable_streaming_engine(self) -> Optional[pulumi.Input[bool]]:
        """
        Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
        """
        return pulumi.get(self, "enable_streaming_engine")

    @enable_streaming_engine.setter
    def enable_streaming_engine(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_streaming_engine", value)

    @property
    @pulumi.getter(name="ipConfiguration")
    def ip_configuration(self) -> Optional[pulumi.Input[str]]:
        """
        The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
        """
        return pulumi.get(self, "ip_configuration")

    @ip_configuration.setter
    def ip_configuration(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "ip_configuration", value)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> Optional[pulumi.Input[str]]:
        """
        The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kms_key_name", value)

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        User labels to be specified for the job. Keys and values should follow the restrictions
        specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
        **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
        Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
        """
        return pulumi.get(self, "labels")

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "labels", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The machine type to use for the job.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="maxWorkers")
    def max_workers(self) -> Optional[pulumi.Input[int]]:
        """
        The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
        """
        return pulumi.get(self, "max_workers")

    @max_workers.setter
    def max_workers(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_workers", value)

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        A unique name for the resource, required by Dataflow.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def network(self) -> Optional[pulumi.Input[str]]:
        """
        The network to which VMs will be assigned. If it is not provided, "default" will be used.
        """
        return pulumi.get(self, "network")

    @network.setter
    def network(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "network", value)

    @property
    @pulumi.getter(name="onDelete")
    def on_delete(self) -> Optional[pulumi.Input[str]]:
        """
        One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
        """
        return pulumi.get(self, "on_delete")

    @on_delete.setter
    def on_delete(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "on_delete", value)

    @property
    @pulumi.getter
    def parameters(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        Key/Value pairs to be passed to the Dataflow job (as used in the template).
        """
        return pulumi.get(self, "parameters")

    @parameters.setter
    def parameters(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "parameters", value)

    @property
    @pulumi.getter
    def project(self) -> Optional[pulumi.Input[str]]:
        """
        The project in which the resource belongs. If it is not provided, the provider project is used.
        """
        return pulumi.get(self, "project")

    @project.setter
    def project(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "project", value)

    @property
    @pulumi.getter
    def region(self) -> Optional[pulumi.Input[str]]:
        """
        The region in which the created job should run.
        """
        return pulumi.get(self, "region")

    @region.setter
    def region(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "region", value)

    @property
    @pulumi.getter(name="serviceAccountEmail")
    def service_account_email(self) -> Optional[pulumi.Input[str]]:
        """
        The Service Account email used to create the job.
        """
        return pulumi.get(self, "service_account_email")

    @service_account_email.setter
    def service_account_email(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_account_email", value)

    @property
    @pulumi.getter(name="skipWaitOnJobTermination")
    def skip_wait_on_job_termination(self) -> Optional[pulumi.Input[bool]]:
        """
        If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
        """
        return pulumi.get(self, "skip_wait_on_job_termination")

    @skip_wait_on_job_termination.setter
    def skip_wait_on_job_termination(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "skip_wait_on_job_termination", value)

    @property
    @pulumi.getter
    def subnetwork(self) -> Optional[pulumi.Input[str]]:
        """
        The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
        """
        return pulumi.get(self, "subnetwork")

    @subnetwork.setter
    def subnetwork(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "subnetwork", value)

    @property
    @pulumi.getter(name="transformNameMapping")
    def transform_name_mapping(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
        """
        return pulumi.get(self, "transform_name_mapping")

    @transform_name_mapping.setter
    def transform_name_mapping(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "transform_name_mapping", value)

    @property
    @pulumi.getter
    def zone(self) -> Optional[pulumi.Input[str]]:
        """
        The zone in which the created job should run. If it is not provided, the provider zone is used.
        """
        return pulumi.get(self, "zone")

    @zone.setter
    def zone(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "zone", value)


@pulumi.input_type
class _JobState:
    def __init__(__self__, *,
                 additional_experiments: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 enable_streaming_engine: Optional[pulumi.Input[bool]] = None,
                 ip_configuration: Optional[pulumi.Input[str]] = None,
                 job_id: Optional[pulumi.Input[str]] = None,
                 kms_key_name: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 max_workers: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 on_delete: Optional[pulumi.Input[str]] = None,
                 parameters: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 project: Optional[pulumi.Input[str]] = None,
                 region: Optional[pulumi.Input[str]] = None,
                 service_account_email: Optional[pulumi.Input[str]] = None,
                 skip_wait_on_job_termination: Optional[pulumi.Input[bool]] = None,
                 state: Optional[pulumi.Input[str]] = None,
                 subnetwork: Optional[pulumi.Input[str]] = None,
                 temp_gcs_location: Optional[pulumi.Input[str]] = None,
                 template_gcs_path: Optional[pulumi.Input[str]] = None,
                 transform_name_mapping: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 type: Optional[pulumi.Input[str]] = None,
                 zone: Optional[pulumi.Input[str]] = None):
        """
        Input properties used for looking up and filtering Job resources.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] additional_experiments: List of experiments that should be used by the job. An example value is `["enable_stackdriver_agent_metrics"]`.
        :param pulumi.Input[bool] enable_streaming_engine: Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
        :param pulumi.Input[str] ip_configuration: The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
        :param pulumi.Input[str] job_id: The unique ID of this job.
        :param pulumi.Input[str] kms_key_name: The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
        :param pulumi.Input[Mapping[str, Any]] labels: User labels to be specified for the job. Keys and values should follow the restrictions
               specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
               **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
               Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
        :param pulumi.Input[str] machine_type: The machine type to use for the job.
        :param pulumi.Input[int] max_workers: The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
        :param pulumi.Input[str] name: A unique name for the resource, required by Dataflow.
        :param pulumi.Input[str] network: The network to which VMs will be assigned. If it is not provided, "default" will be used.
        :param pulumi.Input[str] on_delete: One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
        :param pulumi.Input[Mapping[str, Any]] parameters: Key/Value pairs to be passed to the Dataflow job (as used in the template).
        :param pulumi.Input[str] project: The project in which the resource belongs. If it is not provided, the provider project is used.
        :param pulumi.Input[str] region: The region in which the created job should run.
        :param pulumi.Input[str] service_account_email: The Service Account email used to create the job.
        :param pulumi.Input[bool] skip_wait_on_job_termination: If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
        :param pulumi.Input[str] state: The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
        :param pulumi.Input[str] subnetwork: The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
        :param pulumi.Input[str] temp_gcs_location: A writeable location on GCS for the Dataflow job to dump its temporary data.
               
               - - -
        :param pulumi.Input[str] template_gcs_path: The GCS path to the Dataflow job template.
        :param pulumi.Input[Mapping[str, Any]] transform_name_mapping: Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
        :param pulumi.Input[str] type: The type of this job, selected from the [JobType enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobType)
        :param pulumi.Input[str] zone: The zone in which the created job should run. If it is not provided, the provider zone is used.
        """
        _JobState._configure(
            lambda key, value: pulumi.set(__self__, key, value),
            additional_experiments=additional_experiments,
            enable_streaming_engine=enable_streaming_engine,
            ip_configuration=ip_configuration,
            job_id=job_id,
            kms_key_name=kms_key_name,
            labels=labels,
            machine_type=machine_type,
            max_workers=max_workers,
            name=name,
            network=network,
            on_delete=on_delete,
            parameters=parameters,
            project=project,
            region=region,
            service_account_email=service_account_email,
            skip_wait_on_job_termination=skip_wait_on_job_termination,
            state=state,
            subnetwork=subnetwork,
            temp_gcs_location=temp_gcs_location,
            template_gcs_path=template_gcs_path,
            transform_name_mapping=transform_name_mapping,
            type=type,
            zone=zone,
        )
    @staticmethod
    def _configure(
             _setter: Callable[[Any, Any], None],
             additional_experiments: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
             enable_streaming_engine: Optional[pulumi.Input[bool]] = None,
             ip_configuration: Optional[pulumi.Input[str]] = None,
             job_id: Optional[pulumi.Input[str]] = None,
             kms_key_name: Optional[pulumi.Input[str]] = None,
             labels: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             machine_type: Optional[pulumi.Input[str]] = None,
             max_workers: Optional[pulumi.Input[int]] = None,
             name: Optional[pulumi.Input[str]] = None,
             network: Optional[pulumi.Input[str]] = None,
             on_delete: Optional[pulumi.Input[str]] = None,
             parameters: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             project: Optional[pulumi.Input[str]] = None,
             region: Optional[pulumi.Input[str]] = None,
             service_account_email: Optional[pulumi.Input[str]] = None,
             skip_wait_on_job_termination: Optional[pulumi.Input[bool]] = None,
             state: Optional[pulumi.Input[str]] = None,
             subnetwork: Optional[pulumi.Input[str]] = None,
             temp_gcs_location: Optional[pulumi.Input[str]] = None,
             template_gcs_path: Optional[pulumi.Input[str]] = None,
             transform_name_mapping: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             type: Optional[pulumi.Input[str]] = None,
             zone: Optional[pulumi.Input[str]] = None,
             opts: Optional[pulumi.ResourceOptions]=None):
        if additional_experiments is not None:
            _setter("additional_experiments", additional_experiments)
        if enable_streaming_engine is not None:
            _setter("enable_streaming_engine", enable_streaming_engine)
        if ip_configuration is not None:
            _setter("ip_configuration", ip_configuration)
        if job_id is not None:
            _setter("job_id", job_id)
        if kms_key_name is not None:
            _setter("kms_key_name", kms_key_name)
        if labels is not None:
            _setter("labels", labels)
        if machine_type is not None:
            _setter("machine_type", machine_type)
        if max_workers is not None:
            _setter("max_workers", max_workers)
        if name is not None:
            _setter("name", name)
        if network is not None:
            _setter("network", network)
        if on_delete is not None:
            _setter("on_delete", on_delete)
        if parameters is not None:
            _setter("parameters", parameters)
        if project is not None:
            _setter("project", project)
        if region is not None:
            _setter("region", region)
        if service_account_email is not None:
            _setter("service_account_email", service_account_email)
        if skip_wait_on_job_termination is not None:
            _setter("skip_wait_on_job_termination", skip_wait_on_job_termination)
        if state is not None:
            _setter("state", state)
        if subnetwork is not None:
            _setter("subnetwork", subnetwork)
        if temp_gcs_location is not None:
            _setter("temp_gcs_location", temp_gcs_location)
        if template_gcs_path is not None:
            _setter("template_gcs_path", template_gcs_path)
        if transform_name_mapping is not None:
            _setter("transform_name_mapping", transform_name_mapping)
        if type is not None:
            _setter("type", type)
        if zone is not None:
            _setter("zone", zone)

    @property
    @pulumi.getter(name="additionalExperiments")
    def additional_experiments(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        List of experiments that should be used by the job. An example value is `["enable_stackdriver_agent_metrics"]`.
        """
        return pulumi.get(self, "additional_experiments")

    @additional_experiments.setter
    def additional_experiments(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "additional_experiments", value)

    @property
    @pulumi.getter(name="enableStreamingEngine")
    def enable_streaming_engine(self) -> Optional[pulumi.Input[bool]]:
        """
        Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
        """
        return pulumi.get(self, "enable_streaming_engine")

    @enable_streaming_engine.setter
    def enable_streaming_engine(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_streaming_engine", value)

    @property
    @pulumi.getter(name="ipConfiguration")
    def ip_configuration(self) -> Optional[pulumi.Input[str]]:
        """
        The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
        """
        return pulumi.get(self, "ip_configuration")

    @ip_configuration.setter
    def ip_configuration(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "ip_configuration", value)

    @property
    @pulumi.getter(name="jobId")
    def job_id(self) -> Optional[pulumi.Input[str]]:
        """
        The unique ID of this job.
        """
        return pulumi.get(self, "job_id")

    @job_id.setter
    def job_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "job_id", value)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> Optional[pulumi.Input[str]]:
        """
        The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "kms_key_name", value)

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        User labels to be specified for the job. Keys and values should follow the restrictions
        specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
        **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
        Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
        """
        return pulumi.get(self, "labels")

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "labels", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        The machine type to use for the job.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="maxWorkers")
    def max_workers(self) -> Optional[pulumi.Input[int]]:
        """
        The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
        """
        return pulumi.get(self, "max_workers")

    @max_workers.setter
    def max_workers(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_workers", value)

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        A unique name for the resource, required by Dataflow.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def network(self) -> Optional[pulumi.Input[str]]:
        """
        The network to which VMs will be assigned. If it is not provided, "default" will be used.
        """
        return pulumi.get(self, "network")

    @network.setter
    def network(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "network", value)

    @property
    @pulumi.getter(name="onDelete")
    def on_delete(self) -> Optional[pulumi.Input[str]]:
        """
        One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
        """
        return pulumi.get(self, "on_delete")

    @on_delete.setter
    def on_delete(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "on_delete", value)

    @property
    @pulumi.getter
    def parameters(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        Key/Value pairs to be passed to the Dataflow job (as used in the template).
        """
        return pulumi.get(self, "parameters")

    @parameters.setter
    def parameters(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "parameters", value)

    @property
    @pulumi.getter
    def project(self) -> Optional[pulumi.Input[str]]:
        """
        The project in which the resource belongs. If it is not provided, the provider project is used.
        """
        return pulumi.get(self, "project")

    @project.setter
    def project(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "project", value)

    @property
    @pulumi.getter
    def region(self) -> Optional[pulumi.Input[str]]:
        """
        The region in which the created job should run.
        """
        return pulumi.get(self, "region")

    @region.setter
    def region(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "region", value)

    @property
    @pulumi.getter(name="serviceAccountEmail")
    def service_account_email(self) -> Optional[pulumi.Input[str]]:
        """
        The Service Account email used to create the job.
        """
        return pulumi.get(self, "service_account_email")

    @service_account_email.setter
    def service_account_email(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_account_email", value)

    @property
    @pulumi.getter(name="skipWaitOnJobTermination")
    def skip_wait_on_job_termination(self) -> Optional[pulumi.Input[bool]]:
        """
        If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
        """
        return pulumi.get(self, "skip_wait_on_job_termination")

    @skip_wait_on_job_termination.setter
    def skip_wait_on_job_termination(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "skip_wait_on_job_termination", value)

    @property
    @pulumi.getter
    def state(self) -> Optional[pulumi.Input[str]]:
        """
        The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
        """
        return pulumi.get(self, "state")

    @state.setter
    def state(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "state", value)

    @property
    @pulumi.getter
    def subnetwork(self) -> Optional[pulumi.Input[str]]:
        """
        The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
        """
        return pulumi.get(self, "subnetwork")

    @subnetwork.setter
    def subnetwork(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "subnetwork", value)

    @property
    @pulumi.getter(name="tempGcsLocation")
    def temp_gcs_location(self) -> Optional[pulumi.Input[str]]:
        """
        A writeable location on GCS for the Dataflow job to dump its temporary data.

        - - -
        """
        return pulumi.get(self, "temp_gcs_location")

    @temp_gcs_location.setter
    def temp_gcs_location(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "temp_gcs_location", value)

    @property
    @pulumi.getter(name="templateGcsPath")
    def template_gcs_path(self) -> Optional[pulumi.Input[str]]:
        """
        The GCS path to the Dataflow job template.
        """
        return pulumi.get(self, "template_gcs_path")

    @template_gcs_path.setter
    def template_gcs_path(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "template_gcs_path", value)

    @property
    @pulumi.getter(name="transformNameMapping")
    def transform_name_mapping(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
        """
        return pulumi.get(self, "transform_name_mapping")

    @transform_name_mapping.setter
    def transform_name_mapping(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "transform_name_mapping", value)

    @property
    @pulumi.getter
    def type(self) -> Optional[pulumi.Input[str]]:
        """
        The type of this job, selected from the [JobType enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobType)
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "type", value)

    @property
    @pulumi.getter
    def zone(self) -> Optional[pulumi.Input[str]]:
        """
        The zone in which the created job should run. If it is not provided, the provider zone is used.
        """
        return pulumi.get(self, "zone")

    @zone.setter
    def zone(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "zone", value)


class Job(pulumi.CustomResource):
    @overload
    def __init__(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 additional_experiments: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 enable_streaming_engine: Optional[pulumi.Input[bool]] = None,
                 ip_configuration: Optional[pulumi.Input[str]] = None,
                 kms_key_name: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 max_workers: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 on_delete: Optional[pulumi.Input[str]] = None,
                 parameters: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 project: Optional[pulumi.Input[str]] = None,
                 region: Optional[pulumi.Input[str]] = None,
                 service_account_email: Optional[pulumi.Input[str]] = None,
                 skip_wait_on_job_termination: Optional[pulumi.Input[bool]] = None,
                 subnetwork: Optional[pulumi.Input[str]] = None,
                 temp_gcs_location: Optional[pulumi.Input[str]] = None,
                 template_gcs_path: Optional[pulumi.Input[str]] = None,
                 transform_name_mapping: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 zone: Optional[pulumi.Input[str]] = None,
                 __props__=None):
        """
        Creates a job on Dataflow, which is an implementation of Apache Beam running on Google Compute Engine. For more information see
        the official documentation for
        [Beam](https://beam.apache.org) and [Dataflow](https://cloud.google.com/dataflow/).

        ## Example Usage

        ```python
        import pulumi
        import pulumi_gcp as gcp

        big_data_job = gcp.dataflow.Job("bigDataJob",
            parameters={
                "baz": "qux",
                "foo": "bar",
            },
            temp_gcs_location="gs://my-bucket/tmp_dir",
            template_gcs_path="gs://my-bucket/templates/template_file")
        ```
        ### Streaming Job

        ```python
        import pulumi
        import pulumi_gcp as gcp

        topic = gcp.pubsub.Topic("topic")
        bucket1 = gcp.storage.Bucket("bucket1",
            location="US",
            force_destroy=True)
        bucket2 = gcp.storage.Bucket("bucket2",
            location="US",
            force_destroy=True)
        pubsub_stream = gcp.dataflow.Job("pubsubStream",
            template_gcs_path="gs://my-bucket/templates/template_file",
            temp_gcs_location="gs://my-bucket/tmp_dir",
            enable_streaming_engine=True,
            parameters={
                "inputFilePattern": bucket1.url.apply(lambda url: f"{url}/*.json"),
                "outputTopic": topic.id,
            },
            transform_name_mapping={
                "name": "test_job",
                "env": "test",
            },
            on_delete="cancel")
        ```
        ## Note on "destroy" / "apply"

        There are many types of Dataflow jobs.  Some Dataflow jobs run constantly, getting new data from (e.g.) a GCS bucket, and outputting data continuously.  Some jobs process a set amount of data then terminate.  All jobs can fail while running due to programming errors or other issues.  In this way, Dataflow jobs are different from most other Google resources.

        The Dataflow resource is considered 'existing' while it is in a nonterminal state.  If it reaches a terminal state (e.g. 'FAILED', 'COMPLETE', 'CANCELLED'), it will be recreated on the next 'apply'.  This is as expected for jobs which run continuously, but may surprise users who use this resource for other kinds of Dataflow jobs.

        A Dataflow job which is 'destroyed' may be "cancelled" or "drained".  If "cancelled", the job terminates - any data written remains where it is, but no new data will be processed.  If "drained", no new data will enter the pipeline, but any data currently in the pipeline will finish being processed.  The default is "drain". When `on_delete` is set to `"drain"` in the configuration, you may experience a long wait for your `pulumi destroy` to complete.

        You can potentially short-circuit the wait by setting `skip_wait_on_job_termination` to `true`, but beware that unless you take active steps to ensure that the job `name` parameter changes between instances, the name will conflict and the launch of the new job will fail. One way to do this is with a random_id resource, for example:

        ```python
        import pulumi
        import pulumi_gcp as gcp
        import pulumi_random as random

        config = pulumi.Config()
        big_data_job_subscription_id = config.get("bigDataJobSubscriptionId")
        if big_data_job_subscription_id is None:
            big_data_job_subscription_id = "projects/myproject/subscriptions/messages"
        big_data_job_name_suffix = random.RandomId("bigDataJobNameSuffix",
            byte_length=4,
            keepers={
                "region": var["region"],
                "subscription_id": big_data_job_subscription_id,
            })
        big_data_job = gcp.dataflow.FlexTemplateJob("bigDataJob",
            region=var["region"],
            container_spec_gcs_path="gs://my-bucket/templates/template.json",
            skip_wait_on_job_termination=True,
            parameters={
                "inputSubscription": big_data_job_subscription_id,
            },
            opts=pulumi.ResourceOptions(provider=google_beta))
        ```

        ## Import

        Dataflow jobs can be imported using the job `id` e.g.

        ```sh
         $ pulumi import gcp:dataflow/job:Job example 2022-07-31_06_25_42-11926927532632678660
        ```

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] additional_experiments: List of experiments that should be used by the job. An example value is `["enable_stackdriver_agent_metrics"]`.
        :param pulumi.Input[bool] enable_streaming_engine: Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
        :param pulumi.Input[str] ip_configuration: The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
        :param pulumi.Input[str] kms_key_name: The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
        :param pulumi.Input[Mapping[str, Any]] labels: User labels to be specified for the job. Keys and values should follow the restrictions
               specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
               **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
               Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
        :param pulumi.Input[str] machine_type: The machine type to use for the job.
        :param pulumi.Input[int] max_workers: The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
        :param pulumi.Input[str] name: A unique name for the resource, required by Dataflow.
        :param pulumi.Input[str] network: The network to which VMs will be assigned. If it is not provided, "default" will be used.
        :param pulumi.Input[str] on_delete: One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
        :param pulumi.Input[Mapping[str, Any]] parameters: Key/Value pairs to be passed to the Dataflow job (as used in the template).
        :param pulumi.Input[str] project: The project in which the resource belongs. If it is not provided, the provider project is used.
        :param pulumi.Input[str] region: The region in which the created job should run.
        :param pulumi.Input[str] service_account_email: The Service Account email used to create the job.
        :param pulumi.Input[bool] skip_wait_on_job_termination: If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
        :param pulumi.Input[str] subnetwork: The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
        :param pulumi.Input[str] temp_gcs_location: A writeable location on GCS for the Dataflow job to dump its temporary data.
               
               - - -
        :param pulumi.Input[str] template_gcs_path: The GCS path to the Dataflow job template.
        :param pulumi.Input[Mapping[str, Any]] transform_name_mapping: Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
        :param pulumi.Input[str] zone: The zone in which the created job should run. If it is not provided, the provider zone is used.
        """
        ...
    @overload
    def __init__(__self__,
                 resource_name: str,
                 args: JobArgs,
                 opts: Optional[pulumi.ResourceOptions] = None):
        """
        Creates a job on Dataflow, which is an implementation of Apache Beam running on Google Compute Engine. For more information see
        the official documentation for
        [Beam](https://beam.apache.org) and [Dataflow](https://cloud.google.com/dataflow/).

        ## Example Usage

        ```python
        import pulumi
        import pulumi_gcp as gcp

        big_data_job = gcp.dataflow.Job("bigDataJob",
            parameters={
                "baz": "qux",
                "foo": "bar",
            },
            temp_gcs_location="gs://my-bucket/tmp_dir",
            template_gcs_path="gs://my-bucket/templates/template_file")
        ```
        ### Streaming Job

        ```python
        import pulumi
        import pulumi_gcp as gcp

        topic = gcp.pubsub.Topic("topic")
        bucket1 = gcp.storage.Bucket("bucket1",
            location="US",
            force_destroy=True)
        bucket2 = gcp.storage.Bucket("bucket2",
            location="US",
            force_destroy=True)
        pubsub_stream = gcp.dataflow.Job("pubsubStream",
            template_gcs_path="gs://my-bucket/templates/template_file",
            temp_gcs_location="gs://my-bucket/tmp_dir",
            enable_streaming_engine=True,
            parameters={
                "inputFilePattern": bucket1.url.apply(lambda url: f"{url}/*.json"),
                "outputTopic": topic.id,
            },
            transform_name_mapping={
                "name": "test_job",
                "env": "test",
            },
            on_delete="cancel")
        ```
        ## Note on "destroy" / "apply"

        There are many types of Dataflow jobs.  Some Dataflow jobs run constantly, getting new data from (e.g.) a GCS bucket, and outputting data continuously.  Some jobs process a set amount of data then terminate.  All jobs can fail while running due to programming errors or other issues.  In this way, Dataflow jobs are different from most other Google resources.

        The Dataflow resource is considered 'existing' while it is in a nonterminal state.  If it reaches a terminal state (e.g. 'FAILED', 'COMPLETE', 'CANCELLED'), it will be recreated on the next 'apply'.  This is as expected for jobs which run continuously, but may surprise users who use this resource for other kinds of Dataflow jobs.

        A Dataflow job which is 'destroyed' may be "cancelled" or "drained".  If "cancelled", the job terminates - any data written remains where it is, but no new data will be processed.  If "drained", no new data will enter the pipeline, but any data currently in the pipeline will finish being processed.  The default is "drain". When `on_delete` is set to `"drain"` in the configuration, you may experience a long wait for your `pulumi destroy` to complete.

        You can potentially short-circuit the wait by setting `skip_wait_on_job_termination` to `true`, but beware that unless you take active steps to ensure that the job `name` parameter changes between instances, the name will conflict and the launch of the new job will fail. One way to do this is with a random_id resource, for example:

        ```python
        import pulumi
        import pulumi_gcp as gcp
        import pulumi_random as random

        config = pulumi.Config()
        big_data_job_subscription_id = config.get("bigDataJobSubscriptionId")
        if big_data_job_subscription_id is None:
            big_data_job_subscription_id = "projects/myproject/subscriptions/messages"
        big_data_job_name_suffix = random.RandomId("bigDataJobNameSuffix",
            byte_length=4,
            keepers={
                "region": var["region"],
                "subscription_id": big_data_job_subscription_id,
            })
        big_data_job = gcp.dataflow.FlexTemplateJob("bigDataJob",
            region=var["region"],
            container_spec_gcs_path="gs://my-bucket/templates/template.json",
            skip_wait_on_job_termination=True,
            parameters={
                "inputSubscription": big_data_job_subscription_id,
            },
            opts=pulumi.ResourceOptions(provider=google_beta))
        ```

        ## Import

        Dataflow jobs can be imported using the job `id` e.g.

        ```sh
         $ pulumi import gcp:dataflow/job:Job example 2022-07-31_06_25_42-11926927532632678660
        ```

        :param str resource_name: The name of the resource.
        :param JobArgs args: The arguments to use to populate this resource's properties.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        ...
    def __init__(__self__, resource_name: str, *args, **kwargs):
        resource_args, opts = _utilities.get_resource_args_opts(JobArgs, pulumi.ResourceOptions, *args, **kwargs)
        if resource_args is not None:
            __self__._internal_init(resource_name, opts, **resource_args.__dict__)
        else:
            kwargs = kwargs or {}
            def _setter(key, value):
                kwargs[key] = value
            JobArgs._configure(_setter, **kwargs)
            __self__._internal_init(resource_name, *args, **kwargs)

    def _internal_init(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 additional_experiments: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 enable_streaming_engine: Optional[pulumi.Input[bool]] = None,
                 ip_configuration: Optional[pulumi.Input[str]] = None,
                 kms_key_name: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 max_workers: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 on_delete: Optional[pulumi.Input[str]] = None,
                 parameters: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 project: Optional[pulumi.Input[str]] = None,
                 region: Optional[pulumi.Input[str]] = None,
                 service_account_email: Optional[pulumi.Input[str]] = None,
                 skip_wait_on_job_termination: Optional[pulumi.Input[bool]] = None,
                 subnetwork: Optional[pulumi.Input[str]] = None,
                 temp_gcs_location: Optional[pulumi.Input[str]] = None,
                 template_gcs_path: Optional[pulumi.Input[str]] = None,
                 transform_name_mapping: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 zone: Optional[pulumi.Input[str]] = None,
                 __props__=None):
        opts = pulumi.ResourceOptions.merge(_utilities.get_resource_opts_defaults(), opts)
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = JobArgs.__new__(JobArgs)

            __props__.__dict__["additional_experiments"] = additional_experiments
            __props__.__dict__["enable_streaming_engine"] = enable_streaming_engine
            __props__.__dict__["ip_configuration"] = ip_configuration
            __props__.__dict__["kms_key_name"] = kms_key_name
            __props__.__dict__["labels"] = labels
            __props__.__dict__["machine_type"] = machine_type
            __props__.__dict__["max_workers"] = max_workers
            __props__.__dict__["name"] = name
            __props__.__dict__["network"] = network
            __props__.__dict__["on_delete"] = on_delete
            __props__.__dict__["parameters"] = parameters
            __props__.__dict__["project"] = project
            __props__.__dict__["region"] = region
            __props__.__dict__["service_account_email"] = service_account_email
            __props__.__dict__["skip_wait_on_job_termination"] = skip_wait_on_job_termination
            __props__.__dict__["subnetwork"] = subnetwork
            if temp_gcs_location is None and not opts.urn:
                raise TypeError("Missing required property 'temp_gcs_location'")
            __props__.__dict__["temp_gcs_location"] = temp_gcs_location
            if template_gcs_path is None and not opts.urn:
                raise TypeError("Missing required property 'template_gcs_path'")
            __props__.__dict__["template_gcs_path"] = template_gcs_path
            __props__.__dict__["transform_name_mapping"] = transform_name_mapping
            __props__.__dict__["zone"] = zone
            __props__.__dict__["job_id"] = None
            __props__.__dict__["state"] = None
            __props__.__dict__["type"] = None
        super(Job, __self__).__init__(
            'gcp:dataflow/job:Job',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name: str,
            id: pulumi.Input[str],
            opts: Optional[pulumi.ResourceOptions] = None,
            additional_experiments: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
            enable_streaming_engine: Optional[pulumi.Input[bool]] = None,
            ip_configuration: Optional[pulumi.Input[str]] = None,
            job_id: Optional[pulumi.Input[str]] = None,
            kms_key_name: Optional[pulumi.Input[str]] = None,
            labels: Optional[pulumi.Input[Mapping[str, Any]]] = None,
            machine_type: Optional[pulumi.Input[str]] = None,
            max_workers: Optional[pulumi.Input[int]] = None,
            name: Optional[pulumi.Input[str]] = None,
            network: Optional[pulumi.Input[str]] = None,
            on_delete: Optional[pulumi.Input[str]] = None,
            parameters: Optional[pulumi.Input[Mapping[str, Any]]] = None,
            project: Optional[pulumi.Input[str]] = None,
            region: Optional[pulumi.Input[str]] = None,
            service_account_email: Optional[pulumi.Input[str]] = None,
            skip_wait_on_job_termination: Optional[pulumi.Input[bool]] = None,
            state: Optional[pulumi.Input[str]] = None,
            subnetwork: Optional[pulumi.Input[str]] = None,
            temp_gcs_location: Optional[pulumi.Input[str]] = None,
            template_gcs_path: Optional[pulumi.Input[str]] = None,
            transform_name_mapping: Optional[pulumi.Input[Mapping[str, Any]]] = None,
            type: Optional[pulumi.Input[str]] = None,
            zone: Optional[pulumi.Input[str]] = None) -> 'Job':
        """
        Get an existing Job resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param pulumi.Input[str] id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] additional_experiments: List of experiments that should be used by the job. An example value is `["enable_stackdriver_agent_metrics"]`.
        :param pulumi.Input[bool] enable_streaming_engine: Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
        :param pulumi.Input[str] ip_configuration: The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
        :param pulumi.Input[str] job_id: The unique ID of this job.
        :param pulumi.Input[str] kms_key_name: The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
        :param pulumi.Input[Mapping[str, Any]] labels: User labels to be specified for the job. Keys and values should follow the restrictions
               specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
               **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
               Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
        :param pulumi.Input[str] machine_type: The machine type to use for the job.
        :param pulumi.Input[int] max_workers: The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
        :param pulumi.Input[str] name: A unique name for the resource, required by Dataflow.
        :param pulumi.Input[str] network: The network to which VMs will be assigned. If it is not provided, "default" will be used.
        :param pulumi.Input[str] on_delete: One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
        :param pulumi.Input[Mapping[str, Any]] parameters: Key/Value pairs to be passed to the Dataflow job (as used in the template).
        :param pulumi.Input[str] project: The project in which the resource belongs. If it is not provided, the provider project is used.
        :param pulumi.Input[str] region: The region in which the created job should run.
        :param pulumi.Input[str] service_account_email: The Service Account email used to create the job.
        :param pulumi.Input[bool] skip_wait_on_job_termination: If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
        :param pulumi.Input[str] state: The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
        :param pulumi.Input[str] subnetwork: The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
        :param pulumi.Input[str] temp_gcs_location: A writeable location on GCS for the Dataflow job to dump its temporary data.
               
               - - -
        :param pulumi.Input[str] template_gcs_path: The GCS path to the Dataflow job template.
        :param pulumi.Input[Mapping[str, Any]] transform_name_mapping: Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
        :param pulumi.Input[str] type: The type of this job, selected from the [JobType enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobType)
        :param pulumi.Input[str] zone: The zone in which the created job should run. If it is not provided, the provider zone is used.
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = _JobState.__new__(_JobState)

        __props__.__dict__["additional_experiments"] = additional_experiments
        __props__.__dict__["enable_streaming_engine"] = enable_streaming_engine
        __props__.__dict__["ip_configuration"] = ip_configuration
        __props__.__dict__["job_id"] = job_id
        __props__.__dict__["kms_key_name"] = kms_key_name
        __props__.__dict__["labels"] = labels
        __props__.__dict__["machine_type"] = machine_type
        __props__.__dict__["max_workers"] = max_workers
        __props__.__dict__["name"] = name
        __props__.__dict__["network"] = network
        __props__.__dict__["on_delete"] = on_delete
        __props__.__dict__["parameters"] = parameters
        __props__.__dict__["project"] = project
        __props__.__dict__["region"] = region
        __props__.__dict__["service_account_email"] = service_account_email
        __props__.__dict__["skip_wait_on_job_termination"] = skip_wait_on_job_termination
        __props__.__dict__["state"] = state
        __props__.__dict__["subnetwork"] = subnetwork
        __props__.__dict__["temp_gcs_location"] = temp_gcs_location
        __props__.__dict__["template_gcs_path"] = template_gcs_path
        __props__.__dict__["transform_name_mapping"] = transform_name_mapping
        __props__.__dict__["type"] = type
        __props__.__dict__["zone"] = zone
        return Job(resource_name, opts=opts, __props__=__props__)

    @property
    @pulumi.getter(name="additionalExperiments")
    def additional_experiments(self) -> pulumi.Output[Sequence[str]]:
        """
        List of experiments that should be used by the job. An example value is `["enable_stackdriver_agent_metrics"]`.
        """
        return pulumi.get(self, "additional_experiments")

    @property
    @pulumi.getter(name="enableStreamingEngine")
    def enable_streaming_engine(self) -> pulumi.Output[Optional[bool]]:
        """
        Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
        """
        return pulumi.get(self, "enable_streaming_engine")

    @property
    @pulumi.getter(name="ipConfiguration")
    def ip_configuration(self) -> pulumi.Output[Optional[str]]:
        """
        The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
        """
        return pulumi.get(self, "ip_configuration")

    @property
    @pulumi.getter(name="jobId")
    def job_id(self) -> pulumi.Output[str]:
        """
        The unique ID of this job.
        """
        return pulumi.get(self, "job_id")

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> pulumi.Output[Optional[str]]:
        """
        The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
        """
        return pulumi.get(self, "kms_key_name")

    @property
    @pulumi.getter
    def labels(self) -> pulumi.Output[Mapping[str, Any]]:
        """
        User labels to be specified for the job. Keys and values should follow the restrictions
        specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
        **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
        Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> pulumi.Output[Optional[str]]:
        """
        The machine type to use for the job.
        """
        return pulumi.get(self, "machine_type")

    @property
    @pulumi.getter(name="maxWorkers")
    def max_workers(self) -> pulumi.Output[Optional[int]]:
        """
        The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
        """
        return pulumi.get(self, "max_workers")

    @property
    @pulumi.getter
    def name(self) -> pulumi.Output[str]:
        """
        A unique name for the resource, required by Dataflow.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def network(self) -> pulumi.Output[Optional[str]]:
        """
        The network to which VMs will be assigned. If it is not provided, "default" will be used.
        """
        return pulumi.get(self, "network")

    @property
    @pulumi.getter(name="onDelete")
    def on_delete(self) -> pulumi.Output[Optional[str]]:
        """
        One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
        """
        return pulumi.get(self, "on_delete")

    @property
    @pulumi.getter
    def parameters(self) -> pulumi.Output[Optional[Mapping[str, Any]]]:
        """
        Key/Value pairs to be passed to the Dataflow job (as used in the template).
        """
        return pulumi.get(self, "parameters")

    @property
    @pulumi.getter
    def project(self) -> pulumi.Output[str]:
        """
        The project in which the resource belongs. If it is not provided, the provider project is used.
        """
        return pulumi.get(self, "project")

    @property
    @pulumi.getter
    def region(self) -> pulumi.Output[Optional[str]]:
        """
        The region in which the created job should run.
        """
        return pulumi.get(self, "region")

    @property
    @pulumi.getter(name="serviceAccountEmail")
    def service_account_email(self) -> pulumi.Output[Optional[str]]:
        """
        The Service Account email used to create the job.
        """
        return pulumi.get(self, "service_account_email")

    @property
    @pulumi.getter(name="skipWaitOnJobTermination")
    def skip_wait_on_job_termination(self) -> pulumi.Output[Optional[bool]]:
        """
        If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
        """
        return pulumi.get(self, "skip_wait_on_job_termination")

    @property
    @pulumi.getter
    def state(self) -> pulumi.Output[str]:
        """
        The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter
    def subnetwork(self) -> pulumi.Output[Optional[str]]:
        """
        The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
        """
        return pulumi.get(self, "subnetwork")

    @property
    @pulumi.getter(name="tempGcsLocation")
    def temp_gcs_location(self) -> pulumi.Output[str]:
        """
        A writeable location on GCS for the Dataflow job to dump its temporary data.

        - - -
        """
        return pulumi.get(self, "temp_gcs_location")

    @property
    @pulumi.getter(name="templateGcsPath")
    def template_gcs_path(self) -> pulumi.Output[str]:
        """
        The GCS path to the Dataflow job template.
        """
        return pulumi.get(self, "template_gcs_path")

    @property
    @pulumi.getter(name="transformNameMapping")
    def transform_name_mapping(self) -> pulumi.Output[Optional[Mapping[str, Any]]]:
        """
        Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
        """
        return pulumi.get(self, "transform_name_mapping")

    @property
    @pulumi.getter
    def type(self) -> pulumi.Output[str]:
        """
        The type of this job, selected from the [JobType enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobType)
        """
        return pulumi.get(self, "type")

    @property
    @pulumi.getter
    def zone(self) -> pulumi.Output[Optional[str]]:
        """
        The zone in which the created job should run. If it is not provided, the provider zone is used.
        """
        return pulumi.get(self, "zone")


// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Vertex.Inputs
{

    public sealed class AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecGetArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The number of accelerators to attach to the machine.
        /// </summary>
        [Input("acceleratorCount")]
        public Input<int>? AcceleratorCount { get; set; }

        /// <summary>
        /// Possible values:
        /// ACCELERATOR_TYPE_UNSPECIFIED
        /// NVIDIA_TESLA_K80
        /// NVIDIA_TESLA_P100
        /// NVIDIA_TESLA_V100
        /// NVIDIA_TESLA_P4
        /// NVIDIA_TESLA_T4
        /// NVIDIA_TESLA_A100
        /// NVIDIA_A100_80GB
        /// NVIDIA_L4
        /// NVIDIA_H100_80GB
        /// NVIDIA_H100_MEGA_80GB
        /// NVIDIA_H200_141GB
        /// NVIDIA_B200
        /// TPU_V2
        /// TPU_V3
        /// TPU_V4_POD
        /// TPU_V5_LITEPOD
        /// </summary>
        [Input("acceleratorType")]
        public Input<string>? AcceleratorType { get; set; }

        /// <summary>
        /// The type of the machine.
        /// See the [list of machine types supported for
        /// prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
        /// See the [list of machine types supported for custom
        /// training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
        /// For DeployedModel this field is optional, and the default
        /// value is `n1-standard-2`. For BatchPredictionJob or as part of
        /// WorkerPoolSpec this field is required.
        /// </summary>
        [Input("machineType")]
        public Input<string>? MachineType { get; set; }

        /// <summary>
        /// The number of nodes per replica for multihost GPU deployments.
        /// </summary>
        [Input("multihostGpuNodeCount")]
        public Input<int>? MultihostGpuNodeCount { get; set; }

        /// <summary>
        /// A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
        /// DeployedModel) to draw its Compute Engine resources from a Shared
        /// Reservation, or exclusively from on-demand capacity.
        /// Structure is documented below.
        /// </summary>
        [Input("reservationAffinity")]
        public Input<Inputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinityGetArgs>? ReservationAffinity { get; set; }

        /// <summary>
        /// The topology of the TPUs. Corresponds to the TPU topologies available from
        /// GKE. (Example: tpu_topology: "2x2x1").
        /// </summary>
        [Input("tpuTopology")]
        public Input<string>? TpuTopology { get; set; }

        public AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecGetArgs()
        {
        }
        public static new AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecGetArgs Empty => new AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecGetArgs();
    }
}

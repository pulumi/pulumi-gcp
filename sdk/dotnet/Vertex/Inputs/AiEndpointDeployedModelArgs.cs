// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Vertex.Inputs
{

    public sealed class AiEndpointDeployedModelArgs : global::Pulumi.ResourceArgs
    {
        [Input("automaticResources")]
        private InputList<Inputs.AiEndpointDeployedModelAutomaticResourceArgs>? _automaticResources;

        /// <summary>
        /// (Output)
        /// A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AiEndpointDeployedModelAutomaticResourceArgs> AutomaticResources
        {
            get => _automaticResources ?? (_automaticResources = new InputList<Inputs.AiEndpointDeployedModelAutomaticResourceArgs>());
            set => _automaticResources = value;
        }

        /// <summary>
        /// (Output)
        /// Output only. Timestamp when the DeployedModel was created.
        /// </summary>
        [Input("createTime")]
        public Input<string>? CreateTime { get; set; }

        [Input("dedicatedResources")]
        private InputList<Inputs.AiEndpointDeployedModelDedicatedResourceArgs>? _dedicatedResources;

        /// <summary>
        /// (Output)
        /// A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AiEndpointDeployedModelDedicatedResourceArgs> DedicatedResources
        {
            get => _dedicatedResources ?? (_dedicatedResources = new InputList<Inputs.AiEndpointDeployedModelDedicatedResourceArgs>());
            set => _dedicatedResources = value;
        }

        /// <summary>
        /// Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        /// </summary>
        [Input("displayName")]
        public Input<string>? DisplayName { get; set; }

        /// <summary>
        /// (Output)
        /// These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        /// </summary>
        [Input("enableAccessLogging")]
        public Input<bool>? EnableAccessLogging { get; set; }

        /// <summary>
        /// (Output)
        /// If true, the container of the DeployedModel instances will send `Stderr` and `Stdout` streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
        /// </summary>
        [Input("enableContainerLogging")]
        public Input<bool>? EnableContainerLogging { get; set; }

        /// <summary>
        /// (Output)
        /// The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
        /// </summary>
        [Input("id")]
        public Input<string>? Id { get; set; }

        /// <summary>
        /// (Output)
        /// The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
        /// </summary>
        [Input("model")]
        public Input<string>? Model { get; set; }

        /// <summary>
        /// (Output)
        /// Output only. The version ID of the model that is deployed.
        /// </summary>
        [Input("modelVersionId")]
        public Input<string>? ModelVersionId { get; set; }

        [Input("privateEndpoints")]
        private InputList<Inputs.AiEndpointDeployedModelPrivateEndpointArgs>? _privateEndpoints;

        /// <summary>
        /// (Output)
        /// Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AiEndpointDeployedModelPrivateEndpointArgs> PrivateEndpoints
        {
            get => _privateEndpoints ?? (_privateEndpoints = new InputList<Inputs.AiEndpointDeployedModelPrivateEndpointArgs>());
            set => _privateEndpoints = value;
        }

        /// <summary>
        /// (Output)
        /// The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        /// </summary>
        [Input("serviceAccount")]
        public Input<string>? ServiceAccount { get; set; }

        /// <summary>
        /// (Output)
        /// The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
        /// </summary>
        [Input("sharedResources")]
        public Input<string>? SharedResources { get; set; }

        public AiEndpointDeployedModelArgs()
        {
        }
        public static new AiEndpointDeployedModelArgs Empty => new AiEndpointDeployedModelArgs();
    }
}

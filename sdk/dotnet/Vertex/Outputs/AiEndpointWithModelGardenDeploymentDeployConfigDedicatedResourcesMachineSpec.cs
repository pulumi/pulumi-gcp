// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Vertex.Outputs
{

    [OutputType]
    public sealed class AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec
    {
        /// <summary>
        /// The number of accelerators to attach to the machine.
        /// </summary>
        public readonly int? AcceleratorCount;
        /// <summary>
        /// Possible values:
        /// ACCELERATOR_TYPE_UNSPECIFIED
        /// NVIDIA_TESLA_K80
        /// NVIDIA_TESLA_P100
        /// NVIDIA_TESLA_V100
        /// NVIDIA_TESLA_P4
        /// NVIDIA_TESLA_T4
        /// NVIDIA_TESLA_A100
        /// NVIDIA_A100_80GB
        /// NVIDIA_L4
        /// NVIDIA_H100_80GB
        /// NVIDIA_H100_MEGA_80GB
        /// NVIDIA_H200_141GB
        /// NVIDIA_B200
        /// TPU_V2
        /// TPU_V3
        /// TPU_V4_POD
        /// TPU_V5_LITEPOD
        /// </summary>
        public readonly string? AcceleratorType;
        /// <summary>
        /// The type of the machine.
        /// See the [list of machine types supported for
        /// prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
        /// See the [list of machine types supported for custom
        /// training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
        /// For DeployedModel this field is optional, and the default
        /// value is `n1-standard-2`. For BatchPredictionJob or as part of
        /// WorkerPoolSpec this field is required.
        /// </summary>
        public readonly string? MachineType;
        /// <summary>
        /// The number of nodes per replica for multihost GPU deployments.
        /// </summary>
        public readonly int? MultihostGpuNodeCount;
        /// <summary>
        /// A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
        /// DeployedModel) to draw its Compute Engine resources from a Shared
        /// Reservation, or exclusively from on-demand capacity.
        /// Structure is documented below.
        /// </summary>
        public readonly Outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity? ReservationAffinity;
        /// <summary>
        /// The topology of the TPUs. Corresponds to the TPU topologies available from
        /// GKE. (Example: tpu_topology: "2x2x1").
        /// </summary>
        public readonly string? TpuTopology;

        [OutputConstructor]
        private AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpec(
            int? acceleratorCount,

            string? acceleratorType,

            string? machineType,

            int? multihostGpuNodeCount,

            Outputs.AiEndpointWithModelGardenDeploymentDeployConfigDedicatedResourcesMachineSpecReservationAffinity? reservationAffinity,

            string? tpuTopology)
        {
            AcceleratorCount = acceleratorCount;
            AcceleratorType = acceleratorType;
            MachineType = machineType;
            MultihostGpuNodeCount = multihostGpuNodeCount;
            ReservationAffinity = reservationAffinity;
            TpuTopology = tpuTopology;
        }
    }
}

// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Container
{
    /// <summary>
    /// &gt; See the Using GKE with Terraform
    /// guide for more information about using GKE with Terraform.
    /// 
    /// Manages a node pool in a Google Kubernetes Engine (GKE) cluster separately from
    /// the cluster control plane. For more information see [the official documentation](https://cloud.google.com/container-engine/docs/node-pools)
    /// and [the API reference](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters.nodePools).
    /// 
    /// ## Import
    /// 
    /// Node pools can be imported using the `Project`, `Location`, `Cluster` and `Name`. If
    /// the project is omitted, the project value in the provider configuration will be used. Examples:
    /// 
    /// * `{{project_id}}/{{location}}/{{cluster_id}}/{{pool_id}}`
    /// * `{{location}}/{{cluster_id}}/{{pool_id}}`
    /// 
    /// When using the `pulumi import` command, node pools can be imported using one of the formats above. For example:
    /// 
    /// ```sh
    /// $ pulumi import gcp:container/nodePool:NodePool default {{project_id}}/{{location}}/{{cluster_id}}/{{pool_id}}
    /// 
    /// $ pulumi import gcp:container/nodePool:NodePool default {{location}}/{{cluster_id}}/{{pool_id}}
    /// ```
    /// </summary>
    [GcpResourceType("gcp:container/nodePool:NodePool")]
    public partial class NodePool : global::Pulumi.CustomResource
    {
        /// <summary>
        /// Configuration required by cluster autoscaler to adjust
        /// the size of the node pool to the current cluster usage. Structure is documented below.
        /// </summary>
        [Output("autoscaling")]
        public Output<Outputs.NodePoolAutoscaling?> Autoscaling { get; private set; } = null!;

        /// <summary>
        /// The cluster to create the node pool for. Cluster must be present in `Location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
        /// 
        /// - - -
        /// </summary>
        [Output("cluster")]
        public Output<string> Cluster { get; private set; } = null!;

        /// <summary>
        /// The initial number of nodes for the pool. In
        /// regional or multi-zonal clusters, this is the number of nodes per zone. Changing
        /// this will force recreation of the resource. WARNING: Resizing your node pool manually
        /// may change this value in your existing cluster, which will trigger destruction
        /// and recreation on the next Terraform run (to rectify the discrepancy).  If you don't
        /// need this value, don't set it.  If you do need it, you can use a lifecycle block to
        /// ignore subsequent changes to this field.
        /// </summary>
        [Output("initialNodeCount")]
        public Output<int> InitialNodeCount { get; private set; } = null!;

        /// <summary>
        /// The resource URLs of the managed instance groups associated with this node pool.
        /// </summary>
        [Output("instanceGroupUrls")]
        public Output<ImmutableArray<string>> InstanceGroupUrls { get; private set; } = null!;

        /// <summary>
        /// The location (region or zone) of the cluster.
        /// 
        /// - - -
        /// </summary>
        [Output("location")]
        public Output<string> Location { get; private set; } = null!;

        /// <summary>
        /// List of instance group URLs which have been assigned to this node pool.
        /// </summary>
        [Output("managedInstanceGroupUrls")]
        public Output<ImmutableArray<string>> ManagedInstanceGroupUrls { get; private set; } = null!;

        /// <summary>
        /// Node management configuration, wherein auto-repair and
        /// auto-upgrade is configured. Structure is documented below.
        /// </summary>
        [Output("management")]
        public Output<Outputs.NodePoolManagement> Management { get; private set; } = null!;

        /// <summary>
        /// The maximum number of pods per node in this node pool.
        /// Note that this does not work on node pools which are "route-based" - that is, node
        /// pools belonging to clusters that do not have IP Aliasing enabled.
        /// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
        /// for more information.
        /// </summary>
        [Output("maxPodsPerNode")]
        public Output<int> MaxPodsPerNode { get; private set; } = null!;

        /// <summary>
        /// The name of the node pool. If left blank, Terraform will
        /// auto-generate a unique name.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// Creates a unique name for the node pool beginning
        /// with the specified prefix. Conflicts with `Name`.
        /// </summary>
        [Output("namePrefix")]
        public Output<string> NamePrefix { get; private set; } = null!;

        /// <summary>
        /// The network configuration of the pool. Such as
        /// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
        /// documented below
        /// </summary>
        [Output("networkConfig")]
        public Output<Outputs.NodePoolNetworkConfig> NetworkConfig { get; private set; } = null!;

        /// <summary>
        /// Parameters used in creating the node pool. See
        /// gcp.container.Cluster for schema.
        /// </summary>
        [Output("nodeConfig")]
        public Output<Outputs.NodePoolNodeConfig> NodeConfig { get; private set; } = null!;

        /// <summary>
        /// The number of nodes per instance group. This field can be used to
        /// update the number of nodes per instance group but should not be used alongside `Autoscaling`.
        /// </summary>
        [Output("nodeCount")]
        public Output<int> NodeCount { get; private set; } = null!;

        /// <summary>
        /// The node drain configuration of the pool. Structure is documented below.
        /// </summary>
        [Output("nodeDrainConfigs")]
        public Output<ImmutableArray<Outputs.NodePoolNodeDrainConfig>> NodeDrainConfigs { get; private set; } = null!;

        /// <summary>
        /// The list of zones in which the node pool's nodes should be located. Nodes must
        /// be in the region of their regional cluster or in the same region as their
        /// cluster's zone for zonal clusters. If unspecified, the cluster-level
        /// `NodeLocations` will be used.
        /// 
        /// &gt; Note: `NodeLocations` will not revert to the cluster's default set of zones
        /// upon being unset. You must manually reconcile the list of zones with your
        /// cluster.
        /// </summary>
        [Output("nodeLocations")]
        public Output<ImmutableArray<string>> NodeLocations { get; private set; } = null!;

        [Output("operation")]
        public Output<string> Operation { get; private set; } = null!;

        /// <summary>
        /// Specifies a custom placement policy for the
        /// nodes.
        /// </summary>
        [Output("placementPolicy")]
        public Output<Outputs.NodePoolPlacementPolicy?> PlacementPolicy { get; private set; } = null!;

        /// <summary>
        /// The ID of the project in which to create the node pool. If blank,
        /// the provider-configured project will be used.
        /// </summary>
        [Output("project")]
        public Output<string> Project { get; private set; } = null!;

        /// <summary>
        /// Specifies node pool-level settings of queued provisioning.
        /// Structure is documented below.
        /// 
        /// &lt;a name="NestedAutoscaling"&gt;&lt;/a&gt;The `Autoscaling` block supports (either total or per zone limits are required):
        /// </summary>
        [Output("queuedProvisioning")]
        public Output<Outputs.NodePoolQueuedProvisioning?> QueuedProvisioning { get; private set; } = null!;

        /// <summary>
        /// Specify node upgrade settings to change how GKE upgrades nodes.
        /// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
        /// </summary>
        [Output("upgradeSettings")]
        public Output<Outputs.NodePoolUpgradeSettings> UpgradeSettings { get; private set; } = null!;

        /// <summary>
        /// The Kubernetes version for the nodes in this pool. Note that if this field
        /// and `AutoUpgrade` are both specified, they will fight each other for what the node version should
        /// be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
        /// recommended that you specify explicit versions as Terraform will see spurious diffs
        /// when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
        /// `VersionPrefix` field to approximate fuzzy versions in a Terraform-compatible way.
        /// </summary>
        [Output("version")]
        public Output<string> Version { get; private set; } = null!;


        /// <summary>
        /// Create a NodePool resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public NodePool(string name, NodePoolArgs args, CustomResourceOptions? options = null)
            : base("gcp:container/nodePool:NodePool", name, args ?? new NodePoolArgs(), MakeResourceOptions(options, ""))
        {
        }

        private NodePool(string name, Input<string> id, NodePoolState? state = null, CustomResourceOptions? options = null)
            : base("gcp:container/nodePool:NodePool", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing NodePool resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static NodePool Get(string name, Input<string> id, NodePoolState? state = null, CustomResourceOptions? options = null)
        {
            return new NodePool(name, id, state, options);
        }
    }

    public sealed class NodePoolArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Configuration required by cluster autoscaler to adjust
        /// the size of the node pool to the current cluster usage. Structure is documented below.
        /// </summary>
        [Input("autoscaling")]
        public Input<Inputs.NodePoolAutoscalingArgs>? Autoscaling { get; set; }

        /// <summary>
        /// The cluster to create the node pool for. Cluster must be present in `Location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
        /// 
        /// - - -
        /// </summary>
        [Input("cluster", required: true)]
        public Input<string> Cluster { get; set; } = null!;

        /// <summary>
        /// The initial number of nodes for the pool. In
        /// regional or multi-zonal clusters, this is the number of nodes per zone. Changing
        /// this will force recreation of the resource. WARNING: Resizing your node pool manually
        /// may change this value in your existing cluster, which will trigger destruction
        /// and recreation on the next Terraform run (to rectify the discrepancy).  If you don't
        /// need this value, don't set it.  If you do need it, you can use a lifecycle block to
        /// ignore subsequent changes to this field.
        /// </summary>
        [Input("initialNodeCount")]
        public Input<int>? InitialNodeCount { get; set; }

        /// <summary>
        /// The location (region or zone) of the cluster.
        /// 
        /// - - -
        /// </summary>
        [Input("location")]
        public Input<string>? Location { get; set; }

        /// <summary>
        /// Node management configuration, wherein auto-repair and
        /// auto-upgrade is configured. Structure is documented below.
        /// </summary>
        [Input("management")]
        public Input<Inputs.NodePoolManagementArgs>? Management { get; set; }

        /// <summary>
        /// The maximum number of pods per node in this node pool.
        /// Note that this does not work on node pools which are "route-based" - that is, node
        /// pools belonging to clusters that do not have IP Aliasing enabled.
        /// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
        /// for more information.
        /// </summary>
        [Input("maxPodsPerNode")]
        public Input<int>? MaxPodsPerNode { get; set; }

        /// <summary>
        /// The name of the node pool. If left blank, Terraform will
        /// auto-generate a unique name.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// Creates a unique name for the node pool beginning
        /// with the specified prefix. Conflicts with `Name`.
        /// </summary>
        [Input("namePrefix")]
        public Input<string>? NamePrefix { get; set; }

        /// <summary>
        /// The network configuration of the pool. Such as
        /// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
        /// documented below
        /// </summary>
        [Input("networkConfig")]
        public Input<Inputs.NodePoolNetworkConfigArgs>? NetworkConfig { get; set; }

        /// <summary>
        /// Parameters used in creating the node pool. See
        /// gcp.container.Cluster for schema.
        /// </summary>
        [Input("nodeConfig")]
        public Input<Inputs.NodePoolNodeConfigArgs>? NodeConfig { get; set; }

        /// <summary>
        /// The number of nodes per instance group. This field can be used to
        /// update the number of nodes per instance group but should not be used alongside `Autoscaling`.
        /// </summary>
        [Input("nodeCount")]
        public Input<int>? NodeCount { get; set; }

        [Input("nodeDrainConfigs")]
        private InputList<Inputs.NodePoolNodeDrainConfigArgs>? _nodeDrainConfigs;

        /// <summary>
        /// The node drain configuration of the pool. Structure is documented below.
        /// </summary>
        public InputList<Inputs.NodePoolNodeDrainConfigArgs> NodeDrainConfigs
        {
            get => _nodeDrainConfigs ?? (_nodeDrainConfigs = new InputList<Inputs.NodePoolNodeDrainConfigArgs>());
            set => _nodeDrainConfigs = value;
        }

        [Input("nodeLocations")]
        private InputList<string>? _nodeLocations;

        /// <summary>
        /// The list of zones in which the node pool's nodes should be located. Nodes must
        /// be in the region of their regional cluster or in the same region as their
        /// cluster's zone for zonal clusters. If unspecified, the cluster-level
        /// `NodeLocations` will be used.
        /// 
        /// &gt; Note: `NodeLocations` will not revert to the cluster's default set of zones
        /// upon being unset. You must manually reconcile the list of zones with your
        /// cluster.
        /// </summary>
        public InputList<string> NodeLocations
        {
            get => _nodeLocations ?? (_nodeLocations = new InputList<string>());
            set => _nodeLocations = value;
        }

        /// <summary>
        /// Specifies a custom placement policy for the
        /// nodes.
        /// </summary>
        [Input("placementPolicy")]
        public Input<Inputs.NodePoolPlacementPolicyArgs>? PlacementPolicy { get; set; }

        /// <summary>
        /// The ID of the project in which to create the node pool. If blank,
        /// the provider-configured project will be used.
        /// </summary>
        [Input("project")]
        public Input<string>? Project { get; set; }

        /// <summary>
        /// Specifies node pool-level settings of queued provisioning.
        /// Structure is documented below.
        /// 
        /// &lt;a name="NestedAutoscaling"&gt;&lt;/a&gt;The `Autoscaling` block supports (either total or per zone limits are required):
        /// </summary>
        [Input("queuedProvisioning")]
        public Input<Inputs.NodePoolQueuedProvisioningArgs>? QueuedProvisioning { get; set; }

        /// <summary>
        /// Specify node upgrade settings to change how GKE upgrades nodes.
        /// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
        /// </summary>
        [Input("upgradeSettings")]
        public Input<Inputs.NodePoolUpgradeSettingsArgs>? UpgradeSettings { get; set; }

        /// <summary>
        /// The Kubernetes version for the nodes in this pool. Note that if this field
        /// and `AutoUpgrade` are both specified, they will fight each other for what the node version should
        /// be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
        /// recommended that you specify explicit versions as Terraform will see spurious diffs
        /// when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
        /// `VersionPrefix` field to approximate fuzzy versions in a Terraform-compatible way.
        /// </summary>
        [Input("version")]
        public Input<string>? Version { get; set; }

        public NodePoolArgs()
        {
        }
        public static new NodePoolArgs Empty => new NodePoolArgs();
    }

    public sealed class NodePoolState : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Configuration required by cluster autoscaler to adjust
        /// the size of the node pool to the current cluster usage. Structure is documented below.
        /// </summary>
        [Input("autoscaling")]
        public Input<Inputs.NodePoolAutoscalingGetArgs>? Autoscaling { get; set; }

        /// <summary>
        /// The cluster to create the node pool for. Cluster must be present in `Location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
        /// 
        /// - - -
        /// </summary>
        [Input("cluster")]
        public Input<string>? Cluster { get; set; }

        /// <summary>
        /// The initial number of nodes for the pool. In
        /// regional or multi-zonal clusters, this is the number of nodes per zone. Changing
        /// this will force recreation of the resource. WARNING: Resizing your node pool manually
        /// may change this value in your existing cluster, which will trigger destruction
        /// and recreation on the next Terraform run (to rectify the discrepancy).  If you don't
        /// need this value, don't set it.  If you do need it, you can use a lifecycle block to
        /// ignore subsequent changes to this field.
        /// </summary>
        [Input("initialNodeCount")]
        public Input<int>? InitialNodeCount { get; set; }

        [Input("instanceGroupUrls")]
        private InputList<string>? _instanceGroupUrls;

        /// <summary>
        /// The resource URLs of the managed instance groups associated with this node pool.
        /// </summary>
        public InputList<string> InstanceGroupUrls
        {
            get => _instanceGroupUrls ?? (_instanceGroupUrls = new InputList<string>());
            set => _instanceGroupUrls = value;
        }

        /// <summary>
        /// The location (region or zone) of the cluster.
        /// 
        /// - - -
        /// </summary>
        [Input("location")]
        public Input<string>? Location { get; set; }

        [Input("managedInstanceGroupUrls")]
        private InputList<string>? _managedInstanceGroupUrls;

        /// <summary>
        /// List of instance group URLs which have been assigned to this node pool.
        /// </summary>
        public InputList<string> ManagedInstanceGroupUrls
        {
            get => _managedInstanceGroupUrls ?? (_managedInstanceGroupUrls = new InputList<string>());
            set => _managedInstanceGroupUrls = value;
        }

        /// <summary>
        /// Node management configuration, wherein auto-repair and
        /// auto-upgrade is configured. Structure is documented below.
        /// </summary>
        [Input("management")]
        public Input<Inputs.NodePoolManagementGetArgs>? Management { get; set; }

        /// <summary>
        /// The maximum number of pods per node in this node pool.
        /// Note that this does not work on node pools which are "route-based" - that is, node
        /// pools belonging to clusters that do not have IP Aliasing enabled.
        /// See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
        /// for more information.
        /// </summary>
        [Input("maxPodsPerNode")]
        public Input<int>? MaxPodsPerNode { get; set; }

        /// <summary>
        /// The name of the node pool. If left blank, Terraform will
        /// auto-generate a unique name.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// Creates a unique name for the node pool beginning
        /// with the specified prefix. Conflicts with `Name`.
        /// </summary>
        [Input("namePrefix")]
        public Input<string>? NamePrefix { get; set; }

        /// <summary>
        /// The network configuration of the pool. Such as
        /// configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
        /// documented below
        /// </summary>
        [Input("networkConfig")]
        public Input<Inputs.NodePoolNetworkConfigGetArgs>? NetworkConfig { get; set; }

        /// <summary>
        /// Parameters used in creating the node pool. See
        /// gcp.container.Cluster for schema.
        /// </summary>
        [Input("nodeConfig")]
        public Input<Inputs.NodePoolNodeConfigGetArgs>? NodeConfig { get; set; }

        /// <summary>
        /// The number of nodes per instance group. This field can be used to
        /// update the number of nodes per instance group but should not be used alongside `Autoscaling`.
        /// </summary>
        [Input("nodeCount")]
        public Input<int>? NodeCount { get; set; }

        [Input("nodeDrainConfigs")]
        private InputList<Inputs.NodePoolNodeDrainConfigGetArgs>? _nodeDrainConfigs;

        /// <summary>
        /// The node drain configuration of the pool. Structure is documented below.
        /// </summary>
        public InputList<Inputs.NodePoolNodeDrainConfigGetArgs> NodeDrainConfigs
        {
            get => _nodeDrainConfigs ?? (_nodeDrainConfigs = new InputList<Inputs.NodePoolNodeDrainConfigGetArgs>());
            set => _nodeDrainConfigs = value;
        }

        [Input("nodeLocations")]
        private InputList<string>? _nodeLocations;

        /// <summary>
        /// The list of zones in which the node pool's nodes should be located. Nodes must
        /// be in the region of their regional cluster or in the same region as their
        /// cluster's zone for zonal clusters. If unspecified, the cluster-level
        /// `NodeLocations` will be used.
        /// 
        /// &gt; Note: `NodeLocations` will not revert to the cluster's default set of zones
        /// upon being unset. You must manually reconcile the list of zones with your
        /// cluster.
        /// </summary>
        public InputList<string> NodeLocations
        {
            get => _nodeLocations ?? (_nodeLocations = new InputList<string>());
            set => _nodeLocations = value;
        }

        [Input("operation")]
        public Input<string>? Operation { get; set; }

        /// <summary>
        /// Specifies a custom placement policy for the
        /// nodes.
        /// </summary>
        [Input("placementPolicy")]
        public Input<Inputs.NodePoolPlacementPolicyGetArgs>? PlacementPolicy { get; set; }

        /// <summary>
        /// The ID of the project in which to create the node pool. If blank,
        /// the provider-configured project will be used.
        /// </summary>
        [Input("project")]
        public Input<string>? Project { get; set; }

        /// <summary>
        /// Specifies node pool-level settings of queued provisioning.
        /// Structure is documented below.
        /// 
        /// &lt;a name="NestedAutoscaling"&gt;&lt;/a&gt;The `Autoscaling` block supports (either total or per zone limits are required):
        /// </summary>
        [Input("queuedProvisioning")]
        public Input<Inputs.NodePoolQueuedProvisioningGetArgs>? QueuedProvisioning { get; set; }

        /// <summary>
        /// Specify node upgrade settings to change how GKE upgrades nodes.
        /// The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
        /// </summary>
        [Input("upgradeSettings")]
        public Input<Inputs.NodePoolUpgradeSettingsGetArgs>? UpgradeSettings { get; set; }

        /// <summary>
        /// The Kubernetes version for the nodes in this pool. Note that if this field
        /// and `AutoUpgrade` are both specified, they will fight each other for what the node version should
        /// be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
        /// recommended that you specify explicit versions as Terraform will see spurious diffs
        /// when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
        /// `VersionPrefix` field to approximate fuzzy versions in a Terraform-compatible way.
        /// </summary>
        [Input("version")]
        public Input<string>? Version { get; set; }

        public NodePoolState()
        {
        }
        public static new NodePoolState Empty => new NodePoolState();
    }
}

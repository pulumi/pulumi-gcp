// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Ces.Inputs
{

    public sealed class AppVersionSnapshotGuardrailArgs : global::Pulumi.ResourceArgs
    {
        [Input("actions")]
        private InputList<Inputs.AppVersionSnapshotGuardrailActionArgs>? _actions;

        /// <summary>
        /// (Output)
        /// Action that is taken when a certain precondition is met.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AppVersionSnapshotGuardrailActionArgs> Actions
        {
            get => _actions ?? (_actions = new InputList<Inputs.AppVersionSnapshotGuardrailActionArgs>());
            set => _actions = value;
        }

        [Input("codeCallbacks")]
        private InputList<Inputs.AppVersionSnapshotGuardrailCodeCallbackArgs>? _codeCallbacks;

        /// <summary>
        /// (Output)
        /// Guardrail that blocks the conversation based on the code callbacks
        /// provided.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AppVersionSnapshotGuardrailCodeCallbackArgs> CodeCallbacks
        {
            get => _codeCallbacks ?? (_codeCallbacks = new InputList<Inputs.AppVersionSnapshotGuardrailCodeCallbackArgs>());
            set => _codeCallbacks = value;
        }

        [Input("contentFilters")]
        private InputList<Inputs.AppVersionSnapshotGuardrailContentFilterArgs>? _contentFilters;

        /// <summary>
        /// (Output)
        /// Guardrail that bans certain content from being used in the conversation.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AppVersionSnapshotGuardrailContentFilterArgs> ContentFilters
        {
            get => _contentFilters ?? (_contentFilters = new InputList<Inputs.AppVersionSnapshotGuardrailContentFilterArgs>());
            set => _contentFilters = value;
        }

        /// <summary>
        /// (Output)
        /// Timestamp when the toolset was created.
        /// </summary>
        [Input("createTime")]
        public Input<string>? CreateTime { get; set; }

        /// <summary>
        /// The description of the app version.
        /// </summary>
        [Input("description")]
        public Input<string>? Description { get; set; }

        /// <summary>
        /// The display name of the app version.
        /// </summary>
        [Input("displayName")]
        public Input<string>? DisplayName { get; set; }

        /// <summary>
        /// (Output)
        /// Whether the guardrail is enabled.
        /// </summary>
        [Input("enabled")]
        public Input<bool>? Enabled { get; set; }

        /// <summary>
        /// (Output)
        /// ETag used to ensure the object hasn't changed during a read-modify-write
        /// operation. If the etag is empty, the update will overwrite any concurrent
        /// changes.
        /// </summary>
        [Input("etag")]
        public Input<string>? Etag { get; set; }

        [Input("llmPolicies")]
        private InputList<Inputs.AppVersionSnapshotGuardrailLlmPolicyArgs>? _llmPolicies;

        /// <summary>
        /// (Output)
        /// Guardrail that blocks the conversation if the LLM response is considered
        /// violating the policy based on the LLM classification.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AppVersionSnapshotGuardrailLlmPolicyArgs> LlmPolicies
        {
            get => _llmPolicies ?? (_llmPolicies = new InputList<Inputs.AppVersionSnapshotGuardrailLlmPolicyArgs>());
            set => _llmPolicies = value;
        }

        [Input("llmPromptSecurities")]
        private InputList<Inputs.AppVersionSnapshotGuardrailLlmPromptSecurityArgs>? _llmPromptSecurities;

        /// <summary>
        /// (Output)
        /// Guardrail that blocks the conversation if the input is considered unsafe
        /// based on the LLM classification.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AppVersionSnapshotGuardrailLlmPromptSecurityArgs> LlmPromptSecurities
        {
            get => _llmPromptSecurities ?? (_llmPromptSecurities = new InputList<Inputs.AppVersionSnapshotGuardrailLlmPromptSecurityArgs>());
            set => _llmPromptSecurities = value;
        }

        [Input("modelSafeties")]
        private InputList<Inputs.AppVersionSnapshotGuardrailModelSafetyArgs>? _modelSafeties;

        /// <summary>
        /// (Output)
        /// Model safety settings overrides. When this is set, it will override the
        /// default settings and trigger the guardrail if the response is considered
        /// unsafe.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.AppVersionSnapshotGuardrailModelSafetyArgs> ModelSafeties
        {
            get => _modelSafeties ?? (_modelSafeties = new InputList<Inputs.AppVersionSnapshotGuardrailModelSafetyArgs>());
            set => _modelSafeties = value;
        }

        /// <summary>
        /// (Output)
        /// Identifier. The unique identifier of the toolset.
        /// Format:
        /// `projects/{project}/locations/{location}/apps/{app}/toolsets/{toolset}`
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// (Output)
        /// Timestamp when the toolset was last updated.
        /// </summary>
        [Input("updateTime")]
        public Input<string>? UpdateTime { get; set; }

        public AppVersionSnapshotGuardrailArgs()
        {
        }
        public static new AppVersionSnapshotGuardrailArgs Empty => new AppVersionSnapshotGuardrailArgs();
    }
}

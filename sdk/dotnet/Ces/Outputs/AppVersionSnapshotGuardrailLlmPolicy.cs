// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Ces.Outputs
{

    [OutputType]
    public sealed class AppVersionSnapshotGuardrailLlmPolicy
    {
        /// <summary>
        /// (Output)
        /// If an error occurs during the policy check, fail open and do not trigger
        /// the guardrail.
        /// </summary>
        public readonly bool? FailOpen;
        /// <summary>
        /// (Output)
        /// When checking this policy, consider the last 'n' messages in the
        /// conversation.
        /// When not set a default value of 10 will be used.
        /// </summary>
        public readonly int? MaxConversationMessages;
        /// <summary>
        /// (Output)
        /// Model settings contains various configurations for the LLM model.
        /// Structure is documented below.
        /// </summary>
        public readonly ImmutableArray<Outputs.AppVersionSnapshotGuardrailLlmPolicyModelSetting> ModelSettings;
        /// <summary>
        /// (Output)
        /// Defines when to apply the policy check during the conversation. If set to
        /// `POLICY_SCOPE_UNSPECIFIED`, the policy will be applied to the user input.
        /// When applying the policy to the agent response, additional latency will
        /// be introduced before the agent can respond.
        /// Possible values:
        /// USER_QUERY
        /// AGENT_RESPONSE
        /// USER_QUERY_AND_AGENT_RESPONSE
        /// </summary>
        public readonly string? PolicyScope;
        /// <summary>
        /// (Output)
        /// The prompt definition. If not set, default prompt will be used.
        /// </summary>
        public readonly string? Prompt;

        [OutputConstructor]
        private AppVersionSnapshotGuardrailLlmPolicy(
            bool? failOpen,

            int? maxConversationMessages,

            ImmutableArray<Outputs.AppVersionSnapshotGuardrailLlmPolicyModelSetting> modelSettings,

            string? policyScope,

            string? prompt)
        {
            FailOpen = failOpen;
            MaxConversationMessages = maxConversationMessages;
            ModelSettings = modelSettings;
            PolicyScope = policyScope;
            Prompt = prompt;
        }
    }
}

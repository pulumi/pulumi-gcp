// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Ces.Outputs
{

    [OutputType]
    public sealed class GuardrailLlmPolicy
    {
        /// <summary>
        /// By default, the LLM policy check is bypassed for short utterances.
        /// Enabling this setting applies the policy check to all utterances,
        /// including those that would normally be skipped.
        /// </summary>
        public readonly bool? AllowShortUtterance;
        /// <summary>
        /// If an error occurs during the policy check, fail open and do not trigger
        /// the guardrail.
        /// </summary>
        public readonly bool? FailOpen;
        /// <summary>
        /// When checking this policy, consider the last 'n' messages in the
        /// conversation.
        /// When not set a default value of 10 will be used.
        /// </summary>
        public readonly int? MaxConversationMessages;
        /// <summary>
        /// Model settings contains various configurations for the LLM model.
        /// Structure is documented below.
        /// </summary>
        public readonly Outputs.GuardrailLlmPolicyModelSettings? ModelSettings;
        /// <summary>
        /// Defines when to apply the policy check during the conversation. If set to
        /// `POLICY_SCOPE_UNSPECIFIED`, the policy will be applied to the user input.
        /// When applying the policy to the agent response, additional latency will
        /// be introduced before the agent can respond.
        /// Possible values:
        /// USER_QUERY
        /// AGENT_RESPONSE
        /// USER_QUERY_AND_AGENT_RESPONSE
        /// Possible values are: `USER_QUERY`, `AGENT_RESPONSE`, `USER_QUERY_AND_AGENT_RESPONSE`.
        /// </summary>
        public readonly string PolicyScope;
        /// <summary>
        /// Policy prompt.
        /// </summary>
        public readonly string Prompt;

        [OutputConstructor]
        private GuardrailLlmPolicy(
            bool? allowShortUtterance,

            bool? failOpen,

            int? maxConversationMessages,

            Outputs.GuardrailLlmPolicyModelSettings? modelSettings,

            string policyScope,

            string prompt)
        {
            AllowShortUtterance = allowShortUtterance;
            FailOpen = failOpen;
            MaxConversationMessages = maxConversationMessages;
            ModelSettings = modelSettings;
            PolicyScope = policyScope;
            Prompt = prompt;
        }
    }
}

// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Dataproc.Inputs
{

    public sealed class WorkflowTemplatePlacementManagedClusterConfigGetArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
        /// </summary>
        [Input("autoscalingConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfigGetArgs>? AutoscalingConfig { get; set; }

        /// <summary>
        /// Encryption settings for the cluster.
        /// </summary>
        [Input("encryptionConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigEncryptionConfigGetArgs>? EncryptionConfig { get; set; }

        /// <summary>
        /// Port/endpoint configuration for this cluster
        /// </summary>
        [Input("endpointConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigEndpointConfigGetArgs>? EndpointConfig { get; set; }

        /// <summary>
        /// The shared Compute Engine config settings for all instances in a cluster.
        /// </summary>
        [Input("gceClusterConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigGetArgs>? GceClusterConfig { get; set; }

        /// <summary>
        /// The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as `GceClusterConfig`, `MasterConfig`, `WorkerConfig`, `SecondaryWorkerConfig`, and `AutoscalingConfig`.
        /// </summary>
        [Input("gkeClusterConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigGetArgs>? GkeClusterConfig { get; set; }

        [Input("initializationActions")]
        private InputList<Inputs.WorkflowTemplatePlacementManagedClusterConfigInitializationActionGetArgs>? _initializationActions;

        /// <summary>
        /// Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's `Role` metadata to run an executable on a master or worker node, as shown below using `Curl` (you can also use `Wget`): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
        /// </summary>
        public InputList<Inputs.WorkflowTemplatePlacementManagedClusterConfigInitializationActionGetArgs> InitializationActions
        {
            get => _initializationActions ?? (_initializationActions = new InputList<Inputs.WorkflowTemplatePlacementManagedClusterConfigInitializationActionGetArgs>());
            set => _initializationActions = value;
        }

        /// <summary>
        /// Lifecycle setting for the cluster.
        /// </summary>
        [Input("lifecycleConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigLifecycleConfigGetArgs>? LifecycleConfig { get; set; }

        /// <summary>
        /// The Compute Engine config settings for additional worker instances in a cluster.
        /// </summary>
        [Input("masterConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigMasterConfigGetArgs>? MasterConfig { get; set; }

        /// <summary>
        /// Metastore configuration.
        /// </summary>
        [Input("metastoreConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigMetastoreConfigGetArgs>? MetastoreConfig { get; set; }

        /// <summary>
        /// The Compute Engine config settings for additional worker instances in a cluster.
        /// </summary>
        [Input("secondaryWorkerConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigGetArgs>? SecondaryWorkerConfig { get; set; }

        /// <summary>
        /// Security settings for the cluster.
        /// </summary>
        [Input("securityConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigSecurityConfigGetArgs>? SecurityConfig { get; set; }

        /// <summary>
        /// The config settings for software inside the cluster.
        /// </summary>
        [Input("softwareConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigSoftwareConfigGetArgs>? SoftwareConfig { get; set; }

        /// <summary>
        /// A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see [Dataproc staging and temp buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
        /// </summary>
        [Input("stagingBucket")]
        public Input<string>? StagingBucket { get; set; }

        /// <summary>
        /// A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
        /// </summary>
        [Input("tempBucket")]
        public Input<string>? TempBucket { get; set; }

        /// <summary>
        /// The Compute Engine config settings for additional worker instances in a cluster.
        /// 
        /// - - -
        /// </summary>
        [Input("workerConfig")]
        public Input<Inputs.WorkflowTemplatePlacementManagedClusterConfigWorkerConfigGetArgs>? WorkerConfig { get; set; }

        public WorkflowTemplatePlacementManagedClusterConfigGetArgs()
        {
        }
        public static new WorkflowTemplatePlacementManagedClusterConfigGetArgs Empty => new WorkflowTemplatePlacementManagedClusterConfigGetArgs();
    }
}

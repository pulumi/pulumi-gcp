// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Dataproc
{
    /// <summary>
    /// Dataproc Serverless Batches lets you run Spark workloads without requiring you to
    /// provision and manage your own Dataproc cluster.
    /// 
    /// To get more information about Batch, see:
    /// 
    /// * [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches)
    /// * How-to Guides
    ///     * [Dataproc Serverless Batches Intro](https://cloud.google.com/dataproc-serverless/docs/overview)
    /// 
    /// ## Example Usage
    /// 
    /// ### Dataproc Batch Spark
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var exampleBatchSpark = new Gcp.Dataproc.Batch("example_batch_spark", new()
    ///     {
    ///         BatchId = "tf-test-batch_22375",
    ///         Location = "us-central1",
    ///         Labels = 
    ///         {
    ///             { "batch_test", "terraform" },
    ///         },
    ///         RuntimeConfig = new Gcp.Dataproc.Inputs.BatchRuntimeConfigArgs
    ///         {
    ///             Properties = 
    ///             {
    ///                 { "spark.dynamicAllocation.enabled", "false" },
    ///                 { "spark.executor.instances", "2" },
    ///             },
    ///         },
    ///         EnvironmentConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigArgs
    ///         {
    ///             ExecutionConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigExecutionConfigArgs
    ///             {
    ///                 SubnetworkUri = "default",
    ///                 Ttl = "3600s",
    ///                 NetworkTags = new[]
    ///                 {
    ///                     "tag1",
    ///                 },
    ///             },
    ///         },
    ///         SparkBatch = new Gcp.Dataproc.Inputs.BatchSparkBatchArgs
    ///         {
    ///             MainClass = "org.apache.spark.examples.SparkPi",
    ///             Args = new[]
    ///             {
    ///                 "10",
    ///             },
    ///             JarFileUris = new[]
    ///             {
    ///                 "file:///usr/lib/spark/examples/jars/spark-examples.jar",
    ///             },
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// ### Dataproc Batch Spark Full
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var project = Gcp.Organizations.GetProject.Invoke();
    /// 
    ///     var gcsAccount = Gcp.Storage.GetProjectServiceAccount.Invoke();
    /// 
    ///     var bucket = new Gcp.Storage.Bucket("bucket", new()
    ///     {
    ///         UniformBucketLevelAccess = true,
    ///         Name = "dataproc-bucket",
    ///         Location = "US",
    ///         ForceDestroy = true,
    ///     });
    /// 
    ///     var cryptoKeyMember1 = new Gcp.Kms.CryptoKeyIAMMember("crypto_key_member_1", new()
    ///     {
    ///         CryptoKeyId = "example-key",
    ///         Role = "roles/cloudkms.cryptoKeyEncrypterDecrypter",
    ///         Member = $"serviceAccount:service-{project.Apply(getProjectResult =&gt; getProjectResult.Number)}@dataproc-accounts.iam.gserviceaccount.com",
    ///     });
    /// 
    ///     var ms = new Gcp.Dataproc.MetastoreService("ms", new()
    ///     {
    ///         ServiceId = "dataproc-batch",
    ///         Location = "us-central1",
    ///         Port = 9080,
    ///         Tier = "DEVELOPER",
    ///         MaintenanceWindow = new Gcp.Dataproc.Inputs.MetastoreServiceMaintenanceWindowArgs
    ///         {
    ///             HourOfDay = 2,
    ///             DayOfWeek = "SUNDAY",
    ///         },
    ///         HiveMetastoreConfig = new Gcp.Dataproc.Inputs.MetastoreServiceHiveMetastoreConfigArgs
    ///         {
    ///             Version = "3.1.2",
    ///         },
    ///     });
    /// 
    ///     var basic = new Gcp.Dataproc.Cluster("basic", new()
    ///     {
    ///         Name = "dataproc-batch",
    ///         Region = "us-central1",
    ///         ClusterConfig = new Gcp.Dataproc.Inputs.ClusterClusterConfigArgs
    ///         {
    ///             SoftwareConfig = new Gcp.Dataproc.Inputs.ClusterClusterConfigSoftwareConfigArgs
    ///             {
    ///                 OverrideProperties = 
    ///                 {
    ///                     { "dataproc:dataproc.allow.zero.workers", "true" },
    ///                     { "spark:spark.history.fs.logDirectory", bucket.Name.Apply(name =&gt; $"gs://{name}/*/spark-job-history") },
    ///                 },
    ///             },
    ///             EndpointConfig = new Gcp.Dataproc.Inputs.ClusterClusterConfigEndpointConfigArgs
    ///             {
    ///                 EnableHttpPortAccess = true,
    ///             },
    ///             MasterConfig = new Gcp.Dataproc.Inputs.ClusterClusterConfigMasterConfigArgs
    ///             {
    ///                 NumInstances = 1,
    ///                 MachineType = "e2-standard-2",
    ///                 DiskConfig = new Gcp.Dataproc.Inputs.ClusterClusterConfigMasterConfigDiskConfigArgs
    ///                 {
    ///                     BootDiskSizeGb = 35,
    ///                 },
    ///             },
    ///             MetastoreConfig = new Gcp.Dataproc.Inputs.ClusterClusterConfigMetastoreConfigArgs
    ///             {
    ///                 DataprocMetastoreService = ms.Name,
    ///             },
    ///         },
    ///     });
    /// 
    ///     var exampleBatchSpark = new Gcp.Dataproc.Batch("example_batch_spark", new()
    ///     {
    ///         BatchId = "dataproc-batch",
    ///         Location = "us-central1",
    ///         Labels = 
    ///         {
    ///             { "batch_test", "terraform" },
    ///         },
    ///         RuntimeConfig = new Gcp.Dataproc.Inputs.BatchRuntimeConfigArgs
    ///         {
    ///             Properties = 
    ///             {
    ///                 { "spark.dynamicAllocation.enabled", "false" },
    ///                 { "spark.executor.instances", "2" },
    ///             },
    ///             Version = "2.2",
    ///         },
    ///         EnvironmentConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigArgs
    ///         {
    ///             ExecutionConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigExecutionConfigArgs
    ///             {
    ///                 Ttl = "3600s",
    ///                 NetworkTags = new[]
    ///                 {
    ///                     "tag1",
    ///                 },
    ///                 KmsKey = "example-key",
    ///                 NetworkUri = "default",
    ///                 ServiceAccount = $"{project.Apply(getProjectResult =&gt; getProjectResult.Number)}-compute@developer.gserviceaccount.com",
    ///                 StagingBucket = bucket.Name,
    ///                 AuthenticationConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigExecutionConfigAuthenticationConfigArgs
    ///                 {
    ///                     UserWorkloadAuthenticationType = "SERVICE_ACCOUNT",
    ///                 },
    ///             },
    ///             PeripheralsConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigPeripheralsConfigArgs
    ///             {
    ///                 MetastoreService = ms.Name,
    ///                 SparkHistoryServerConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigPeripheralsConfigSparkHistoryServerConfigArgs
    ///                 {
    ///                     DataprocCluster = basic.Id,
    ///                 },
    ///             },
    ///         },
    ///         SparkBatch = new Gcp.Dataproc.Inputs.BatchSparkBatchArgs
    ///         {
    ///             MainClass = "org.apache.spark.examples.SparkPi",
    ///             Args = new[]
    ///             {
    ///                 "10",
    ///             },
    ///             JarFileUris = new[]
    ///             {
    ///                 "file:///usr/lib/spark/examples/jars/spark-examples.jar",
    ///             },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         DependsOn =
    ///         {
    ///             cryptoKeyMember1,
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// ### Dataproc Batch Sparksql
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var exampleBatchSparsql = new Gcp.Dataproc.Batch("example_batch_sparsql", new()
    ///     {
    ///         BatchId = "tf-test-batch_29439",
    ///         Location = "us-central1",
    ///         RuntimeConfig = new Gcp.Dataproc.Inputs.BatchRuntimeConfigArgs
    ///         {
    ///             Properties = 
    ///             {
    ///                 { "spark.dynamicAllocation.enabled", "false" },
    ///                 { "spark.executor.instances", "2" },
    ///             },
    ///         },
    ///         EnvironmentConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigArgs
    ///         {
    ///             ExecutionConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigExecutionConfigArgs
    ///             {
    ///                 SubnetworkUri = "default",
    ///             },
    ///         },
    ///         SparkSqlBatch = new Gcp.Dataproc.Inputs.BatchSparkSqlBatchArgs
    ///         {
    ///             QueryFileUri = "gs://dataproc-examples/spark-sql/natality/cigarette_correlations.sql",
    ///             JarFileUris = new[]
    ///             {
    ///                 "file:///usr/lib/spark/examples/jars/spark-examples.jar",
    ///             },
    ///             QueryVariables = 
    ///             {
    ///                 { "name", "value" },
    ///             },
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// ### Dataproc Batch Pyspark
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var exampleBatchPyspark = new Gcp.Dataproc.Batch("example_batch_pyspark", new()
    ///     {
    ///         BatchId = "tf-test-batch_87786",
    ///         Location = "us-central1",
    ///         RuntimeConfig = new Gcp.Dataproc.Inputs.BatchRuntimeConfigArgs
    ///         {
    ///             Properties = 
    ///             {
    ///                 { "spark.dynamicAllocation.enabled", "false" },
    ///                 { "spark.executor.instances", "2" },
    ///             },
    ///         },
    ///         EnvironmentConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigArgs
    ///         {
    ///             ExecutionConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigExecutionConfigArgs
    ///             {
    ///                 SubnetworkUri = "default",
    ///             },
    ///         },
    ///         PysparkBatch = new Gcp.Dataproc.Inputs.BatchPysparkBatchArgs
    ///         {
    ///             MainPythonFileUri = "https://storage.googleapis.com/terraform-batches/test_util.py",
    ///             Args = new[]
    ///             {
    ///                 "10",
    ///             },
    ///             JarFileUris = new[]
    ///             {
    ///                 "file:///usr/lib/spark/examples/jars/spark-examples.jar",
    ///             },
    ///             PythonFileUris = new[]
    ///             {
    ///                 "gs://dataproc-examples/pyspark/hello-world/hello-world.py",
    ///             },
    ///             ArchiveUris = new[]
    ///             {
    ///                 "https://storage.googleapis.com/terraform-batches/animals.txt.tar.gz#unpacked",
    ///                 "https://storage.googleapis.com/terraform-batches/animals.txt.jar",
    ///                 "https://storage.googleapis.com/terraform-batches/animals.txt",
    ///             },
    ///             FileUris = new[]
    ///             {
    ///                 "https://storage.googleapis.com/terraform-batches/people.txt",
    ///             },
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// ### Dataproc Batch Sparkr
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var exampleBatchSparkr = new Gcp.Dataproc.Batch("example_batch_sparkr", new()
    ///     {
    ///         BatchId = "tf-test-batch_2067",
    ///         Location = "us-central1",
    ///         Labels = 
    ///         {
    ///             { "batch_test", "terraform" },
    ///         },
    ///         RuntimeConfig = new Gcp.Dataproc.Inputs.BatchRuntimeConfigArgs
    ///         {
    ///             Properties = 
    ///             {
    ///                 { "spark.dynamicAllocation.enabled", "false" },
    ///                 { "spark.executor.instances", "2" },
    ///             },
    ///         },
    ///         EnvironmentConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigArgs
    ///         {
    ///             ExecutionConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigExecutionConfigArgs
    ///             {
    ///                 SubnetworkUri = "default",
    ///                 Ttl = "3600s",
    ///                 NetworkTags = new[]
    ///                 {
    ///                     "tag1",
    ///                 },
    ///             },
    ///         },
    ///         SparkRBatch = new Gcp.Dataproc.Inputs.BatchSparkRBatchArgs
    ///         {
    ///             MainRFileUri = "https://storage.googleapis.com/terraform-batches/spark-r-flights.r",
    ///             Args = new[]
    ///             {
    ///                 "https://storage.googleapis.com/terraform-batches/flights.csv",
    ///             },
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// ### Dataproc Batch Autotuning
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var exampleBatchAutotuning = new Gcp.Dataproc.Batch("example_batch_autotuning", new()
    ///     {
    ///         BatchId = "tf-test-batch_40785",
    ///         Location = "us-central1",
    ///         Labels = 
    ///         {
    ///             { "batch_test", "terraform" },
    ///         },
    ///         RuntimeConfig = new Gcp.Dataproc.Inputs.BatchRuntimeConfigArgs
    ///         {
    ///             Version = "2.2",
    ///             Properties = 
    ///             {
    ///                 { "spark.dynamicAllocation.enabled", "false" },
    ///                 { "spark.executor.instances", "2" },
    ///             },
    ///             Cohort = "tf-dataproc-batch-example",
    ///             AutotuningConfig = new Gcp.Dataproc.Inputs.BatchRuntimeConfigAutotuningConfigArgs
    ///             {
    ///                 Scenarios = new[]
    ///                 {
    ///                     "SCALING",
    ///                     "MEMORY",
    ///                 },
    ///             },
    ///         },
    ///         EnvironmentConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigArgs
    ///         {
    ///             ExecutionConfig = new Gcp.Dataproc.Inputs.BatchEnvironmentConfigExecutionConfigArgs
    ///             {
    ///                 SubnetworkUri = "default",
    ///                 Ttl = "3600s",
    ///             },
    ///         },
    ///         SparkBatch = new Gcp.Dataproc.Inputs.BatchSparkBatchArgs
    ///         {
    ///             MainClass = "org.apache.spark.examples.SparkPi",
    ///             Args = new[]
    ///             {
    ///                 "10",
    ///             },
    ///             JarFileUris = new[]
    ///             {
    ///                 "file:///usr/lib/spark/examples/jars/spark-examples.jar",
    ///             },
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ## Import
    /// 
    /// Batch can be imported using any of these accepted formats:
    /// 
    /// * `projects/{{project}}/locations/{{location}}/batches/{{batch_id}}`
    /// 
    /// * `{{project}}/{{location}}/{{batch_id}}`
    /// 
    /// * `{{location}}/{{batch_id}}`
    /// 
    /// When using the `pulumi import` command, Batch can be imported using one of the formats above. For example:
    /// 
    /// ```sh
    /// $ pulumi import gcp:dataproc/batch:Batch default projects/{{project}}/locations/{{location}}/batches/{{batch_id}}
    /// ```
    /// 
    /// ```sh
    /// $ pulumi import gcp:dataproc/batch:Batch default {{project}}/{{location}}/{{batch_id}}
    /// ```
    /// 
    /// ```sh
    /// $ pulumi import gcp:dataproc/batch:Batch default {{location}}/{{batch_id}}
    /// ```
    /// </summary>
    [GcpResourceType("gcp:dataproc/batch:Batch")]
    public partial class Batch : global::Pulumi.CustomResource
    {
        /// <summary>
        /// The ID to use for the batch, which will become the final component of the batch's resource name.
        /// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
        /// </summary>
        [Output("batchId")]
        public Output<string?> BatchId { get; private set; } = null!;

        /// <summary>
        /// The time when the batch was created.
        /// </summary>
        [Output("createTime")]
        public Output<string> CreateTime { get; private set; } = null!;

        /// <summary>
        /// The email address of the user who created the batch.
        /// </summary>
        [Output("creator")]
        public Output<string> Creator { get; private set; } = null!;

        /// <summary>
        /// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
        /// </summary>
        [Output("effectiveLabels")]
        public Output<ImmutableDictionary<string, string>> EffectiveLabels { get; private set; } = null!;

        /// <summary>
        /// Environment configuration for the batch execution.
        /// Structure is documented below.
        /// </summary>
        [Output("environmentConfig")]
        public Output<Outputs.BatchEnvironmentConfig?> EnvironmentConfig { get; private set; } = null!;

        /// <summary>
        /// The labels to associate with this batch.
        /// 
        /// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
        /// Please refer to the field `EffectiveLabels` for all of the labels present on the resource.
        /// </summary>
        [Output("labels")]
        public Output<ImmutableDictionary<string, string>?> Labels { get; private set; } = null!;

        /// <summary>
        /// The location in which the batch will be created in.
        /// </summary>
        [Output("location")]
        public Output<string?> Location { get; private set; } = null!;

        /// <summary>
        /// The resource name of the batch.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// The resource name of the operation associated with this batch.
        /// </summary>
        [Output("operation")]
        public Output<string> Operation { get; private set; } = null!;

        /// <summary>
        /// The ID of the project in which the resource belongs.
        /// If it is not provided, the provider project is used.
        /// </summary>
        [Output("project")]
        public Output<string> Project { get; private set; } = null!;

        /// <summary>
        /// The combination of labels configured directly on the resource
        /// and default labels configured on the provider.
        /// </summary>
        [Output("pulumiLabels")]
        public Output<ImmutableDictionary<string, string>> PulumiLabels { get; private set; } = null!;

        /// <summary>
        /// PySpark batch config.
        /// Structure is documented below.
        /// </summary>
        [Output("pysparkBatch")]
        public Output<Outputs.BatchPysparkBatch?> PysparkBatch { get; private set; } = null!;

        /// <summary>
        /// Runtime configuration for the batch execution.
        /// Structure is documented below.
        /// </summary>
        [Output("runtimeConfig")]
        public Output<Outputs.BatchRuntimeConfig?> RuntimeConfig { get; private set; } = null!;

        /// <summary>
        /// Runtime information about batch execution.
        /// Structure is documented below.
        /// </summary>
        [Output("runtimeInfos")]
        public Output<ImmutableArray<Outputs.BatchRuntimeInfo>> RuntimeInfos { get; private set; } = null!;

        /// <summary>
        /// Spark batch config.
        /// Structure is documented below.
        /// </summary>
        [Output("sparkBatch")]
        public Output<Outputs.BatchSparkBatch?> SparkBatch { get; private set; } = null!;

        /// <summary>
        /// SparkR batch config.
        /// Structure is documented below.
        /// </summary>
        [Output("sparkRBatch")]
        public Output<Outputs.BatchSparkRBatch?> SparkRBatch { get; private set; } = null!;

        /// <summary>
        /// Spark SQL batch config.
        /// Structure is documented below.
        /// </summary>
        [Output("sparkSqlBatch")]
        public Output<Outputs.BatchSparkSqlBatch?> SparkSqlBatch { get; private set; } = null!;

        /// <summary>
        /// (Output)
        /// The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
        /// </summary>
        [Output("state")]
        public Output<string> State { get; private set; } = null!;

        /// <summary>
        /// Historical state information for the batch.
        /// Structure is documented below.
        /// </summary>
        [Output("stateHistories")]
        public Output<ImmutableArray<Outputs.BatchStateHistory>> StateHistories { get; private set; } = null!;

        /// <summary>
        /// (Output)
        /// Details about the state at this point in history.
        /// </summary>
        [Output("stateMessage")]
        public Output<string> StateMessage { get; private set; } = null!;

        /// <summary>
        /// Batch state details, such as a failure description if the state is FAILED.
        /// </summary>
        [Output("stateTime")]
        public Output<string> StateTime { get; private set; } = null!;

        /// <summary>
        /// A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
        /// </summary>
        [Output("uuid")]
        public Output<string> Uuid { get; private set; } = null!;


        /// <summary>
        /// Create a Batch resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public Batch(string name, BatchArgs? args = null, CustomResourceOptions? options = null)
            : base("gcp:dataproc/batch:Batch", name, args ?? new BatchArgs(), MakeResourceOptions(options, ""))
        {
        }

        private Batch(string name, Input<string> id, BatchState? state = null, CustomResourceOptions? options = null)
            : base("gcp:dataproc/batch:Batch", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
                AdditionalSecretOutputs =
                {
                    "effectiveLabels",
                    "pulumiLabels",
                },
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing Batch resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static Batch Get(string name, Input<string> id, BatchState? state = null, CustomResourceOptions? options = null)
        {
            return new Batch(name, id, state, options);
        }
    }

    public sealed class BatchArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The ID to use for the batch, which will become the final component of the batch's resource name.
        /// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
        /// </summary>
        [Input("batchId")]
        public Input<string>? BatchId { get; set; }

        /// <summary>
        /// Environment configuration for the batch execution.
        /// Structure is documented below.
        /// </summary>
        [Input("environmentConfig")]
        public Input<Inputs.BatchEnvironmentConfigArgs>? EnvironmentConfig { get; set; }

        [Input("labels")]
        private InputMap<string>? _labels;

        /// <summary>
        /// The labels to associate with this batch.
        /// 
        /// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
        /// Please refer to the field `EffectiveLabels` for all of the labels present on the resource.
        /// </summary>
        public InputMap<string> Labels
        {
            get => _labels ?? (_labels = new InputMap<string>());
            set => _labels = value;
        }

        /// <summary>
        /// The location in which the batch will be created in.
        /// </summary>
        [Input("location")]
        public Input<string>? Location { get; set; }

        /// <summary>
        /// The ID of the project in which the resource belongs.
        /// If it is not provided, the provider project is used.
        /// </summary>
        [Input("project")]
        public Input<string>? Project { get; set; }

        /// <summary>
        /// PySpark batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("pysparkBatch")]
        public Input<Inputs.BatchPysparkBatchArgs>? PysparkBatch { get; set; }

        /// <summary>
        /// Runtime configuration for the batch execution.
        /// Structure is documented below.
        /// </summary>
        [Input("runtimeConfig")]
        public Input<Inputs.BatchRuntimeConfigArgs>? RuntimeConfig { get; set; }

        /// <summary>
        /// Spark batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("sparkBatch")]
        public Input<Inputs.BatchSparkBatchArgs>? SparkBatch { get; set; }

        /// <summary>
        /// SparkR batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("sparkRBatch")]
        public Input<Inputs.BatchSparkRBatchArgs>? SparkRBatch { get; set; }

        /// <summary>
        /// Spark SQL batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("sparkSqlBatch")]
        public Input<Inputs.BatchSparkSqlBatchArgs>? SparkSqlBatch { get; set; }

        public BatchArgs()
        {
        }
        public static new BatchArgs Empty => new BatchArgs();
    }

    public sealed class BatchState : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The ID to use for the batch, which will become the final component of the batch's resource name.
        /// This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.
        /// </summary>
        [Input("batchId")]
        public Input<string>? BatchId { get; set; }

        /// <summary>
        /// The time when the batch was created.
        /// </summary>
        [Input("createTime")]
        public Input<string>? CreateTime { get; set; }

        /// <summary>
        /// The email address of the user who created the batch.
        /// </summary>
        [Input("creator")]
        public Input<string>? Creator { get; set; }

        [Input("effectiveLabels")]
        private InputMap<string>? _effectiveLabels;

        /// <summary>
        /// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
        /// </summary>
        public InputMap<string> EffectiveLabels
        {
            get => _effectiveLabels ?? (_effectiveLabels = new InputMap<string>());
            set
            {
                var emptySecret = Output.CreateSecret(ImmutableDictionary.Create<string, string>());
                _effectiveLabels = Output.All(value, emptySecret).Apply(v => v[0]);
            }
        }

        /// <summary>
        /// Environment configuration for the batch execution.
        /// Structure is documented below.
        /// </summary>
        [Input("environmentConfig")]
        public Input<Inputs.BatchEnvironmentConfigGetArgs>? EnvironmentConfig { get; set; }

        [Input("labels")]
        private InputMap<string>? _labels;

        /// <summary>
        /// The labels to associate with this batch.
        /// 
        /// **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
        /// Please refer to the field `EffectiveLabels` for all of the labels present on the resource.
        /// </summary>
        public InputMap<string> Labels
        {
            get => _labels ?? (_labels = new InputMap<string>());
            set => _labels = value;
        }

        /// <summary>
        /// The location in which the batch will be created in.
        /// </summary>
        [Input("location")]
        public Input<string>? Location { get; set; }

        /// <summary>
        /// The resource name of the batch.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// The resource name of the operation associated with this batch.
        /// </summary>
        [Input("operation")]
        public Input<string>? Operation { get; set; }

        /// <summary>
        /// The ID of the project in which the resource belongs.
        /// If it is not provided, the provider project is used.
        /// </summary>
        [Input("project")]
        public Input<string>? Project { get; set; }

        [Input("pulumiLabels")]
        private InputMap<string>? _pulumiLabels;

        /// <summary>
        /// The combination of labels configured directly on the resource
        /// and default labels configured on the provider.
        /// </summary>
        public InputMap<string> PulumiLabels
        {
            get => _pulumiLabels ?? (_pulumiLabels = new InputMap<string>());
            set
            {
                var emptySecret = Output.CreateSecret(ImmutableDictionary.Create<string, string>());
                _pulumiLabels = Output.All(value, emptySecret).Apply(v => v[0]);
            }
        }

        /// <summary>
        /// PySpark batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("pysparkBatch")]
        public Input<Inputs.BatchPysparkBatchGetArgs>? PysparkBatch { get; set; }

        /// <summary>
        /// Runtime configuration for the batch execution.
        /// Structure is documented below.
        /// </summary>
        [Input("runtimeConfig")]
        public Input<Inputs.BatchRuntimeConfigGetArgs>? RuntimeConfig { get; set; }

        [Input("runtimeInfos")]
        private InputList<Inputs.BatchRuntimeInfoGetArgs>? _runtimeInfos;

        /// <summary>
        /// Runtime information about batch execution.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.BatchRuntimeInfoGetArgs> RuntimeInfos
        {
            get => _runtimeInfos ?? (_runtimeInfos = new InputList<Inputs.BatchRuntimeInfoGetArgs>());
            set => _runtimeInfos = value;
        }

        /// <summary>
        /// Spark batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("sparkBatch")]
        public Input<Inputs.BatchSparkBatchGetArgs>? SparkBatch { get; set; }

        /// <summary>
        /// SparkR batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("sparkRBatch")]
        public Input<Inputs.BatchSparkRBatchGetArgs>? SparkRBatch { get; set; }

        /// <summary>
        /// Spark SQL batch config.
        /// Structure is documented below.
        /// </summary>
        [Input("sparkSqlBatch")]
        public Input<Inputs.BatchSparkSqlBatchGetArgs>? SparkSqlBatch { get; set; }

        /// <summary>
        /// (Output)
        /// The state of the batch at this point in history. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).
        /// </summary>
        [Input("state")]
        public Input<string>? State { get; set; }

        [Input("stateHistories")]
        private InputList<Inputs.BatchStateHistoryGetArgs>? _stateHistories;

        /// <summary>
        /// Historical state information for the batch.
        /// Structure is documented below.
        /// </summary>
        public InputList<Inputs.BatchStateHistoryGetArgs> StateHistories
        {
            get => _stateHistories ?? (_stateHistories = new InputList<Inputs.BatchStateHistoryGetArgs>());
            set => _stateHistories = value;
        }

        /// <summary>
        /// (Output)
        /// Details about the state at this point in history.
        /// </summary>
        [Input("stateMessage")]
        public Input<string>? StateMessage { get; set; }

        /// <summary>
        /// Batch state details, such as a failure description if the state is FAILED.
        /// </summary>
        [Input("stateTime")]
        public Input<string>? StateTime { get; set; }

        /// <summary>
        /// A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
        /// </summary>
        [Input("uuid")]
        public Input<string>? Uuid { get; set; }

        public BatchState()
        {
        }
        public static new BatchState Empty => new BatchState();
    }
}

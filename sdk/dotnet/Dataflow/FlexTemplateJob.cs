// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Gcp.Dataflow
{
    /// <summary>
    /// ## Example Usage
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var bigDataJob = new Gcp.Dataflow.FlexTemplateJob("bigDataJob", new()
    ///     {
    ///         ContainerSpecGcsPath = "gs://my-bucket/templates/template.json",
    ///         Parameters = 
    ///         {
    ///             { "inputSubscription", "messages" },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         Provider = google_beta,
    ///     });
    /// 
    /// });
    /// ```
    /// ## Note on "destroy" / "apply"
    /// 
    /// There are many types of Dataflow jobs.  Some Dataflow jobs run constantly,
    /// getting new data from (e.g.) a GCS bucket, and outputting data continuously.
    /// Some jobs process a set amount of data then terminate. All jobs can fail while
    /// running due to programming errors or other issues. In this way, Dataflow jobs
    /// are different from most other provider / Google resources.
    /// 
    /// The Dataflow resource is considered 'existing' while it is in a nonterminal
    /// state.  If it reaches a terminal state (e.g. 'FAILED', 'COMPLETE',
    /// 'CANCELLED'), it will be recreated on the next 'apply'.  This is as expected for
    /// jobs which run continuously, but may surprise users who use this resource for
    /// other kinds of Dataflow jobs.
    /// 
    /// A Dataflow job which is 'destroyed' may be "cancelled" or "drained".  If
    /// "cancelled", the job terminates - any data written remains where it is, but no
    /// new data will be processed.  If "drained", no new data will enter the pipeline,
    /// but any data currently in the pipeline will finish being processed.  The default
    /// is "cancelled", but if a user sets `on_delete` to `"drain"` in the
    /// configuration, you may experience a long wait for your `pulumi destroy` to
    /// complete.
    /// 
    /// You can potentially short-circuit the wait by setting `skip_wait_on_job_termination`
    /// to `true`, but beware that unless you take active steps to ensure that the job
    /// `name` parameter changes between instances, the name will conflict and the launch
    /// of the new job will fail. One way to do this is with a
    /// random_id
    /// resource, for example:
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using Gcp = Pulumi.Gcp;
    /// using Random = Pulumi.Random;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     var config = new Config();
    ///     var bigDataJobSubscriptionId = config.Get("bigDataJobSubscriptionId") ?? "projects/myproject/subscriptions/messages";
    ///     var bigDataJobNameSuffix = new Random.RandomId("bigDataJobNameSuffix", new()
    ///     {
    ///         ByteLength = 4,
    ///         Keepers = 
    ///         {
    ///             { "region", @var.Region },
    ///             { "subscription_id", bigDataJobSubscriptionId },
    ///         },
    ///     });
    /// 
    ///     var bigDataJob = new Gcp.Dataflow.FlexTemplateJob("bigDataJob", new()
    ///     {
    ///         Region = @var.Region,
    ///         ContainerSpecGcsPath = "gs://my-bucket/templates/template.json",
    ///         SkipWaitOnJobTermination = true,
    ///         Parameters = 
    ///         {
    ///             { "inputSubscription", bigDataJobSubscriptionId },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         Provider = google_beta,
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ## Import
    /// 
    /// This resource does not support import.
    /// </summary>
    [GcpResourceType("gcp:dataflow/flexTemplateJob:FlexTemplateJob")]
    public partial class FlexTemplateJob : global::Pulumi.CustomResource
    {
        /// <summary>
        /// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
        /// </summary>
        [Output("additionalExperiments")]
        public Output<ImmutableArray<string>> AdditionalExperiments { get; private set; } = null!;

        /// <summary>
        /// The algorithm to use for autoscaling
        /// </summary>
        [Output("autoscalingAlgorithm")]
        public Output<string> AutoscalingAlgorithm { get; private set; } = null!;

        /// <summary>
        /// The GCS path to the Dataflow job Flex
        /// Template.
        /// 
        /// - - -
        /// </summary>
        [Output("containerSpecGcsPath")]
        public Output<string> ContainerSpecGcsPath { get; private set; } = null!;

        /// <summary>
        /// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other
        /// clients and services.
        /// </summary>
        [Output("effectiveLabels")]
        public Output<ImmutableDictionary<string, string>> EffectiveLabels { get; private set; } = null!;

        /// <summary>
        /// Indicates if the job should use the streaming engine feature.
        /// </summary>
        [Output("enableStreamingEngine")]
        public Output<bool?> EnableStreamingEngine { get; private set; } = null!;

        /// <summary>
        /// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
        /// </summary>
        [Output("ipConfiguration")]
        public Output<string?> IpConfiguration { get; private set; } = null!;

        /// <summary>
        /// The unique ID of this job.
        /// </summary>
        [Output("jobId")]
        public Output<string> JobId { get; private set; } = null!;

        /// <summary>
        /// The name for the Cloud KMS key for the job. Key format is:
        /// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
        /// </summary>
        [Output("kmsKeyName")]
        public Output<string> KmsKeyName { get; private set; } = null!;

        /// <summary>
        /// User labels to be specified for the job. Keys and values
        /// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
        /// page. **Note**: This field is marked as deprecated as the API does not currently
        /// support adding labels.
        /// **NOTE**: Google-provided Dataflow templates often provide default labels
        /// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
        /// labels will be ignored to prevent diffs on re-apply.
        /// </summary>
        [Output("labels")]
        public Output<ImmutableDictionary<string, object>?> Labels { get; private set; } = null!;

        /// <summary>
        /// The machine type to use for launching the job. The default is n1-standard-1.
        /// </summary>
        [Output("launcherMachineType")]
        public Output<string> LauncherMachineType { get; private set; } = null!;

        /// <summary>
        /// The machine type to use for the job.
        /// </summary>
        [Output("machineType")]
        public Output<string> MachineType { get; private set; } = null!;

        /// <summary>
        /// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
        /// 1000.
        /// </summary>
        [Output("maxWorkers")]
        public Output<int> MaxWorkers { get; private set; } = null!;

        /// <summary>
        /// A unique name for the resource, required by Dataflow.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// The network to which VMs will be assigned. If it is not provided, "default" will be used.
        /// </summary>
        [Output("network")]
        public Output<string> Network { get; private set; } = null!;

        /// <summary>
        /// The initial number of Google Compute Engine instances for the job.
        /// </summary>
        [Output("numWorkers")]
        public Output<int> NumWorkers { get; private set; } = null!;

        /// <summary>
        /// One of "drain" or "cancel". Specifies behavior of
        /// deletion during `pulumi destroy`.  See above note.
        /// </summary>
        [Output("onDelete")]
        public Output<string?> OnDelete { get; private set; } = null!;

        /// <summary>
        /// Key/Value pairs to be passed to the Dataflow job (as
        /// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
        /// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
        /// </summary>
        [Output("parameters")]
        public Output<ImmutableDictionary<string, object>?> Parameters { get; private set; } = null!;

        /// <summary>
        /// The project in which the resource belongs. If it is not
        /// provided, the provider project is used.
        /// </summary>
        [Output("project")]
        public Output<string> Project { get; private set; } = null!;

        /// <summary>
        /// The combination of labels configured directly on the resource and default labels configured on the provider.
        /// </summary>
        [Output("pulumiLabels")]
        public Output<ImmutableDictionary<string, string>> PulumiLabels { get; private set; } = null!;

        /// <summary>
        /// The region in which the created job should run.
        /// </summary>
        [Output("region")]
        public Output<string> Region { get; private set; } = null!;

        /// <summary>
        /// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
        /// the SDK. Note this field is only valid for portable pipelines.
        /// </summary>
        [Output("sdkContainerImage")]
        public Output<string> SdkContainerImage { get; private set; } = null!;

        /// <summary>
        /// The Service Account email used to create the job.
        /// </summary>
        [Output("serviceAccountEmail")]
        public Output<string> ServiceAccountEmail { get; private set; } = null!;

        /// <summary>
        /// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
        /// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
        /// different, e.g. by embedding a release ID or by using a random_id.
        /// </summary>
        [Output("skipWaitOnJobTermination")]
        public Output<bool?> SkipWaitOnJobTermination { get; private set; } = null!;

        /// <summary>
        /// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
        /// </summary>
        [Output("stagingLocation")]
        public Output<string> StagingLocation { get; private set; } = null!;

        /// <summary>
        /// The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
        /// </summary>
        [Output("state")]
        public Output<string> State { get; private set; } = null!;

        /// <summary>
        /// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
        /// </summary>
        [Output("subnetwork")]
        public Output<string> Subnetwork { get; private set; } = null!;

        /// <summary>
        /// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
        /// </summary>
        [Output("tempLocation")]
        public Output<string> TempLocation { get; private set; } = null!;

        /// <summary>
        /// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
        /// corresponding name prefixes of the new job.
        /// </summary>
        [Output("transformNameMapping")]
        public Output<ImmutableDictionary<string, object>?> TransformNameMapping { get; private set; } = null!;

        /// <summary>
        /// The type of this job, selected from the JobType enum.
        /// </summary>
        [Output("type")]
        public Output<string> Type { get; private set; } = null!;


        /// <summary>
        /// Create a FlexTemplateJob resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public FlexTemplateJob(string name, FlexTemplateJobArgs args, CustomResourceOptions? options = null)
            : base("gcp:dataflow/flexTemplateJob:FlexTemplateJob", name, args ?? new FlexTemplateJobArgs(), MakeResourceOptions(options, ""))
        {
        }

        private FlexTemplateJob(string name, Input<string> id, FlexTemplateJobState? state = null, CustomResourceOptions? options = null)
            : base("gcp:dataflow/flexTemplateJob:FlexTemplateJob", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing FlexTemplateJob resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static FlexTemplateJob Get(string name, Input<string> id, FlexTemplateJobState? state = null, CustomResourceOptions? options = null)
        {
            return new FlexTemplateJob(name, id, state, options);
        }
    }

    public sealed class FlexTemplateJobArgs : global::Pulumi.ResourceArgs
    {
        [Input("additionalExperiments")]
        private InputList<string>? _additionalExperiments;

        /// <summary>
        /// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
        /// </summary>
        public InputList<string> AdditionalExperiments
        {
            get => _additionalExperiments ?? (_additionalExperiments = new InputList<string>());
            set => _additionalExperiments = value;
        }

        /// <summary>
        /// The algorithm to use for autoscaling
        /// </summary>
        [Input("autoscalingAlgorithm")]
        public Input<string>? AutoscalingAlgorithm { get; set; }

        /// <summary>
        /// The GCS path to the Dataflow job Flex
        /// Template.
        /// 
        /// - - -
        /// </summary>
        [Input("containerSpecGcsPath", required: true)]
        public Input<string> ContainerSpecGcsPath { get; set; } = null!;

        /// <summary>
        /// Indicates if the job should use the streaming engine feature.
        /// </summary>
        [Input("enableStreamingEngine")]
        public Input<bool>? EnableStreamingEngine { get; set; }

        /// <summary>
        /// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
        /// </summary>
        [Input("ipConfiguration")]
        public Input<string>? IpConfiguration { get; set; }

        /// <summary>
        /// The name for the Cloud KMS key for the job. Key format is:
        /// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
        /// </summary>
        [Input("kmsKeyName")]
        public Input<string>? KmsKeyName { get; set; }

        [Input("labels")]
        private InputMap<object>? _labels;

        /// <summary>
        /// User labels to be specified for the job. Keys and values
        /// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
        /// page. **Note**: This field is marked as deprecated as the API does not currently
        /// support adding labels.
        /// **NOTE**: Google-provided Dataflow templates often provide default labels
        /// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
        /// labels will be ignored to prevent diffs on re-apply.
        /// </summary>
        public InputMap<object> Labels
        {
            get => _labels ?? (_labels = new InputMap<object>());
            set => _labels = value;
        }

        /// <summary>
        /// The machine type to use for launching the job. The default is n1-standard-1.
        /// </summary>
        [Input("launcherMachineType")]
        public Input<string>? LauncherMachineType { get; set; }

        /// <summary>
        /// The machine type to use for the job.
        /// </summary>
        [Input("machineType")]
        public Input<string>? MachineType { get; set; }

        /// <summary>
        /// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
        /// 1000.
        /// </summary>
        [Input("maxWorkers")]
        public Input<int>? MaxWorkers { get; set; }

        /// <summary>
        /// A unique name for the resource, required by Dataflow.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// The network to which VMs will be assigned. If it is not provided, "default" will be used.
        /// </summary>
        [Input("network")]
        public Input<string>? Network { get; set; }

        /// <summary>
        /// The initial number of Google Compute Engine instances for the job.
        /// </summary>
        [Input("numWorkers")]
        public Input<int>? NumWorkers { get; set; }

        /// <summary>
        /// One of "drain" or "cancel". Specifies behavior of
        /// deletion during `pulumi destroy`.  See above note.
        /// </summary>
        [Input("onDelete")]
        public Input<string>? OnDelete { get; set; }

        [Input("parameters")]
        private InputMap<object>? _parameters;

        /// <summary>
        /// Key/Value pairs to be passed to the Dataflow job (as
        /// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
        /// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
        /// </summary>
        public InputMap<object> Parameters
        {
            get => _parameters ?? (_parameters = new InputMap<object>());
            set => _parameters = value;
        }

        /// <summary>
        /// The project in which the resource belongs. If it is not
        /// provided, the provider project is used.
        /// </summary>
        [Input("project")]
        public Input<string>? Project { get; set; }

        /// <summary>
        /// The region in which the created job should run.
        /// </summary>
        [Input("region")]
        public Input<string>? Region { get; set; }

        /// <summary>
        /// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
        /// the SDK. Note this field is only valid for portable pipelines.
        /// </summary>
        [Input("sdkContainerImage")]
        public Input<string>? SdkContainerImage { get; set; }

        /// <summary>
        /// The Service Account email used to create the job.
        /// </summary>
        [Input("serviceAccountEmail")]
        public Input<string>? ServiceAccountEmail { get; set; }

        /// <summary>
        /// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
        /// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
        /// different, e.g. by embedding a release ID or by using a random_id.
        /// </summary>
        [Input("skipWaitOnJobTermination")]
        public Input<bool>? SkipWaitOnJobTermination { get; set; }

        /// <summary>
        /// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
        /// </summary>
        [Input("stagingLocation")]
        public Input<string>? StagingLocation { get; set; }

        /// <summary>
        /// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
        /// </summary>
        [Input("subnetwork")]
        public Input<string>? Subnetwork { get; set; }

        /// <summary>
        /// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
        /// </summary>
        [Input("tempLocation")]
        public Input<string>? TempLocation { get; set; }

        [Input("transformNameMapping")]
        private InputMap<object>? _transformNameMapping;

        /// <summary>
        /// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
        /// corresponding name prefixes of the new job.
        /// </summary>
        public InputMap<object> TransformNameMapping
        {
            get => _transformNameMapping ?? (_transformNameMapping = new InputMap<object>());
            set => _transformNameMapping = value;
        }

        public FlexTemplateJobArgs()
        {
        }
        public static new FlexTemplateJobArgs Empty => new FlexTemplateJobArgs();
    }

    public sealed class FlexTemplateJobState : global::Pulumi.ResourceArgs
    {
        [Input("additionalExperiments")]
        private InputList<string>? _additionalExperiments;

        /// <summary>
        /// List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
        /// </summary>
        public InputList<string> AdditionalExperiments
        {
            get => _additionalExperiments ?? (_additionalExperiments = new InputList<string>());
            set => _additionalExperiments = value;
        }

        /// <summary>
        /// The algorithm to use for autoscaling
        /// </summary>
        [Input("autoscalingAlgorithm")]
        public Input<string>? AutoscalingAlgorithm { get; set; }

        /// <summary>
        /// The GCS path to the Dataflow job Flex
        /// Template.
        /// 
        /// - - -
        /// </summary>
        [Input("containerSpecGcsPath")]
        public Input<string>? ContainerSpecGcsPath { get; set; }

        [Input("effectiveLabels")]
        private InputMap<string>? _effectiveLabels;

        /// <summary>
        /// All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other
        /// clients and services.
        /// </summary>
        public InputMap<string> EffectiveLabels
        {
            get => _effectiveLabels ?? (_effectiveLabels = new InputMap<string>());
            set => _effectiveLabels = value;
        }

        /// <summary>
        /// Indicates if the job should use the streaming engine feature.
        /// </summary>
        [Input("enableStreamingEngine")]
        public Input<bool>? EnableStreamingEngine { get; set; }

        /// <summary>
        /// The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
        /// </summary>
        [Input("ipConfiguration")]
        public Input<string>? IpConfiguration { get; set; }

        /// <summary>
        /// The unique ID of this job.
        /// </summary>
        [Input("jobId")]
        public Input<string>? JobId { get; set; }

        /// <summary>
        /// The name for the Cloud KMS key for the job. Key format is:
        /// projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
        /// </summary>
        [Input("kmsKeyName")]
        public Input<string>? KmsKeyName { get; set; }

        [Input("labels")]
        private InputMap<object>? _labels;

        /// <summary>
        /// User labels to be specified for the job. Keys and values
        /// should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
        /// page. **Note**: This field is marked as deprecated as the API does not currently
        /// support adding labels.
        /// **NOTE**: Google-provided Dataflow templates often provide default labels
        /// that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
        /// labels will be ignored to prevent diffs on re-apply.
        /// </summary>
        public InputMap<object> Labels
        {
            get => _labels ?? (_labels = new InputMap<object>());
            set => _labels = value;
        }

        /// <summary>
        /// The machine type to use for launching the job. The default is n1-standard-1.
        /// </summary>
        [Input("launcherMachineType")]
        public Input<string>? LauncherMachineType { get; set; }

        /// <summary>
        /// The machine type to use for the job.
        /// </summary>
        [Input("machineType")]
        public Input<string>? MachineType { get; set; }

        /// <summary>
        /// The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
        /// 1000.
        /// </summary>
        [Input("maxWorkers")]
        public Input<int>? MaxWorkers { get; set; }

        /// <summary>
        /// A unique name for the resource, required by Dataflow.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// The network to which VMs will be assigned. If it is not provided, "default" will be used.
        /// </summary>
        [Input("network")]
        public Input<string>? Network { get; set; }

        /// <summary>
        /// The initial number of Google Compute Engine instances for the job.
        /// </summary>
        [Input("numWorkers")]
        public Input<int>? NumWorkers { get; set; }

        /// <summary>
        /// One of "drain" or "cancel". Specifies behavior of
        /// deletion during `pulumi destroy`.  See above note.
        /// </summary>
        [Input("onDelete")]
        public Input<string>? OnDelete { get; set; }

        [Input("parameters")]
        private InputMap<object>? _parameters;

        /// <summary>
        /// Key/Value pairs to be passed to the Dataflow job (as
        /// used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
        /// such as `serviceAccount`, `workerMachineType`, etc can be specified here.
        /// </summary>
        public InputMap<object> Parameters
        {
            get => _parameters ?? (_parameters = new InputMap<object>());
            set => _parameters = value;
        }

        /// <summary>
        /// The project in which the resource belongs. If it is not
        /// provided, the provider project is used.
        /// </summary>
        [Input("project")]
        public Input<string>? Project { get; set; }

        [Input("pulumiLabels")]
        private InputMap<string>? _pulumiLabels;

        /// <summary>
        /// The combination of labels configured directly on the resource and default labels configured on the provider.
        /// </summary>
        public InputMap<string> PulumiLabels
        {
            get => _pulumiLabels ?? (_pulumiLabels = new InputMap<string>());
            set => _pulumiLabels = value;
        }

        /// <summary>
        /// The region in which the created job should run.
        /// </summary>
        [Input("region")]
        public Input<string>? Region { get; set; }

        /// <summary>
        /// Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
        /// the SDK. Note this field is only valid for portable pipelines.
        /// </summary>
        [Input("sdkContainerImage")]
        public Input<string>? SdkContainerImage { get; set; }

        /// <summary>
        /// The Service Account email used to create the job.
        /// </summary>
        [Input("serviceAccountEmail")]
        public Input<string>? ServiceAccountEmail { get; set; }

        /// <summary>
        /// If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
        /// terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
        /// different, e.g. by embedding a release ID or by using a random_id.
        /// </summary>
        [Input("skipWaitOnJobTermination")]
        public Input<bool>? SkipWaitOnJobTermination { get; set; }

        /// <summary>
        /// The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
        /// </summary>
        [Input("stagingLocation")]
        public Input<string>? StagingLocation { get; set; }

        /// <summary>
        /// The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
        /// </summary>
        [Input("state")]
        public Input<string>? State { get; set; }

        /// <summary>
        /// The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
        /// </summary>
        [Input("subnetwork")]
        public Input<string>? Subnetwork { get; set; }

        /// <summary>
        /// The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
        /// </summary>
        [Input("tempLocation")]
        public Input<string>? TempLocation { get; set; }

        [Input("transformNameMapping")]
        private InputMap<object>? _transformNameMapping;

        /// <summary>
        /// Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
        /// corresponding name prefixes of the new job.
        /// </summary>
        public InputMap<object> TransformNameMapping
        {
            get => _transformNameMapping ?? (_transformNameMapping = new InputMap<object>());
            set => _transformNameMapping = value;
        }

        /// <summary>
        /// The type of this job, selected from the JobType enum.
        /// </summary>
        [Input("type")]
        public Input<string>? Type { get; set; }

        public FlexTemplateJobState()
        {
        }
        public static new FlexTemplateJobState Empty => new FlexTemplateJobState();
    }
}

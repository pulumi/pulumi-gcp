// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";
import * as utilities from "../utilities";

/**
 * A Spark application is a single Spark workload run on a GDC cluster.
 *
 * To get more information about SparkApplication, see:
 *
 * * [API documentation](https://cloud.google.com/dataproc-gdc/docs/reference/rest/v1/projects.locations.serviceInstances.sparkApplications)
 * * How-to Guides
 *     * [Dataproc Intro](https://cloud.google.com/dataproc/)
 *
 * ## Example Usage
 *
 * ### Dataprocgdc Sparkapplication Basic
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-spark-app-basic",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     sparkApplicationConfig: {
 *         mainClass: "org.apache.spark.examples.SparkPi",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         args: ["10000"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const appEnv = new gcp.dataproc.GdcApplicationEnvironment("app_env", {
 *     applicationEnvironmentId: "tf-e2e-spark-app-env",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 * });
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-spark-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     labels: {
 *         "test-label": "label-value",
 *     },
 *     annotations: {
 *         an_annotation: "annotation_value",
 *     },
 *     properties: {
 *         "spark.executor.instances": "2",
 *     },
 *     applicationEnvironment: appEnv.name,
 *     version: "1.2",
 *     sparkApplicationConfig: {
 *         mainJarFileUri: "file:///usr/lib/spark/examples/jars/spark-examples.jar",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         archiveUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         fileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Pyspark
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-pyspark-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A Pyspark application for a Terraform create test",
 *     dependencyImages: ["gcr.io/some/image"],
 *     pysparkApplicationConfig: {
 *         mainPythonFileUri: "gs://goog-dataproc-initialization-actions-us-west2/conda/test_conda.py",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         pythonFileUris: ["gs://goog-dataproc-initialization-actions-us-west2/conda/get-sys-exec.py"],
 *         fileUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         archiveUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         args: ["10"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Sparkr
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-sparkr-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A SparkR application for a Terraform create test",
 *     sparkRApplicationConfig: {
 *         mainRFileUri: "gs://some-bucket/something.R",
 *         fileUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         archiveUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         args: ["10"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Sparksql
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-sparksql-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A SparkSql application for a Terraform create test",
 *     sparkSqlApplicationConfig: {
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         queryList: {
 *             queries: ["show tables;"],
 *         },
 *         scriptVariables: {
 *             MY_VAR: "1",
 *         },
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Sparksql Query File
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-sparksql-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A SparkSql application for a Terraform create test",
 *     sparkSqlApplicationConfig: {
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         queryFileUri: "gs://some-bucket/something.sql",
 *         scriptVariables: {
 *             MY_VAR: "1",
 *         },
 *     },
 * });
 * ```
 *
 * ## Import
 *
 * SparkApplication can be imported using any of these accepted formats:
 *
 * * `projects/{{project}}/locations/{{location}}/serviceInstances/{{serviceinstance}}/sparkApplications/{{spark_application_id}}`
 *
 * * `{{project}}/{{location}}/{{serviceinstance}}/{{spark_application_id}}`
 *
 * * `{{location}}/{{serviceinstance}}/{{spark_application_id}}`
 *
 * When using the `pulumi import` command, SparkApplication can be imported using one of the formats above. For example:
 *
 * ```sh
 * $ pulumi import gcp:dataproc/gdcSparkApplication:GdcSparkApplication default projects/{{project}}/locations/{{location}}/serviceInstances/{{serviceinstance}}/sparkApplications/{{spark_application_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:dataproc/gdcSparkApplication:GdcSparkApplication default {{project}}/{{location}}/{{serviceinstance}}/{{spark_application_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:dataproc/gdcSparkApplication:GdcSparkApplication default {{location}}/{{serviceinstance}}/{{spark_application_id}}
 * ```
 */
export class GdcSparkApplication extends pulumi.CustomResource {
    /**
     * Get an existing GdcSparkApplication resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: GdcSparkApplicationState, opts?: pulumi.CustomResourceOptions): GdcSparkApplication {
        return new GdcSparkApplication(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'gcp:dataproc/gdcSparkApplication:GdcSparkApplication';

    /**
     * Returns true if the given object is an instance of GdcSparkApplication.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is GdcSparkApplication {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === GdcSparkApplication.__pulumiType;
    }

    /**
     * The annotations to associate with this application. Annotations may be used to store client information, but are not used by the server.
     * **Note**: This field is non-authoritative, and will only manage the annotations present in your configuration.
     * Please refer to the field `effectiveAnnotations` for all of the annotations present on the resource.
     */
    declare public readonly annotations: pulumi.Output<{[key: string]: string} | undefined>;
    /**
     * An ApplicationEnvironment from which to inherit configuration properties.
     */
    declare public readonly applicationEnvironment: pulumi.Output<string | undefined>;
    /**
     * The timestamp when the resource was created.
     */
    declare public /*out*/ readonly createTime: pulumi.Output<string>;
    /**
     * List of container image uris for additional file dependencies. Dependent files are sequentially copied from each image. If a file with the same name exists in 2 images then the file from later image is used.
     */
    declare public readonly dependencyImages: pulumi.Output<string[] | undefined>;
    /**
     * User-provided human-readable name to be used in user interfaces.
     */
    declare public readonly displayName: pulumi.Output<string | undefined>;
    declare public /*out*/ readonly effectiveAnnotations: pulumi.Output<{[key: string]: string}>;
    /**
     * All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
     */
    declare public /*out*/ readonly effectiveLabels: pulumi.Output<{[key: string]: string}>;
    /**
     * The labels to associate with this application. Labels may be used for filtering and billing tracking.
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field `effectiveLabels` for all of the labels present on the resource.
     */
    declare public readonly labels: pulumi.Output<{[key: string]: string} | undefined>;
    /**
     * The location of the spark application.
     */
    declare public readonly location: pulumi.Output<string>;
    /**
     * URL for a monitoring UI for this application (for eventual Spark PHS/UI support) Out of scope for private GA
     */
    declare public /*out*/ readonly monitoringEndpoint: pulumi.Output<string>;
    /**
     * Identifier. The name of the application. Format: projects/{project}/locations/{location}/serviceInstances/{service_instance}/sparkApplications/{application}
     */
    declare public /*out*/ readonly name: pulumi.Output<string>;
    /**
     * The Kubernetes namespace in which to create the application. This namespace must already exist on the cluster.
     */
    declare public readonly namespace: pulumi.Output<string | undefined>;
    /**
     * An HCFS URI pointing to the location of stdout and stdout of the application Mainly useful for Pantheon and gcloud Not in scope for private GA
     */
    declare public /*out*/ readonly outputUri: pulumi.Output<string>;
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     */
    declare public readonly project: pulumi.Output<string>;
    /**
     * application-specific properties.
     */
    declare public readonly properties: pulumi.Output<{[key: string]: string} | undefined>;
    /**
     * The combination of labels configured directly on the resource
     * and default labels configured on the provider.
     */
    declare public /*out*/ readonly pulumiLabels: pulumi.Output<{[key: string]: string}>;
    /**
     * Represents the PySparkApplicationConfig.
     * Structure is documented below.
     */
    declare public readonly pysparkApplicationConfig: pulumi.Output<outputs.dataproc.GdcSparkApplicationPysparkApplicationConfig | undefined>;
    /**
     * Whether the application is currently reconciling. True if the current state of the resource does not match the intended state, and the system is working to reconcile them, whether or not the change was user initiated.
     */
    declare public /*out*/ readonly reconciling: pulumi.Output<boolean>;
    /**
     * The id of the service instance to which this spark application belongs.
     */
    declare public readonly serviceinstance: pulumi.Output<string>;
    /**
     * Represents the SparkApplicationConfig.
     * Structure is documented below.
     */
    declare public readonly sparkApplicationConfig: pulumi.Output<outputs.dataproc.GdcSparkApplicationSparkApplicationConfig | undefined>;
    /**
     * The id of the application
     */
    declare public readonly sparkApplicationId: pulumi.Output<string>;
    /**
     * Represents the SparkRApplicationConfig.
     * Structure is documented below.
     */
    declare public readonly sparkRApplicationConfig: pulumi.Output<outputs.dataproc.GdcSparkApplicationSparkRApplicationConfig | undefined>;
    /**
     * Represents the SparkRApplicationConfig.
     * Structure is documented below.
     */
    declare public readonly sparkSqlApplicationConfig: pulumi.Output<outputs.dataproc.GdcSparkApplicationSparkSqlApplicationConfig | undefined>;
    /**
     * The current state.
     * Possible values:
     * * `STATE_UNSPECIFIED`
     * * `PENDING`
     * * `RUNNING`
     * * `CANCELLING`
     * * `CANCELLED`
     * * `SUCCEEDED`
     * * `FAILED`
     */
    declare public /*out*/ readonly state: pulumi.Output<string>;
    /**
     * A message explaining the current state.
     */
    declare public /*out*/ readonly stateMessage: pulumi.Output<string>;
    /**
     * System generated unique identifier for this application, formatted as UUID4.
     */
    declare public /*out*/ readonly uid: pulumi.Output<string>;
    /**
     * The timestamp when the resource was most recently updated.
     */
    declare public /*out*/ readonly updateTime: pulumi.Output<string>;
    /**
     * The Dataproc version of this application.
     */
    declare public readonly version: pulumi.Output<string | undefined>;

    /**
     * Create a GdcSparkApplication resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: GdcSparkApplicationArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: GdcSparkApplicationArgs | GdcSparkApplicationState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as GdcSparkApplicationState | undefined;
            resourceInputs["annotations"] = state?.annotations;
            resourceInputs["applicationEnvironment"] = state?.applicationEnvironment;
            resourceInputs["createTime"] = state?.createTime;
            resourceInputs["dependencyImages"] = state?.dependencyImages;
            resourceInputs["displayName"] = state?.displayName;
            resourceInputs["effectiveAnnotations"] = state?.effectiveAnnotations;
            resourceInputs["effectiveLabels"] = state?.effectiveLabels;
            resourceInputs["labels"] = state?.labels;
            resourceInputs["location"] = state?.location;
            resourceInputs["monitoringEndpoint"] = state?.monitoringEndpoint;
            resourceInputs["name"] = state?.name;
            resourceInputs["namespace"] = state?.namespace;
            resourceInputs["outputUri"] = state?.outputUri;
            resourceInputs["project"] = state?.project;
            resourceInputs["properties"] = state?.properties;
            resourceInputs["pulumiLabels"] = state?.pulumiLabels;
            resourceInputs["pysparkApplicationConfig"] = state?.pysparkApplicationConfig;
            resourceInputs["reconciling"] = state?.reconciling;
            resourceInputs["serviceinstance"] = state?.serviceinstance;
            resourceInputs["sparkApplicationConfig"] = state?.sparkApplicationConfig;
            resourceInputs["sparkApplicationId"] = state?.sparkApplicationId;
            resourceInputs["sparkRApplicationConfig"] = state?.sparkRApplicationConfig;
            resourceInputs["sparkSqlApplicationConfig"] = state?.sparkSqlApplicationConfig;
            resourceInputs["state"] = state?.state;
            resourceInputs["stateMessage"] = state?.stateMessage;
            resourceInputs["uid"] = state?.uid;
            resourceInputs["updateTime"] = state?.updateTime;
            resourceInputs["version"] = state?.version;
        } else {
            const args = argsOrState as GdcSparkApplicationArgs | undefined;
            if (args?.location === undefined && !opts.urn) {
                throw new Error("Missing required property 'location'");
            }
            if (args?.serviceinstance === undefined && !opts.urn) {
                throw new Error("Missing required property 'serviceinstance'");
            }
            if (args?.sparkApplicationId === undefined && !opts.urn) {
                throw new Error("Missing required property 'sparkApplicationId'");
            }
            resourceInputs["annotations"] = args?.annotations;
            resourceInputs["applicationEnvironment"] = args?.applicationEnvironment;
            resourceInputs["dependencyImages"] = args?.dependencyImages;
            resourceInputs["displayName"] = args?.displayName;
            resourceInputs["labels"] = args?.labels;
            resourceInputs["location"] = args?.location;
            resourceInputs["namespace"] = args?.namespace;
            resourceInputs["project"] = args?.project;
            resourceInputs["properties"] = args?.properties;
            resourceInputs["pysparkApplicationConfig"] = args?.pysparkApplicationConfig;
            resourceInputs["serviceinstance"] = args?.serviceinstance;
            resourceInputs["sparkApplicationConfig"] = args?.sparkApplicationConfig;
            resourceInputs["sparkApplicationId"] = args?.sparkApplicationId;
            resourceInputs["sparkRApplicationConfig"] = args?.sparkRApplicationConfig;
            resourceInputs["sparkSqlApplicationConfig"] = args?.sparkSqlApplicationConfig;
            resourceInputs["version"] = args?.version;
            resourceInputs["createTime"] = undefined /*out*/;
            resourceInputs["effectiveAnnotations"] = undefined /*out*/;
            resourceInputs["effectiveLabels"] = undefined /*out*/;
            resourceInputs["monitoringEndpoint"] = undefined /*out*/;
            resourceInputs["name"] = undefined /*out*/;
            resourceInputs["outputUri"] = undefined /*out*/;
            resourceInputs["pulumiLabels"] = undefined /*out*/;
            resourceInputs["reconciling"] = undefined /*out*/;
            resourceInputs["state"] = undefined /*out*/;
            resourceInputs["stateMessage"] = undefined /*out*/;
            resourceInputs["uid"] = undefined /*out*/;
            resourceInputs["updateTime"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        const secretOpts = { additionalSecretOutputs: ["effectiveLabels", "pulumiLabels"] };
        opts = pulumi.mergeOptions(opts, secretOpts);
        super(GdcSparkApplication.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering GdcSparkApplication resources.
 */
export interface GdcSparkApplicationState {
    /**
     * The annotations to associate with this application. Annotations may be used to store client information, but are not used by the server.
     * **Note**: This field is non-authoritative, and will only manage the annotations present in your configuration.
     * Please refer to the field `effectiveAnnotations` for all of the annotations present on the resource.
     */
    annotations?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * An ApplicationEnvironment from which to inherit configuration properties.
     */
    applicationEnvironment?: pulumi.Input<string>;
    /**
     * The timestamp when the resource was created.
     */
    createTime?: pulumi.Input<string>;
    /**
     * List of container image uris for additional file dependencies. Dependent files are sequentially copied from each image. If a file with the same name exists in 2 images then the file from later image is used.
     */
    dependencyImages?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * User-provided human-readable name to be used in user interfaces.
     */
    displayName?: pulumi.Input<string>;
    effectiveAnnotations?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Pulumi, other clients and services.
     */
    effectiveLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The labels to associate with this application. Labels may be used for filtering and billing tracking.
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field `effectiveLabels` for all of the labels present on the resource.
     */
    labels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The location of the spark application.
     */
    location?: pulumi.Input<string>;
    /**
     * URL for a monitoring UI for this application (for eventual Spark PHS/UI support) Out of scope for private GA
     */
    monitoringEndpoint?: pulumi.Input<string>;
    /**
     * Identifier. The name of the application. Format: projects/{project}/locations/{location}/serviceInstances/{service_instance}/sparkApplications/{application}
     */
    name?: pulumi.Input<string>;
    /**
     * The Kubernetes namespace in which to create the application. This namespace must already exist on the cluster.
     */
    namespace?: pulumi.Input<string>;
    /**
     * An HCFS URI pointing to the location of stdout and stdout of the application Mainly useful for Pantheon and gcloud Not in scope for private GA
     */
    outputUri?: pulumi.Input<string>;
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * application-specific properties.
     */
    properties?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The combination of labels configured directly on the resource
     * and default labels configured on the provider.
     */
    pulumiLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * Represents the PySparkApplicationConfig.
     * Structure is documented below.
     */
    pysparkApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationPysparkApplicationConfig>;
    /**
     * Whether the application is currently reconciling. True if the current state of the resource does not match the intended state, and the system is working to reconcile them, whether or not the change was user initiated.
     */
    reconciling?: pulumi.Input<boolean>;
    /**
     * The id of the service instance to which this spark application belongs.
     */
    serviceinstance?: pulumi.Input<string>;
    /**
     * Represents the SparkApplicationConfig.
     * Structure is documented below.
     */
    sparkApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationSparkApplicationConfig>;
    /**
     * The id of the application
     */
    sparkApplicationId?: pulumi.Input<string>;
    /**
     * Represents the SparkRApplicationConfig.
     * Structure is documented below.
     */
    sparkRApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationSparkRApplicationConfig>;
    /**
     * Represents the SparkRApplicationConfig.
     * Structure is documented below.
     */
    sparkSqlApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationSparkSqlApplicationConfig>;
    /**
     * The current state.
     * Possible values:
     * * `STATE_UNSPECIFIED`
     * * `PENDING`
     * * `RUNNING`
     * * `CANCELLING`
     * * `CANCELLED`
     * * `SUCCEEDED`
     * * `FAILED`
     */
    state?: pulumi.Input<string>;
    /**
     * A message explaining the current state.
     */
    stateMessage?: pulumi.Input<string>;
    /**
     * System generated unique identifier for this application, formatted as UUID4.
     */
    uid?: pulumi.Input<string>;
    /**
     * The timestamp when the resource was most recently updated.
     */
    updateTime?: pulumi.Input<string>;
    /**
     * The Dataproc version of this application.
     */
    version?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a GdcSparkApplication resource.
 */
export interface GdcSparkApplicationArgs {
    /**
     * The annotations to associate with this application. Annotations may be used to store client information, but are not used by the server.
     * **Note**: This field is non-authoritative, and will only manage the annotations present in your configuration.
     * Please refer to the field `effectiveAnnotations` for all of the annotations present on the resource.
     */
    annotations?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * An ApplicationEnvironment from which to inherit configuration properties.
     */
    applicationEnvironment?: pulumi.Input<string>;
    /**
     * List of container image uris for additional file dependencies. Dependent files are sequentially copied from each image. If a file with the same name exists in 2 images then the file from later image is used.
     */
    dependencyImages?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * User-provided human-readable name to be used in user interfaces.
     */
    displayName?: pulumi.Input<string>;
    /**
     * The labels to associate with this application. Labels may be used for filtering and billing tracking.
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field `effectiveLabels` for all of the labels present on the resource.
     */
    labels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The location of the spark application.
     */
    location: pulumi.Input<string>;
    /**
     * The Kubernetes namespace in which to create the application. This namespace must already exist on the cluster.
     */
    namespace?: pulumi.Input<string>;
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * application-specific properties.
     */
    properties?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * Represents the PySparkApplicationConfig.
     * Structure is documented below.
     */
    pysparkApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationPysparkApplicationConfig>;
    /**
     * The id of the service instance to which this spark application belongs.
     */
    serviceinstance: pulumi.Input<string>;
    /**
     * Represents the SparkApplicationConfig.
     * Structure is documented below.
     */
    sparkApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationSparkApplicationConfig>;
    /**
     * The id of the application
     */
    sparkApplicationId: pulumi.Input<string>;
    /**
     * Represents the SparkRApplicationConfig.
     * Structure is documented below.
     */
    sparkRApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationSparkRApplicationConfig>;
    /**
     * Represents the SparkRApplicationConfig.
     * Structure is documented below.
     */
    sparkSqlApplicationConfig?: pulumi.Input<inputs.dataproc.GdcSparkApplicationSparkSqlApplicationConfig>;
    /**
     * The Dataproc version of this application.
     */
    version?: pulumi.Input<string>;
}

// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";
import * as utilities from "../utilities";

/**
 * Manages a node pool in a Google Kubernetes Engine (GKE) cluster separately from
 * the cluster control plane. For more information see [the official documentation](https://cloud.google.com/container-engine/docs/node-pools)
 * and [the API reference](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters.nodePools).
 *
 * ## Example Usage
 *
 * ### Using A Separately Managed Node Pool (Recommended)
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const _default = new gcp.serviceaccount.Account("default", {
 *     accountId: "service-account-id",
 *     displayName: "Service Account",
 * });
 * const primary = new gcp.container.Cluster("primary", {
 *     name: "my-gke-cluster",
 *     location: "us-central1",
 *     removeDefaultNodePool: true,
 *     initialNodeCount: 1,
 * });
 * const primaryPreemptibleNodes = new gcp.container.NodePool("primary_preemptible_nodes", {
 *     name: "my-node-pool",
 *     cluster: primary.id,
 *     nodeCount: 1,
 *     nodeConfig: {
 *         preemptible: true,
 *         machineType: "e2-medium",
 *         serviceAccount: _default.email,
 *         oauthScopes: ["https://www.googleapis.com/auth/cloud-platform"],
 *     },
 * });
 * ```
 *
 * ### 2 Node Pools, 1 Separately Managed + The Default Node Pool
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const _default = new gcp.serviceaccount.Account("default", {
 *     accountId: "service-account-id",
 *     displayName: "Service Account",
 * });
 * const primary = new gcp.container.Cluster("primary", {
 *     name: "marcellus-wallace",
 *     location: "us-central1-a",
 *     initialNodeCount: 3,
 *     nodeLocations: ["us-central1-c"],
 *     nodeConfig: {
 *         serviceAccount: _default.email,
 *         oauthScopes: ["https://www.googleapis.com/auth/cloud-platform"],
 *         guestAccelerators: [{
 *             type: "nvidia-tesla-k80",
 *             count: 1,
 *         }],
 *     },
 * });
 * const np = new gcp.container.NodePool("np", {
 *     name: "my-node-pool",
 *     cluster: primary.id,
 *     nodeConfig: {
 *         machineType: "e2-medium",
 *         serviceAccount: _default.email,
 *         oauthScopes: ["https://www.googleapis.com/auth/cloud-platform"],
 *     },
 * });
 * ```
 *
 * ## Import
 *
 * Node pools can be imported using the `project`, `location`, `cluster` and `name`. If
 *
 * the project is omitted, the project value in the provider configuration will be used. Examples:
 *
 * * `{{project_id}}/{{location}}/{{cluster_id}}/{{pool_id}}`
 *
 * * `{{location}}/{{cluster_id}}/{{pool_id}}`
 *
 * When using the `pulumi import` command, node pools can be imported using one of the formats above. For example:
 *
 * ```sh
 * $ pulumi import gcp:container/nodePool:NodePool default {{project_id}}/{{location}}/{{cluster_id}}/{{pool_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:container/nodePool:NodePool default {{location}}/{{cluster_id}}/{{pool_id}}
 * ```
 */
export class NodePool extends pulumi.CustomResource {
    /**
     * Get an existing NodePool resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: NodePoolState, opts?: pulumi.CustomResourceOptions): NodePool {
        return new NodePool(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'gcp:container/nodePool:NodePool';

    /**
     * Returns true if the given object is an instance of NodePool.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is NodePool {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === NodePool.__pulumiType;
    }

    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    public readonly autoscaling!: pulumi.Output<outputs.container.NodePoolAutoscaling | undefined>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     *
     * - - -
     */
    public readonly cluster!: pulumi.Output<string>;
    /**
     * The initial number of nodes for the pool. In
     * regional or multi-zonal clusters, this is the number of nodes per zone. Changing
     * this will force recreation of the resource. WARNING: Resizing your node pool manually
     * may change this value in your existing cluster, which will trigger destruction
     * and recreation on the next provider run (to rectify the discrepancy).  If you don't
     * need this value, don't set it.  If you do need it, you can use a lifecycle block to
     * ignore subsqeuent changes to this field.
     */
    public readonly initialNodeCount!: pulumi.Output<number>;
    /**
     * The resource URLs of the managed instance groups associated with this node pool.
     */
    public /*out*/ readonly instanceGroupUrls!: pulumi.Output<string[]>;
    /**
     * The location (region or zone) of the cluster.
     *
     * - - -
     */
    public readonly location!: pulumi.Output<string>;
    /**
     * List of instance group URLs which have been assigned to this node pool.
     */
    public /*out*/ readonly managedInstanceGroupUrls!: pulumi.Output<string[]>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    public readonly management!: pulumi.Output<outputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    public readonly maxPodsPerNode!: pulumi.Output<number>;
    /**
     * The name of the node pool. If left blank, the provider will
     * auto-generate a unique name.
     */
    public readonly name!: pulumi.Output<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    public readonly namePrefix!: pulumi.Output<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    public readonly networkConfig!: pulumi.Output<outputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    public readonly nodeConfig!: pulumi.Output<outputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    public readonly nodeCount!: pulumi.Output<number>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     *
     * > Note: `nodeLocations` will not revert to the cluster's default set of zones
     * upon being unset. You must manually reconcile the list of zones with your
     * cluster.
     */
    public readonly nodeLocations!: pulumi.Output<string[]>;
    public /*out*/ readonly operation!: pulumi.Output<string>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    public readonly placementPolicy!: pulumi.Output<outputs.container.NodePoolPlacementPolicy | undefined>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    public readonly project!: pulumi.Output<string>;
    /**
     * Specifies node pool-level settings of queued provisioning.
     * Structure is documented below.
     *
     * <a name="nestedAutoscaling"></a>The `autoscaling` block supports (either total or per zone limits are required):
     */
    public readonly queuedProvisioning!: pulumi.Output<outputs.container.NodePoolQueuedProvisioning | undefined>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    public readonly upgradeSettings!: pulumi.Output<outputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field
     * and `autoUpgrade` are both specified, they will fight each other for what the node version should
     * be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
     * recommended that you specify explicit versions as the provider will see spurious diffs
     * when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
     * `versionPrefix` field to approximate fuzzy versions in a provider-compatible way.
     */
    public readonly version!: pulumi.Output<string>;

    /**
     * Create a NodePool resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: NodePoolArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: NodePoolArgs | NodePoolState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as NodePoolState | undefined;
            resourceInputs["autoscaling"] = state ? state.autoscaling : undefined;
            resourceInputs["cluster"] = state ? state.cluster : undefined;
            resourceInputs["initialNodeCount"] = state ? state.initialNodeCount : undefined;
            resourceInputs["instanceGroupUrls"] = state ? state.instanceGroupUrls : undefined;
            resourceInputs["location"] = state ? state.location : undefined;
            resourceInputs["managedInstanceGroupUrls"] = state ? state.managedInstanceGroupUrls : undefined;
            resourceInputs["management"] = state ? state.management : undefined;
            resourceInputs["maxPodsPerNode"] = state ? state.maxPodsPerNode : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["namePrefix"] = state ? state.namePrefix : undefined;
            resourceInputs["networkConfig"] = state ? state.networkConfig : undefined;
            resourceInputs["nodeConfig"] = state ? state.nodeConfig : undefined;
            resourceInputs["nodeCount"] = state ? state.nodeCount : undefined;
            resourceInputs["nodeLocations"] = state ? state.nodeLocations : undefined;
            resourceInputs["operation"] = state ? state.operation : undefined;
            resourceInputs["placementPolicy"] = state ? state.placementPolicy : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["queuedProvisioning"] = state ? state.queuedProvisioning : undefined;
            resourceInputs["upgradeSettings"] = state ? state.upgradeSettings : undefined;
            resourceInputs["version"] = state ? state.version : undefined;
        } else {
            const args = argsOrState as NodePoolArgs | undefined;
            if ((!args || args.cluster === undefined) && !opts.urn) {
                throw new Error("Missing required property 'cluster'");
            }
            resourceInputs["autoscaling"] = args ? args.autoscaling : undefined;
            resourceInputs["cluster"] = args ? args.cluster : undefined;
            resourceInputs["initialNodeCount"] = args ? args.initialNodeCount : undefined;
            resourceInputs["location"] = args ? args.location : undefined;
            resourceInputs["management"] = args ? args.management : undefined;
            resourceInputs["maxPodsPerNode"] = args ? args.maxPodsPerNode : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["namePrefix"] = args ? args.namePrefix : undefined;
            resourceInputs["networkConfig"] = args ? args.networkConfig : undefined;
            resourceInputs["nodeConfig"] = args ? args.nodeConfig : undefined;
            resourceInputs["nodeCount"] = args ? args.nodeCount : undefined;
            resourceInputs["nodeLocations"] = args ? args.nodeLocations : undefined;
            resourceInputs["placementPolicy"] = args ? args.placementPolicy : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["queuedProvisioning"] = args ? args.queuedProvisioning : undefined;
            resourceInputs["upgradeSettings"] = args ? args.upgradeSettings : undefined;
            resourceInputs["version"] = args ? args.version : undefined;
            resourceInputs["instanceGroupUrls"] = undefined /*out*/;
            resourceInputs["managedInstanceGroupUrls"] = undefined /*out*/;
            resourceInputs["operation"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(NodePool.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering NodePool resources.
 */
export interface NodePoolState {
    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    autoscaling?: pulumi.Input<inputs.container.NodePoolAutoscaling>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     *
     * - - -
     */
    cluster?: pulumi.Input<string>;
    /**
     * The initial number of nodes for the pool. In
     * regional or multi-zonal clusters, this is the number of nodes per zone. Changing
     * this will force recreation of the resource. WARNING: Resizing your node pool manually
     * may change this value in your existing cluster, which will trigger destruction
     * and recreation on the next provider run (to rectify the discrepancy).  If you don't
     * need this value, don't set it.  If you do need it, you can use a lifecycle block to
     * ignore subsqeuent changes to this field.
     */
    initialNodeCount?: pulumi.Input<number>;
    /**
     * The resource URLs of the managed instance groups associated with this node pool.
     */
    instanceGroupUrls?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The location (region or zone) of the cluster.
     *
     * - - -
     */
    location?: pulumi.Input<string>;
    /**
     * List of instance group URLs which have been assigned to this node pool.
     */
    managedInstanceGroupUrls?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    management?: pulumi.Input<inputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    maxPodsPerNode?: pulumi.Input<number>;
    /**
     * The name of the node pool. If left blank, the provider will
     * auto-generate a unique name.
     */
    name?: pulumi.Input<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    namePrefix?: pulumi.Input<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    networkConfig?: pulumi.Input<inputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    nodeConfig?: pulumi.Input<inputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     *
     * > Note: `nodeLocations` will not revert to the cluster's default set of zones
     * upon being unset. You must manually reconcile the list of zones with your
     * cluster.
     */
    nodeLocations?: pulumi.Input<pulumi.Input<string>[]>;
    operation?: pulumi.Input<string>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    placementPolicy?: pulumi.Input<inputs.container.NodePoolPlacementPolicy>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    project?: pulumi.Input<string>;
    /**
     * Specifies node pool-level settings of queued provisioning.
     * Structure is documented below.
     *
     * <a name="nestedAutoscaling"></a>The `autoscaling` block supports (either total or per zone limits are required):
     */
    queuedProvisioning?: pulumi.Input<inputs.container.NodePoolQueuedProvisioning>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    upgradeSettings?: pulumi.Input<inputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field
     * and `autoUpgrade` are both specified, they will fight each other for what the node version should
     * be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
     * recommended that you specify explicit versions as the provider will see spurious diffs
     * when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
     * `versionPrefix` field to approximate fuzzy versions in a provider-compatible way.
     */
    version?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a NodePool resource.
 */
export interface NodePoolArgs {
    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    autoscaling?: pulumi.Input<inputs.container.NodePoolAutoscaling>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     *
     * - - -
     */
    cluster: pulumi.Input<string>;
    /**
     * The initial number of nodes for the pool. In
     * regional or multi-zonal clusters, this is the number of nodes per zone. Changing
     * this will force recreation of the resource. WARNING: Resizing your node pool manually
     * may change this value in your existing cluster, which will trigger destruction
     * and recreation on the next provider run (to rectify the discrepancy).  If you don't
     * need this value, don't set it.  If you do need it, you can use a lifecycle block to
     * ignore subsqeuent changes to this field.
     */
    initialNodeCount?: pulumi.Input<number>;
    /**
     * The location (region or zone) of the cluster.
     *
     * - - -
     */
    location?: pulumi.Input<string>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    management?: pulumi.Input<inputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    maxPodsPerNode?: pulumi.Input<number>;
    /**
     * The name of the node pool. If left blank, the provider will
     * auto-generate a unique name.
     */
    name?: pulumi.Input<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    namePrefix?: pulumi.Input<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    networkConfig?: pulumi.Input<inputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    nodeConfig?: pulumi.Input<inputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     *
     * > Note: `nodeLocations` will not revert to the cluster's default set of zones
     * upon being unset. You must manually reconcile the list of zones with your
     * cluster.
     */
    nodeLocations?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    placementPolicy?: pulumi.Input<inputs.container.NodePoolPlacementPolicy>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    project?: pulumi.Input<string>;
    /**
     * Specifies node pool-level settings of queued provisioning.
     * Structure is documented below.
     *
     * <a name="nestedAutoscaling"></a>The `autoscaling` block supports (either total or per zone limits are required):
     */
    queuedProvisioning?: pulumi.Input<inputs.container.NodePoolQueuedProvisioning>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    upgradeSettings?: pulumi.Input<inputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field
     * and `autoUpgrade` are both specified, they will fight each other for what the node version should
     * be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
     * recommended that you specify explicit versions as the provider will see spurious diffs
     * when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
     * `versionPrefix` field to approximate fuzzy versions in a provider-compatible way.
     */
    version?: pulumi.Input<string>;
}

// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";
import * as utilities from "../utilities";

/**
 * Manages a node pool in a Google Kubernetes Engine (GKE) cluster separately from
 * the cluster control plane. For more information see [the official documentation](https://cloud.google.com/container-engine/docs/node-pools)
 * and [the API reference](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters.nodePools).
 *
 * ## Example Usage
 *
 * ### Using A Separately Managed Node Pool (Recommended)
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const _default = new gcp.serviceaccount.Account("default", {
 *     accountId: "service-account-id",
 *     displayName: "Service Account",
 * });
 * const primary = new gcp.container.Cluster("primary", {
 *     name: "my-gke-cluster",
 *     location: "us-central1",
 *     removeDefaultNodePool: true,
 *     initialNodeCount: 1,
 * });
 * const primaryPreemptibleNodes = new gcp.container.NodePool("primary_preemptible_nodes", {
 *     name: "my-node-pool",
 *     cluster: primary.id,
 *     nodeCount: 1,
 *     nodeConfig: {
 *         preemptible: true,
 *         machineType: "e2-medium",
 *         serviceAccount: _default.email,
 *         oauthScopes: ["https://www.googleapis.com/auth/cloud-platform"],
 *     },
 * });
 * ```
 *
 * ### 2 Node Pools, 1 Separately Managed + The Default Node Pool
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const _default = new gcp.serviceaccount.Account("default", {
 *     accountId: "service-account-id",
 *     displayName: "Service Account",
 * });
 * const primary = new gcp.container.Cluster("primary", {
 *     name: "marcellus-wallace",
 *     location: "us-central1-a",
 *     initialNodeCount: 3,
 *     nodeLocations: ["us-central1-c"],
 *     nodeConfig: {
 *         serviceAccount: _default.email,
 *         oauthScopes: ["https://www.googleapis.com/auth/cloud-platform"],
 *         guestAccelerators: [{
 *             type: "nvidia-tesla-k80",
 *             count: 1,
 *         }],
 *     },
 * });
 * const np = new gcp.container.NodePool("np", {
 *     name: "my-node-pool",
 *     cluster: primary.id,
 *     nodeConfig: {
 *         machineType: "e2-medium",
 *         serviceAccount: _default.email,
 *         oauthScopes: ["https://www.googleapis.com/auth/cloud-platform"],
 *     },
 * });
 * ```
 *
 * ## Import
 *
 * Node pools can be imported using the `project`, `location`, `cluster` and `name`. If
 *
 * the project is omitted, the project value in the provider configuration will be used. Examples:
 *
 * * `{{project_id}}/{{location}}/{{cluster_id}}/{{pool_id}}`
 *
 * * `{{location}}/{{cluster_id}}/{{pool_id}}`
 *
 * When using the `pulumi import` command, node pools can be imported using one of the formats above. For example:
 *
 * ```sh
 * $ pulumi import gcp:container/nodePool:NodePool default {{project_id}}/{{location}}/{{cluster_id}}/{{pool_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:container/nodePool:NodePool default {{location}}/{{cluster_id}}/{{pool_id}}
 * ```
 */
export class NodePool extends pulumi.CustomResource {
    /**
     * Get an existing NodePool resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: NodePoolState, opts?: pulumi.CustomResourceOptions): NodePool {
        return new NodePool(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'gcp:container/nodePool:NodePool';

    /**
     * Returns true if the given object is an instance of NodePool.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is NodePool {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === NodePool.__pulumiType;
    }

    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    declare public readonly autoscaling: pulumi.Output<outputs.container.NodePoolAutoscaling | undefined>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     *
     * - - -
     */
    declare public readonly cluster: pulumi.Output<string>;
    /**
     * The initial number of nodes for the pool. In
     * regional or multi-zonal clusters, this is the number of nodes per zone. Changing
     * this will force recreation of the resource. WARNING: Resizing your node pool manually
     * may change this value in your existing cluster, which will trigger destruction
     * and recreation on the next provider run (to rectify the discrepancy).  If you don't
     * need this value, don't set it.  If you do need it, you can use a lifecycle block to
     * ignore subsqeuent changes to this field.
     */
    declare public readonly initialNodeCount: pulumi.Output<number>;
    /**
     * The resource URLs of the managed instance groups associated with this node pool.
     */
    declare public /*out*/ readonly instanceGroupUrls: pulumi.Output<string[]>;
    /**
     * The location (region or zone) of the cluster.
     *
     * - - -
     */
    declare public readonly location: pulumi.Output<string>;
    /**
     * List of instance group URLs which have been assigned to this node pool.
     */
    declare public /*out*/ readonly managedInstanceGroupUrls: pulumi.Output<string[]>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    declare public readonly management: pulumi.Output<outputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    declare public readonly maxPodsPerNode: pulumi.Output<number>;
    /**
     * The name of the node pool. If left blank, the provider will
     * auto-generate a unique name.
     */
    declare public readonly name: pulumi.Output<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    declare public readonly namePrefix: pulumi.Output<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    declare public readonly networkConfig: pulumi.Output<outputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    declare public readonly nodeConfig: pulumi.Output<outputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    declare public readonly nodeCount: pulumi.Output<number>;
    /**
     * The node drain configuration of the pool. Structure is documented below.
     */
    declare public readonly nodeDrainConfigs: pulumi.Output<outputs.container.NodePoolNodeDrainConfig[]>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     *
     * > Note: `nodeLocations` will not revert to the cluster's default set of zones
     * upon being unset. You must manually reconcile the list of zones with your
     * cluster.
     */
    declare public readonly nodeLocations: pulumi.Output<string[]>;
    declare public /*out*/ readonly operation: pulumi.Output<string>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    declare public readonly placementPolicy: pulumi.Output<outputs.container.NodePoolPlacementPolicy | undefined>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    declare public readonly project: pulumi.Output<string>;
    /**
     * Specifies node pool-level settings of queued provisioning.
     * Structure is documented below.
     *
     * <a name="nestedAutoscaling"></a>The `autoscaling` block supports (either total or per zone limits are required):
     */
    declare public readonly queuedProvisioning: pulumi.Output<outputs.container.NodePoolQueuedProvisioning | undefined>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    declare public readonly upgradeSettings: pulumi.Output<outputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field
     * and `autoUpgrade` are both specified, they will fight each other for what the node version should
     * be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
     * recommended that you specify explicit versions as the provider will see spurious diffs
     * when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
     * `versionPrefix` field to approximate fuzzy versions in a provider-compatible way.
     */
    declare public readonly version: pulumi.Output<string>;

    /**
     * Create a NodePool resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: NodePoolArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: NodePoolArgs | NodePoolState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as NodePoolState | undefined;
            resourceInputs["autoscaling"] = state?.autoscaling;
            resourceInputs["cluster"] = state?.cluster;
            resourceInputs["initialNodeCount"] = state?.initialNodeCount;
            resourceInputs["instanceGroupUrls"] = state?.instanceGroupUrls;
            resourceInputs["location"] = state?.location;
            resourceInputs["managedInstanceGroupUrls"] = state?.managedInstanceGroupUrls;
            resourceInputs["management"] = state?.management;
            resourceInputs["maxPodsPerNode"] = state?.maxPodsPerNode;
            resourceInputs["name"] = state?.name;
            resourceInputs["namePrefix"] = state?.namePrefix;
            resourceInputs["networkConfig"] = state?.networkConfig;
            resourceInputs["nodeConfig"] = state?.nodeConfig;
            resourceInputs["nodeCount"] = state?.nodeCount;
            resourceInputs["nodeDrainConfigs"] = state?.nodeDrainConfigs;
            resourceInputs["nodeLocations"] = state?.nodeLocations;
            resourceInputs["operation"] = state?.operation;
            resourceInputs["placementPolicy"] = state?.placementPolicy;
            resourceInputs["project"] = state?.project;
            resourceInputs["queuedProvisioning"] = state?.queuedProvisioning;
            resourceInputs["upgradeSettings"] = state?.upgradeSettings;
            resourceInputs["version"] = state?.version;
        } else {
            const args = argsOrState as NodePoolArgs | undefined;
            if (args?.cluster === undefined && !opts.urn) {
                throw new Error("Missing required property 'cluster'");
            }
            resourceInputs["autoscaling"] = args?.autoscaling;
            resourceInputs["cluster"] = args?.cluster;
            resourceInputs["initialNodeCount"] = args?.initialNodeCount;
            resourceInputs["location"] = args?.location;
            resourceInputs["management"] = args?.management;
            resourceInputs["maxPodsPerNode"] = args?.maxPodsPerNode;
            resourceInputs["name"] = args?.name;
            resourceInputs["namePrefix"] = args?.namePrefix;
            resourceInputs["networkConfig"] = args?.networkConfig;
            resourceInputs["nodeConfig"] = args?.nodeConfig;
            resourceInputs["nodeCount"] = args?.nodeCount;
            resourceInputs["nodeDrainConfigs"] = args?.nodeDrainConfigs;
            resourceInputs["nodeLocations"] = args?.nodeLocations;
            resourceInputs["placementPolicy"] = args?.placementPolicy;
            resourceInputs["project"] = args?.project;
            resourceInputs["queuedProvisioning"] = args?.queuedProvisioning;
            resourceInputs["upgradeSettings"] = args?.upgradeSettings;
            resourceInputs["version"] = args?.version;
            resourceInputs["instanceGroupUrls"] = undefined /*out*/;
            resourceInputs["managedInstanceGroupUrls"] = undefined /*out*/;
            resourceInputs["operation"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(NodePool.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering NodePool resources.
 */
export interface NodePoolState {
    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    autoscaling?: pulumi.Input<inputs.container.NodePoolAutoscaling>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     *
     * - - -
     */
    cluster?: pulumi.Input<string>;
    /**
     * The initial number of nodes for the pool. In
     * regional or multi-zonal clusters, this is the number of nodes per zone. Changing
     * this will force recreation of the resource. WARNING: Resizing your node pool manually
     * may change this value in your existing cluster, which will trigger destruction
     * and recreation on the next provider run (to rectify the discrepancy).  If you don't
     * need this value, don't set it.  If you do need it, you can use a lifecycle block to
     * ignore subsqeuent changes to this field.
     */
    initialNodeCount?: pulumi.Input<number>;
    /**
     * The resource URLs of the managed instance groups associated with this node pool.
     */
    instanceGroupUrls?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The location (region or zone) of the cluster.
     *
     * - - -
     */
    location?: pulumi.Input<string>;
    /**
     * List of instance group URLs which have been assigned to this node pool.
     */
    managedInstanceGroupUrls?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    management?: pulumi.Input<inputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    maxPodsPerNode?: pulumi.Input<number>;
    /**
     * The name of the node pool. If left blank, the provider will
     * auto-generate a unique name.
     */
    name?: pulumi.Input<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    namePrefix?: pulumi.Input<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    networkConfig?: pulumi.Input<inputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    nodeConfig?: pulumi.Input<inputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The node drain configuration of the pool. Structure is documented below.
     */
    nodeDrainConfigs?: pulumi.Input<pulumi.Input<inputs.container.NodePoolNodeDrainConfig>[]>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     *
     * > Note: `nodeLocations` will not revert to the cluster's default set of zones
     * upon being unset. You must manually reconcile the list of zones with your
     * cluster.
     */
    nodeLocations?: pulumi.Input<pulumi.Input<string>[]>;
    operation?: pulumi.Input<string>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    placementPolicy?: pulumi.Input<inputs.container.NodePoolPlacementPolicy>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    project?: pulumi.Input<string>;
    /**
     * Specifies node pool-level settings of queued provisioning.
     * Structure is documented below.
     *
     * <a name="nestedAutoscaling"></a>The `autoscaling` block supports (either total or per zone limits are required):
     */
    queuedProvisioning?: pulumi.Input<inputs.container.NodePoolQueuedProvisioning>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    upgradeSettings?: pulumi.Input<inputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field
     * and `autoUpgrade` are both specified, they will fight each other for what the node version should
     * be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
     * recommended that you specify explicit versions as the provider will see spurious diffs
     * when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
     * `versionPrefix` field to approximate fuzzy versions in a provider-compatible way.
     */
    version?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a NodePool resource.
 */
export interface NodePoolArgs {
    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    autoscaling?: pulumi.Input<inputs.container.NodePoolAutoscaling>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     *
     * - - -
     */
    cluster: pulumi.Input<string>;
    /**
     * The initial number of nodes for the pool. In
     * regional or multi-zonal clusters, this is the number of nodes per zone. Changing
     * this will force recreation of the resource. WARNING: Resizing your node pool manually
     * may change this value in your existing cluster, which will trigger destruction
     * and recreation on the next provider run (to rectify the discrepancy).  If you don't
     * need this value, don't set it.  If you do need it, you can use a lifecycle block to
     * ignore subsqeuent changes to this field.
     */
    initialNodeCount?: pulumi.Input<number>;
    /**
     * The location (region or zone) of the cluster.
     *
     * - - -
     */
    location?: pulumi.Input<string>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    management?: pulumi.Input<inputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    maxPodsPerNode?: pulumi.Input<number>;
    /**
     * The name of the node pool. If left blank, the provider will
     * auto-generate a unique name.
     */
    name?: pulumi.Input<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    namePrefix?: pulumi.Input<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    networkConfig?: pulumi.Input<inputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    nodeConfig?: pulumi.Input<inputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The node drain configuration of the pool. Structure is documented below.
     */
    nodeDrainConfigs?: pulumi.Input<pulumi.Input<inputs.container.NodePoolNodeDrainConfig>[]>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     *
     * > Note: `nodeLocations` will not revert to the cluster's default set of zones
     * upon being unset. You must manually reconcile the list of zones with your
     * cluster.
     */
    nodeLocations?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    placementPolicy?: pulumi.Input<inputs.container.NodePoolPlacementPolicy>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    project?: pulumi.Input<string>;
    /**
     * Specifies node pool-level settings of queued provisioning.
     * Structure is documented below.
     *
     * <a name="nestedAutoscaling"></a>The `autoscaling` block supports (either total or per zone limits are required):
     */
    queuedProvisioning?: pulumi.Input<inputs.container.NodePoolQueuedProvisioning>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    upgradeSettings?: pulumi.Input<inputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field
     * and `autoUpgrade` are both specified, they will fight each other for what the node version should
     * be, so setting both is highly discouraged. While a fuzzy version can be specified, it's
     * recommended that you specify explicit versions as the provider will see spurious diffs
     * when fuzzy versions are used. See the `gcp.container.getEngineVersions` data source's
     * `versionPrefix` field to approximate fuzzy versions in a provider-compatible way.
     */
    version?: pulumi.Input<string>;
}

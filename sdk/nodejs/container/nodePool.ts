// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";
import * as utilities from "../utilities";

/**
 * ## Import
 *
 * Node pools can be imported using the `project`, `location`, `cluster` and `name`. If the project is omitted, the project value in the provider configuration will be used. Examples
 *
 * ```sh
 *  $ pulumi import gcp:container/nodePool:NodePool mainpool my-gcp-project/us-east1-a/my-cluster/main-pool
 * ```
 *
 * ```sh
 *  $ pulumi import gcp:container/nodePool:NodePool mainpool us-east1/my-cluster/main-pool
 * ```
 */
export class NodePool extends pulumi.CustomResource {
    /**
     * Get an existing NodePool resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: NodePoolState, opts?: pulumi.CustomResourceOptions): NodePool {
        return new NodePool(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'gcp:container/nodePool:NodePool';

    /**
     * Returns true if the given object is an instance of NodePool.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is NodePool {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === NodePool.__pulumiType;
    }

    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    public readonly autoscaling!: pulumi.Output<outputs.container.NodePoolAutoscaling | undefined>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     */
    public readonly cluster!: pulumi.Output<string>;
    /**
     * The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
     * Changing this will force recreation of the resource.
     */
    public readonly initialNodeCount!: pulumi.Output<number>;
    /**
     * The resource URLs of the managed instance groups associated with this node pool.
     */
    public /*out*/ readonly instanceGroupUrls!: pulumi.Output<string[]>;
    /**
     * The location (region or zone) of the cluster.
     */
    public readonly location!: pulumi.Output<string>;
    /**
     * List of instance group URLs which have been assigned to this node pool.
     */
    public /*out*/ readonly managedInstanceGroupUrls!: pulumi.Output<string[]>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    public readonly management!: pulumi.Output<outputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    public readonly maxPodsPerNode!: pulumi.Output<number>;
    /**
     * The name of the node pool. If left blank, Terraform will auto-generate a unique name.
     */
    public readonly name!: pulumi.Output<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    public readonly namePrefix!: pulumi.Output<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    public readonly networkConfig!: pulumi.Output<outputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    public readonly nodeConfig!: pulumi.Output<outputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    public readonly nodeCount!: pulumi.Output<number>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     */
    public readonly nodeLocations!: pulumi.Output<string[]>;
    public /*out*/ readonly operation!: pulumi.Output<string>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    public readonly placementPolicy!: pulumi.Output<outputs.container.NodePoolPlacementPolicy | undefined>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    public readonly project!: pulumi.Output<string>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    public readonly upgradeSettings!: pulumi.Output<outputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
     * will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
     * can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
     * versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
     * versions in a Terraform-compatible way.
     */
    public readonly version!: pulumi.Output<string>;

    /**
     * Create a NodePool resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: NodePoolArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: NodePoolArgs | NodePoolState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as NodePoolState | undefined;
            resourceInputs["autoscaling"] = state ? state.autoscaling : undefined;
            resourceInputs["cluster"] = state ? state.cluster : undefined;
            resourceInputs["initialNodeCount"] = state ? state.initialNodeCount : undefined;
            resourceInputs["instanceGroupUrls"] = state ? state.instanceGroupUrls : undefined;
            resourceInputs["location"] = state ? state.location : undefined;
            resourceInputs["managedInstanceGroupUrls"] = state ? state.managedInstanceGroupUrls : undefined;
            resourceInputs["management"] = state ? state.management : undefined;
            resourceInputs["maxPodsPerNode"] = state ? state.maxPodsPerNode : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["namePrefix"] = state ? state.namePrefix : undefined;
            resourceInputs["networkConfig"] = state ? state.networkConfig : undefined;
            resourceInputs["nodeConfig"] = state ? state.nodeConfig : undefined;
            resourceInputs["nodeCount"] = state ? state.nodeCount : undefined;
            resourceInputs["nodeLocations"] = state ? state.nodeLocations : undefined;
            resourceInputs["operation"] = state ? state.operation : undefined;
            resourceInputs["placementPolicy"] = state ? state.placementPolicy : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["upgradeSettings"] = state ? state.upgradeSettings : undefined;
            resourceInputs["version"] = state ? state.version : undefined;
        } else {
            const args = argsOrState as NodePoolArgs | undefined;
            if ((!args || args.cluster === undefined) && !opts.urn) {
                throw new Error("Missing required property 'cluster'");
            }
            resourceInputs["autoscaling"] = args ? args.autoscaling : undefined;
            resourceInputs["cluster"] = args ? args.cluster : undefined;
            resourceInputs["initialNodeCount"] = args ? args.initialNodeCount : undefined;
            resourceInputs["location"] = args ? args.location : undefined;
            resourceInputs["management"] = args ? args.management : undefined;
            resourceInputs["maxPodsPerNode"] = args ? args.maxPodsPerNode : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["namePrefix"] = args ? args.namePrefix : undefined;
            resourceInputs["networkConfig"] = args ? args.networkConfig : undefined;
            resourceInputs["nodeConfig"] = args ? args.nodeConfig : undefined;
            resourceInputs["nodeCount"] = args ? args.nodeCount : undefined;
            resourceInputs["nodeLocations"] = args ? args.nodeLocations : undefined;
            resourceInputs["placementPolicy"] = args ? args.placementPolicy : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["upgradeSettings"] = args ? args.upgradeSettings : undefined;
            resourceInputs["version"] = args ? args.version : undefined;
            resourceInputs["instanceGroupUrls"] = undefined /*out*/;
            resourceInputs["managedInstanceGroupUrls"] = undefined /*out*/;
            resourceInputs["operation"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(NodePool.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering NodePool resources.
 */
export interface NodePoolState {
    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    autoscaling?: pulumi.Input<inputs.container.NodePoolAutoscaling>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     */
    cluster?: pulumi.Input<string>;
    /**
     * The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
     * Changing this will force recreation of the resource.
     */
    initialNodeCount?: pulumi.Input<number>;
    /**
     * The resource URLs of the managed instance groups associated with this node pool.
     */
    instanceGroupUrls?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The location (region or zone) of the cluster.
     */
    location?: pulumi.Input<string>;
    /**
     * List of instance group URLs which have been assigned to this node pool.
     */
    managedInstanceGroupUrls?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    management?: pulumi.Input<inputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    maxPodsPerNode?: pulumi.Input<number>;
    /**
     * The name of the node pool. If left blank, Terraform will auto-generate a unique name.
     */
    name?: pulumi.Input<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    namePrefix?: pulumi.Input<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    networkConfig?: pulumi.Input<inputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    nodeConfig?: pulumi.Input<inputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     */
    nodeLocations?: pulumi.Input<pulumi.Input<string>[]>;
    operation?: pulumi.Input<string>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    placementPolicy?: pulumi.Input<inputs.container.NodePoolPlacementPolicy>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    project?: pulumi.Input<string>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    upgradeSettings?: pulumi.Input<inputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
     * will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
     * can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
     * versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
     * versions in a Terraform-compatible way.
     */
    version?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a NodePool resource.
 */
export interface NodePoolArgs {
    /**
     * Configuration required by cluster autoscaler to adjust
     * the size of the node pool to the current cluster usage. Structure is documented below.
     */
    autoscaling?: pulumi.Input<inputs.container.NodePoolAutoscaling>;
    /**
     * The cluster to create the node pool for. Cluster must be present in `location` provided for clusters. May be specified in the format `projects/{{project}}/locations/{{location}}/clusters/{{cluster}}` or as just the name of the cluster.
     */
    cluster: pulumi.Input<string>;
    /**
     * The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone.
     * Changing this will force recreation of the resource.
     */
    initialNodeCount?: pulumi.Input<number>;
    /**
     * The location (region or zone) of the cluster.
     */
    location?: pulumi.Input<string>;
    /**
     * Node management configuration, wherein auto-repair and
     * auto-upgrade is configured. Structure is documented below.
     */
    management?: pulumi.Input<inputs.container.NodePoolManagement>;
    /**
     * The maximum number of pods per node in this node pool.
     * Note that this does not work on node pools which are "route-based" - that is, node
     * pools belonging to clusters that do not have IP Aliasing enabled.
     * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr)
     * for more information.
     */
    maxPodsPerNode?: pulumi.Input<number>;
    /**
     * The name of the node pool. If left blank, Terraform will auto-generate a unique name.
     */
    name?: pulumi.Input<string>;
    /**
     * Creates a unique name for the node pool beginning
     * with the specified prefix. Conflicts with `name`.
     */
    namePrefix?: pulumi.Input<string>;
    /**
     * The network configuration of the pool. Such as
     * configuration for [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Or enabling private nodes. Structure is
     * documented below
     */
    networkConfig?: pulumi.Input<inputs.container.NodePoolNetworkConfig>;
    /**
     * Parameters used in creating the node pool. See
     * gcp.container.Cluster for schema.
     */
    nodeConfig?: pulumi.Input<inputs.container.NodePoolNodeConfig>;
    /**
     * The number of nodes per instance group. This field can be used to
     * update the number of nodes per instance group but should not be used alongside `autoscaling`.
     */
    nodeCount?: pulumi.Input<number>;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must
     * be in the region of their regional cluster or in the same region as their
     * cluster's zone for zonal clusters. If unspecified, the cluster-level
     * `nodeLocations` will be used.
     */
    nodeLocations?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Specifies a custom placement policy for the
     * nodes.
     */
    placementPolicy?: pulumi.Input<inputs.container.NodePoolPlacementPolicy>;
    /**
     * The ID of the project in which to create the node pool. If blank,
     * the provider-configured project will be used.
     */
    project?: pulumi.Input<string>;
    /**
     * Specify node upgrade settings to change how GKE upgrades nodes.
     * The maximum number of nodes upgraded simultaneously is limited to 20. Structure is documented below.
     */
    upgradeSettings?: pulumi.Input<inputs.container.NodePoolUpgradeSettings>;
    /**
     * The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they
     * will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version
     * can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy
     * versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy
     * versions in a Terraform-compatible way.
     */
    version?: pulumi.Input<string>;
}

// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as utilities from "../utilities";

/**
 * ## Import
 *
 * Dataflow jobs can be imported using the job `id` e.g.
 *
 * ```sh
 *  $ pulumi import gcp:dataflow/job:Job example 2022-07-31_06_25_42-11926927532632678660
 * ```
 */
export class Job extends pulumi.CustomResource {
    /**
     * Get an existing Job resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: JobState, opts?: pulumi.CustomResourceOptions): Job {
        return new Job(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'gcp:dataflow/job:Job';

    /**
     * Returns true if the given object is an instance of Job.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is Job {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === Job.__pulumiType;
    }

    /**
     * List of experiments that should be used by the job. An example value is `["enableStackdriverAgentMetrics"]`.
     */
    public readonly additionalExperiments!: pulumi.Output<string[]>;
    /**
     * Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
     */
    public readonly enableStreamingEngine!: pulumi.Output<boolean | undefined>;
    /**
     * The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
     */
    public readonly ipConfiguration!: pulumi.Output<string | undefined>;
    /**
     * The unique ID of this job.
     */
    public /*out*/ readonly jobId!: pulumi.Output<string>;
    /**
     * The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
     */
    public readonly kmsKeyName!: pulumi.Output<string | undefined>;
    /**
     * User labels to be specified for the job. Keys and values should follow the restrictions
     * specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
     * **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
     * Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
     */
    public readonly labels!: pulumi.Output<{[key: string]: any} | undefined>;
    /**
     * The machine type to use for the job.
     */
    public readonly machineType!: pulumi.Output<string | undefined>;
    /**
     * The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
     */
    public readonly maxWorkers!: pulumi.Output<number | undefined>;
    /**
     * A unique name for the resource, required by Dataflow.
     */
    public readonly name!: pulumi.Output<string>;
    /**
     * The network to which VMs will be assigned. If it is not provided, "default" will be used.
     */
    public readonly network!: pulumi.Output<string | undefined>;
    /**
     * One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
     */
    public readonly onDelete!: pulumi.Output<string | undefined>;
    /**
     * Key/Value pairs to be passed to the Dataflow job (as used in the template).
     */
    public readonly parameters!: pulumi.Output<{[key: string]: any} | undefined>;
    /**
     * The project in which the resource belongs. If it is not provided, the provider project is used.
     */
    public readonly project!: pulumi.Output<string>;
    /**
     * The region in which the created job should run.
     */
    public readonly region!: pulumi.Output<string | undefined>;
    /**
     * The Service Account email used to create the job.
     */
    public readonly serviceAccountEmail!: pulumi.Output<string | undefined>;
    /**
     * If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
     */
    public readonly skipWaitOnJobTermination!: pulumi.Output<boolean | undefined>;
    /**
     * The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
     */
    public /*out*/ readonly state!: pulumi.Output<string>;
    /**
     * The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
     */
    public readonly subnetwork!: pulumi.Output<string | undefined>;
    /**
     * A writeable location on GCS for the Dataflow job to dump its temporary data.
     */
    public readonly tempGcsLocation!: pulumi.Output<string>;
    /**
     * The GCS path to the Dataflow job template.
     */
    public readonly templateGcsPath!: pulumi.Output<string>;
    /**
     * Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
     */
    public readonly transformNameMapping!: pulumi.Output<{[key: string]: any} | undefined>;
    /**
     * The type of this job, selected from the [JobType enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobType)
     */
    public /*out*/ readonly type!: pulumi.Output<string>;
    /**
     * The zone in which the created job should run. If it is not provided, the provider zone is used.
     */
    public readonly zone!: pulumi.Output<string | undefined>;

    /**
     * Create a Job resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: JobArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: JobArgs | JobState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as JobState | undefined;
            resourceInputs["additionalExperiments"] = state ? state.additionalExperiments : undefined;
            resourceInputs["enableStreamingEngine"] = state ? state.enableStreamingEngine : undefined;
            resourceInputs["ipConfiguration"] = state ? state.ipConfiguration : undefined;
            resourceInputs["jobId"] = state ? state.jobId : undefined;
            resourceInputs["kmsKeyName"] = state ? state.kmsKeyName : undefined;
            resourceInputs["labels"] = state ? state.labels : undefined;
            resourceInputs["machineType"] = state ? state.machineType : undefined;
            resourceInputs["maxWorkers"] = state ? state.maxWorkers : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["network"] = state ? state.network : undefined;
            resourceInputs["onDelete"] = state ? state.onDelete : undefined;
            resourceInputs["parameters"] = state ? state.parameters : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["region"] = state ? state.region : undefined;
            resourceInputs["serviceAccountEmail"] = state ? state.serviceAccountEmail : undefined;
            resourceInputs["skipWaitOnJobTermination"] = state ? state.skipWaitOnJobTermination : undefined;
            resourceInputs["state"] = state ? state.state : undefined;
            resourceInputs["subnetwork"] = state ? state.subnetwork : undefined;
            resourceInputs["tempGcsLocation"] = state ? state.tempGcsLocation : undefined;
            resourceInputs["templateGcsPath"] = state ? state.templateGcsPath : undefined;
            resourceInputs["transformNameMapping"] = state ? state.transformNameMapping : undefined;
            resourceInputs["type"] = state ? state.type : undefined;
            resourceInputs["zone"] = state ? state.zone : undefined;
        } else {
            const args = argsOrState as JobArgs | undefined;
            if ((!args || args.tempGcsLocation === undefined) && !opts.urn) {
                throw new Error("Missing required property 'tempGcsLocation'");
            }
            if ((!args || args.templateGcsPath === undefined) && !opts.urn) {
                throw new Error("Missing required property 'templateGcsPath'");
            }
            resourceInputs["additionalExperiments"] = args ? args.additionalExperiments : undefined;
            resourceInputs["enableStreamingEngine"] = args ? args.enableStreamingEngine : undefined;
            resourceInputs["ipConfiguration"] = args ? args.ipConfiguration : undefined;
            resourceInputs["kmsKeyName"] = args ? args.kmsKeyName : undefined;
            resourceInputs["labels"] = args ? args.labels : undefined;
            resourceInputs["machineType"] = args ? args.machineType : undefined;
            resourceInputs["maxWorkers"] = args ? args.maxWorkers : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["network"] = args ? args.network : undefined;
            resourceInputs["onDelete"] = args ? args.onDelete : undefined;
            resourceInputs["parameters"] = args ? args.parameters : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["region"] = args ? args.region : undefined;
            resourceInputs["serviceAccountEmail"] = args ? args.serviceAccountEmail : undefined;
            resourceInputs["skipWaitOnJobTermination"] = args ? args.skipWaitOnJobTermination : undefined;
            resourceInputs["subnetwork"] = args ? args.subnetwork : undefined;
            resourceInputs["tempGcsLocation"] = args ? args.tempGcsLocation : undefined;
            resourceInputs["templateGcsPath"] = args ? args.templateGcsPath : undefined;
            resourceInputs["transformNameMapping"] = args ? args.transformNameMapping : undefined;
            resourceInputs["zone"] = args ? args.zone : undefined;
            resourceInputs["jobId"] = undefined /*out*/;
            resourceInputs["state"] = undefined /*out*/;
            resourceInputs["type"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(Job.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering Job resources.
 */
export interface JobState {
    /**
     * List of experiments that should be used by the job. An example value is `["enableStackdriverAgentMetrics"]`.
     */
    additionalExperiments?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
     */
    enableStreamingEngine?: pulumi.Input<boolean>;
    /**
     * The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
     */
    ipConfiguration?: pulumi.Input<string>;
    /**
     * The unique ID of this job.
     */
    jobId?: pulumi.Input<string>;
    /**
     * The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
     */
    kmsKeyName?: pulumi.Input<string>;
    /**
     * User labels to be specified for the job. Keys and values should follow the restrictions
     * specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
     * **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
     * Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
     */
    labels?: pulumi.Input<{[key: string]: any}>;
    /**
     * The machine type to use for the job.
     */
    machineType?: pulumi.Input<string>;
    /**
     * The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
     */
    maxWorkers?: pulumi.Input<number>;
    /**
     * A unique name for the resource, required by Dataflow.
     */
    name?: pulumi.Input<string>;
    /**
     * The network to which VMs will be assigned. If it is not provided, "default" will be used.
     */
    network?: pulumi.Input<string>;
    /**
     * One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
     */
    onDelete?: pulumi.Input<string>;
    /**
     * Key/Value pairs to be passed to the Dataflow job (as used in the template).
     */
    parameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * The project in which the resource belongs. If it is not provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The region in which the created job should run.
     */
    region?: pulumi.Input<string>;
    /**
     * The Service Account email used to create the job.
     */
    serviceAccountEmail?: pulumi.Input<string>;
    /**
     * If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
     */
    skipWaitOnJobTermination?: pulumi.Input<boolean>;
    /**
     * The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
     */
    state?: pulumi.Input<string>;
    /**
     * The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
     */
    subnetwork?: pulumi.Input<string>;
    /**
     * A writeable location on GCS for the Dataflow job to dump its temporary data.
     */
    tempGcsLocation?: pulumi.Input<string>;
    /**
     * The GCS path to the Dataflow job template.
     */
    templateGcsPath?: pulumi.Input<string>;
    /**
     * Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
     */
    transformNameMapping?: pulumi.Input<{[key: string]: any}>;
    /**
     * The type of this job, selected from the [JobType enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobType)
     */
    type?: pulumi.Input<string>;
    /**
     * The zone in which the created job should run. If it is not provided, the provider zone is used.
     */
    zone?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a Job resource.
 */
export interface JobArgs {
    /**
     * List of experiments that should be used by the job. An example value is `["enableStackdriverAgentMetrics"]`.
     */
    additionalExperiments?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Enable/disable the use of [Streaming Engine](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) for the job. Note that Streaming Engine is enabled by default for pipelines developed against the Beam SDK for Python v2.21.0 or later when using Python 3.
     */
    enableStreamingEngine?: pulumi.Input<boolean>;
    /**
     * The configuration for VM IPs.  Options are `"WORKER_IP_PUBLIC"` or `"WORKER_IP_PRIVATE"`.
     */
    ipConfiguration?: pulumi.Input<string>;
    /**
     * The name for the Cloud KMS key for the job. Key format is: `projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY`
     */
    kmsKeyName?: pulumi.Input<string>;
    /**
     * User labels to be specified for the job. Keys and values should follow the restrictions
     * specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page.
     * **NOTE**: Google-provided Dataflow templates often provide default labels that begin with `goog-dataflow-provided`.
     * Unless explicitly set in config, these labels will be ignored to prevent diffs on re-apply.
     */
    labels?: pulumi.Input<{[key: string]: any}>;
    /**
     * The machine type to use for the job.
     */
    machineType?: pulumi.Input<string>;
    /**
     * The number of workers permitted to work on the job.  More workers may improve processing speed at additional cost.
     */
    maxWorkers?: pulumi.Input<number>;
    /**
     * A unique name for the resource, required by Dataflow.
     */
    name?: pulumi.Input<string>;
    /**
     * The network to which VMs will be assigned. If it is not provided, "default" will be used.
     */
    network?: pulumi.Input<string>;
    /**
     * One of "drain" or "cancel".  Specifies behavior of deletion during `pulumi destroy`.  See above note.
     */
    onDelete?: pulumi.Input<string>;
    /**
     * Key/Value pairs to be passed to the Dataflow job (as used in the template).
     */
    parameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * The project in which the resource belongs. If it is not provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The region in which the created job should run.
     */
    region?: pulumi.Input<string>;
    /**
     * The Service Account email used to create the job.
     */
    serviceAccountEmail?: pulumi.Input<string>;
    /**
     * If set to `true`, Pulumi will treat `DRAINING` and `CANCELLING` as terminal states when deleting the resource, and will remove the resource from Pulumi state and move on.  See above note.
     */
    skipWaitOnJobTermination?: pulumi.Input<boolean>;
    /**
     * The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK". If the [subnetwork is located in a Shared VPC network](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared), you must use the complete URL. For example `"googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME"`
     */
    subnetwork?: pulumi.Input<string>;
    /**
     * A writeable location on GCS for the Dataflow job to dump its temporary data.
     */
    tempGcsLocation: pulumi.Input<string>;
    /**
     * The GCS path to the Dataflow job template.
     */
    templateGcsPath: pulumi.Input<string>;
    /**
     * Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the corresponding name prefixes of the new job. This field is not used outside of update.
     */
    transformNameMapping?: pulumi.Input<{[key: string]: any}>;
    /**
     * The zone in which the created job should run. If it is not provided, the provider zone is used.
     */
    zone?: pulumi.Input<string>;
}

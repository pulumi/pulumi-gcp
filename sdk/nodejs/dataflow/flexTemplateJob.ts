// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as utilities from "../utilities";

/**
 * Creates a [Flex Template](https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates)
 * job on Dataflow, which is an implementation of Apache Beam running on Google
 * Compute Engine. For more information see the official documentation for [Beam](https://beam.apache.org)
 * and [Dataflow](https://cloud.google.com/dataflow/).
 *
 * ## Example Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const bigDataJob = new gcp.dataflow.FlexTemplateJob("bigDataJob", {
 *     containerSpecGcsPath: "gs://my-bucket/templates/template.json",
 *     parameters: {
 *         inputSubscription: "messages",
 *     },
 * }, {
 *     provider: google_beta,
 * });
 * ```
 * ## Note on "destroy" / "apply"
 *
 * There are many types of Dataflow jobs.  Some Dataflow jobs run constantly,
 * getting new data from (e.g.) a GCS bucket, and outputting data continuously.
 * Some jobs process a set amount of data then terminate. All jobs can fail while
 * running due to programming errors or other issues. In this way, Dataflow jobs
 * are different from most other provider / Google resources.
 *
 * The Dataflow resource is considered 'existing' while it is in a nonterminal
 * state.  If it reaches a terminal state (e.g. 'FAILED', 'COMPLETE',
 * 'CANCELLED'), it will be recreated on the next 'apply'.  This is as expected for
 * jobs which run continuously, but may surprise users who use this resource for
 * other kinds of Dataflow jobs.
 *
 * A Dataflow job which is 'destroyed' may be "cancelled" or "drained".  If
 * "cancelled", the job terminates - any data written remains where it is, but no
 * new data will be processed.  If "drained", no new data will enter the pipeline,
 * but any data currently in the pipeline will finish being processed.  The default
 * is "cancelled", but if a user sets `onDelete` to `"drain"` in the
 * configuration, you may experience a long wait for your `pulumi destroy` to
 * complete.
 *
 * You can potentially short-circuit the wait by setting `skipWaitOnJobTermination`
 * to `true`, but beware that unless you take active steps to ensure that the job
 * `name` parameter changes between instances, the name will conflict and the launch
 * of the new job will fail. One way to do this is with a
 * randomId
 * resource, for example:
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 * import * as random from "@pulumi/random";
 *
 * const config = new pulumi.Config();
 * const bigDataJobSubscriptionId = config.get("bigDataJobSubscriptionId") || "projects/myproject/subscriptions/messages";
 * const bigDataJobNameSuffix = new random.RandomId("bigDataJobNameSuffix", {
 *     byteLength: 4,
 *     keepers: {
 *         region: _var.region,
 *         subscription_id: bigDataJobSubscriptionId,
 *     },
 * });
 * const bigDataJob = new gcp.dataflow.FlexTemplateJob("bigDataJob", {
 *     region: _var.region,
 *     containerSpecGcsPath: "gs://my-bucket/templates/template.json",
 *     skipWaitOnJobTermination: true,
 *     parameters: {
 *         inputSubscription: bigDataJobSubscriptionId,
 *     },
 * }, {
 *     provider: google_beta,
 * });
 * ```
 *
 * ## Import
 *
 * This resource does not support import.
 */
export class FlexTemplateJob extends pulumi.CustomResource {
    /**
     * Get an existing FlexTemplateJob resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: FlexTemplateJobState, opts?: pulumi.CustomResourceOptions): FlexTemplateJob {
        return new FlexTemplateJob(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'gcp:dataflow/flexTemplateJob:FlexTemplateJob';

    /**
     * Returns true if the given object is an instance of FlexTemplateJob.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is FlexTemplateJob {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === FlexTemplateJob.__pulumiType;
    }

    /**
     * List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
     */
    public readonly additionalExperiments!: pulumi.Output<string[]>;
    /**
     * The algorithm to use for autoscaling
     */
    public readonly autoscalingAlgorithm!: pulumi.Output<string | undefined>;
    /**
     * The GCS path to the Dataflow job Flex
     * Template.
     *
     * - - -
     */
    public readonly containerSpecGcsPath!: pulumi.Output<string>;
    /**
     * Indicates if the job should use the streaming engine feature.
     */
    public readonly enableStreamingEngine!: pulumi.Output<boolean | undefined>;
    /**
     * The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
     */
    public readonly ipConfiguration!: pulumi.Output<string | undefined>;
    /**
     * The unique ID of this job.
     */
    public /*out*/ readonly jobId!: pulumi.Output<string>;
    /**
     * The name for the Cloud KMS key for the job. Key format is:
     * projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
     */
    public readonly kmsKeyName!: pulumi.Output<string | undefined>;
    /**
     * User labels to be specified for the job. Keys and values
     * should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
     * page. **Note**: This field is marked as deprecated as the API does not currently
     * support adding labels.
     * **NOTE**: Google-provided Dataflow templates often provide default labels
     * that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
     * labels will be ignored to prevent diffs on re-apply.
     */
    public readonly labels!: pulumi.Output<{[key: string]: any} | undefined>;
    /**
     * The machine type to use for launching the job. The default is n1-standard-1.
     */
    public readonly launcherMachineType!: pulumi.Output<string | undefined>;
    /**
     * The machine type to use for the job.
     */
    public readonly machineType!: pulumi.Output<string | undefined>;
    /**
     * The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
     * 1000.
     */
    public readonly maxWorkers!: pulumi.Output<number | undefined>;
    /**
     * A unique name for the resource, required by Dataflow.
     */
    public readonly name!: pulumi.Output<string>;
    /**
     * The network to which VMs will be assigned. If it is not provided, "default" will be used.
     */
    public readonly network!: pulumi.Output<string | undefined>;
    /**
     * The initial number of Google Compute Engine instances for the job.
     */
    public readonly numWorkers!: pulumi.Output<number | undefined>;
    /**
     * One of "drain" or "cancel". Specifies behavior of
     * deletion during `pulumi destroy`.  See above note.
     */
    public readonly onDelete!: pulumi.Output<string | undefined>;
    /**
     * Key/Value pairs to be passed to the Dataflow job (as
     * used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
     * such as `serviceAccount`, `workerMachineType`, etc can be specified here.
     */
    public readonly parameters!: pulumi.Output<{[key: string]: any} | undefined>;
    /**
     * The project in which the resource belongs. If it is not
     * provided, the provider project is used.
     */
    public readonly project!: pulumi.Output<string>;
    /**
     * The region in which the created job should run.
     */
    public readonly region!: pulumi.Output<string>;
    /**
     * Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
     * the SDK. Note this field is only valid for portable pipelines.
     */
    public readonly sdkContainerImage!: pulumi.Output<string | undefined>;
    /**
     * The Service Account email used to create the job.
     */
    public readonly serviceAccountEmail!: pulumi.Output<string>;
    /**
     * If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
     * terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
     * different, e.g. by embedding a release ID or by using a random_id.
     */
    public readonly skipWaitOnJobTermination!: pulumi.Output<boolean | undefined>;
    /**
     * The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    public readonly stagingLocation!: pulumi.Output<string>;
    /**
     * The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
     */
    public /*out*/ readonly state!: pulumi.Output<string>;
    /**
     * The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
     */
    public readonly subnetwork!: pulumi.Output<string | undefined>;
    /**
     * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    public readonly tempLocation!: pulumi.Output<string>;
    /**
     * Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
     * corresponding name prefixes of the new job.
     */
    public readonly transformNameMapping!: pulumi.Output<{[key: string]: any} | undefined>;
    /**
     * The type of this job, selected from the JobType enum.
     */
    public /*out*/ readonly type!: pulumi.Output<string>;

    /**
     * Create a FlexTemplateJob resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: FlexTemplateJobArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: FlexTemplateJobArgs | FlexTemplateJobState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as FlexTemplateJobState | undefined;
            resourceInputs["additionalExperiments"] = state ? state.additionalExperiments : undefined;
            resourceInputs["autoscalingAlgorithm"] = state ? state.autoscalingAlgorithm : undefined;
            resourceInputs["containerSpecGcsPath"] = state ? state.containerSpecGcsPath : undefined;
            resourceInputs["enableStreamingEngine"] = state ? state.enableStreamingEngine : undefined;
            resourceInputs["ipConfiguration"] = state ? state.ipConfiguration : undefined;
            resourceInputs["jobId"] = state ? state.jobId : undefined;
            resourceInputs["kmsKeyName"] = state ? state.kmsKeyName : undefined;
            resourceInputs["labels"] = state ? state.labels : undefined;
            resourceInputs["launcherMachineType"] = state ? state.launcherMachineType : undefined;
            resourceInputs["machineType"] = state ? state.machineType : undefined;
            resourceInputs["maxWorkers"] = state ? state.maxWorkers : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["network"] = state ? state.network : undefined;
            resourceInputs["numWorkers"] = state ? state.numWorkers : undefined;
            resourceInputs["onDelete"] = state ? state.onDelete : undefined;
            resourceInputs["parameters"] = state ? state.parameters : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["region"] = state ? state.region : undefined;
            resourceInputs["sdkContainerImage"] = state ? state.sdkContainerImage : undefined;
            resourceInputs["serviceAccountEmail"] = state ? state.serviceAccountEmail : undefined;
            resourceInputs["skipWaitOnJobTermination"] = state ? state.skipWaitOnJobTermination : undefined;
            resourceInputs["stagingLocation"] = state ? state.stagingLocation : undefined;
            resourceInputs["state"] = state ? state.state : undefined;
            resourceInputs["subnetwork"] = state ? state.subnetwork : undefined;
            resourceInputs["tempLocation"] = state ? state.tempLocation : undefined;
            resourceInputs["transformNameMapping"] = state ? state.transformNameMapping : undefined;
            resourceInputs["type"] = state ? state.type : undefined;
        } else {
            const args = argsOrState as FlexTemplateJobArgs | undefined;
            if ((!args || args.containerSpecGcsPath === undefined) && !opts.urn) {
                throw new Error("Missing required property 'containerSpecGcsPath'");
            }
            resourceInputs["additionalExperiments"] = args ? args.additionalExperiments : undefined;
            resourceInputs["autoscalingAlgorithm"] = args ? args.autoscalingAlgorithm : undefined;
            resourceInputs["containerSpecGcsPath"] = args ? args.containerSpecGcsPath : undefined;
            resourceInputs["enableStreamingEngine"] = args ? args.enableStreamingEngine : undefined;
            resourceInputs["ipConfiguration"] = args ? args.ipConfiguration : undefined;
            resourceInputs["kmsKeyName"] = args ? args.kmsKeyName : undefined;
            resourceInputs["labels"] = args ? args.labels : undefined;
            resourceInputs["launcherMachineType"] = args ? args.launcherMachineType : undefined;
            resourceInputs["machineType"] = args ? args.machineType : undefined;
            resourceInputs["maxWorkers"] = args ? args.maxWorkers : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["network"] = args ? args.network : undefined;
            resourceInputs["numWorkers"] = args ? args.numWorkers : undefined;
            resourceInputs["onDelete"] = args ? args.onDelete : undefined;
            resourceInputs["parameters"] = args ? args.parameters : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["region"] = args ? args.region : undefined;
            resourceInputs["sdkContainerImage"] = args ? args.sdkContainerImage : undefined;
            resourceInputs["serviceAccountEmail"] = args ? args.serviceAccountEmail : undefined;
            resourceInputs["skipWaitOnJobTermination"] = args ? args.skipWaitOnJobTermination : undefined;
            resourceInputs["stagingLocation"] = args ? args.stagingLocation : undefined;
            resourceInputs["subnetwork"] = args ? args.subnetwork : undefined;
            resourceInputs["tempLocation"] = args ? args.tempLocation : undefined;
            resourceInputs["transformNameMapping"] = args ? args.transformNameMapping : undefined;
            resourceInputs["jobId"] = undefined /*out*/;
            resourceInputs["state"] = undefined /*out*/;
            resourceInputs["type"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(FlexTemplateJob.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering FlexTemplateJob resources.
 */
export interface FlexTemplateJobState {
    /**
     * List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
     */
    additionalExperiments?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The algorithm to use for autoscaling
     */
    autoscalingAlgorithm?: pulumi.Input<string>;
    /**
     * The GCS path to the Dataflow job Flex
     * Template.
     *
     * - - -
     */
    containerSpecGcsPath?: pulumi.Input<string>;
    /**
     * Indicates if the job should use the streaming engine feature.
     */
    enableStreamingEngine?: pulumi.Input<boolean>;
    /**
     * The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
     */
    ipConfiguration?: pulumi.Input<string>;
    /**
     * The unique ID of this job.
     */
    jobId?: pulumi.Input<string>;
    /**
     * The name for the Cloud KMS key for the job. Key format is:
     * projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
     */
    kmsKeyName?: pulumi.Input<string>;
    /**
     * User labels to be specified for the job. Keys and values
     * should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
     * page. **Note**: This field is marked as deprecated as the API does not currently
     * support adding labels.
     * **NOTE**: Google-provided Dataflow templates often provide default labels
     * that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
     * labels will be ignored to prevent diffs on re-apply.
     */
    labels?: pulumi.Input<{[key: string]: any}>;
    /**
     * The machine type to use for launching the job. The default is n1-standard-1.
     */
    launcherMachineType?: pulumi.Input<string>;
    /**
     * The machine type to use for the job.
     */
    machineType?: pulumi.Input<string>;
    /**
     * The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
     * 1000.
     */
    maxWorkers?: pulumi.Input<number>;
    /**
     * A unique name for the resource, required by Dataflow.
     */
    name?: pulumi.Input<string>;
    /**
     * The network to which VMs will be assigned. If it is not provided, "default" will be used.
     */
    network?: pulumi.Input<string>;
    /**
     * The initial number of Google Compute Engine instances for the job.
     */
    numWorkers?: pulumi.Input<number>;
    /**
     * One of "drain" or "cancel". Specifies behavior of
     * deletion during `pulumi destroy`.  See above note.
     */
    onDelete?: pulumi.Input<string>;
    /**
     * Key/Value pairs to be passed to the Dataflow job (as
     * used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
     * such as `serviceAccount`, `workerMachineType`, etc can be specified here.
     */
    parameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * The project in which the resource belongs. If it is not
     * provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The region in which the created job should run.
     */
    region?: pulumi.Input<string>;
    /**
     * Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
     * the SDK. Note this field is only valid for portable pipelines.
     */
    sdkContainerImage?: pulumi.Input<string>;
    /**
     * The Service Account email used to create the job.
     */
    serviceAccountEmail?: pulumi.Input<string>;
    /**
     * If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
     * terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
     * different, e.g. by embedding a release ID or by using a random_id.
     */
    skipWaitOnJobTermination?: pulumi.Input<boolean>;
    /**
     * The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    stagingLocation?: pulumi.Input<string>;
    /**
     * The current state of the resource, selected from the [JobState enum](https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState)
     */
    state?: pulumi.Input<string>;
    /**
     * The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
     */
    subnetwork?: pulumi.Input<string>;
    /**
     * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    tempLocation?: pulumi.Input<string>;
    /**
     * Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
     * corresponding name prefixes of the new job.
     */
    transformNameMapping?: pulumi.Input<{[key: string]: any}>;
    /**
     * The type of this job, selected from the JobType enum.
     */
    type?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a FlexTemplateJob resource.
 */
export interface FlexTemplateJobArgs {
    /**
     * List of experiments that should be used by the job. An example value is ["enable_stackdriver_agent_metrics"].
     */
    additionalExperiments?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The algorithm to use for autoscaling
     */
    autoscalingAlgorithm?: pulumi.Input<string>;
    /**
     * The GCS path to the Dataflow job Flex
     * Template.
     *
     * - - -
     */
    containerSpecGcsPath: pulumi.Input<string>;
    /**
     * Indicates if the job should use the streaming engine feature.
     */
    enableStreamingEngine?: pulumi.Input<boolean>;
    /**
     * The configuration for VM IPs. Options are "WORKER_IP_PUBLIC" or "WORKER_IP_PRIVATE".
     */
    ipConfiguration?: pulumi.Input<string>;
    /**
     * The name for the Cloud KMS key for the job. Key format is:
     * projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
     */
    kmsKeyName?: pulumi.Input<string>;
    /**
     * User labels to be specified for the job. Keys and values
     * should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
     * page. **Note**: This field is marked as deprecated as the API does not currently
     * support adding labels.
     * **NOTE**: Google-provided Dataflow templates often provide default labels
     * that begin with `goog-dataflow-provided`. Unless explicitly set in config, these
     * labels will be ignored to prevent diffs on re-apply.
     */
    labels?: pulumi.Input<{[key: string]: any}>;
    /**
     * The machine type to use for launching the job. The default is n1-standard-1.
     */
    launcherMachineType?: pulumi.Input<string>;
    /**
     * The machine type to use for the job.
     */
    machineType?: pulumi.Input<string>;
    /**
     * The maximum number of Google Compute Engine instances to be made available to your pipeline during execution, from 1 to
     * 1000.
     */
    maxWorkers?: pulumi.Input<number>;
    /**
     * A unique name for the resource, required by Dataflow.
     */
    name?: pulumi.Input<string>;
    /**
     * The network to which VMs will be assigned. If it is not provided, "default" will be used.
     */
    network?: pulumi.Input<string>;
    /**
     * The initial number of Google Compute Engine instances for the job.
     */
    numWorkers?: pulumi.Input<number>;
    /**
     * One of "drain" or "cancel". Specifies behavior of
     * deletion during `pulumi destroy`.  See above note.
     */
    onDelete?: pulumi.Input<string>;
    /**
     * Key/Value pairs to be passed to the Dataflow job (as
     * used in the template). Additional [pipeline options](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)
     * such as `serviceAccount`, `workerMachineType`, etc can be specified here.
     */
    parameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * The project in which the resource belongs. If it is not
     * provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The region in which the created job should run.
     */
    region?: pulumi.Input<string>;
    /**
     * Docker registry location of container image to use for the 'worker harness. Default is the container for the version of
     * the SDK. Note this field is only valid for portable pipelines.
     */
    sdkContainerImage?: pulumi.Input<string>;
    /**
     * The Service Account email used to create the job.
     */
    serviceAccountEmail?: pulumi.Input<string>;
    /**
     * If true, treat DRAINING and CANCELLING as terminal job states and do not wait for further changes before removing from
     * terraform state and moving on. WARNING: this will lead to job name conflicts if you do not ensure that the job names are
     * different, e.g. by embedding a release ID or by using a random_id.
     */
    skipWaitOnJobTermination?: pulumi.Input<boolean>;
    /**
     * The Cloud Storage path to use for staging files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    stagingLocation?: pulumi.Input<string>;
    /**
     * The subnetwork to which VMs will be assigned. Should be of the form "regions/REGION/subnetworks/SUBNETWORK".
     */
    subnetwork?: pulumi.Input<string>;
    /**
     * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    tempLocation?: pulumi.Input<string>;
    /**
     * Only applicable when updating a pipeline. Map of transform name prefixes of the job to be replaced with the
     * corresponding name prefixes of the new job.
     */
    transformNameMapping?: pulumi.Input<{[key: string]: any}>;
}

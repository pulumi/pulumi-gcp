// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";
import * as utilities from "../utilities";

/**
 * Represents a machine learning solution.
 *
 * A model can have multiple versions, each of which is a deployed, trained model
 * ready to receive prediction requests. The model itself is just a container.
 *
 * To get more information about Model, see:
 *
 * * [API documentation](https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects.models)
 * * How-to Guides
 *     * [Official Documentation](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
 *
 * ## Example Usage
 * ### Ml Model Basic
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const _default = new gcp.ml.EngineModel("default", {
 *     description: "My model",
 *     regions: "us-central1",
 * });
 * ```
 * ### Ml Model Full
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const _default = new gcp.ml.EngineModel("default", {
 *     description: "My model",
 *     labels: {
 *         my_model: "foo",
 *     },
 *     onlinePredictionConsoleLogging: true,
 *     onlinePredictionLogging: true,
 *     regions: "us-central1",
 * });
 * ```
 *
 * ## Import
 *
 * Model can be imported using any of these accepted formats
 *
 * ```sh
 *  $ pulumi import gcp:ml/engineModel:EngineModel default projects/{{project}}/models/{{name}}
 * ```
 *
 * ```sh
 *  $ pulumi import gcp:ml/engineModel:EngineModel default {{project}}/{{name}}
 * ```
 *
 * ```sh
 *  $ pulumi import gcp:ml/engineModel:EngineModel default {{name}}
 * ```
 */
export class EngineModel extends pulumi.CustomResource {
    /**
     * Get an existing EngineModel resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: EngineModelState, opts?: pulumi.CustomResourceOptions): EngineModel {
        return new EngineModel(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'gcp:ml/engineModel:EngineModel';

    /**
     * Returns true if the given object is an instance of EngineModel.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is EngineModel {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === EngineModel.__pulumiType;
    }

    /**
     * The default version of the model. This version will be used to handle
     * prediction requests that do not specify a version.
     * Structure is documented below.
     */
    public readonly defaultVersion!: pulumi.Output<outputs.ml.EngineModelDefaultVersion | undefined>;
    /**
     * The description specified for the model when it was created.
     */
    public readonly description!: pulumi.Output<string | undefined>;
    /**
     * One or more labels that you can add, to organize your models.
     */
    public readonly labels!: pulumi.Output<{[key: string]: string} | undefined>;
    /**
     * The name specified for the model.
     */
    public readonly name!: pulumi.Output<string>;
    /**
     * If true, online prediction nodes send stderr and stdout streams to Stackdriver Logging
     */
    public readonly onlinePredictionConsoleLogging!: pulumi.Output<boolean | undefined>;
    /**
     * If true, online prediction access logs are sent to StackDriver Logging.
     */
    public readonly onlinePredictionLogging!: pulumi.Output<boolean | undefined>;
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     */
    public readonly project!: pulumi.Output<string>;
    /**
     * The list of regions where the model is going to be deployed.
     * Currently only one region per model is supported
     */
    public readonly regions!: pulumi.Output<string | undefined>;

    /**
     * Create a EngineModel resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args?: EngineModelArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: EngineModelArgs | EngineModelState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as EngineModelState | undefined;
            resourceInputs["defaultVersion"] = state ? state.defaultVersion : undefined;
            resourceInputs["description"] = state ? state.description : undefined;
            resourceInputs["labels"] = state ? state.labels : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["onlinePredictionConsoleLogging"] = state ? state.onlinePredictionConsoleLogging : undefined;
            resourceInputs["onlinePredictionLogging"] = state ? state.onlinePredictionLogging : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["regions"] = state ? state.regions : undefined;
        } else {
            const args = argsOrState as EngineModelArgs | undefined;
            resourceInputs["defaultVersion"] = args ? args.defaultVersion : undefined;
            resourceInputs["description"] = args ? args.description : undefined;
            resourceInputs["labels"] = args ? args.labels : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["onlinePredictionConsoleLogging"] = args ? args.onlinePredictionConsoleLogging : undefined;
            resourceInputs["onlinePredictionLogging"] = args ? args.onlinePredictionLogging : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["regions"] = args ? args.regions : undefined;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(EngineModel.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering EngineModel resources.
 */
export interface EngineModelState {
    /**
     * The default version of the model. This version will be used to handle
     * prediction requests that do not specify a version.
     * Structure is documented below.
     */
    defaultVersion?: pulumi.Input<inputs.ml.EngineModelDefaultVersion>;
    /**
     * The description specified for the model when it was created.
     */
    description?: pulumi.Input<string>;
    /**
     * One or more labels that you can add, to organize your models.
     */
    labels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The name specified for the model.
     */
    name?: pulumi.Input<string>;
    /**
     * If true, online prediction nodes send stderr and stdout streams to Stackdriver Logging
     */
    onlinePredictionConsoleLogging?: pulumi.Input<boolean>;
    /**
     * If true, online prediction access logs are sent to StackDriver Logging.
     */
    onlinePredictionLogging?: pulumi.Input<boolean>;
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The list of regions where the model is going to be deployed.
     * Currently only one region per model is supported
     */
    regions?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a EngineModel resource.
 */
export interface EngineModelArgs {
    /**
     * The default version of the model. This version will be used to handle
     * prediction requests that do not specify a version.
     * Structure is documented below.
     */
    defaultVersion?: pulumi.Input<inputs.ml.EngineModelDefaultVersion>;
    /**
     * The description specified for the model when it was created.
     */
    description?: pulumi.Input<string>;
    /**
     * One or more labels that you can add, to organize your models.
     */
    labels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The name specified for the model.
     */
    name?: pulumi.Input<string>;
    /**
     * If true, online prediction nodes send stderr and stdout streams to Stackdriver Logging
     */
    onlinePredictionConsoleLogging?: pulumi.Input<boolean>;
    /**
     * If true, online prediction access logs are sent to StackDriver Logging.
     */
    onlinePredictionLogging?: pulumi.Input<boolean>;
    /**
     * The ID of the project in which the resource belongs.
     * If it is not provided, the provider project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The list of regions where the model is going to be deployed.
     * Currently only one region per model is supported
     */
    regions?: pulumi.Input<string>;
}

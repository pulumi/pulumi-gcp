// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export namespace accesscontextmanager {
    export interface AccessLevelBasic {
        /**
         * How the conditions list should be combined to determine if a request
         * is granted this AccessLevel. If AND is used, each Condition in
         * conditions must be satisfied for the AccessLevel to be applied. If
         * OR is used, at least one Condition in conditions must be satisfied
         * for the AccessLevel to be applied.
         * Default value is `AND`.
         * Possible values are: `AND`, `OR`.
         */
        combiningFunction?: string;
        /**
         * A set of requirements for the AccessLevel to be granted.
         * Structure is documented below.
         */
        conditions: outputs.accesscontextmanager.AccessLevelBasicCondition[];
    }

    export interface AccessLevelBasicCondition {
        /**
         * Device specific restrictions, all restrictions must hold for
         * the Condition to be true. If not specified, all devices are
         * allowed.
         * Structure is documented below.
         */
        devicePolicy?: outputs.accesscontextmanager.AccessLevelBasicConditionDevicePolicy;
        /**
         * A list of CIDR block IP subnetwork specification. May be IPv4
         * or IPv6.
         * Note that for a CIDR IP address block, the specified IP address
         * portion must be properly truncated (i.e. all the host bits must
         * be zero) or the input is considered malformed. For example,
         * "192.0.2.0/24" is accepted but "192.0.2.1/24" is not. Similarly,
         * for IPv6, "2001:db8::/32" is accepted whereas "2001:db8::1/32"
         * is not. The originating IP of a request must be in one of the
         * listed subnets in order for this Condition to be true.
         * If empty, all IP addresses are allowed.
         */
        ipSubnetworks?: string[];
        /**
         * An allowed list of members (users, service accounts).
         * Using groups is not supported yet.
         * The signed-in user originating the request must be a part of one
         * of the provided members. If not specified, a request may come
         * from any user (logged in/not logged in, not present in any
         * groups, etc.).
         * Formats: `user:{emailid}`, `serviceAccount:{emailid}`
         */
        members?: string[];
        /**
         * Whether to negate the Condition. If true, the Condition becomes
         * a NAND over its non-empty fields, each field must be false for
         * the Condition overall to be satisfied. Defaults to false.
         */
        negate?: boolean;
        /**
         * The request must originate from one of the provided
         * countries/regions.
         * Format: A valid ISO 3166-1 alpha-2 code.
         */
        regions?: string[];
        /**
         * A list of other access levels defined in the same Policy,
         * referenced by resource name. Referencing an AccessLevel which
         * does not exist is an error. All access levels listed must be
         * granted for the Condition to be true.
         * Format: accessPolicies/{policy_id}/accessLevels/{short_name}
         */
        requiredAccessLevels?: string[];
    }

    export interface AccessLevelBasicConditionDevicePolicy {
        /**
         * A list of allowed device management levels.
         * An empty list allows all management levels.
         * Each value may be one of: `MANAGEMENT_UNSPECIFIED`, `NONE`, `BASIC`, `COMPLETE`.
         */
        allowedDeviceManagementLevels?: string[];
        /**
         * A list of allowed encryptions statuses.
         * An empty list allows all statuses.
         * Each value may be one of: `ENCRYPTION_UNSPECIFIED`, `ENCRYPTION_UNSUPPORTED`, `UNENCRYPTED`, `ENCRYPTED`.
         */
        allowedEncryptionStatuses?: string[];
        /**
         * A list of allowed OS versions.
         * An empty list allows all types and all versions.
         * Structure is documented below.
         */
        osConstraints?: outputs.accesscontextmanager.AccessLevelBasicConditionDevicePolicyOsConstraint[];
        /**
         * Whether the device needs to be approved by the customer admin.
         */
        requireAdminApproval?: boolean;
        /**
         * Whether the device needs to be corp owned.
         */
        requireCorpOwned?: boolean;
        /**
         * Whether or not screenlock is required for the DevicePolicy
         * to be true. Defaults to false.
         */
        requireScreenLock?: boolean;
    }

    export interface AccessLevelBasicConditionDevicePolicyOsConstraint {
        /**
         * The minimum allowed OS version. If not set, any version
         * of this OS satisfies the constraint.
         * Format: "major.minor.patch" such as "10.5.301", "9.2.1".
         */
        minimumVersion?: string;
        /**
         * The operating system type of the device.
         * Possible values are: `OS_UNSPECIFIED`, `DESKTOP_MAC`, `DESKTOP_WINDOWS`, `DESKTOP_LINUX`, `DESKTOP_CHROME_OS`, `ANDROID`, `IOS`.
         */
        osType: string;
        /**
         * If you specify DESKTOP_CHROME_OS for osType, you can optionally include requireVerifiedChromeOs to require Chrome Verified Access.
         */
        requireVerifiedChromeOs?: boolean;
    }

    export interface AccessLevelConditionDevicePolicy {
        /**
         * A list of allowed device management levels.
         * An empty list allows all management levels.
         * Each value may be one of: `MANAGEMENT_UNSPECIFIED`, `NONE`, `BASIC`, `COMPLETE`.
         */
        allowedDeviceManagementLevels?: string[];
        /**
         * A list of allowed encryptions statuses.
         * An empty list allows all statuses.
         * Each value may be one of: `ENCRYPTION_UNSPECIFIED`, `ENCRYPTION_UNSUPPORTED`, `UNENCRYPTED`, `ENCRYPTED`.
         */
        allowedEncryptionStatuses?: string[];
        /**
         * A list of allowed OS versions.
         * An empty list allows all types and all versions.
         * Structure is documented below.
         */
        osConstraints?: outputs.accesscontextmanager.AccessLevelConditionDevicePolicyOsConstraint[];
        /**
         * Whether the device needs to be approved by the customer admin.
         */
        requireAdminApproval?: boolean;
        /**
         * Whether the device needs to be corp owned.
         */
        requireCorpOwned?: boolean;
        /**
         * Whether or not screenlock is required for the DevicePolicy
         * to be true. Defaults to false.
         */
        requireScreenLock?: boolean;
    }

    export interface AccessLevelConditionDevicePolicyOsConstraint {
        /**
         * The minimum allowed OS version. If not set, any version
         * of this OS satisfies the constraint.
         * Format: "major.minor.patch" such as "10.5.301", "9.2.1".
         */
        minimumVersion?: string;
        /**
         * The operating system type of the device.
         * Possible values are: `OS_UNSPECIFIED`, `DESKTOP_MAC`, `DESKTOP_WINDOWS`, `DESKTOP_LINUX`, `DESKTOP_CHROME_OS`, `ANDROID`, `IOS`.
         */
        osType: string;
    }

    export interface AccessLevelCustom {
        /**
         * Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language.
         * This page details the objects and attributes that are used to the build the CEL expressions for
         * custom access levels - https://cloud.google.com/access-context-manager/docs/custom-access-level-spec.
         * Structure is documented below.
         */
        expr: outputs.accesscontextmanager.AccessLevelCustomExpr;
    }

    export interface AccessLevelCustomExpr {
        /**
         * Description of the expression
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose.
         */
        title?: string;
    }

    export interface AccessLevelsAccessLevel {
        /**
         * A set of predefined conditions for the access level and a combining function.
         * Structure is documented below.
         */
        basic?: outputs.accesscontextmanager.AccessLevelsAccessLevelBasic;
        /**
         * Custom access level conditions are set using the Cloud Common Expression Language to represent the necessary conditions for the level to apply to a request.
         * See CEL spec at: https://github.com/google/cel-spec.
         * Structure is documented below.
         */
        custom?: outputs.accesscontextmanager.AccessLevelsAccessLevelCustom;
        /**
         * Description of the AccessLevel and its use. Does not affect behavior.
         */
        description?: string;
        /**
         * Resource name for the Access Level. The shortName component must begin
         * with a letter and only include alphanumeric and '_'.
         * Format: accessPolicies/{policy_id}/accessLevels/{short_name}
         */
        name: string;
        /**
         * Human readable title. Must be unique within the Policy.
         */
        title: string;
    }

    export interface AccessLevelsAccessLevelBasic {
        /**
         * How the conditions list should be combined to determine if a request
         * is granted this AccessLevel. If AND is used, each Condition in
         * conditions must be satisfied for the AccessLevel to be applied. If
         * OR is used, at least one Condition in conditions must be satisfied
         * for the AccessLevel to be applied.
         * Default value is `AND`.
         * Possible values are: `AND`, `OR`.
         */
        combiningFunction?: string;
        /**
         * A set of requirements for the AccessLevel to be granted.
         * Structure is documented below.
         */
        conditions: outputs.accesscontextmanager.AccessLevelsAccessLevelBasicCondition[];
    }

    export interface AccessLevelsAccessLevelBasicCondition {
        /**
         * Device specific restrictions, all restrictions must hold for
         * the Condition to be true. If not specified, all devices are
         * allowed.
         * Structure is documented below.
         */
        devicePolicy?: outputs.accesscontextmanager.AccessLevelsAccessLevelBasicConditionDevicePolicy;
        /**
         * A list of CIDR block IP subnetwork specification. May be IPv4
         * or IPv6.
         * Note that for a CIDR IP address block, the specified IP address
         * portion must be properly truncated (i.e. all the host bits must
         * be zero) or the input is considered malformed. For example,
         * "192.0.2.0/24" is accepted but "192.0.2.1/24" is not. Similarly,
         * for IPv6, "2001:db8::/32" is accepted whereas "2001:db8::1/32"
         * is not. The originating IP of a request must be in one of the
         * listed subnets in order for this Condition to be true.
         * If empty, all IP addresses are allowed.
         */
        ipSubnetworks?: string[];
        /**
         * An allowed list of members (users, service accounts).
         * Using groups is not supported yet.
         * The signed-in user originating the request must be a part of one
         * of the provided members. If not specified, a request may come
         * from any user (logged in/not logged in, not present in any
         * groups, etc.).
         * Formats: `user:{emailid}`, `serviceAccount:{emailid}`
         */
        members?: string[];
        /**
         * Whether to negate the Condition. If true, the Condition becomes
         * a NAND over its non-empty fields, each field must be false for
         * the Condition overall to be satisfied. Defaults to false.
         */
        negate?: boolean;
        /**
         * The request must originate from one of the provided
         * countries/regions.
         * Format: A valid ISO 3166-1 alpha-2 code.
         */
        regions?: string[];
        /**
         * A list of other access levels defined in the same Policy,
         * referenced by resource name. Referencing an AccessLevel which
         * does not exist is an error. All access levels listed must be
         * granted for the Condition to be true.
         * Format: accessPolicies/{policy_id}/accessLevels/{short_name}
         */
        requiredAccessLevels?: string[];
    }

    export interface AccessLevelsAccessLevelBasicConditionDevicePolicy {
        /**
         * A list of allowed device management levels.
         * An empty list allows all management levels.
         * Each value may be one of: `MANAGEMENT_UNSPECIFIED`, `NONE`, `BASIC`, `COMPLETE`.
         */
        allowedDeviceManagementLevels?: string[];
        /**
         * A list of allowed encryptions statuses.
         * An empty list allows all statuses.
         * Each value may be one of: `ENCRYPTION_UNSPECIFIED`, `ENCRYPTION_UNSUPPORTED`, `UNENCRYPTED`, `ENCRYPTED`.
         */
        allowedEncryptionStatuses?: string[];
        /**
         * A list of allowed OS versions.
         * An empty list allows all types and all versions.
         * Structure is documented below.
         */
        osConstraints?: outputs.accesscontextmanager.AccessLevelsAccessLevelBasicConditionDevicePolicyOsConstraint[];
        /**
         * Whether the device needs to be approved by the customer admin.
         */
        requireAdminApproval?: boolean;
        /**
         * Whether the device needs to be corp owned.
         */
        requireCorpOwned?: boolean;
        /**
         * Whether or not screenlock is required for the DevicePolicy
         * to be true. Defaults to false.
         */
        requireScreenLock?: boolean;
    }

    export interface AccessLevelsAccessLevelBasicConditionDevicePolicyOsConstraint {
        /**
         * The minimum allowed OS version. If not set, any version
         * of this OS satisfies the constraint.
         * Format: "major.minor.patch" such as "10.5.301", "9.2.1".
         */
        minimumVersion?: string;
        /**
         * The operating system type of the device.
         * Possible values are: `OS_UNSPECIFIED`, `DESKTOP_MAC`, `DESKTOP_WINDOWS`, `DESKTOP_LINUX`, `DESKTOP_CHROME_OS`, `ANDROID`, `IOS`.
         */
        osType: string;
    }

    export interface AccessLevelsAccessLevelCustom {
        /**
         * Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language.
         * This page details the objects and attributes that are used to the build the CEL expressions for
         * custom access levels - https://cloud.google.com/access-context-manager/docs/custom-access-level-spec.
         * Structure is documented below.
         */
        expr: outputs.accesscontextmanager.AccessLevelsAccessLevelCustomExpr;
    }

    export interface AccessLevelsAccessLevelCustomExpr {
        /**
         * Description of the expression
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose.
         */
        title?: string;
    }

    export interface AccessPolicyIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AccessPolicyIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServicePerimeterEgressPolicyEgressFrom {
        /**
         * A list of identities that are allowed access through this `EgressPolicy`.
         * Should be in the format of email address. The email address should
         * represent individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access to outside the
         * perimeter. If left unspecified, then members of `identities` field will
         * be allowed access.
         * Possible values are: `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
    }

    export interface ServicePerimeterEgressPolicyEgressTo {
        /**
         * A list of external resources that are allowed to be accessed. A request
         * matches if it contains an external resource in this list (Example:
         * s3://bucket/path). Currently '*' is not allowed.
         */
        externalResources?: string[];
        /**
         * A list of `ApiOperations` that this egress rule applies to. A request matches
         * if it contains an operation/service in this list.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimeterEgressPolicyEgressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, that match this to stanza. A request matches
         * if it contains a resource in this list. If * is specified for resources,
         * then this `EgressTo` rule will authorize access to all resources outside
         * the perimeter.
         */
        resources?: string[];
    }

    export interface ServicePerimeterEgressPolicyEgressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimeterEgressPolicyEgressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimeterEgressPolicyEgressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimeterIngressPolicyIngressFrom {
        /**
         * A list of identities that are allowed access through this ingress policy.
         * Should be in the format of email address. The email address should represent
         * individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access from outside the
         * perimeter. If left unspecified, then members of `identities` field will be
         * allowed access.
         * Possible values are: `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
        /**
         * Sources that this `IngressPolicy` authorizes access from.
         * Structure is documented below.
         */
        sources?: outputs.accesscontextmanager.ServicePerimeterIngressPolicyIngressFromSource[];
    }

    export interface ServicePerimeterIngressPolicyIngressFromSource {
        /**
         * An `AccessLevel` resource name that allow resources within the
         * `ServicePerimeters` to be accessed from the internet. `AccessLevels` listed
         * must be in the same policy as this `ServicePerimeter`. Referencing a nonexistent
         * `AccessLevel` will cause an error. If no `AccessLevel` names are listed,
         * resources within the perimeter can only be accessed via Google Cloud calls
         * with request origins within the perimeter.
         * Example `accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.`
         * If * is specified, then all IngressSources will be allowed.
         */
        accessLevel?: string;
        /**
         * A Google Cloud resource that is allowed to ingress the perimeter.
         * Requests from these resources will be allowed to access perimeter data.
         * Currently only projects are allowed. Format `projects/{project_number}`
         * The project may be in any Google Cloud organization, not just the
         * organization that the perimeter is defined in. `*` is not allowed, the case
         * of allowing all Google Cloud resources only is not supported.
         */
        resource?: string;
    }

    export interface ServicePerimeterIngressPolicyIngressTo {
        /**
         * A list of `ApiOperations` the sources specified in corresponding `IngressFrom`
         * are allowed to perform in this `ServicePerimeter`.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimeterIngressPolicyIngressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, protected by this `ServicePerimeter`
         * that are allowed to be accessed by sources defined in the
         * corresponding `IngressFrom`. A request matches if it contains
         * a resource in this list. If `*` is specified for resources,
         * then this `IngressTo` rule will authorize access to all
         * resources inside the perimeter, provided that the request
         * also matches the `operations` field.
         */
        resources?: string[];
    }

    export interface ServicePerimeterIngressPolicyIngressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong to
         * the service specified by serviceName field. A single `MethodSelector` entry
         * with `*` specified for the method field will allow all methods AND
         * permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimeterIngressPolicyIngressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with `serviceName`
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimeterIngressPolicyIngressToOperationMethodSelector {
        /**
         * Value for method should be a valid method name for the corresponding
         * serviceName in `ApiOperation`. If `*` used as value for `method`, then
         * ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimeterSpec {
        /**
         * A list of AccessLevel resource names that allow resources within
         * the ServicePerimeter to be accessed from the internet.
         * AccessLevels listed must be in the same policy as this
         * ServicePerimeter. Referencing a nonexistent AccessLevel is a
         * syntax error. If no AccessLevel names are listed, resources within
         * the perimeter can only be accessed via GCP calls with request
         * origins within the perimeter. For Service Perimeter Bridge, must
         * be empty.
         * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
         */
        accessLevels?: string[];
        /**
         * List of EgressPolicies to apply to the perimeter. A perimeter may
         * have multiple EgressPolicies, each of which is evaluated separately.
         * Access is granted if any EgressPolicy grants it. Must be empty for
         * a perimeter bridge.
         * Structure is documented below.
         */
        egressPolicies?: outputs.accesscontextmanager.ServicePerimeterSpecEgressPolicy[];
        /**
         * List of `IngressPolicies` to apply to the perimeter. A perimeter may
         * have multiple `IngressPolicies`, each of which is evaluated
         * separately. Access is granted if any `Ingress Policy` grants it.
         * Must be empty for a perimeter bridge.
         * Structure is documented below.
         */
        ingressPolicies?: outputs.accesscontextmanager.ServicePerimeterSpecIngressPolicy[];
        /**
         * A list of GCP resources that are inside of the service perimeter.
         * Currently only projects are allowed.
         * Format: projects/{project_number}
         */
        resources?: string[];
        /**
         * GCP services that are subject to the Service Perimeter
         * restrictions. Must contain a list of services. For example, if
         * `storage.googleapis.com` is specified, access to the storage
         * buckets inside the perimeter must meet the perimeter's access
         * restrictions.
         */
        restrictedServices?: string[];
        /**
         * Specifies how APIs are allowed to communicate within the Service
         * Perimeter.
         * Structure is documented below.
         */
        vpcAccessibleServices?: outputs.accesscontextmanager.ServicePerimeterSpecVpcAccessibleServices;
    }

    export interface ServicePerimeterSpecEgressPolicy {
        /**
         * Defines conditions on the source of a request causing this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressFrom?: outputs.accesscontextmanager.ServicePerimeterSpecEgressPolicyEgressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and destination resources that
         * cause this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressTo?: outputs.accesscontextmanager.ServicePerimeterSpecEgressPolicyEgressTo;
    }

    export interface ServicePerimeterSpecEgressPolicyEgressFrom {
        /**
         * A list of identities that are allowed access through this `EgressPolicy`.
         * Should be in the format of email address. The email address should
         * represent individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access to outside the
         * perimeter. If left unspecified, then members of `identities` field will
         * be allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
    }

    export interface ServicePerimeterSpecEgressPolicyEgressTo {
        /**
         * A list of external resources that are allowed to be accessed. A request
         * matches if it contains an external resource in this list (Example:
         * s3://bucket/path). Currently '*' is not allowed.
         */
        externalResources?: string[];
        /**
         * A list of `ApiOperations` that this egress rule applies to. A request matches
         * if it contains an operation/service in this list.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimeterSpecEgressPolicyEgressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, that match this to stanza. A request matches
         * if it contains a resource in this list. If * is specified for resources,
         * then this `EgressTo` rule will authorize access to all resources outside
         * the perimeter.
         */
        resources?: string[];
    }

    export interface ServicePerimeterSpecEgressPolicyEgressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimeterSpecIngressPolicy {
        /**
         * Defines the conditions on the source of a request causing this `IngressPolicy`
         * to apply.
         * Structure is documented below.
         */
        ingressFrom?: outputs.accesscontextmanager.ServicePerimeterSpecIngressPolicyIngressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and request destination that cause
         * this `IngressPolicy` to apply.
         * Structure is documented below.
         */
        ingressTo?: outputs.accesscontextmanager.ServicePerimeterSpecIngressPolicyIngressTo;
    }

    export interface ServicePerimeterSpecIngressPolicyIngressFrom {
        /**
         * A list of identities that are allowed access through this ingress policy.
         * Should be in the format of email address. The email address should represent
         * individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access from outside the
         * perimeter. If left unspecified, then members of `identities` field will be
         * allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
        /**
         * Sources that this `IngressPolicy` authorizes access from.
         * Structure is documented below.
         */
        sources?: outputs.accesscontextmanager.ServicePerimeterSpecIngressPolicyIngressFromSource[];
    }

    export interface ServicePerimeterSpecIngressPolicyIngressFromSource {
        /**
         * An `AccessLevel` resource name that allow resources within the
         * `ServicePerimeters` to be accessed from the internet. `AccessLevels` listed
         * must be in the same policy as this `ServicePerimeter`. Referencing a nonexistent
         * `AccessLevel` will cause an error. If no `AccessLevel` names are listed,
         * resources within the perimeter can only be accessed via Google Cloud calls
         * with request origins within the perimeter.
         * Example `accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.`
         * If * is specified, then all IngressSources will be allowed.
         */
        accessLevel?: string;
        /**
         * A Google Cloud resource that is allowed to ingress the perimeter.
         * Requests from these resources will be allowed to access perimeter data.
         * Currently only projects are allowed. Format `projects/{project_number}`
         * The project may be in any Google Cloud organization, not just the
         * organization that the perimeter is defined in. `*` is not allowed, the case
         * of allowing all Google Cloud resources only is not supported.
         */
        resource?: string;
    }

    export interface ServicePerimeterSpecIngressPolicyIngressTo {
        /**
         * A list of `ApiOperations` the sources specified in corresponding `IngressFrom`
         * are allowed to perform in this `ServicePerimeter`.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimeterSpecIngressPolicyIngressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, protected by this `ServicePerimeter`
         * that are allowed to be accessed by sources defined in the
         * corresponding `IngressFrom`. A request matches if it contains
         * a resource in this list. If `*` is specified for resources,
         * then this `IngressTo` rule will authorize access to all
         * resources inside the perimeter, provided that the request
         * also matches the `operations` field.
         */
        resources?: string[];
    }

    export interface ServicePerimeterSpecIngressPolicyIngressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimeterSpecVpcAccessibleServices {
        /**
         * The list of APIs usable within the Service Perimeter.
         * Must be empty unless `enableRestriction` is True.
         */
        allowedServices?: string[];
        /**
         * Whether to restrict API calls within the Service Perimeter to the
         * list of APIs specified in 'allowedServices'.
         */
        enableRestriction?: boolean;
    }

    export interface ServicePerimeterStatus {
        /**
         * A list of AccessLevel resource names that allow resources within
         * the ServicePerimeter to be accessed from the internet.
         * AccessLevels listed must be in the same policy as this
         * ServicePerimeter. Referencing a nonexistent AccessLevel is a
         * syntax error. If no AccessLevel names are listed, resources within
         * the perimeter can only be accessed via GCP calls with request
         * origins within the perimeter. For Service Perimeter Bridge, must
         * be empty.
         * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
         */
        accessLevels?: string[];
        /**
         * List of EgressPolicies to apply to the perimeter. A perimeter may
         * have multiple EgressPolicies, each of which is evaluated separately.
         * Access is granted if any EgressPolicy grants it. Must be empty for
         * a perimeter bridge.
         * Structure is documented below.
         */
        egressPolicies?: outputs.accesscontextmanager.ServicePerimeterStatusEgressPolicy[];
        /**
         * List of `IngressPolicies` to apply to the perimeter. A perimeter may
         * have multiple `IngressPolicies`, each of which is evaluated
         * separately. Access is granted if any `Ingress Policy` grants it.
         * Must be empty for a perimeter bridge.
         * Structure is documented below.
         */
        ingressPolicies?: outputs.accesscontextmanager.ServicePerimeterStatusIngressPolicy[];
        /**
         * A list of GCP resources that are inside of the service perimeter.
         * Currently only projects are allowed.
         * Format: projects/{project_number}
         */
        resources?: string[];
        /**
         * GCP services that are subject to the Service Perimeter
         * restrictions. Must contain a list of services. For example, if
         * `storage.googleapis.com` is specified, access to the storage
         * buckets inside the perimeter must meet the perimeter's access
         * restrictions.
         */
        restrictedServices?: string[];
        /**
         * Specifies how APIs are allowed to communicate within the Service
         * Perimeter.
         * Structure is documented below.
         */
        vpcAccessibleServices?: outputs.accesscontextmanager.ServicePerimeterStatusVpcAccessibleServices;
    }

    export interface ServicePerimeterStatusEgressPolicy {
        /**
         * Defines conditions on the source of a request causing this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressFrom?: outputs.accesscontextmanager.ServicePerimeterStatusEgressPolicyEgressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and destination resources that
         * cause this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressTo?: outputs.accesscontextmanager.ServicePerimeterStatusEgressPolicyEgressTo;
    }

    export interface ServicePerimeterStatusEgressPolicyEgressFrom {
        /**
         * A list of identities that are allowed access through this `EgressPolicy`.
         * Should be in the format of email address. The email address should
         * represent individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access to outside the
         * perimeter. If left unspecified, then members of `identities` field will
         * be allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
    }

    export interface ServicePerimeterStatusEgressPolicyEgressTo {
        /**
         * A list of external resources that are allowed to be accessed. A request
         * matches if it contains an external resource in this list (Example:
         * s3://bucket/path). Currently '*' is not allowed.
         */
        externalResources?: string[];
        /**
         * A list of `ApiOperations` that this egress rule applies to. A request matches
         * if it contains an operation/service in this list.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimeterStatusEgressPolicyEgressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, that match this to stanza. A request matches
         * if it contains a resource in this list. If * is specified for resources,
         * then this `EgressTo` rule will authorize access to all resources outside
         * the perimeter.
         */
        resources?: string[];
    }

    export interface ServicePerimeterStatusEgressPolicyEgressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimeterStatusIngressPolicy {
        /**
         * Defines the conditions on the source of a request causing this `IngressPolicy`
         * to apply.
         * Structure is documented below.
         */
        ingressFrom?: outputs.accesscontextmanager.ServicePerimeterStatusIngressPolicyIngressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and request destination that cause
         * this `IngressPolicy` to apply.
         * Structure is documented below.
         */
        ingressTo?: outputs.accesscontextmanager.ServicePerimeterStatusIngressPolicyIngressTo;
    }

    export interface ServicePerimeterStatusIngressPolicyIngressFrom {
        /**
         * A list of identities that are allowed access through this ingress policy.
         * Should be in the format of email address. The email address should represent
         * individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access from outside the
         * perimeter. If left unspecified, then members of `identities` field will be
         * allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
        /**
         * Sources that this `IngressPolicy` authorizes access from.
         * Structure is documented below.
         */
        sources?: outputs.accesscontextmanager.ServicePerimeterStatusIngressPolicyIngressFromSource[];
    }

    export interface ServicePerimeterStatusIngressPolicyIngressFromSource {
        /**
         * An `AccessLevel` resource name that allow resources within the
         * `ServicePerimeters` to be accessed from the internet. `AccessLevels` listed
         * must be in the same policy as this `ServicePerimeter`. Referencing a nonexistent
         * `AccessLevel` will cause an error. If no `AccessLevel` names are listed,
         * resources within the perimeter can only be accessed via Google Cloud calls
         * with request origins within the perimeter.
         * Example `accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.`
         * If * is specified, then all IngressSources will be allowed.
         */
        accessLevel?: string;
        /**
         * A Google Cloud resource that is allowed to ingress the perimeter.
         * Requests from these resources will be allowed to access perimeter data.
         * Currently only projects are allowed. Format `projects/{project_number}`
         * The project may be in any Google Cloud organization, not just the
         * organization that the perimeter is defined in. `*` is not allowed, the case
         * of allowing all Google Cloud resources only is not supported.
         */
        resource?: string;
    }

    export interface ServicePerimeterStatusIngressPolicyIngressTo {
        /**
         * A list of `ApiOperations` the sources specified in corresponding `IngressFrom`
         * are allowed to perform in this `ServicePerimeter`.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimeterStatusIngressPolicyIngressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, protected by this `ServicePerimeter`
         * that are allowed to be accessed by sources defined in the
         * corresponding `IngressFrom`. A request matches if it contains
         * a resource in this list. If `*` is specified for resources,
         * then this `IngressTo` rule will authorize access to all
         * resources inside the perimeter, provided that the request
         * also matches the `operations` field.
         */
        resources?: string[];
    }

    export interface ServicePerimeterStatusIngressPolicyIngressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimeterStatusVpcAccessibleServices {
        /**
         * The list of APIs usable within the Service Perimeter.
         * Must be empty unless `enableRestriction` is True.
         */
        allowedServices?: string[];
        /**
         * Whether to restrict API calls within the Service Perimeter to the
         * list of APIs specified in 'allowedServices'.
         */
        enableRestriction?: boolean;
    }

    export interface ServicePerimetersServicePerimeter {
        /**
         * (Output)
         * Time the AccessPolicy was created in UTC.
         */
        createTime: string;
        /**
         * Description of the ServicePerimeter and its use. Does not affect
         * behavior.
         */
        description?: string;
        /**
         * Resource name for the ServicePerimeter. The shortName component must
         * begin with a letter and only include alphanumeric and '_'.
         * Format: accessPolicies/{policy_id}/servicePerimeters/{short_name}
         */
        name: string;
        /**
         * Specifies the type of the Perimeter. There are two types: regular and
         * bridge. Regular Service Perimeter contains resources, access levels,
         * and restricted services. Every resource can be in at most
         * ONE regular Service Perimeter.
         * In addition to being in a regular service perimeter, a resource can also
         * be in zero or more perimeter bridges. A perimeter bridge only contains
         * resources. Cross project operations are permitted if all effected
         * resources share some perimeter (whether bridge or regular). Perimeter
         * Bridge does not contain access levels or services: those are governed
         * entirely by the regular perimeter that resource is in.
         * Perimeter Bridges are typically useful when building more complex
         * topologies with many independent perimeters that need to share some data
         * with a common perimeter, but should not be able to share data among
         * themselves.
         * Default value is `PERIMETER_TYPE_REGULAR`.
         * Possible values are: `PERIMETER_TYPE_REGULAR`, `PERIMETER_TYPE_BRIDGE`.
         */
        perimeterType?: string;
        /**
         * Proposed (or dry run) ServicePerimeter configuration.
         * This configuration allows to specify and test ServicePerimeter configuration
         * without enforcing actual access restrictions. Only allowed to be set when
         * the `useExplicitDryRunSpec` flag is set.
         * Structure is documented below.
         */
        spec?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpec;
        /**
         * ServicePerimeter configuration. Specifies sets of resources,
         * restricted services and access levels that determine
         * perimeter content and boundaries.
         * Structure is documented below.
         */
        status?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatus;
        /**
         * Human readable title. Must be unique within the Policy.
         */
        title: string;
        /**
         * (Output)
         * Time the AccessPolicy was updated in UTC.
         */
        updateTime: string;
        /**
         * Use explicit dry run spec flag. Ordinarily, a dry-run spec implicitly exists
         * for all Service Perimeters, and that spec is identical to the status for those
         * Service Perimeters. When this flag is set, it inhibits the generation of the
         * implicit spec, thereby allowing the user to explicitly provide a
         * configuration ("spec") to use in a dry-run version of the Service Perimeter.
         * This allows the user to test changes to the enforced config ("status") without
         * actually enforcing them. This testing is done through analyzing the differences
         * between currently enforced and suggested restrictions. useExplicitDryRunSpec must
         * bet set to True if any of the fields in the spec are set to non-default values.
         */
        useExplicitDryRunSpec?: boolean;
    }

    export interface ServicePerimetersServicePerimeterSpec {
        /**
         * A list of AccessLevel resource names that allow resources within
         * the ServicePerimeter to be accessed from the internet.
         * AccessLevels listed must be in the same policy as this
         * ServicePerimeter. Referencing a nonexistent AccessLevel is a
         * syntax error. If no AccessLevel names are listed, resources within
         * the perimeter can only be accessed via GCP calls with request
         * origins within the perimeter. For Service Perimeter Bridge, must
         * be empty.
         * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
         */
        accessLevels?: string[];
        /**
         * List of EgressPolicies to apply to the perimeter. A perimeter may
         * have multiple EgressPolicies, each of which is evaluated separately.
         * Access is granted if any EgressPolicy grants it. Must be empty for
         * a perimeter bridge.
         * Structure is documented below.
         */
        egressPolicies?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecEgressPolicy[];
        /**
         * List of `IngressPolicies` to apply to the perimeter. A perimeter may
         * have multiple `IngressPolicies`, each of which is evaluated
         * separately. Access is granted if any `Ingress Policy` grants it.
         * Must be empty for a perimeter bridge.
         * Structure is documented below.
         */
        ingressPolicies?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecIngressPolicy[];
        /**
         * A list of GCP resources that are inside of the service perimeter.
         * Currently only projects are allowed.
         * Format: projects/{project_number}
         */
        resources?: string[];
        /**
         * GCP services that are subject to the Service Perimeter
         * restrictions. Must contain a list of services. For example, if
         * `storage.googleapis.com` is specified, access to the storage
         * buckets inside the perimeter must meet the perimeter's access
         * restrictions.
         */
        restrictedServices?: string[];
        /**
         * Specifies how APIs are allowed to communicate within the Service
         * Perimeter.
         * Structure is documented below.
         */
        vpcAccessibleServices?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecVpcAccessibleServices;
    }

    export interface ServicePerimetersServicePerimeterSpecEgressPolicy {
        /**
         * Defines conditions on the source of a request causing this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressFrom?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecEgressPolicyEgressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and destination resources that
         * cause this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressTo?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecEgressPolicyEgressTo;
    }

    export interface ServicePerimetersServicePerimeterSpecEgressPolicyEgressFrom {
        /**
         * A list of identities that are allowed access through this `EgressPolicy`.
         * Should be in the format of email address. The email address should
         * represent individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access to outside the
         * perimeter. If left unspecified, then members of `identities` field will
         * be allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
    }

    export interface ServicePerimetersServicePerimeterSpecEgressPolicyEgressTo {
        /**
         * A list of external resources that are allowed to be accessed. A request
         * matches if it contains an external resource in this list (Example:
         * s3://bucket/path). Currently '*' is not allowed.
         */
        externalResources?: string[];
        /**
         * A list of `ApiOperations` that this egress rule applies to. A request matches
         * if it contains an operation/service in this list.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, that match this to stanza. A request matches
         * if it contains a resource in this list. If * is specified for resources,
         * then this `EgressTo` rule will authorize access to all resources outside
         * the perimeter.
         */
        resources?: string[];
    }

    export interface ServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimetersServicePerimeterSpecIngressPolicy {
        /**
         * Defines the conditions on the source of a request causing this `IngressPolicy`
         * to apply.
         * Structure is documented below.
         */
        ingressFrom?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecIngressPolicyIngressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and request destination that cause
         * this `IngressPolicy` to apply.
         * Structure is documented below.
         */
        ingressTo?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecIngressPolicyIngressTo;
    }

    export interface ServicePerimetersServicePerimeterSpecIngressPolicyIngressFrom {
        /**
         * A list of identities that are allowed access through this ingress policy.
         * Should be in the format of email address. The email address should represent
         * individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access from outside the
         * perimeter. If left unspecified, then members of `identities` field will be
         * allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
        /**
         * Sources that this `IngressPolicy` authorizes access from.
         * Structure is documented below.
         */
        sources?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecIngressPolicyIngressFromSource[];
    }

    export interface ServicePerimetersServicePerimeterSpecIngressPolicyIngressFromSource {
        /**
         * An `AccessLevel` resource name that allow resources within the
         * `ServicePerimeters` to be accessed from the internet. `AccessLevels` listed
         * must be in the same policy as this `ServicePerimeter`. Referencing a nonexistent
         * `AccessLevel` will cause an error. If no `AccessLevel` names are listed,
         * resources within the perimeter can only be accessed via Google Cloud calls
         * with request origins within the perimeter.
         * Example `accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.`
         * If * is specified, then all IngressSources will be allowed.
         */
        accessLevel?: string;
        /**
         * A Google Cloud resource that is allowed to ingress the perimeter.
         * Requests from these resources will be allowed to access perimeter data.
         * Currently only projects are allowed. Format `projects/{project_number}`
         * The project may be in any Google Cloud organization, not just the
         * organization that the perimeter is defined in. `*` is not allowed, the case
         * of allowing all Google Cloud resources only is not supported.
         */
        resource?: string;
    }

    export interface ServicePerimetersServicePerimeterSpecIngressPolicyIngressTo {
        /**
         * A list of `ApiOperations` the sources specified in corresponding `IngressFrom`
         * are allowed to perform in this `ServicePerimeter`.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, protected by this `ServicePerimeter`
         * that are allowed to be accessed by sources defined in the
         * corresponding `IngressFrom`. A request matches if it contains
         * a resource in this list. If `*` is specified for resources,
         * then this `IngressTo` rule will authorize access to all
         * resources inside the perimeter, provided that the request
         * also matches the `operations` field.
         */
        resources?: string[];
    }

    export interface ServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimetersServicePerimeterSpecVpcAccessibleServices {
        /**
         * The list of APIs usable within the Service Perimeter.
         * Must be empty unless `enableRestriction` is True.
         */
        allowedServices?: string[];
        /**
         * Whether to restrict API calls within the Service Perimeter to the
         * list of APIs specified in 'allowedServices'.
         */
        enableRestriction?: boolean;
    }

    export interface ServicePerimetersServicePerimeterStatus {
        /**
         * A list of AccessLevel resource names that allow resources within
         * the ServicePerimeter to be accessed from the internet.
         * AccessLevels listed must be in the same policy as this
         * ServicePerimeter. Referencing a nonexistent AccessLevel is a
         * syntax error. If no AccessLevel names are listed, resources within
         * the perimeter can only be accessed via GCP calls with request
         * origins within the perimeter. For Service Perimeter Bridge, must
         * be empty.
         * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
         */
        accessLevels?: string[];
        /**
         * List of EgressPolicies to apply to the perimeter. A perimeter may
         * have multiple EgressPolicies, each of which is evaluated separately.
         * Access is granted if any EgressPolicy grants it. Must be empty for
         * a perimeter bridge.
         * Structure is documented below.
         */
        egressPolicies?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusEgressPolicy[];
        /**
         * List of `IngressPolicies` to apply to the perimeter. A perimeter may
         * have multiple `IngressPolicies`, each of which is evaluated
         * separately. Access is granted if any `Ingress Policy` grants it.
         * Must be empty for a perimeter bridge.
         * Structure is documented below.
         */
        ingressPolicies?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusIngressPolicy[];
        /**
         * A list of GCP resources that are inside of the service perimeter.
         * Currently only projects are allowed.
         * Format: projects/{project_number}
         */
        resources?: string[];
        /**
         * GCP services that are subject to the Service Perimeter
         * restrictions. Must contain a list of services. For example, if
         * `storage.googleapis.com` is specified, access to the storage
         * buckets inside the perimeter must meet the perimeter's access
         * restrictions.
         */
        restrictedServices?: string[];
        /**
         * Specifies how APIs are allowed to communicate within the Service
         * Perimeter.
         * Structure is documented below.
         */
        vpcAccessibleServices?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusVpcAccessibleServices;
    }

    export interface ServicePerimetersServicePerimeterStatusEgressPolicy {
        /**
         * Defines conditions on the source of a request causing this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressFrom?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusEgressPolicyEgressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and destination resources that
         * cause this `EgressPolicy` to apply.
         * Structure is documented below.
         */
        egressTo?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusEgressPolicyEgressTo;
    }

    export interface ServicePerimetersServicePerimeterStatusEgressPolicyEgressFrom {
        /**
         * A list of identities that are allowed access through this `EgressPolicy`.
         * Should be in the format of email address. The email address should
         * represent individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access to outside the
         * perimeter. If left unspecified, then members of `identities` field will
         * be allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
    }

    export interface ServicePerimetersServicePerimeterStatusEgressPolicyEgressTo {
        /**
         * A list of external resources that are allowed to be accessed. A request
         * matches if it contains an external resource in this list (Example:
         * s3://bucket/path). Currently '*' is not allowed.
         */
        externalResources?: string[];
        /**
         * A list of `ApiOperations` that this egress rule applies to. A request matches
         * if it contains an operation/service in this list.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, that match this to stanza. A request matches
         * if it contains a resource in this list. If * is specified for resources,
         * then this `EgressTo` rule will authorize access to all resources outside
         * the perimeter.
         */
        resources?: string[];
    }

    export interface ServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimetersServicePerimeterStatusIngressPolicy {
        /**
         * Defines the conditions on the source of a request causing this `IngressPolicy`
         * to apply.
         * Structure is documented below.
         */
        ingressFrom?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusIngressPolicyIngressFrom;
        /**
         * Defines the conditions on the `ApiOperation` and request destination that cause
         * this `IngressPolicy` to apply.
         * Structure is documented below.
         */
        ingressTo?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusIngressPolicyIngressTo;
    }

    export interface ServicePerimetersServicePerimeterStatusIngressPolicyIngressFrom {
        /**
         * A list of identities that are allowed access through this ingress policy.
         * Should be in the format of email address. The email address should represent
         * individual user or service account only.
         */
        identities?: string[];
        /**
         * Specifies the type of identities that are allowed access from outside the
         * perimeter. If left unspecified, then members of `identities` field will be
         * allowed access.
         * Possible values are: `IDENTITY_TYPE_UNSPECIFIED`, `ANY_IDENTITY`, `ANY_USER_ACCOUNT`, `ANY_SERVICE_ACCOUNT`.
         */
        identityType?: string;
        /**
         * Sources that this `IngressPolicy` authorizes access from.
         * Structure is documented below.
         */
        sources?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusIngressPolicyIngressFromSource[];
    }

    export interface ServicePerimetersServicePerimeterStatusIngressPolicyIngressFromSource {
        /**
         * An `AccessLevel` resource name that allow resources within the
         * `ServicePerimeters` to be accessed from the internet. `AccessLevels` listed
         * must be in the same policy as this `ServicePerimeter`. Referencing a nonexistent
         * `AccessLevel` will cause an error. If no `AccessLevel` names are listed,
         * resources within the perimeter can only be accessed via Google Cloud calls
         * with request origins within the perimeter.
         * Example `accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.`
         * If * is specified, then all IngressSources will be allowed.
         */
        accessLevel?: string;
        /**
         * A Google Cloud resource that is allowed to ingress the perimeter.
         * Requests from these resources will be allowed to access perimeter data.
         * Currently only projects are allowed. Format `projects/{project_number}`
         * The project may be in any Google Cloud organization, not just the
         * organization that the perimeter is defined in. `*` is not allowed, the case
         * of allowing all Google Cloud resources only is not supported.
         */
        resource?: string;
    }

    export interface ServicePerimetersServicePerimeterStatusIngressPolicyIngressTo {
        /**
         * A list of `ApiOperations` the sources specified in corresponding `IngressFrom`
         * are allowed to perform in this `ServicePerimeter`.
         * Structure is documented below.
         */
        operations?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperation[];
        /**
         * A list of resources, currently only projects in the form
         * `projects/<projectnumber>`, protected by this `ServicePerimeter`
         * that are allowed to be accessed by sources defined in the
         * corresponding `IngressFrom`. A request matches if it contains
         * a resource in this list. If `*` is specified for resources,
         * then this `IngressTo` rule will authorize access to all
         * resources inside the perimeter, provided that the request
         * also matches the `operations` field.
         */
        resources?: string[];
    }

    export interface ServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperation {
        /**
         * API methods or permissions to allow. Method or permission must belong
         * to the service specified by `serviceName` field. A single MethodSelector
         * entry with `*` specified for the `method` field will allow all methods
         * AND permissions for the service specified in `serviceName`.
         * Structure is documented below.
         */
        methodSelectors?: outputs.accesscontextmanager.ServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector[];
        /**
         * The name of the API whose methods or permissions the `IngressPolicy` or
         * `EgressPolicy` want to allow. A single `ApiOperation` with serviceName
         * field set to `*` will allow all methods AND permissions for all services.
         */
        serviceName?: string;
    }

    export interface ServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector {
        /**
         * Value for `method` should be a valid method name for the corresponding
         * `serviceName` in `ApiOperation`. If `*` used as value for method,
         * then ALL methods and permissions are allowed.
         */
        method?: string;
        /**
         * Value for permission should be a valid Cloud IAM permission for the
         * corresponding `serviceName` in `ApiOperation`.
         */
        permission?: string;
    }

    export interface ServicePerimetersServicePerimeterStatusVpcAccessibleServices {
        /**
         * The list of APIs usable within the Service Perimeter.
         * Must be empty unless `enableRestriction` is True.
         */
        allowedServices?: string[];
        /**
         * Whether to restrict API calls within the Service Perimeter to the
         * list of APIs specified in 'allowedServices'.
         */
        enableRestriction?: boolean;
    }

}

export namespace alloydb {
    export interface BackupEncryptionConfig {
        /**
         * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
         */
        kmsKeyName?: string;
    }

    export interface BackupEncryptionInfo {
        /**
         * (Output)
         * Output only. Type of encryption.
         */
        encryptionType: string;
        /**
         * (Output)
         * Output only. Cloud KMS key versions that are being used to protect the database or the backup.
         */
        kmsKeyVersions: string[];
    }

    export interface ClusterAutomatedBackupPolicy {
        /**
         * The length of the time window during which a backup can be taken. If a backup does not succeed within this time window, it will be canceled and considered failed.
         * The backup window must be at least 5 minutes long. There is no upper bound on the window. If not set, it will default to 1 hour.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        backupWindow: string;
        /**
         * Whether automated backups are enabled.
         */
        enabled: boolean;
        /**
         * EncryptionConfig describes the encryption config of a cluster or a backup that is encrypted with a CMEK (customer-managed encryption key).
         * Structure is documented below.
         */
        encryptionConfig?: outputs.alloydb.ClusterAutomatedBackupPolicyEncryptionConfig;
        /**
         * Labels to apply to backups created using this configuration.
         */
        labels?: {[key: string]: string};
        /**
         * The location where the backup will be stored. Currently, the only supported option is to store the backup in the same region as the cluster.
         */
        location: string;
        /**
         * Quantity-based Backup retention policy to retain recent backups. Conflicts with 'time_based_retention', both can't be set together.
         * Structure is documented below.
         */
        quantityBasedRetention?: outputs.alloydb.ClusterAutomatedBackupPolicyQuantityBasedRetention;
        /**
         * Time-based Backup retention policy. Conflicts with 'quantity_based_retention', both can't be set together.
         * Structure is documented below.
         */
        timeBasedRetention?: outputs.alloydb.ClusterAutomatedBackupPolicyTimeBasedRetention;
        /**
         * Weekly schedule for the Backup.
         * Structure is documented below.
         */
        weeklySchedule: outputs.alloydb.ClusterAutomatedBackupPolicyWeeklySchedule;
    }

    export interface ClusterAutomatedBackupPolicyEncryptionConfig {
        /**
         * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
         */
        kmsKeyName?: string;
    }

    export interface ClusterAutomatedBackupPolicyQuantityBasedRetention {
        /**
         * The number of backups to retain.
         */
        count?: number;
    }

    export interface ClusterAutomatedBackupPolicyTimeBasedRetention {
        /**
         * The retention period.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        retentionPeriod?: string;
    }

    export interface ClusterAutomatedBackupPolicyWeeklySchedule {
        /**
         * The days of the week to perform a backup. At least one day of the week must be provided.
         * Each value may be one of: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        daysOfWeeks?: string[];
        /**
         * The times during the day to start a backup. At least one start time must be provided. The start times are assumed to be in UTC and to be an exact hour (e.g., 04:00:00).
         * Structure is documented below.
         */
        startTimes: outputs.alloydb.ClusterAutomatedBackupPolicyWeeklyScheduleStartTime[];
    }

    export interface ClusterAutomatedBackupPolicyWeeklyScheduleStartTime {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Currently, only the value 0 is supported.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Currently, only the value 0 is supported.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Currently, only the value 0 is supported.
         */
        seconds?: number;
    }

    export interface ClusterBackupSource {
        /**
         * The name of the backup that this cluster is restored from.
         */
        backupName?: string;
    }

    export interface ClusterContinuousBackupConfig {
        /**
         * Whether continuous backup recovery is enabled. If not set, defaults to true.
         */
        enabled?: boolean;
        /**
         * EncryptionConfig describes the encryption config of a cluster or a backup that is encrypted with a CMEK (customer-managed encryption key).
         * Structure is documented below.
         */
        encryptionConfig?: outputs.alloydb.ClusterContinuousBackupConfigEncryptionConfig;
        /**
         * The numbers of days that are eligible to restore from using PITR. To support the entire recovery window, backups and logs are retained for one day more than the recovery window.
         * If not set, defaults to 14 days.
         */
        recoveryWindowDays: number;
    }

    export interface ClusterContinuousBackupConfigEncryptionConfig {
        /**
         * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
         */
        kmsKeyName?: string;
    }

    export interface ClusterContinuousBackupInfo {
        /**
         * (Output)
         * The earliest restorable time that can be restored to. Output only field.
         */
        earliestRestorableTime: string;
        /**
         * (Output)
         * When ContinuousBackup was most recently enabled. Set to null if ContinuousBackup is not enabled.
         */
        enabledTime: string;
        /**
         * (Output)
         * Output only. The encryption information for the WALs and backups required for ContinuousBackup.
         * Structure is documented below.
         */
        encryptionInfos: outputs.alloydb.ClusterContinuousBackupInfoEncryptionInfo[];
        /**
         * (Output)
         * Days of the week on which a continuous backup is taken. Output only field. Ignored if passed into the request.
         */
        schedules: string[];
    }

    export interface ClusterContinuousBackupInfoEncryptionInfo {
        /**
         * (Output)
         * Output only. Type of encryption.
         */
        encryptionType: string;
        /**
         * (Output)
         * Output only. Cloud KMS key versions that are being used to protect the database or the backup.
         */
        kmsKeyVersions: string[];
    }

    export interface ClusterEncryptionConfig {
        /**
         * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
         */
        kmsKeyName?: string;
    }

    export interface ClusterEncryptionInfo {
        /**
         * (Output)
         * Output only. Type of encryption.
         */
        encryptionType: string;
        /**
         * (Output)
         * Output only. Cloud KMS key versions that are being used to protect the database or the backup.
         */
        kmsKeyVersions: string[];
    }

    export interface ClusterInitialUser {
        /**
         * The initial password for the user.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * The database username.
         */
        user?: string;
    }

    export interface ClusterMigrationSource {
        /**
         * The host and port of the on-premises instance in host:port format
         */
        hostPort?: string;
        /**
         * Place holder for the external source identifier(e.g DMS job name) that created the cluster.
         */
        referenceId?: string;
        /**
         * Type of migration source.
         */
        sourceType?: string;
    }

    export interface ClusterNetworkConfig {
        /**
         * The name of the allocated IP range for the private IP AlloyDB cluster. For example: "google-managed-services-default".
         * If set, the instance IPs for this cluster will be created in the allocated range.
         */
        allocatedIpRange?: string;
        /**
         * The resource link for the VPC network in which cluster resources are created and from which they are accessible via Private IP. The network must belong to the same project as the cluster.
         * It is specified in the form: "projects/{projectNumber}/global/networks/{network_id}".
         */
        network?: string;
    }

    export interface ClusterRestoreBackupSource {
        /**
         * The name of the backup that this cluster is restored from.
         */
        backupName: string;
    }

    export interface ClusterRestoreContinuousBackupSource {
        /**
         * The name of the source cluster that this cluster is restored from.
         */
        cluster: string;
        /**
         * The point in time that this cluster is restored to, in RFC 3339 format.
         */
        pointInTime: string;
    }

    export interface GetLocationsLocation {
        /**
         * The friendly name for this location, typically a nearby city name. For example, "Tokyo".
         */
        displayName: string;
        /**
         * Cross-service attributes for the location. For example `{"cloud.googleapis.com/region": "us-east1"}`.
         */
        labels: {[key: string]: string};
        /**
         * The canonical id for this location. For example: "us-east1"..
         */
        locationId: string;
        /**
         * Service-specific metadata. For example the available capacity at the given location.
         */
        metadata: {[key: string]: string};
        /**
         * Resource name for the location, which may vary between implementations. For example: "projects/example-project/locations/us-east1".
         */
        name: string;
    }

    export interface GetSupportedDatabaseFlagsSupportedDatabaseFlag {
        /**
         * Whether the database flag accepts multiple values. If true, a comma-separated list of stringified values may be specified.
         */
        acceptsMultipleValues: boolean;
        /**
         * The name of the database flag, e.g. "maxAllowedPackets". The is a possibly key for the Instance.database_flags map field.
         */
        flagName: string;
        /**
         * Restriction on `INTEGER` type value. Specifies the minimum value and the maximum value that can be specified, if applicable.
         */
        integerRestrictions: outputs.alloydb.GetSupportedDatabaseFlagsSupportedDatabaseFlagIntegerRestrictions;
        /**
         * The name of the flag resource, following Google Cloud conventions, e.g.: * projects/{project}/locations/{location}/flags/{flag} This field currently has no semantic meaning.
         */
        name: string;
        /**
         * Whether setting or updating this flag on an Instance requires a database restart. If a flag that requires database restart is set, the backend will automatically restart the database (making sure to satisfy any availability SLO's).
         */
        requiresDbRestart: boolean;
        /**
         * Restriction on `STRING` type value. The list of allowed values, if bounded. This field will be empty if there is a unbounded number of allowed values.
         */
        stringRestrictions: outputs.alloydb.GetSupportedDatabaseFlagsSupportedDatabaseFlagStringRestrictions;
        /**
         * Major database engine versions for which this flag is supported. The supported values are `POSTGRES_14` and `DATABASE_VERSION_UNSPECIFIED`.
         */
        supportedDbVersions: string[];
        /**
         * ValueType describes the semantic type of the value that the flag accepts. Regardless of the ValueType, the Instance.database_flags field accepts the stringified version of the value, i.e. "20" or "3.14". The supported values are `VALUE_TYPE_UNSPECIFIED`, `STRING`, `INTEGER`, `FLOAT` and `NONE`.
         */
        valueType: string;
    }

    export interface GetSupportedDatabaseFlagsSupportedDatabaseFlagIntegerRestrictions {
        maxValue: string;
        minValue: string;
    }

    export interface GetSupportedDatabaseFlagsSupportedDatabaseFlagStringRestrictions {
        allowedValues: string[];
    }

    export interface InstanceMachineConfig {
        /**
         * The number of CPU's in the VM instance.
         */
        cpuCount: number;
    }

    export interface InstanceReadPoolConfig {
        /**
         * Read capacity, i.e. number of nodes in a read pool instance.
         */
        nodeCount?: number;
    }

}

export namespace apigateway {
    export interface ApiConfigGatewayConfig {
        /**
         * Backend settings that are applied to all backends of the Gateway.
         * Structure is documented below.
         */
        backendConfig: outputs.apigateway.ApiConfigGatewayConfigBackendConfig;
    }

    export interface ApiConfigGatewayConfigBackendConfig {
        /**
         * Google Cloud IAM service account used to sign OIDC tokens for backends that have authentication configured
         * (https://cloud.google.com/service-infrastructure/docs/service-management/reference/rest/v1/services.configs#backend).
         */
        googleServiceAccount: string;
    }

    export interface ApiConfigGrpcService {
        /**
         * Input only. File descriptor set, generated by protoc.
         * To generate, use protoc with imports and source info included. For an example test.proto file, the following command would put the value in a new file named out.pb.
         * $ protoc --include_imports --include_source_info test.proto -o out.pb
         * Structure is documented below.
         */
        fileDescriptorSet: outputs.apigateway.ApiConfigGrpcServiceFileDescriptorSet;
        /**
         * Uncompiled proto files associated with the descriptor set, used for display purposes (server-side compilation is not supported). These should match the inputs to 'protoc' command used to generate fileDescriptorSet.
         * Structure is documented below.
         */
        sources?: outputs.apigateway.ApiConfigGrpcServiceSource[];
    }

    export interface ApiConfigGrpcServiceFileDescriptorSet {
        /**
         * Base64 encoded content of the file.
         */
        contents: string;
        /**
         * The file path (full or relative path). This is typically the path of the file when it is uploaded.
         */
        path: string;
    }

    export interface ApiConfigGrpcServiceSource {
        /**
         * Base64 encoded content of the file.
         */
        contents: string;
        /**
         * The file path (full or relative path). This is typically the path of the file when it is uploaded.
         */
        path: string;
    }

    export interface ApiConfigIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ApiConfigIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ApiConfigManagedServiceConfig {
        /**
         * Base64 encoded content of the file.
         */
        contents: string;
        /**
         * The file path (full or relative path). This is typically the path of the file when it is uploaded.
         */
        path: string;
    }

    export interface ApiConfigOpenapiDocument {
        /**
         * The OpenAPI Specification document file.
         * Structure is documented below.
         */
        document: outputs.apigateway.ApiConfigOpenapiDocumentDocument;
    }

    export interface ApiConfigOpenapiDocumentDocument {
        /**
         * Base64 encoded content of the file.
         */
        contents: string;
        /**
         * The file path (full or relative path). This is typically the path of the file when it is uploaded.
         */
        path: string;
    }

    export interface ApiIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ApiIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface GatewayIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface GatewayIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace apigee {
    export interface AddonsConfigAddonsConfig {
        /**
         * Configuration for the Monetization add-on.
         * Structure is documented below.
         */
        advancedApiOpsConfig?: outputs.apigee.AddonsConfigAddonsConfigAdvancedApiOpsConfig;
        /**
         * Configuration for the Monetization add-on.
         * Structure is documented below.
         */
        apiSecurityConfig?: outputs.apigee.AddonsConfigAddonsConfigApiSecurityConfig;
        /**
         * Configuration for the Monetization add-on.
         * Structure is documented below.
         */
        connectorsPlatformConfig?: outputs.apigee.AddonsConfigAddonsConfigConnectorsPlatformConfig;
        /**
         * Configuration for the Monetization add-on.
         * Structure is documented below.
         */
        integrationConfig?: outputs.apigee.AddonsConfigAddonsConfigIntegrationConfig;
        /**
         * Configuration for the Monetization add-on.
         * Structure is documented below.
         */
        monetizationConfig?: outputs.apigee.AddonsConfigAddonsConfigMonetizationConfig;
    }

    export interface AddonsConfigAddonsConfigAdvancedApiOpsConfig {
        /**
         * Flag that specifies whether the Advanced API Ops add-on is enabled.
         */
        enabled?: boolean;
    }

    export interface AddonsConfigAddonsConfigApiSecurityConfig {
        /**
         * Flag that specifies whether the Advanced API Ops add-on is enabled.
         */
        enabled?: boolean;
        /**
         * (Output)
         * Flag that specifies whether the Advanced API Ops add-on is enabled.
         */
        expiresAt: string;
    }

    export interface AddonsConfigAddonsConfigConnectorsPlatformConfig {
        /**
         * Flag that specifies whether the Advanced API Ops add-on is enabled.
         */
        enabled?: boolean;
        /**
         * (Output)
         * Flag that specifies whether the Advanced API Ops add-on is enabled.
         */
        expiresAt: string;
    }

    export interface AddonsConfigAddonsConfigIntegrationConfig {
        /**
         * Flag that specifies whether the Advanced API Ops add-on is enabled.
         */
        enabled?: boolean;
    }

    export interface AddonsConfigAddonsConfigMonetizationConfig {
        /**
         * Flag that specifies whether the Advanced API Ops add-on is enabled.
         */
        enabled?: boolean;
    }

    export interface EnvironmentIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface EnvironmentIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface EnvironmentNodeConfig {
        /**
         * (Output)
         * The current total number of gateway nodes that each environment currently has across
         * all instances.
         */
        currentAggregateNodeCount: string;
        /**
         * The maximum total number of gateway nodes that the is reserved for all instances that
         * has the specified environment. If not specified, the default is determined by the
         * recommended maximum number of nodes for that gateway.
         */
        maxNodeCount?: string;
        /**
         * The minimum total number of gateway nodes that the is reserved for all instances that
         * has the specified environment. If not specified, the default is determined by the
         * recommended minimum number of nodes for that gateway.
         */
        minNodeCount?: string;
    }

    export interface KeystoresAliasesKeyCertFileCertsInfo {
        /**
         * (Output)
         * List of all properties in the object.
         * Structure is documented below.
         */
        certInfos: outputs.apigee.KeystoresAliasesKeyCertFileCertsInfoCertInfo[];
    }

    export interface KeystoresAliasesKeyCertFileCertsInfoCertInfo {
        /**
         * (Output)
         * X.509 basic constraints extension.
         */
        basicConstraints: string;
        /**
         * (Output)
         * X.509 notAfter validity period in milliseconds since epoch.
         */
        expiryDate: string;
        /**
         * (Output)
         * Flag that specifies whether the certificate is valid.
         * Flag is set to Yes if the certificate is valid, No if expired, or Not yet if not yet valid.
         */
        isValid: string;
        /**
         * (Output)
         * X.509 issuer.
         */
        issuer: string;
        /**
         * (Output)
         * Public key component of the X.509 subject public key info.
         */
        publicKey: string;
        /**
         * (Output)
         * X.509 serial number.
         */
        serialNumber: string;
        /**
         * (Output)
         * X.509 signatureAlgorithm.
         */
        sigAlgName: string;
        /**
         * (Output)
         * X.509 subject.
         */
        subject: string;
        /**
         * (Output)
         * X.509 subject alternative names (SANs) extension.
         */
        subjectAlternativeNames: string[];
        /**
         * (Output)
         * X.509 notBefore validity period in milliseconds since epoch.
         */
        validFrom: string;
        /**
         * (Output)
         * X.509 version.
         */
        version: number;
    }

    export interface KeystoresAliasesPkcs12CertsInfo {
        /**
         * (Output)
         * List of all properties in the object.
         * Structure is documented below.
         */
        certInfos: outputs.apigee.KeystoresAliasesPkcs12CertsInfoCertInfo[];
    }

    export interface KeystoresAliasesPkcs12CertsInfoCertInfo {
        /**
         * (Output)
         * X.509 basic constraints extension.
         */
        basicConstraints: string;
        /**
         * (Output)
         * X.509 notAfter validity period in milliseconds since epoch.
         */
        expiryDate: string;
        /**
         * (Output)
         * Flag that specifies whether the certificate is valid.
         * Flag is set to Yes if the certificate is valid, No if expired, or Not yet if not yet valid.
         */
        isValid: string;
        /**
         * (Output)
         * X.509 issuer.
         */
        issuer: string;
        /**
         * (Output)
         * Public key component of the X.509 subject public key info.
         */
        publicKey: string;
        /**
         * (Output)
         * X.509 serial number.
         */
        serialNumber: string;
        /**
         * (Output)
         * X.509 signatureAlgorithm.
         */
        sigAlgName: string;
        /**
         * (Output)
         * X.509 subject.
         */
        subject: string;
        /**
         * (Output)
         * X.509 subject alternative names (SANs) extension.
         */
        subjectAlternativeNames: string[];
        /**
         * (Output)
         * X.509 notBefore validity period in milliseconds since epoch.
         */
        validFrom: string;
        /**
         * (Output)
         * X.509 version.
         */
        version: number;
    }

    export interface KeystoresAliasesSelfSignedCertCertsInfo {
        /**
         * (Output)
         * List of all properties in the object.
         * Structure is documented below.
         */
        certInfos: outputs.apigee.KeystoresAliasesSelfSignedCertCertsInfoCertInfo[];
    }

    export interface KeystoresAliasesSelfSignedCertCertsInfoCertInfo {
        /**
         * (Output)
         * X.509 basic constraints extension.
         */
        basicConstraints: string;
        /**
         * (Output)
         * X.509 notAfter validity period in milliseconds since epoch.
         */
        expiryDate: string;
        /**
         * (Output)
         * Flag that specifies whether the certificate is valid.
         * Flag is set to Yes if the certificate is valid, No if expired, or Not yet if not yet valid.
         */
        isValid: string;
        /**
         * (Output)
         * X.509 issuer.
         */
        issuer: string;
        /**
         * (Output)
         * Public key component of the X.509 subject public key info.
         */
        publicKey: string;
        /**
         * (Output)
         * X.509 serial number.
         */
        serialNumber: string;
        /**
         * (Output)
         * X.509 signatureAlgorithm.
         */
        sigAlgName: string;
        /**
         * Subject details.
         * Structure is documented below.
         */
        subject: string;
        /**
         * (Output)
         * X.509 subject alternative names (SANs) extension.
         */
        subjectAlternativeNames: string[];
        /**
         * (Output)
         * X.509 notBefore validity period in milliseconds since epoch.
         */
        validFrom: string;
        /**
         * (Output)
         * X.509 version.
         */
        version: number;
    }

    export interface KeystoresAliasesSelfSignedCertSubject {
        /**
         * Common name of the organization. Maximum length is 64 characters.
         */
        commonName?: string;
        /**
         * Two-letter country code. Example, IN for India, US for United States of America.
         */
        countryCode?: string;
        /**
         * Email address. Max 255 characters.
         *
         * - - -
         */
        email?: string;
        /**
         * City or town name. Maximum length is 128 characters.
         */
        locality?: string;
        /**
         * Organization name. Maximum length is 64 characters.
         */
        org?: string;
        /**
         * Organization team name. Maximum length is 64 characters.
         */
        orgUnit?: string;
        /**
         * State or district name. Maximum length is 128 characters.
         */
        state?: string;
    }

    export interface KeystoresAliasesSelfSignedCertSubjectAlternativeDnsNames {
        /**
         * Subject Alternative Name
         */
        subjectAlternativeName?: string;
    }

    export interface OrganizationProperties {
        /**
         * List of all properties in the object.
         * Structure is documented below.
         */
        properties?: outputs.apigee.OrganizationPropertiesProperty[];
    }

    export interface OrganizationPropertiesProperty {
        /**
         * Name of the property.
         */
        name?: string;
        /**
         * Value of the property.
         */
        value?: string;
    }

    export interface SharedflowMetaData {
        /**
         * Time at which the API proxy was created, in milliseconds since epoch.
         */
        createdAt: string;
        /**
         * Time at which the API proxy was most recently modified, in milliseconds since epoch.
         */
        lastModifiedAt: string;
        /**
         * The type of entity described
         */
        subType: string;
    }

    export interface TargetServerSSlInfo {
        /**
         * The SSL/TLS cipher suites to be used. For programmable proxies, it must be one of the cipher suite names listed in: http://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#ciphersuites. For configurable proxies, it must follow the configuration specified in: https://commondatastorage.googleapis.com/chromium-boringssl-docs/ssl.h.html#Cipher-suite-configuration. This setting has no effect for configurable proxies when negotiating TLS 1.3.
         */
        ciphers?: string[];
        /**
         * Enables two-way TLS.
         */
        clientAuthEnabled?: boolean;
        /**
         * The TLS Common Name of the certificate.
         * Structure is documented below.
         */
        commonName?: outputs.apigee.TargetServerSSlInfoCommonName;
        /**
         * Enables TLS. If false, neither one-way nor two-way TLS will be enabled.
         */
        enabled: boolean;
        /**
         * If true, Edge ignores TLS certificate errors. Valid when configuring TLS for target servers and target endpoints, and when configuring virtual hosts that use 2-way TLS. When used with a target endpoint/target server, if the backend system uses SNI and returns a cert with a subject Distinguished Name (DN) that does not match the hostname, there is no way to ignore the error and the connection fails.
         */
        ignoreValidationErrors?: boolean;
        /**
         * Required if clientAuthEnabled is true. The resource ID for the alias containing the private key and cert.
         */
        keyAlias?: string;
        /**
         * Required if clientAuthEnabled is true. The resource ID of the keystore.
         */
        keyStore?: string;
        /**
         * The TLS versioins to be used.
         */
        protocols?: string[];
        /**
         * The resource ID of the truststore.
         */
        trustStore?: string;
    }

    export interface TargetServerSSlInfoCommonName {
        /**
         * The TLS Common Name string of the certificate.
         */
        value?: string;
        /**
         * Indicates whether the cert should be matched against as a wildcard cert.
         */
        wildcardMatch?: boolean;
    }

}

export namespace appengine {
    export interface ApplicationFeatureSettings {
        /**
         * Set to false to use the legacy health check instead of the readiness
         * and liveness checks.
         */
        splitHealthChecks: boolean;
    }

    export interface ApplicationIap {
        /**
         * (Optional) Whether the serving infrastructure will authenticate and authorize all incoming requests. 
         * (default is false)
         */
        enabled?: boolean;
        /**
         * OAuth2 client ID to use for the authentication flow.
         */
        oauth2ClientId: string;
        /**
         * OAuth2 client secret to use for the authentication flow.
         * The SHA-256 hash of the value is returned in the oauth2ClientSecretSha256 field.
         */
        oauth2ClientSecret: string;
        /**
         * Hex-encoded SHA-256 hash of the client secret.
         */
        oauth2ClientSecretSha256: string;
    }

    export interface ApplicationUrlDispatchRule {
        domain: string;
        path: string;
        service: string;
    }

    export interface ApplicationUrlDispatchRulesDispatchRule {
        /**
         * Domain name to match against. The wildcard "*" is supported if specified before a period: "*.".
         * Defaults to matching all domains: "*".
         */
        domain?: string;
        /**
         * Pathname within the host. Must start with a "/". A single "*" can be included at the end of the path.
         * The sum of the lengths of the domain and path may not exceed 100 characters.
         */
        path: string;
        /**
         * Pathname within the host. Must start with a "/". A single "*" can be included at the end of the path.
         * The sum of the lengths of the domain and path may not exceed 100 characters.
         *
         * - - -
         */
        service: string;
    }

    export interface DomainMappingResourceRecord {
        /**
         * Relative name of the object affected by this record. Only applicable for CNAME records. Example: 'www'.
         */
        name?: string;
        /**
         * Data for this record. Values vary by record type, as defined in RFC 1035 (section 5) and RFC 1034 (section 3.6.1).
         */
        rrdata?: string;
        /**
         * Resource record type. Example: `AAAA`.
         * Possible values are: `A`, `AAAA`, `CNAME`.
         */
        type?: string;
    }

    export interface DomainMappingSslSettings {
        /**
         * ID of the AuthorizedCertificate resource configuring SSL for the application. Clearing this field will
         * remove SSL support.
         * By default, a managed certificate is automatically created for every domain mapping. To omit SSL support
         * or to configure SSL manually, specify `SslManagementType.MANUAL` on a `CREATE` or `UPDATE` request. You must be
         * authorized to administer the `AuthorizedCertificate` resource to manually map it to a DomainMapping resource.
         * Example: 12345.
         */
        certificateId: string;
        /**
         * (Output)
         * ID of the managed `AuthorizedCertificate` resource currently being provisioned, if applicable. Until the new
         * managed certificate has been successfully provisioned, the previous SSL state will be preserved. Once the
         * provisioning process completes, the `certificateId` field will reflect the new managed certificate and this
         * field will be left empty. To remove SSL support while there is still a pending managed certificate, clear the
         * `certificateId` field with an update request.
         */
        pendingManagedCertificateId: string;
        /**
         * SSL management type for this domain. If `AUTOMATIC`, a managed certificate is automatically provisioned.
         * If `MANUAL`, `certificateId` must be manually specified in order to configure SSL for this domain.
         * Possible values are: `AUTOMATIC`, `MANUAL`.
         */
        sslManagementType: string;
    }

    export interface EngineSplitTrafficSplit {
        /**
         * Mapping from version IDs within the service to fractional (0.000, 1] allocations of traffic for that version. Each version can be specified only once, but some versions in the service may not have any traffic allocation. Services that have traffic allocated cannot be deleted until either the service is deleted or their traffic allocation is removed. Allocations must sum to 1. Up to two decimal place precision is supported for IP-based splits and up to three decimal places is supported for cookie-based splits.
         *
         * - - -
         */
        allocations: {[key: string]: string};
        /**
         * Mechanism used to determine which version a request is sent to. The traffic selection algorithm will be stable for either type until allocations are changed.
         * Possible values are: `UNSPECIFIED`, `COOKIE`, `IP`, `RANDOM`.
         */
        shardBy?: string;
    }

    export interface FlexibleAppVersionApiConfig {
        /**
         * Action to take when users access resources that require authentication.
         * Default value is `AUTH_FAIL_ACTION_REDIRECT`.
         * Possible values are: `AUTH_FAIL_ACTION_REDIRECT`, `AUTH_FAIL_ACTION_UNAUTHORIZED`.
         */
        authFailAction?: string;
        /**
         * Level of login required to access this resource.
         * Default value is `LOGIN_OPTIONAL`.
         * Possible values are: `LOGIN_OPTIONAL`, `LOGIN_ADMIN`, `LOGIN_REQUIRED`.
         */
        login?: string;
        /**
         * Path to the script from the application root directory.
         */
        script: string;
        /**
         * Security (HTTPS) enforcement for this URL.
         * Possible values are: `SECURE_DEFAULT`, `SECURE_NEVER`, `SECURE_OPTIONAL`, `SECURE_ALWAYS`.
         */
        securityLevel?: string;
        /**
         * URL to serve the endpoint at.
         */
        url?: string;
    }

    export interface FlexibleAppVersionAutomaticScaling {
        /**
         * The time period that the Autoscaler should wait before it starts collecting information from a new instance.
         * This prevents the autoscaler from collecting information when the instance is initializing,
         * during which the collected usage would not be reliable. Default: 120s
         */
        coolDownPeriod?: string;
        /**
         * Target scaling by CPU usage.
         * Structure is documented below.
         */
        cpuUtilization: outputs.appengine.FlexibleAppVersionAutomaticScalingCpuUtilization;
        /**
         * Target scaling by disk usage.
         * Structure is documented below.
         */
        diskUtilization?: outputs.appengine.FlexibleAppVersionAutomaticScalingDiskUtilization;
        /**
         * Number of concurrent requests an automatic scaling instance can accept before the scheduler spawns a new instance.
         * Defaults to a runtime-specific value.
         */
        maxConcurrentRequests: number;
        /**
         * Maximum number of idle instances that should be maintained for this version.
         */
        maxIdleInstances?: number;
        /**
         * Maximum amount of time that a request should wait in the pending queue before starting a new instance to handle it.
         */
        maxPendingLatency?: string;
        /**
         * Maximum number of instances that should be started to handle requests for this version. Default: 20
         */
        maxTotalInstances?: number;
        /**
         * Minimum number of idle instances that should be maintained for this version. Only applicable for the default version of a service.
         */
        minIdleInstances?: number;
        /**
         * Minimum amount of time a request should wait in the pending queue before starting a new instance to handle it.
         */
        minPendingLatency?: string;
        /**
         * Minimum number of running instances that should be maintained for this version. Default: 2
         */
        minTotalInstances?: number;
        /**
         * Target scaling by network usage.
         * Structure is documented below.
         */
        networkUtilization?: outputs.appengine.FlexibleAppVersionAutomaticScalingNetworkUtilization;
        /**
         * Target scaling by request utilization.
         * Structure is documented below.
         */
        requestUtilization?: outputs.appengine.FlexibleAppVersionAutomaticScalingRequestUtilization;
    }

    export interface FlexibleAppVersionAutomaticScalingCpuUtilization {
        /**
         * Period of time over which CPU utilization is calculated.
         */
        aggregationWindowLength?: string;
        /**
         * Target CPU utilization ratio to maintain when scaling. Must be between 0 and 1.
         */
        targetUtilization: number;
    }

    export interface FlexibleAppVersionAutomaticScalingDiskUtilization {
        /**
         * Target bytes read per second.
         */
        targetReadBytesPerSecond?: number;
        /**
         * Target ops read per seconds.
         */
        targetReadOpsPerSecond?: number;
        /**
         * Target bytes written per second.
         */
        targetWriteBytesPerSecond?: number;
        /**
         * Target ops written per second.
         */
        targetWriteOpsPerSecond?: number;
    }

    export interface FlexibleAppVersionAutomaticScalingNetworkUtilization {
        /**
         * Target bytes received per second.
         */
        targetReceivedBytesPerSecond?: number;
        /**
         * Target packets received per second.
         */
        targetReceivedPacketsPerSecond?: number;
        /**
         * Target bytes sent per second.
         */
        targetSentBytesPerSecond?: number;
        /**
         * Target packets sent per second.
         */
        targetSentPacketsPerSecond?: number;
    }

    export interface FlexibleAppVersionAutomaticScalingRequestUtilization {
        /**
         * Target number of concurrent requests.
         */
        targetConcurrentRequests?: number;
        /**
         * Target requests per second.
         */
        targetRequestCountPerSecond?: string;
    }

    export interface FlexibleAppVersionDeployment {
        /**
         * Options for the build operations performed as a part of the version deployment. Only applicable when creating a version using source code directly.
         * Structure is documented below.
         */
        cloudBuildOptions?: outputs.appengine.FlexibleAppVersionDeploymentCloudBuildOptions;
        /**
         * The Docker image for the container that runs the version.
         * Structure is documented below.
         */
        container: outputs.appengine.FlexibleAppVersionDeploymentContainer;
        /**
         * Manifest of the files stored in Google Cloud Storage that are included as part of this version.
         * All files must be readable using the credentials supplied with this call.
         * Structure is documented below.
         */
        files?: outputs.appengine.FlexibleAppVersionDeploymentFile[];
        /**
         * Zip File
         * Structure is documented below.
         */
        zip?: outputs.appengine.FlexibleAppVersionDeploymentZip;
    }

    export interface FlexibleAppVersionDeploymentCloudBuildOptions {
        /**
         * Path to the yaml file used in deployment, used to determine runtime configuration details.
         */
        appYamlPath: string;
        /**
         * The Cloud Build timeout used as part of any dependent builds performed by version creation. Defaults to 10 minutes.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        cloudBuildTimeout?: string;
    }

    export interface FlexibleAppVersionDeploymentContainer {
        /**
         * URI to the hosted container image in Google Container Registry. The URI must be fully qualified and include a tag or digest.
         * Examples: "gcr.io/my-project/image:tag" or "gcr.io/my-project/image@digest"
         */
        image: string;
    }

    export interface FlexibleAppVersionDeploymentFile {
        /**
         * The identifier for this object. Format specified above.
         */
        name: string;
        /**
         * SHA1 checksum of the file
         */
        sha1Sum?: string;
        /**
         * Source URL
         */
        sourceUrl: string;
    }

    export interface FlexibleAppVersionDeploymentZip {
        /**
         * files count
         */
        filesCount?: number;
        /**
         * Source URL
         */
        sourceUrl: string;
    }

    export interface FlexibleAppVersionEndpointsApiService {
        /**
         * Endpoints service configuration ID as specified by the Service Management API. For example "2016-09-19r1".
         * By default, the rollout strategy for Endpoints is "FIXED". This means that Endpoints starts up with a particular configuration ID.
         * When a new configuration is rolled out, Endpoints must be given the new configuration ID. The configId field is used to give the configuration ID
         * and is required in this case.
         * Endpoints also has a rollout strategy called "MANAGED". When using this, Endpoints fetches the latest configuration and does not need
         * the configuration ID. In this case, configId must be omitted.
         */
        configId?: string;
        /**
         * Enable or disable trace sampling. By default, this is set to false for enabled.
         */
        disableTraceSampling?: boolean;
        /**
         * Endpoints service name which is the name of the "service" resource in the Service Management API.
         * For example "myapi.endpoints.myproject.cloud.goog"
         */
        name: string;
        /**
         * Endpoints rollout strategy. If FIXED, configId must be specified. If MANAGED, configId must be omitted.
         * Default value is `FIXED`.
         * Possible values are: `FIXED`, `MANAGED`.
         */
        rolloutStrategy?: string;
    }

    export interface FlexibleAppVersionEntrypoint {
        /**
         * The format should be a shell command that can be fed to bash -c.
         */
        shell: string;
    }

    export interface FlexibleAppVersionHandler {
        /**
         * Actions to take when the user is not logged in.
         * Possible values are: `AUTH_FAIL_ACTION_REDIRECT`, `AUTH_FAIL_ACTION_UNAUTHORIZED`.
         */
        authFailAction?: string;
        /**
         * Methods to restrict access to a URL based on login status.
         * Possible values are: `LOGIN_OPTIONAL`, `LOGIN_ADMIN`, `LOGIN_REQUIRED`.
         */
        login?: string;
        /**
         * 30x code to use when performing redirects for the secure field.
         * Possible values are: `REDIRECT_HTTP_RESPONSE_CODE_301`, `REDIRECT_HTTP_RESPONSE_CODE_302`, `REDIRECT_HTTP_RESPONSE_CODE_303`, `REDIRECT_HTTP_RESPONSE_CODE_307`.
         */
        redirectHttpResponseCode?: string;
        /**
         * Executes a script to handle the requests that match this URL pattern.
         * Only the auto value is supported for Node.js in the App Engine standard environment, for example "script:" "auto".
         * Structure is documented below.
         */
        script?: outputs.appengine.FlexibleAppVersionHandlerScript;
        /**
         * Security (HTTPS) enforcement for this URL.
         * Possible values are: `SECURE_DEFAULT`, `SECURE_NEVER`, `SECURE_OPTIONAL`, `SECURE_ALWAYS`.
         */
        securityLevel?: string;
        /**
         * Files served directly to the user for a given URL, such as images, CSS stylesheets, or JavaScript source files.
         * Static file handlers describe which files in the application directory are static files, and which URLs serve them.
         * Structure is documented below.
         */
        staticFiles?: outputs.appengine.FlexibleAppVersionHandlerStaticFiles;
        /**
         * URL prefix. Uses regular expression syntax, which means regexp special characters must be escaped, but should not contain groupings.
         * All URLs that begin with this prefix are handled by this handler, using the portion of the URL after the prefix as part of the file path.
         */
        urlRegex?: string;
    }

    export interface FlexibleAppVersionHandlerScript {
        /**
         * Path to the script from the application root directory.
         */
        scriptPath: string;
    }

    export interface FlexibleAppVersionHandlerStaticFiles {
        /**
         * Whether files should also be uploaded as code data. By default, files declared in static file handlers are
         * uploaded as static data and are only served to end users; they cannot be read by the application. If enabled,
         * uploads are charged against both your code and static data storage resource quotas.
         */
        applicationReadable?: boolean;
        /**
         * Time a static file served by this handler should be cached by web proxies and browsers.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example "3.5s".
         * Default is '0s'
         */
        expiration?: string;
        /**
         * HTTP headers to use for all responses from these URLs.
         * An object containing a list of "key:value" value pairs.".
         */
        httpHeaders?: {[key: string]: string};
        /**
         * MIME type used to serve all files served by this handler.
         * Defaults to file-specific MIME types, which are derived from each file's filename extension.
         */
        mimeType?: string;
        /**
         * Path to the static files matched by the URL pattern, from the application root directory.
         * The path can refer to text matched in groupings in the URL pattern.
         */
        path?: string;
        /**
         * Whether this handler should match the request if the file referenced by the handler does not exist.
         */
        requireMatchingFile?: boolean;
        /**
         * Regular expression that matches the file paths for all files that should be referenced by this handler.
         */
        uploadPathRegex?: string;
    }

    export interface FlexibleAppVersionLivenessCheck {
        /**
         * Interval between health checks.
         */
        checkInterval?: string;
        /**
         * Number of consecutive failed checks required before considering the VM unhealthy. Default: 4.
         */
        failureThreshold?: number;
        /**
         * Host header to send when performing a HTTP Readiness check. Example: "myapp.appspot.com"
         */
        host?: string;
        /**
         * The initial delay before starting to execute the checks. Default: "300s"
         *
         * - - -
         */
        initialDelay?: string;
        /**
         * The request path.
         */
        path: string;
        /**
         * Number of consecutive successful checks required before considering the VM healthy. Default: 2.
         */
        successThreshold?: number;
        /**
         * Time before the check is considered failed. Default: "4s"
         */
        timeout?: string;
    }

    export interface FlexibleAppVersionManualScaling {
        /**
         * Number of instances to assign to the service at the start.
         * **Note:** When managing the number of instances at runtime through the App Engine Admin API or the (now deprecated) Python 2
         * Modules API set_num_instances() you must use `lifecycle.ignore_changes = ["manualScaling"[0].instances]` to prevent drift detection.
         */
        instances: number;
    }

    export interface FlexibleAppVersionNetwork {
        /**
         * List of ports, or port pairs, to forward from the virtual machine to the application container.
         */
        forwardedPorts?: string[];
        /**
         * Tag to apply to the instance during creation.
         */
        instanceTag?: string;
        /**
         * Google Compute Engine network where the virtual machines are created. Specify the short name, not the resource path.
         */
        name: string;
        /**
         * Enable session affinity.
         */
        sessionAffinity?: boolean;
        /**
         * Google Cloud Platform sub-network where the virtual machines are created. Specify the short name, not the resource path.
         * If the network that the instance is being created in is a Legacy network, then the IP address is allocated from the IPv4Range.
         * If the network that the instance is being created in is an auto Subnet Mode Network, then only network name should be specified (not the subnetworkName) and the IP address is created from the IPCidrRange of the subnetwork that exists in that zone for that network.
         * If the network that the instance is being created in is a custom Subnet Mode Network, then the subnetworkName must be specified and the IP address is created from the IPCidrRange of the subnetwork.
         * If specified, the subnetwork must exist in the same region as the App Engine flexible environment application.
         */
        subnetwork?: string;
    }

    export interface FlexibleAppVersionReadinessCheck {
        /**
         * A maximum time limit on application initialization, measured from moment the application successfully
         * replies to a healthcheck until it is ready to serve traffic. Default: "300s"
         */
        appStartTimeout?: string;
        /**
         * Interval between health checks.  Default: "5s".
         */
        checkInterval?: string;
        /**
         * Number of consecutive failed checks required before removing traffic. Default: 2.
         */
        failureThreshold?: number;
        /**
         * Host header to send when performing a HTTP Readiness check. Example: "myapp.appspot.com"
         */
        host?: string;
        /**
         * The request path.
         */
        path: string;
        /**
         * Number of consecutive successful checks required before receiving traffic. Default: 2.
         */
        successThreshold?: number;
        /**
         * Time before the check is considered failed. Default: "4s"
         */
        timeout?: string;
    }

    export interface FlexibleAppVersionResources {
        /**
         * Number of CPU cores needed.
         */
        cpu?: number;
        /**
         * Disk size (GB) needed.
         */
        diskGb?: number;
        /**
         * Memory (GB) needed.
         */
        memoryGb?: number;
        /**
         * List of ports, or port pairs, to forward from the virtual machine to the application container.
         * Structure is documented below.
         */
        volumes?: outputs.appengine.FlexibleAppVersionResourcesVolume[];
    }

    export interface FlexibleAppVersionResourcesVolume {
        /**
         * Unique name for the volume.
         */
        name: string;
        /**
         * Volume size in gigabytes.
         */
        sizeGb: number;
        /**
         * Underlying volume type, e.g. 'tmpfs'.
         */
        volumeType: string;
    }

    export interface FlexibleAppVersionVpcAccessConnector {
        /**
         * Full Serverless VPC Access Connector name e.g. /projects/my-project/locations/us-central1/connectors/c1.
         */
        name: string;
    }

    export interface ServiceNetworkSettingsNetworkSettings {
        /**
         * The ingress settings for version or service.
         * Default value is `INGRESS_TRAFFIC_ALLOWED_UNSPECIFIED`.
         * Possible values are: `INGRESS_TRAFFIC_ALLOWED_UNSPECIFIED`, `INGRESS_TRAFFIC_ALLOWED_ALL`, `INGRESS_TRAFFIC_ALLOWED_INTERNAL_ONLY`, `INGRESS_TRAFFIC_ALLOWED_INTERNAL_AND_LB`.
         *
         * - - -
         */
        ingressTrafficAllowed?: string;
    }

    export interface StandardAppVersionAutomaticScaling {
        /**
         * Number of concurrent requests an automatic scaling instance can accept before the scheduler spawns a new instance.
         * Defaults to a runtime-specific value.
         */
        maxConcurrentRequests?: number;
        /**
         * Maximum number of idle instances that should be maintained for this version.
         */
        maxIdleInstances?: number;
        /**
         * Maximum amount of time that a request should wait in the pending queue before starting a new instance to handle it.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        maxPendingLatency?: string;
        /**
         * Minimum number of idle instances that should be maintained for this version. Only applicable for the default version of a service.
         */
        minIdleInstances?: number;
        /**
         * Minimum amount of time a request should wait in the pending queue before starting a new instance to handle it.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        minPendingLatency?: string;
        /**
         * Scheduler settings for standard environment.
         * Structure is documented below.
         */
        standardSchedulerSettings?: outputs.appengine.StandardAppVersionAutomaticScalingStandardSchedulerSettings;
    }

    export interface StandardAppVersionAutomaticScalingStandardSchedulerSettings {
        /**
         * Maximum number of instances to run for this version. Set to zero to disable maxInstances configuration.
         */
        maxInstances?: number;
        /**
         * Minimum number of instances to run for this version. Set to zero to disable minInstances configuration.
         */
        minInstances?: number;
        /**
         * Target CPU utilization ratio to maintain when scaling. Should be a value in the range [0.50, 0.95], zero, or a negative value.
         */
        targetCpuUtilization?: number;
        /**
         * Target throughput utilization ratio to maintain when scaling. Should be a value in the range [0.50, 0.95], zero, or a negative value.
         */
        targetThroughputUtilization?: number;
    }

    export interface StandardAppVersionBasicScaling {
        /**
         * Duration of time after the last request that an instance must wait before the instance is shut down.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.
         */
        idleTimeout?: string;
        /**
         * Maximum number of instances to create for this version. Must be in the range [1.0, 200.0].
         */
        maxInstances: number;
    }

    export interface StandardAppVersionDeployment {
        /**
         * Manifest of the files stored in Google Cloud Storage that are included as part of this version.
         * All files must be readable using the credentials supplied with this call.
         * Structure is documented below.
         */
        files?: outputs.appengine.StandardAppVersionDeploymentFile[];
        /**
         * Zip File
         * Structure is documented below.
         */
        zip?: outputs.appengine.StandardAppVersionDeploymentZip;
    }

    export interface StandardAppVersionDeploymentFile {
        /**
         * The identifier for this object. Format specified above.
         */
        name: string;
        /**
         * SHA1 checksum of the file
         */
        sha1Sum?: string;
        /**
         * Source URL
         */
        sourceUrl: string;
    }

    export interface StandardAppVersionDeploymentZip {
        /**
         * files count
         */
        filesCount?: number;
        /**
         * Source URL
         */
        sourceUrl: string;
    }

    export interface StandardAppVersionEntrypoint {
        /**
         * The format should be a shell command that can be fed to bash -c.
         *
         * - - -
         */
        shell: string;
    }

    export interface StandardAppVersionHandler {
        /**
         * Actions to take when the user is not logged in.
         * Possible values are: `AUTH_FAIL_ACTION_REDIRECT`, `AUTH_FAIL_ACTION_UNAUTHORIZED`.
         */
        authFailAction?: string;
        /**
         * Methods to restrict access to a URL based on login status.
         * Possible values are: `LOGIN_OPTIONAL`, `LOGIN_ADMIN`, `LOGIN_REQUIRED`.
         */
        login?: string;
        /**
         * 30x code to use when performing redirects for the secure field.
         * Possible values are: `REDIRECT_HTTP_RESPONSE_CODE_301`, `REDIRECT_HTTP_RESPONSE_CODE_302`, `REDIRECT_HTTP_RESPONSE_CODE_303`, `REDIRECT_HTTP_RESPONSE_CODE_307`.
         */
        redirectHttpResponseCode?: string;
        /**
         * Executes a script to handle the requests that match this URL pattern.
         * Only the auto value is supported for Node.js in the App Engine standard environment, for example "script:" "auto".
         * Structure is documented below.
         */
        script?: outputs.appengine.StandardAppVersionHandlerScript;
        /**
         * Security (HTTPS) enforcement for this URL.
         * Possible values are: `SECURE_DEFAULT`, `SECURE_NEVER`, `SECURE_OPTIONAL`, `SECURE_ALWAYS`.
         */
        securityLevel?: string;
        /**
         * Files served directly to the user for a given URL, such as images, CSS stylesheets, or JavaScript source files. Static file handlers describe which files in the application directory are static files, and which URLs serve them.
         * Structure is documented below.
         */
        staticFiles?: outputs.appengine.StandardAppVersionHandlerStaticFiles;
        /**
         * URL prefix. Uses regular expression syntax, which means regexp special characters must be escaped, but should not contain groupings.
         * All URLs that begin with this prefix are handled by this handler, using the portion of the URL after the prefix as part of the file path.
         */
        urlRegex?: string;
    }

    export interface StandardAppVersionHandlerScript {
        /**
         * Path to the script from the application root directory.
         */
        scriptPath: string;
    }

    export interface StandardAppVersionHandlerStaticFiles {
        /**
         * Whether files should also be uploaded as code data. By default, files declared in static file handlers are uploaded as
         * static data and are only served to end users; they cannot be read by the application. If enabled, uploads are charged
         * against both your code and static data storage resource quotas.
         */
        applicationReadable?: boolean;
        /**
         * Time a static file served by this handler should be cached by web proxies and browsers.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example "3.5s".
         */
        expiration?: string;
        /**
         * HTTP headers to use for all responses from these URLs.
         * An object containing a list of "key:value" value pairs.".
         */
        httpHeaders?: {[key: string]: string};
        /**
         * MIME type used to serve all files served by this handler.
         * Defaults to file-specific MIME types, which are derived from each file's filename extension.
         */
        mimeType?: string;
        /**
         * Path to the static files matched by the URL pattern, from the application root directory. The path can refer to text matched in groupings in the URL pattern.
         */
        path?: string;
        /**
         * Whether this handler should match the request if the file referenced by the handler does not exist.
         */
        requireMatchingFile?: boolean;
        /**
         * Regular expression that matches the file paths for all files that should be referenced by this handler.
         */
        uploadPathRegex?: string;
    }

    export interface StandardAppVersionLibrary {
        /**
         * Name of the library. Example "django".
         */
        name?: string;
        /**
         * Version of the library to select, or "latest".
         */
        version?: string;
    }

    export interface StandardAppVersionManualScaling {
        /**
         * Number of instances to assign to the service at the start.
         * **Note:** When managing the number of instances at runtime through the App Engine Admin API or the (now deprecated) Python 2
         * Modules API set_num_instances() you must use `lifecycle.ignore_changes = ["manualScaling"[0].instances]` to prevent drift detection.
         */
        instances: number;
    }

    export interface StandardAppVersionVpcAccessConnector {
        /**
         * The egress setting for the connector, controlling what traffic is diverted through it.
         */
        egressSetting?: string;
        /**
         * Full Serverless VPC Access Connector name e.g. /projects/my-project/locations/us-central1/connectors/c1.
         */
        name: string;
    }

}

export namespace artifactregistry {
    export interface GetRepositoryCleanupPolicy {
        action: string;
        conditions: outputs.artifactregistry.GetRepositoryCleanupPolicyCondition[];
        id: string;
        mostRecentVersions: outputs.artifactregistry.GetRepositoryCleanupPolicyMostRecentVersion[];
    }

    export interface GetRepositoryCleanupPolicyCondition {
        newerThan: string;
        olderThan: string;
        packageNamePrefixes: string[];
        tagPrefixes: string[];
        tagState: string;
        versionNamePrefixes: string[];
    }

    export interface GetRepositoryCleanupPolicyMostRecentVersion {
        keepCount: number;
        packageNamePrefixes: string[];
    }

    export interface GetRepositoryDockerConfig {
        immutableTags: boolean;
    }

    export interface GetRepositoryMavenConfig {
        allowSnapshotOverwrites: boolean;
        versionPolicy: string;
    }

    export interface GetRepositoryRemoteRepositoryConfig {
        description: string;
        dockerRepositories: outputs.artifactregistry.GetRepositoryRemoteRepositoryConfigDockerRepository[];
        mavenRepositories: outputs.artifactregistry.GetRepositoryRemoteRepositoryConfigMavenRepository[];
        npmRepositories: outputs.artifactregistry.GetRepositoryRemoteRepositoryConfigNpmRepository[];
        pythonRepositories: outputs.artifactregistry.GetRepositoryRemoteRepositoryConfigPythonRepository[];
    }

    export interface GetRepositoryRemoteRepositoryConfigDockerRepository {
        publicRepository: string;
    }

    export interface GetRepositoryRemoteRepositoryConfigMavenRepository {
        publicRepository: string;
    }

    export interface GetRepositoryRemoteRepositoryConfigNpmRepository {
        publicRepository: string;
    }

    export interface GetRepositoryRemoteRepositoryConfigPythonRepository {
        publicRepository: string;
    }

    export interface GetRepositoryVirtualRepositoryConfig {
        upstreamPolicies: outputs.artifactregistry.GetRepositoryVirtualRepositoryConfigUpstreamPolicy[];
    }

    export interface GetRepositoryVirtualRepositoryConfigUpstreamPolicy {
        id: string;
        priority: number;
        repository: string;
    }

    export interface RepositoryCleanupPolicy {
        action?: string;
        condition?: outputs.artifactregistry.RepositoryCleanupPolicyCondition;
        /**
         * The identifier for this object. Format specified above.
         */
        id: string;
        mostRecentVersions?: outputs.artifactregistry.RepositoryCleanupPolicyMostRecentVersions;
    }

    export interface RepositoryCleanupPolicyCondition {
        newerThan?: string;
        olderThan?: string;
        packageNamePrefixes?: string[];
        tagPrefixes?: string[];
        tagState?: string;
        versionNamePrefixes?: string[];
    }

    export interface RepositoryCleanupPolicyMostRecentVersions {
        keepCount?: number;
        packageNamePrefixes?: string[];
    }

    export interface RepositoryDockerConfig {
        /**
         * The repository which enabled this flag prevents all tags from being modified, moved or deleted. This does not prevent tags from being created.
         */
        immutableTags?: boolean;
    }

    export interface RepositoryIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RepositoryIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RepositoryMavenConfig {
        /**
         * The repository with this flag will allow publishing the same
         * snapshot versions.
         */
        allowSnapshotOverwrites?: boolean;
        /**
         * Version policy defines the versions that the registry will accept.
         * Default value is `VERSION_POLICY_UNSPECIFIED`.
         * Possible values are: `VERSION_POLICY_UNSPECIFIED`, `RELEASE`, `SNAPSHOT`.
         */
        versionPolicy?: string;
    }

    export interface RepositoryRemoteRepositoryConfig {
        /**
         * The description of the remote source.
         */
        description?: string;
        /**
         * Specific settings for a Docker remote repository.
         * Structure is documented below.
         */
        dockerRepository?: outputs.artifactregistry.RepositoryRemoteRepositoryConfigDockerRepository;
        /**
         * Specific settings for a Maven remote repository.
         * Structure is documented below.
         */
        mavenRepository?: outputs.artifactregistry.RepositoryRemoteRepositoryConfigMavenRepository;
        /**
         * Specific settings for an Npm remote repository.
         * Structure is documented below.
         */
        npmRepository?: outputs.artifactregistry.RepositoryRemoteRepositoryConfigNpmRepository;
        /**
         * Specific settings for a Python remote repository.
         * Structure is documented below.
         */
        pythonRepository?: outputs.artifactregistry.RepositoryRemoteRepositoryConfigPythonRepository;
    }

    export interface RepositoryRemoteRepositoryConfigDockerRepository {
        /**
         * Address of the remote repository.
         * Default value is `DOCKER_HUB`.
         * Possible values are: `DOCKER_HUB`.
         */
        publicRepository?: string;
    }

    export interface RepositoryRemoteRepositoryConfigMavenRepository {
        /**
         * Address of the remote repository.
         * Default value is `MAVEN_CENTRAL`.
         * Possible values are: `MAVEN_CENTRAL`.
         */
        publicRepository?: string;
    }

    export interface RepositoryRemoteRepositoryConfigNpmRepository {
        /**
         * Address of the remote repository.
         * Default value is `NPMJS`.
         * Possible values are: `NPMJS`.
         */
        publicRepository?: string;
    }

    export interface RepositoryRemoteRepositoryConfigPythonRepository {
        /**
         * Address of the remote repository.
         * Default value is `PYPI`.
         * Possible values are: `PYPI`.
         */
        publicRepository?: string;
    }

    export interface RepositoryVirtualRepositoryConfig {
        /**
         * Policies that configure the upstream artifacts distributed by the Virtual
         * Repository. Upstream policies cannot be set on a standard repository.
         * Structure is documented below.
         */
        upstreamPolicies?: outputs.artifactregistry.RepositoryVirtualRepositoryConfigUpstreamPolicy[];
    }

    export interface RepositoryVirtualRepositoryConfigUpstreamPolicy {
        /**
         * The user-provided ID of the upstream policy.
         */
        id?: string;
        /**
         * Entries with a greater priority value take precedence in the pull order.
         */
        priority?: number;
        /**
         * A reference to the repository resource, for example:
         * "projects/p1/locations/us-central1/repository/repo1".
         */
        repository?: string;
    }

}

export namespace assuredworkloads {
    export interface WorkloadKmsSettings {
        /**
         * Required. Input only. Immutable. The time at which the Key Management Service will automatically create a new version of the crypto key and mark it as the primary.
         */
        nextRotationTime: string;
        /**
         * Required. Input only. Immutable. will be advanced by this period when the Key Management Service automatically rotates a key. Must be at least 24 hours and at most 876,000 hours.
         */
        rotationPeriod: string;
    }

    export interface WorkloadResource {
        /**
         * Resource identifier. For a project this represents project_number. If the project is already taken, the workload creation will fail.
         */
        resourceId: number;
        /**
         * Indicates the type of resource. This field should be specified to correspond the id to the right project type (CONSUMER_PROJECT or ENCRYPTION_KEYS_PROJECT) Possible values: RESOURCE_TYPE_UNSPECIFIED, CONSUMER_PROJECT, ENCRYPTION_KEYS_PROJECT, KEYRING, CONSUMER_FOLDER
         */
        resourceType: string;
    }

    export interface WorkloadResourceSetting {
        /**
         * Resource identifier. For a project this represents project_number. If the project is already taken, the workload creation will fail.
         */
        resourceId?: string;
        /**
         * Indicates the type of resource. This field should be specified to correspond the id to the right project type (CONSUMER_PROJECT or ENCRYPTION_KEYS_PROJECT) Possible values: RESOURCE_TYPE_UNSPECIFIED, CONSUMER_PROJECT, ENCRYPTION_KEYS_PROJECT, KEYRING, CONSUMER_FOLDER
         */
        resourceType?: string;
    }

}

export namespace backupdisasterrecovery {
    export interface ManagementServerManagementUri {
        /**
         * (Output)
         * The management console api endpoint.
         */
        api: string;
        /**
         * (Output)
         * The management console webUi.
         */
        webUi: string;
    }

    export interface ManagementServerNetwork {
        /**
         * Network with format `projects/{{project_id}}/global/networks/{{network_id}}`
         */
        network: string;
        /**
         * Type of Network peeringMode
         * Default value is `PRIVATE_SERVICE_ACCESS`.
         * Possible values are: `PRIVATE_SERVICE_ACCESS`.
         *
         * - - -
         */
        peeringMode?: string;
    }

}

export namespace beyondcorp {
    export interface AppConnectionApplicationEndpoint {
        /**
         * Hostname or IP address of the remote application endpoint.
         */
        host: string;
        /**
         * Port of the remote application endpoint.
         *
         * - - -
         */
        port: number;
    }

    export interface AppConnectionGateway {
        /**
         * AppGateway name in following format: projects/{project_id}/locations/{locationId}/appgateways/{gateway_id}.
         */
        appGateway: string;
        /**
         * (Output)
         * Ingress port reserved on the gateways for this AppConnection, if not specified or zero, the default port is 19443.
         */
        ingressPort: number;
        /**
         * The type of hosting used by the gateway. Refer to
         * https://cloud.google.com/beyondcorp/docs/reference/rest/v1/projects.locations.appConnections#Type_1
         * for a list of possible values.
         */
        type?: string;
        /**
         * (Output)
         * Server-defined URI for this resource.
         */
        uri: string;
    }

    export interface AppConnectorPrincipalInfo {
        /**
         * ServiceAccount represents a GCP service account.
         * Structure is documented below.
         */
        serviceAccount: outputs.beyondcorp.AppConnectorPrincipalInfoServiceAccount;
    }

    export interface AppConnectorPrincipalInfoServiceAccount {
        /**
         * Email address of the service account.
         *
         * - - -
         */
        email: string;
    }

    export interface AppGatewayAllocatedConnection {
        /**
         * The ingress port of an allocated connection.
         */
        ingressPort?: number;
        /**
         * The PSC uri of an allocated connection.
         */
        pscUri?: string;
    }

    export interface GetAppConnectionApplicationEndpoint {
        host: string;
        port: number;
    }

    export interface GetAppConnectionGateway {
        appGateway: string;
        ingressPort: number;
        type: string;
        uri: string;
    }

    export interface GetAppConnectorPrincipalInfo {
        serviceAccounts: outputs.beyondcorp.GetAppConnectorPrincipalInfoServiceAccount[];
    }

    export interface GetAppConnectorPrincipalInfoServiceAccount {
        email: string;
    }

    export interface GetAppGatewayAllocatedConnection {
        ingressPort: number;
        pscUri: string;
    }

}

export namespace biglake {
    export interface DatabaseHiveOptions {
        /**
         * Cloud Storage folder URI where the database data is stored, starting with "gs://".
         */
        locationUri?: string;
        /**
         * Stores user supplied Hive database parameters. An object containing a
         * list of"key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         *
         * - - -
         */
        parameters?: {[key: string]: string};
    }

    export interface TableHiveOptions {
        /**
         * Stores user supplied Hive table parameters. An object containing a
         * list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        parameters?: {[key: string]: string};
        /**
         * Stores physical storage information on the data.
         * Structure is documented below.
         */
        storageDescriptor?: outputs.biglake.TableHiveOptionsStorageDescriptor;
        /**
         * Hive table type. For example, MANAGED_TABLE, EXTERNAL_TABLE.
         */
        tableType?: string;
    }

    export interface TableHiveOptionsStorageDescriptor {
        /**
         * The fully qualified Java class name of the input format.
         */
        inputFormat?: string;
        /**
         * Cloud Storage folder URI where the table data is stored, starting with "gs://".
         */
        locationUri?: string;
        /**
         * The fully qualified Java class name of the output format.
         */
        outputFormat?: string;
    }

}

export namespace bigquery {
    export interface AppProfileSingleClusterRouting {
        /**
         * If true, CheckAndMutateRow and ReadModifyWriteRow requests are allowed by this app profile.
         * It is unsafe to send these requests to the same table/row/column in multiple clusters.
         */
        allowTransactionalWrites?: boolean;
        /**
         * The cluster to which read/write requests should be routed.
         */
        clusterId: string;
    }

    export interface BiReservationPreferredTable {
        /**
         * The ID of the dataset in the above project.
         */
        datasetId?: string;
        /**
         * The assigned project ID of the project.
         */
        projectId?: string;
        /**
         * The ID of the table in the above dataset.
         */
        tableId?: string;
    }

    export interface ConnectionAws {
        /**
         * Authentication using Google owned service account to assume into customer's AWS IAM Role.
         * Structure is documented below.
         */
        accessRole: outputs.bigquery.ConnectionAwsAccessRole;
    }

    export interface ConnectionAwsAccessRole {
        /**
         * The users AWS IAM Role that trusts the Google-owned AWS IAM user Connection.
         */
        iamRoleId: string;
        /**
         * (Output)
         * A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's AWS IAM Role.
         */
        identity: string;
    }

    export interface ConnectionAzure {
        /**
         * (Output)
         * The name of the Azure Active Directory Application.
         */
        application: string;
        /**
         * (Output)
         * The client id of the Azure Active Directory Application.
         */
        clientId: string;
        /**
         * The id of customer's directory that host the data.
         */
        customerTenantId: string;
        /**
         * The Azure Application (client) ID where the federated credentials will be hosted.
         */
        federatedApplicationClientId?: string;
        /**
         * (Output)
         * A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's Azure Active Directory Application.
         */
        identity: string;
        /**
         * (Output)
         * The object id of the Azure Active Directory Application.
         */
        objectId: string;
        /**
         * (Output)
         * The URL user will be redirected to after granting consent during connection setup.
         */
        redirectUri: string;
    }

    export interface ConnectionCloudResource {
        /**
         * (Output)
         * The account ID of the service created for the purpose of this connection.
         */
        serviceAccountId: string;
    }

    export interface ConnectionCloudSpanner {
        /**
         * Cloud Spanner database in the form `project/instance/database'
         */
        database: string;
        /**
         * If parallelism should be used when reading from Cloud Spanner
         */
        useParallelism?: boolean;
        /**
         * If the serverless analytics service should be used to read data from Cloud Spanner. useParallelism must be set when using serverless analytics
         */
        useServerlessAnalytics?: boolean;
    }

    export interface ConnectionCloudSql {
        /**
         * Cloud SQL properties.
         * Structure is documented below.
         */
        credential: outputs.bigquery.ConnectionCloudSqlCredential;
        /**
         * Database name.
         */
        database: string;
        /**
         * Cloud SQL instance ID in the form project:location:instance.
         */
        instanceId: string;
        /**
         * (Output)
         * When the connection is used in the context of an operation in BigQuery, this service account will serve as the identity being used for connecting to the CloudSQL instance specified in this connection.
         */
        serviceAccountId: string;
        /**
         * Type of the Cloud SQL database.
         * Possible values are: `DATABASE_TYPE_UNSPECIFIED`, `POSTGRES`, `MYSQL`.
         */
        type: string;
    }

    export interface ConnectionCloudSqlCredential {
        /**
         * Password for database.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * Username for database.
         */
        username: string;
    }

    export interface ConnectionIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ConnectionIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DataTransferConfigEmailPreferences {
        /**
         * If true, email notifications will be sent on transfer run failures.
         */
        enableFailureEmail: boolean;
    }

    export interface DataTransferConfigScheduleOptions {
        /**
         * If true, automatic scheduling of data transfer runs for this
         * configuration will be disabled. The runs can be started on ad-hoc
         * basis using transferConfigs.startManualRuns API. When automatic
         * scheduling is disabled, the TransferConfig.schedule field will
         * be ignored.
         */
        disableAutoScheduling?: boolean;
        /**
         * Defines time to stop scheduling transfer runs. A transfer run cannot be
         * scheduled at or after the end time. The end time can be changed at any
         * moment. The time when a data transfer can be triggered manually is not
         * limited by this option.
         */
        endTime?: string;
        /**
         * Specifies time to start scheduling transfer runs. The first run will be
         * scheduled at or after the start time according to a recurrence pattern
         * defined in the schedule string. The start time can be changed at any
         * moment. The time when a data transfer can be triggered manually is not
         * limited by this option.
         */
        startTime?: string;
    }

    export interface DataTransferConfigSensitiveParams {
        /**
         * The Secret Access Key of the AWS account transferring data from.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        secretAccessKey: string;
    }

    export interface DatasetAccess {
        /**
         * Grants all resources of particular types in a particular dataset read access to the current dataset.
         * Structure is documented below.
         */
        dataset?: outputs.bigquery.DatasetAccessDataset;
        /**
         * A domain to grant access to. Any users signed in with the
         * domain specified will be granted the specified access
         */
        domain?: string;
        /**
         * An email address of a Google Group to grant access to.
         */
        groupByEmail?: string;
        /**
         * Describes the rights granted to the user specified by the other
         * member of the access object. Basic, predefined, and custom roles
         * are supported. Predefined roles that have equivalent basic roles
         * are swapped by the API to their basic counterparts. See
         * [official docs](https://cloud.google.com/bigquery/docs/access-control).
         */
        role?: string;
        /**
         * A routine from a different dataset to grant access to. Queries
         * executed against that routine will have read access to tables in
         * this dataset. The role field is not required when this field is
         * set. If that routine is updated by any user, access to the routine
         * needs to be granted again via an update operation.
         * Structure is documented below.
         */
        routine?: outputs.bigquery.DatasetAccessRoutine;
        /**
         * A special group to grant access to. Possible values include:
         */
        specialGroup?: string;
        /**
         * An email address of a user to grant access to. For example:
         * fred@example.com
         */
        userByEmail?: string;
        /**
         * A view from a different dataset to grant access to. Queries
         * executed against that view will have read access to tables in
         * this dataset. The role field is not required when this field is
         * set. If that view is updated by any user, access to the view
         * needs to be granted again via an update operation.
         * Structure is documented below.
         */
        view?: outputs.bigquery.DatasetAccessView;
    }

    export interface DatasetAccessAuthorizedDataset {
        /**
         * The dataset this entry applies to
         * Structure is documented below.
         */
        dataset: outputs.bigquery.DatasetAccessAuthorizedDatasetDataset;
        /**
         * Which resources in the dataset this entry applies to. Currently, only views are supported,
         * but additional target types may be added in the future. Possible values: VIEWS
         */
        targetTypes: string[];
    }

    export interface DatasetAccessAuthorizedDatasetDataset {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
    }

    export interface DatasetAccessDataset {
        /**
         * The dataset this entry applies to
         * Structure is documented below.
         */
        dataset: outputs.bigquery.DatasetAccessDatasetDataset;
        /**
         * Which resources in the dataset this entry applies to. Currently, only views are supported,
         * but additional target types may be added in the future. Possible values: VIEWS
         */
        targetTypes: string[];
    }

    export interface DatasetAccessDatasetDataset {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
    }

    export interface DatasetAccessRoutine {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The ID of the routine. The ID must contain only letters (a-z,
         * A-Z), numbers (0-9), or underscores (_). The maximum length
         * is 256 characters.
         */
        routineId: string;
    }

    export interface DatasetAccessView {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The ID of the table. The ID must contain only letters (a-z,
         * A-Z), numbers (0-9), or underscores (_). The maximum length
         * is 1,024 characters.
         */
        tableId: string;
    }

    export interface DatasetDefaultEncryptionConfiguration {
        /**
         * Describes the Cloud KMS encryption key that will be used to protect destination
         * BigQuery table. The BigQuery Service Account associated with your project requires
         * access to this encryption key.
         */
        kmsKeyName: string;
    }

    export interface DatasetIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DatasetIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface IamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, this provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface IamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, this provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface JobCopy {
        /**
         * Specifies whether the job is allowed to create new tables. The following values are supported:
         * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
         * CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
         * Creation, truncation and append actions occur as one atomic update upon job completion
         * Default value is `CREATE_IF_NEEDED`.
         * Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
         */
        createDisposition?: string;
        /**
         * Custom encryption configuration (e.g., Cloud KMS keys)
         * Structure is documented below.
         */
        destinationEncryptionConfiguration?: outputs.bigquery.JobCopyDestinationEncryptionConfiguration;
        /**
         * The destination table.
         * Structure is documented below.
         */
        destinationTable?: outputs.bigquery.JobCopyDestinationTable;
        /**
         * Source tables to copy.
         * Structure is documented below.
         */
        sourceTables: outputs.bigquery.JobCopySourceTable[];
        /**
         * Specifies the action that occurs if the destination table already exists. The following values are supported:
         * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
         * WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
         * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
         * Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
         * Creation, truncation and append actions occur as one atomic update upon job completion.
         * Default value is `WRITE_EMPTY`.
         * Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
         */
        writeDisposition?: string;
    }

    export interface JobCopyDestinationEncryptionConfiguration {
        /**
         * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
         * The BigQuery Service Account associated with your project requires access to this encryption key.
         */
        kmsKeyName: string;
        /**
         * (Output)
         * Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
         */
        kmsKeyVersion: string;
    }

    export interface JobCopyDestinationTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The table. Can be specified `{{table_id}}` if `projectId` and `datasetId` are also set,
         * or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
         */
        tableId: string;
    }

    export interface JobCopySourceTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The table. Can be specified `{{table_id}}` if `projectId` and `datasetId` are also set,
         * or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
         */
        tableId: string;
    }

    export interface JobExtract {
        /**
         * The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
         * The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
         */
        compression?: string;
        /**
         * The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
         * The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
         * The default value for models is SAVED_MODEL.
         */
        destinationFormat: string;
        /**
         * A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
         */
        destinationUris: string[];
        /**
         * When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
         * Default is ','
         */
        fieldDelimiter: string;
        /**
         * Whether to print out a header row in the results. Default is true.
         */
        printHeader?: boolean;
        /**
         * A reference to the model being exported.
         * Structure is documented below.
         */
        sourceModel?: outputs.bigquery.JobExtractSourceModel;
        /**
         * A reference to the table being exported.
         * Structure is documented below.
         */
        sourceTable?: outputs.bigquery.JobExtractSourceTable;
        /**
         * Whether to use logical types when extracting to AVRO format.
         */
        useAvroLogicalTypes?: boolean;
    }

    export interface JobExtractSourceModel {
        /**
         * The ID of the dataset containing this model.
         */
        datasetId: string;
        /**
         * The ID of the model.
         *
         * - - -
         */
        modelId: string;
        /**
         * The ID of the project containing this model.
         */
        projectId: string;
    }

    export interface JobExtractSourceTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The table. Can be specified `{{table_id}}` if `projectId` and `datasetId` are also set,
         * or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
         */
        tableId: string;
    }

    export interface JobLoad {
        /**
         * Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
         * If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
         * an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
         */
        allowJaggedRows?: boolean;
        /**
         * Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
         * The default value is false.
         */
        allowQuotedNewlines?: boolean;
        /**
         * Indicates if we should automatically infer the options and schema for CSV and JSON sources.
         */
        autodetect?: boolean;
        /**
         * Specifies whether the job is allowed to create new tables. The following values are supported:
         * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
         * CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
         * Creation, truncation and append actions occur as one atomic update upon job completion
         * Default value is `CREATE_IF_NEEDED`.
         * Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
         */
        createDisposition?: string;
        /**
         * Custom encryption configuration (e.g., Cloud KMS keys)
         * Structure is documented below.
         */
        destinationEncryptionConfiguration?: outputs.bigquery.JobLoadDestinationEncryptionConfiguration;
        /**
         * The destination table to load the data into.
         * Structure is documented below.
         */
        destinationTable: outputs.bigquery.JobLoadDestinationTable;
        /**
         * The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
         * The default value is UTF-8. BigQuery decodes the data after the raw, binary data
         * has been split using the values of the quote and fieldDelimiter properties.
         */
        encoding?: string;
        /**
         * The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
         * To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
         * the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
         * data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
         * The default value is a comma (',').
         */
        fieldDelimiter: string;
        /**
         * Indicates if BigQuery should allow extra values that are not represented in the table schema.
         * If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
         * and if there are too many bad records, an invalid error is returned in the job result.
         * The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
         * CSV: Trailing columns
         * JSON: Named values that don't match any column names
         */
        ignoreUnknownValues?: boolean;
        /**
         * If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
         * For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
         * GeoJSON: set to GEOJSON.
         */
        jsonExtension?: string;
        /**
         * The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
         * an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
         */
        maxBadRecords?: number;
        /**
         * Specifies a string that represents a null value in a CSV file. The default value is the empty string. If you set this
         * property to a custom value, BigQuery throws an error if an
         * empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
         * an empty value.
         */
        nullMarker?: string;
        /**
         * Parquet Options for load and make external tables.
         * Structure is documented below.
         */
        parquetOptions?: outputs.bigquery.JobLoadParquetOptions;
        /**
         * If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
         * Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
         * If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
         */
        projectionFields?: string[];
        /**
         * The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
         * and then uses the first byte of the encoded string to split the data in its raw, binary state.
         * The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
         * If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
         */
        quote: string;
        /**
         * Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
         * supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
         * when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
         * For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
         * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
         * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
         */
        schemaUpdateOptions?: string[];
        /**
         * The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
         * The default value is 0. This property is useful if you have header rows in the file that should be skipped.
         * When autodetect is on, the behavior is the following:
         * skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
         * the row is read as data. Otherwise data is read starting from the second row.
         * skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
         * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
         * row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
         */
        skipLeadingRows?: number;
        /**
         * The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
         * For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
         * For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
         * The default value is CSV.
         */
        sourceFormat?: string;
        /**
         * The fully-qualified URIs that point to your data in Google Cloud.
         * For Google Cloud Storage URIs: Each URI can contain one '\*' wildcard character
         * and it must come after the 'bucket' name. Size limits related to load jobs apply
         * to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
         * specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
         * For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '\*' wildcard character is not allowed.
         */
        sourceUris: string[];
        /**
         * Time-based partitioning specification for the destination table.
         * Structure is documented below.
         */
        timePartitioning?: outputs.bigquery.JobLoadTimePartitioning;
        /**
         * Specifies the action that occurs if the destination table already exists. The following values are supported:
         * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
         * WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
         * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
         * Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
         * Creation, truncation and append actions occur as one atomic update upon job completion.
         * Default value is `WRITE_EMPTY`.
         * Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
         */
        writeDisposition?: string;
    }

    export interface JobLoadDestinationEncryptionConfiguration {
        /**
         * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
         * The BigQuery Service Account associated with your project requires access to this encryption key.
         */
        kmsKeyName: string;
        /**
         * (Output)
         * Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
         */
        kmsKeyVersion: string;
    }

    export interface JobLoadDestinationTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The table. Can be specified `{{table_id}}` if `projectId` and `datasetId` are also set,
         * or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
         */
        tableId: string;
    }

    export interface JobLoadParquetOptions {
        /**
         * If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
         */
        enableListInference?: boolean;
        /**
         * If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
         */
        enumAsString?: boolean;
    }

    export interface JobLoadTimePartitioning {
        /**
         * Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
         */
        expirationMs?: string;
        /**
         * If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
         * The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
         * A wrapper is used here because an empty string is an invalid value.
         */
        field?: string;
        /**
         * The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
         * but in OnePlatform the field will be treated as unset.
         */
        type: string;
    }

    export interface JobQuery {
        /**
         * If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
         * Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
         * However, you must still set destinationTable when result size exceeds the allowed maximum response size.
         */
        allowLargeResults?: boolean;
        /**
         * Specifies whether the job is allowed to create new tables. The following values are supported:
         * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
         * CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
         * Creation, truncation and append actions occur as one atomic update upon job completion
         * Default value is `CREATE_IF_NEEDED`.
         * Possible values are: `CREATE_IF_NEEDED`, `CREATE_NEVER`.
         */
        createDisposition?: string;
        /**
         * Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
         * Structure is documented below.
         */
        defaultDataset?: outputs.bigquery.JobQueryDefaultDataset;
        /**
         * Custom encryption configuration (e.g., Cloud KMS keys)
         * Structure is documented below.
         */
        destinationEncryptionConfiguration?: outputs.bigquery.JobQueryDestinationEncryptionConfiguration;
        /**
         * Describes the table where the query results should be stored.
         * This property must be set for large results that exceed the maximum response size.
         * For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
         * Structure is documented below.
         */
        destinationTable: outputs.bigquery.JobQueryDestinationTable;
        /**
         * If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
         * allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
         */
        flattenResults?: boolean;
        /**
         * Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
         * If unspecified, this will be set to your project default.
         */
        maximumBillingTier?: number;
        /**
         * Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
         * If unspecified, this will be set to your project default.
         */
        maximumBytesBilled?: string;
        /**
         * Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
         */
        parameterMode?: string;
        /**
         * Specifies a priority for the query.
         * Default value is `INTERACTIVE`.
         * Possible values are: `INTERACTIVE`, `BATCH`.
         */
        priority?: string;
        /**
         * SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
         * *NOTE*: queries containing [DML language](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language)
         * (`DELETE`, `UPDATE`, `MERGE`, `INSERT`) must specify `createDisposition = ""` and `writeDisposition = ""`.
         */
        query: string;
        /**
         * Allows the schema of the destination table to be updated as a side effect of the query job.
         * Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
         * when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
         * specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
         * One or more of the following values are specified:
         * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
         * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
         */
        schemaUpdateOptions?: string[];
        /**
         * Options controlling the execution of scripts.
         * Structure is documented below.
         */
        scriptOptions?: outputs.bigquery.JobQueryScriptOptions;
        /**
         * Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
         * If set to false, the query will use BigQuery's standard SQL.
         */
        useLegacySql?: boolean;
        /**
         * Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
         * tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
         * The default value is true.
         */
        useQueryCache?: boolean;
        /**
         * Describes user-defined function resources used in the query.
         * Structure is documented below.
         */
        userDefinedFunctionResources?: outputs.bigquery.JobQueryUserDefinedFunctionResource[];
        /**
         * Specifies the action that occurs if the destination table already exists. The following values are supported:
         * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
         * WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
         * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
         * Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
         * Creation, truncation and append actions occur as one atomic update upon job completion.
         * Default value is `WRITE_EMPTY`.
         * Possible values are: `WRITE_TRUNCATE`, `WRITE_APPEND`, `WRITE_EMPTY`.
         */
        writeDisposition?: string;
    }

    export interface JobQueryDefaultDataset {
        /**
         * The dataset. Can be specified `{{dataset_id}}` if `projectId` is also set,
         * or of the form `projects/{{project}}/datasets/{{dataset_id}}` if not.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
    }

    export interface JobQueryDestinationEncryptionConfiguration {
        /**
         * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
         * The BigQuery Service Account associated with your project requires access to this encryption key.
         */
        kmsKeyName: string;
        /**
         * (Output)
         * Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
         */
        kmsKeyVersion: string;
    }

    export interface JobQueryDestinationTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The table. Can be specified `{{table_id}}` if `projectId` and `datasetId` are also set,
         * or of the form `projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}` if not.
         */
        tableId: string;
    }

    export interface JobQueryScriptOptions {
        /**
         * Determines which statement in the script represents the "key result",
         * used to populate the schema and query results of the script job.
         * Possible values are: `LAST`, `FIRST_SELECT`.
         */
        keyResultStatement?: string;
        /**
         * Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
         */
        statementByteBudget?: string;
        /**
         * Timeout period for each statement in a script.
         */
        statementTimeoutMs?: string;
    }

    export interface JobQueryUserDefinedFunctionResource {
        /**
         * An inline resource that contains code for a user-defined function (UDF).
         * Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
         */
        inlineCode?: string;
        /**
         * A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
         */
        resourceUri?: string;
    }

    export interface JobStatus {
        /**
         * (Output)
         * Final error result of the job. If present, indicates that the job has completed and was unsuccessful.
         * Structure is documented below.
         */
        errorResults: outputs.bigquery.JobStatusErrorResult[];
        /**
         * (Output)
         * The first errors encountered during the running of the job. The final message
         * includes the number of errors that caused the process to stop. Errors here do
         * not necessarily mean that the job has not completed or was unsuccessful.
         * Structure is documented below.
         */
        errors: outputs.bigquery.JobStatusError[];
        /**
         * (Output)
         * Running state of the job. Valid states include 'PENDING', 'RUNNING', and 'DONE'.
         */
        state: string;
    }

    export interface JobStatusError {
        /**
         * The geographic location of the job. The default value is US.
         */
        location?: string;
        /**
         * A human-readable description of the error.
         */
        message?: string;
        /**
         * A short error code that summarizes the error.
         */
        reason?: string;
    }

    export interface JobStatusErrorResult {
        /**
         * The geographic location of the job. The default value is US.
         */
        location?: string;
        /**
         * A human-readable description of the error.
         */
        message?: string;
        /**
         * A short error code that summarizes the error.
         */
        reason?: string;
    }

    export interface ReservationAutoscale {
        /**
         * (Output)
         * The slot capacity added to this reservation when autoscale happens. Will be between [0, maxSlots].
         */
        currentSlots: number;
        /**
         * Number of slots to be scaled when needed.
         */
        maxSlots?: number;
    }

    export interface RoutineArgument {
        /**
         * Defaults to FIXED_TYPE.
         * Default value is `FIXED_TYPE`.
         * Possible values are: `FIXED_TYPE`, `ANY_TYPE`.
         */
        argumentKind?: string;
        /**
         * A JSON schema for the data type. Required unless argumentKind = ANY_TYPE.
         * ~>**NOTE**: Because this field expects a JSON string, any changes to the string
         * will create a diff, even if the JSON itself hasn't changed. If the API returns
         * a different value for the same schema, e.g. it switched the order of values
         * or replaced STRUCT field type with RECORD field type, we currently cannot
         * suppress the recurring diff this causes. As a workaround, we recommend using
         * the schema as returned by the API.
         */
        dataType?: string;
        /**
         * Specifies whether the argument is input or output. Can be set for procedures only.
         * Possible values are: `IN`, `OUT`, `INOUT`.
         */
        mode?: string;
        /**
         * The name of this argument. Can be absent for function return argument.
         */
        name?: string;
    }

    export interface TableEncryptionConfiguration {
        /**
         * The self link or full name of a key which should be used to
         * encrypt this table.  Note that the default bigquery service account will need to have
         * encrypt/decrypt permissions on this key - you may want to see the
         * `gcp.bigquery.getDefaultServiceAccount` datasource and the
         * `gcp.kms.CryptoKeyIAMBinding` resource.
         */
        kmsKeyName: string;
        /**
         * The self link or full name of the kms key version used to encrypt this table.
         */
        kmsKeyVersion: string;
    }

    export interface TableExternalDataConfiguration {
        /**
         * Let BigQuery try to autodetect the schema
         * and format of the table.
         */
        autodetect: boolean;
        /**
         * Additional options if `sourceFormat` is set to
         * "AVRO".  Structure is documented below.
         */
        avroOptions?: outputs.bigquery.TableExternalDataConfigurationAvroOptions;
        /**
         * The compression type of the data source.
         * Valid values are "NONE" or "GZIP".
         */
        compression?: string;
        /**
         * The connection specifying the credentials to be used to read
         * external storage, such as Azure Blob, Cloud Storage, or S3. The `connectionId` can have
         * the form `{{project}}.{{location}}.{{connection_id}}`
         * or `projects/{{project}}/locations/{{location}}/connections/{{connection_id}}`.
         *
         * ~>**NOTE:** If you set `external_data_configuration.connection_id`, the
         * table schema must be specified using the top-level `schema` field
         * documented above.
         */
        connectionId?: string;
        /**
         * Additional properties to set if
         * `sourceFormat` is set to "CSV". Structure is documented below.
         */
        csvOptions?: outputs.bigquery.TableExternalDataConfigurationCsvOptions;
        /**
         * Specifies how source URIs are interpreted for constructing the file set to load.
         * By default source URIs are expanded against the underlying storage.
         * Other options include specifying manifest files. Only applicable to object storage systems. Docs
         */
        fileSetSpecType?: string;
        /**
         * Additional options if
         * `sourceFormat` is set to "GOOGLE_SHEETS". Structure is
         * documented below.
         */
        googleSheetsOptions?: outputs.bigquery.TableExternalDataConfigurationGoogleSheetsOptions;
        /**
         * When set, configures hive partitioning
         * support. Not all storage formats support hive partitioning -- requesting hive
         * partitioning on an unsupported format will lead to an error, as will providing
         * an invalid specification. Structure is documented below.
         */
        hivePartitioningOptions?: outputs.bigquery.TableExternalDataConfigurationHivePartitioningOptions;
        /**
         * Indicates if BigQuery should
         * allow extra values that are not represented in the table schema.
         * If true, the extra values are ignored. If false, records with
         * extra columns are treated as bad records, and if there are too
         * many bad records, an invalid error is returned in the job result.
         * The default value is false.
         */
        ignoreUnknownValues?: boolean;
        /**
         * Additional properties to set if
         * `sourceFormat` is set to "JSON". Structure is documented below.
         */
        jsonOptions?: outputs.bigquery.TableExternalDataConfigurationJsonOptions;
        /**
         * The maximum number of bad records that
         * BigQuery can ignore when reading data.
         */
        maxBadRecords?: number;
        /**
         * Metadata Cache Mode for the table. Set this to enable caching of metadata from external data source. Valid values are `AUTOMATIC` and `MANUAL`.
         */
        metadataCacheMode?: string;
        /**
         * Object Metadata is used to create Object Tables. Object Tables contain a listing of objects (with their metadata) found at the sourceUris. If `objectMetadata` is set, `sourceFormat` should be omitted.
         */
        objectMetadata?: string;
        /**
         * Additional properties to set if
         * `sourceFormat` is set to "PARQUET". Structure is documented below.
         */
        parquetOptions?: outputs.bigquery.TableExternalDataConfigurationParquetOptions;
        /**
         * When creating an external table, the user can provide a reference file with the table schema. This is enabled for the following formats: AVRO, PARQUET, ORC.
         */
        referenceFileSchemaUri?: string;
        /**
         * A JSON schema for the external table. Schema is required
         * for CSV and JSON formats if autodetect is not on. Schema is disallowed
         * for Google Cloud Bigtable, Cloud Datastore backups, Avro, Iceberg, ORC and Parquet formats.
         * ~>**NOTE:** Because this field expects a JSON string, any changes to the
         * string will create a diff, even if the JSON itself hasn't changed.
         * Furthermore drift for this field cannot not be detected because BigQuery
         * only uses this schema to compute the effective schema for the table, therefore
         * any changes on the configured value will force the table to be recreated.
         * This schema is effectively only applied when creating a table from an external
         * datasource, after creation the computed schema will be stored in
         * `google_bigquery_table.schema`
         *
         * ~>**NOTE:** If you set `external_data_configuration.connection_id`, the
         * table schema must be specified using the top-level `schema` field
         * documented above.
         */
        schema: string;
        /**
         * The data format. Please see sourceFormat under
         * [ExternalDataConfiguration](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration)
         * in Bigquery's public API documentation for supported formats. To use "GOOGLE_SHEETS"
         * the `scopes` must include "https://www.googleapis.com/auth/drive.readonly".
         */
        sourceFormat?: string;
        /**
         * A list of the fully-qualified URIs that point to
         * your data in Google Cloud.
         */
        sourceUris: string[];
    }

    export interface TableExternalDataConfigurationAvroOptions {
        /**
         * If is set to true, indicates whether
         * to interpret logical types as the corresponding BigQuery data type
         * (for example, TIMESTAMP), instead of using the raw type (for example, INTEGER).
         */
        useAvroLogicalTypes: boolean;
    }

    export interface TableExternalDataConfigurationCsvOptions {
        /**
         * Indicates if BigQuery should accept rows
         * that are missing trailing optional columns.
         */
        allowJaggedRows?: boolean;
        /**
         * Indicates if BigQuery should allow
         * quoted data sections that contain newline characters in a CSV file.
         * The default value is false.
         */
        allowQuotedNewlines?: boolean;
        /**
         * The character encoding of the data. The supported
         * values are UTF-8 or ISO-8859-1.
         */
        encoding?: string;
        /**
         * The separator for fields in a CSV file.
         */
        fieldDelimiter?: string;
        /**
         * The value that is used to quote data sections in a
         * CSV file. If your data does not contain quoted sections, set the
         * property value to an empty string. If your data contains quoted newline
         * characters, you must also set the `allowQuotedNewlines` property to true.
         * The API-side default is `"`, specified in the provider escaped as `\"`. Due to
         * limitations with default values, this value is required to be
         * explicitly set.
         */
        quote: string;
        /**
         * The number of rows at the top of a CSV
         * file that BigQuery will skip when reading the data.
         */
        skipLeadingRows?: number;
    }

    export interface TableExternalDataConfigurationGoogleSheetsOptions {
        /**
         * Range of a sheet to query from. Only used when
         * non-empty. At least one of `range` or `skipLeadingRows` must be set.
         * Typical format: "sheet_name!top_left_cell_id:bottom_right_cell_id"
         * For example: "sheet1!A1:B20"
         */
        range?: string;
        /**
         * The number of rows at the top of the sheet
         * that BigQuery will skip when reading the data. At least one of `range` or
         * `skipLeadingRows` must be set.
         */
        skipLeadingRows?: number;
    }

    export interface TableExternalDataConfigurationHivePartitioningOptions {
        /**
         * When set, what mode of hive partitioning to use when
         * reading data. The following modes are supported.
         * * AUTO: automatically infer partition key name(s) and type(s).
         * * STRINGS: automatically infer partition key name(s). All types are
         * Not all storage formats support hive partitioning. Requesting hive
         * partitioning on an unsupported format will lead to an error.
         * Currently supported formats are: JSON, CSV, ORC, Avro and Parquet.
         * * CUSTOM: when set to `CUSTOM`, you must encode the partition key schema within the `sourceUriPrefix` by setting `sourceUriPrefix` to `gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}`.
         */
        mode?: string;
        /**
         * If set to true, queries over this table
         * require a partition filter that can be used for partition elimination to be
         * specified.
         */
        requirePartitionFilter?: boolean;
        /**
         * When hive partition detection is requested,
         * a common for all source uris must be required. The prefix must end immediately
         * before the partition key encoding begins. For example, consider files following
         * this data layout. `gs://bucket/path_to_table/dt=2019-06-01/country=USA/id=7/file.avro`
         * `gs://bucket/path_to_table/dt=2019-05-31/country=CA/id=3/file.avro` When hive
         * partitioning is requested with either AUTO or STRINGS detection, the common prefix
         * can be either of `gs://bucket/path_to_table` or `gs://bucket/path_to_table/`.
         * Note that when `mode` is set to `CUSTOM`, you must encode the partition key schema within the `sourceUriPrefix` by setting `sourceUriPrefix` to `gs://bucket/path_to_table/{key1:TYPE1}/{key2:TYPE2}/{key3:TYPE3}`.
         */
        sourceUriPrefix?: string;
    }

    export interface TableExternalDataConfigurationJsonOptions {
        /**
         * The character encoding of the data. The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is UTF-8.
         */
        encoding?: string;
    }

    export interface TableExternalDataConfigurationParquetOptions {
        /**
         * Indicates whether to use schema inference specifically for Parquet LIST logical type.
         */
        enableListInference?: boolean;
        /**
         * Indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
         */
        enumAsString?: boolean;
    }

    export interface TableMaterializedView {
        /**
         * Allow non incremental materialized view definition.
         * The default value is false.
         */
        allowNonIncrementalDefinition?: boolean;
        /**
         * Specifies whether to use BigQuery's automatic refresh for this materialized view when the base table is updated.
         * The default value is true.
         */
        enableRefresh?: boolean;
        /**
         * A query whose result is persisted.
         */
        query: string;
        /**
         * The maximum frequency at which this materialized view will be refreshed.
         * The default value is 1800000
         */
        refreshIntervalMs?: number;
    }

    export interface TableRangePartitioning {
        /**
         * The field used to determine how to create a range-based
         * partition.
         */
        field: string;
        /**
         * Information required to partition based on ranges.
         * Structure is documented below.
         */
        range: outputs.bigquery.TableRangePartitioningRange;
    }

    export interface TableRangePartitioningRange {
        /**
         * End of the range partitioning, exclusive.
         */
        end: number;
        /**
         * The width of each range within the partition.
         */
        interval: number;
        /**
         * Start of the range partitioning, inclusive.
         */
        start: number;
    }

    export interface TableTableConstraints {
        /**
         * Present only if the table has a foreign key.
         * The foreign key is not enforced.
         * Structure is documented below.
         */
        foreignKeys?: outputs.bigquery.TableTableConstraintsForeignKey[];
        /**
         * Represents the primary key constraint
         * on a table's columns. Present only if the table has a primary key.
         * The primary key is not enforced.
         * Structure is documented below.
         */
        primaryKey?: outputs.bigquery.TableTableConstraintsPrimaryKey;
    }

    export interface TableTableConstraintsForeignKey {
        /**
         * The pair of the foreign key column and primary key column.
         * Structure is documented below.
         */
        columnReferences: outputs.bigquery.TableTableConstraintsForeignKeyColumnReferences;
        /**
         * Set only if the foreign key constraint is named.
         */
        name?: string;
        /**
         * The table that holds the primary key
         * and is referenced by this foreign key.
         * Structure is documented below.
         */
        referencedTable: outputs.bigquery.TableTableConstraintsForeignKeyReferencedTable;
    }

    export interface TableTableConstraintsForeignKeyColumnReferences {
        /**
         * The column in the primary key that are
         * referenced by the referencingColumn
         */
        referencedColumn: string;
        /**
         * The column that composes the foreign key.
         */
        referencingColumn: string;
    }

    export interface TableTableConstraintsForeignKeyReferencedTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The ID of the table. The ID must contain only
         * letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum
         * length is 1,024 characters. Certain operations allow suffixing of
         * the table ID with a partition decorator, such as
         * sample_table$20190123.
         */
        tableId: string;
    }

    export interface TableTableConstraintsPrimaryKey {
        /**
         * The columns that are composed of the primary key constraint.
         */
        columns: string[];
    }

    export interface TableTimePartitioning {
        /**
         * Number of milliseconds for which to keep the
         * storage for a partition.
         */
        expirationMs: number;
        /**
         * The field used to determine how to create a time-based
         * partition. If time-based partitioning is enabled without this value, the
         * table is partitioned based on the load time.
         */
        field?: string;
        /**
         * If set to true, queries over this table
         * require a partition filter that can be used for partition elimination to be
         * specified.
         */
        requirePartitionFilter?: boolean;
        /**
         * The supported types are DAY, HOUR, MONTH, and YEAR,
         * which will generate one partition per day, hour, month, and year, respectively.
         */
        type: string;
    }

    export interface TableView {
        /**
         * A query that BigQuery executes when the view is referenced.
         */
        query: string;
        /**
         * Specifies whether to use BigQuery's legacy SQL for this view.
         * The default value is true. If set to false, the view will use BigQuery's standard SQL.
         */
        useLegacySql?: boolean;
    }

}

export namespace bigqueryanalyticshub {
    export interface DataExchangeIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DataExchangeIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ListingBigqueryDataset {
        /**
         * Resource name of the dataset source for this listing. e.g. projects/myproject/datasets/123
         *
         * - - -
         */
        dataset: string;
    }

    export interface ListingDataProvider {
        /**
         * Name of the data provider.
         */
        name: string;
        /**
         * Email or URL of the data provider.
         */
        primaryContact?: string;
    }

    export interface ListingIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ListingIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ListingPublisher {
        /**
         * Name of the listing publisher.
         */
        name: string;
        /**
         * Email or URL of the listing publisher.
         */
        primaryContact?: string;
    }

}

export namespace bigquerydatapolicy {
    export interface DataPolicyDataMaskingPolicy {
        /**
         * The available masking rules. Learn more here: https://cloud.google.com/bigquery/docs/column-data-masking-intro#masking_options.
         * Possible values are: `SHA256`, `ALWAYS_NULL`, `DEFAULT_MASKING_VALUE`, `LAST_FOUR_CHARACTERS`, `FIRST_FOUR_CHARACTERS`, `EMAIL_MASK`, `DATE_YEAR_MASK`.
         */
        predefinedExpression: string;
    }

    export interface DataPolicyIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DataPolicyIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace bigtable {
    export interface GCPolicyMaxAge {
        /**
         * Number of days before applying GC policy.
         *
         * @deprecated Deprecated in favor of duration
         */
        days: number;
        /**
         * Duration before applying GC policy (ex. "8h"). This is required when `days` isn't set
         *
         * -----
         */
        duration: string;
    }

    export interface GCPolicyMaxVersion {
        /**
         * Number of version before applying the GC policy.
         *
         * -----
         * `gcRules` include 2 fields:
         */
        number: number;
    }

    export interface InstanceCluster {
        /**
         * [Autoscaling](https://cloud.google.com/bigtable/docs/autoscaling#parameters) config for the cluster, contains the following arguments:
         */
        autoscalingConfig?: outputs.bigtable.InstanceClusterAutoscalingConfig;
        /**
         * The ID of the Cloud Bigtable cluster. Must be 6-30 characters and must only contain hyphens, lowercase letters and numbers.
         */
        clusterId: string;
        /**
         * Describes the Cloud KMS encryption key that will be used to protect the destination Bigtable cluster. The requirements for this key are: 1) The Cloud Bigtable service account associated with the project that contains this cluster must be granted the `cloudkms.cryptoKeyEncrypterDecrypter` role on the CMEK key. 2) Only regional keys can be used and the region of the CMEK key must match the region of the cluster.
         *
         * > **Note**: Removing the field entirely from the config will cause the provider to default to the backend value.
         *
         * !> **Warning**: Modifying this field will cause the provider to delete/recreate the entire resource.
         *
         * !> **Warning:** Modifying the `storageType`, `zone` or `kmsKeyName` of an existing cluster (by
         * `clusterId`) will cause the provider to delete/recreate the entire
         * `gcp.bigtable.Instance` resource. If these values are changing, use a new
         * `clusterId`.
         */
        kmsKeyName: string;
        /**
         * The number of nodes in the cluster.
         * If no value is set, Cloud Bigtable automatically allocates nodes based on your data footprint and optimized for 50% storage utilization.
         */
        numNodes: number;
        /**
         * The storage type to use. One of `"SSD"` or
         * `"HDD"`. Defaults to `"SSD"`.
         */
        storageType?: string;
        /**
         * The zone to create the Cloud Bigtable cluster in. If it not
         * specified, the provider zone is used. Each cluster must have a different zone in the same region. Zones that support
         * Bigtable instances are noted on the [Cloud Bigtable locations page](https://cloud.google.com/bigtable/docs/locations).
         */
        zone: string;
    }

    export interface InstanceClusterAutoscalingConfig {
        /**
         * The target CPU utilization for autoscaling, in percentage. Must be between 10 and 80.
         */
        cpuTarget: number;
        /**
         * The maximum number of nodes for autoscaling.
         */
        maxNodes: number;
        /**
         * The minimum number of nodes for autoscaling.
         */
        minNodes: number;
        /**
         * The target storage utilization for autoscaling, in GB, for each node in a cluster. This number is limited between 2560 (2.5TiB) and 5120 (5TiB) for a SSD cluster and between 8192 (8TiB) and 16384 (16 TiB) for an HDD cluster. If not set, whatever is already set for the cluster will not change, or if the cluster is just being created, it will use the default value of 2560 for SSD clusters and 8192 for HDD clusters.
         *
         * !> **Warning**: Only one of `autoscalingConfig` or `numNodes` should be set for a cluster. If both are set, `numNodes` is ignored. If none is set, autoscaling will be disabled and sized to the current node count.
         */
        storageTarget: number;
    }

    export interface InstanceIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface InstanceIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TableColumnFamily {
        /**
         * The name of the column family.
         */
        family: string;
    }

    export interface TableIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TableIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace billing {
    export interface AccountIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AccountIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface BudgetAllUpdatesRule {
        /**
         * Boolean. When set to true, disables default notifications sent
         * when a threshold is exceeded. Default recipients are
         * those with Billing Account Administrators and Billing
         * Account Users IAM roles for the target account.
         */
        disableDefaultIamRecipients?: boolean;
        /**
         * The full resource name of a monitoring notification
         * channel in the form
         * projects/{project_id}/notificationChannels/{channel_id}.
         * A maximum of 5 channels are allowed.
         */
        monitoringNotificationChannels?: string[];
        /**
         * The name of the Cloud Pub/Sub topic where budget related
         * messages will be published, in the form
         * projects/{project_id}/topics/{topic_id}. Updates are sent
         * at regular intervals to the topic.
         */
        pubsubTopic?: string;
        /**
         * The schema version of the notification. Only "1.0" is
         * accepted. It represents the JSON schema as defined in
         * https://cloud.google.com/billing/docs/how-to/budgets#notification_format.
         */
        schemaVersion?: string;
    }

    export interface BudgetAmount {
        /**
         * Configures a budget amount that is automatically set to 100% of
         * last period's spend.
         * Boolean. Set value to true to use. Do not set to false, instead
         * use the `specifiedAmount` block.
         */
        lastPeriodAmount?: boolean;
        /**
         * A specified amount to use as the budget. currencyCode is
         * optional. If specified, it must match the currency of the
         * billing account. The currencyCode is provided on output.
         * Structure is documented below.
         */
        specifiedAmount?: outputs.billing.BudgetAmountSpecifiedAmount;
    }

    export interface BudgetAmountSpecifiedAmount {
        /**
         * The 3-letter currency code defined in ISO 4217.
         */
        currencyCode: string;
        /**
         * Number of nano (10^-9) units of the amount.
         * The value must be between -999,999,999 and +999,999,999
         * inclusive. If units is positive, nanos must be positive or
         * zero. If units is zero, nanos can be positive, zero, or
         * negative. If units is negative, nanos must be negative or
         * zero. For example $-1.75 is represented as units=-1 and
         * nanos=-750,000,000.
         *
         * - - -
         */
        nanos?: number;
        /**
         * The whole units of the amount. For example if currencyCode
         * is "USD", then 1 unit is one US dollar.
         */
        units?: string;
    }

    export interface BudgetBudgetFilter {
        /**
         * A CalendarPeriod represents the abstract concept of a recurring time period that has a
         * canonical start. Grammatically, "the start of the current CalendarPeriod".
         * All calendar times begin at 12 AM US and Canadian Pacific Time (UTC-8).
         * Exactly one of `calendarPeriod`, `customPeriod` must be provided.
         * Possible values are: `MONTH`, `QUARTER`, `YEAR`, `CALENDAR_PERIOD_UNSPECIFIED`.
         */
        calendarPeriod?: string;
        /**
         * Optional. If creditTypesTreatment is INCLUDE_SPECIFIED_CREDITS,
         * this is a list of credit types to be subtracted from gross cost to determine the spend for threshold calculations. See a list of acceptable credit type values.
         * If creditTypesTreatment is not INCLUDE_SPECIFIED_CREDITS, this field must be empty.
         * **Note:** If the field has a value in the config and needs to be removed, the field has to be an emtpy array in the config.
         */
        creditTypes: string[];
        /**
         * Specifies how credits should be treated when determining spend
         * for threshold calculations.
         * Default value is `INCLUDE_ALL_CREDITS`.
         * Possible values are: `INCLUDE_ALL_CREDITS`, `EXCLUDE_ALL_CREDITS`, `INCLUDE_SPECIFIED_CREDITS`.
         */
        creditTypesTreatment?: string;
        /**
         * Specifies to track usage from any start date (required) to any end date (optional).
         * This time period is static, it does not recur.
         * Exactly one of `calendarPeriod`, `customPeriod` must be provided.
         * Structure is documented below.
         */
        customPeriod?: outputs.billing.BudgetBudgetFilterCustomPeriod;
        /**
         * A single label and value pair specifying that usage from only
         * this set of labeled resources should be included in the budget.
         */
        labels: {[key: string]: string};
        /**
         * A set of projects of the form projects/{project_number},
         * specifying that usage from only this set of projects should be
         * included in the budget. If omitted, the report will include
         * all usage for the billing account, regardless of which project
         * the usage occurred on.
         */
        projects?: string[];
        /**
         * A set of folder and organization names of the form folders/{folderId} or organizations/{organizationId},
         * specifying that usage from only this set of folders and organizations should be included in the budget.
         * If omitted, the budget includes all usage that the billing account pays for. If the folder or organization
         * contains projects that are paid for by a different Cloud Billing account, the budget doesn't apply to those projects.
         */
        resourceAncestors?: string[];
        /**
         * A set of services of the form services/{service_id},
         * specifying that usage from only this set of services should be
         * included in the budget. If omitted, the report will include
         * usage for all the services. The service names are available
         * through the Catalog API:
         * https://cloud.google.com/billing/v1/how-tos/catalog-api.
         */
        services: string[];
        /**
         * A set of subaccounts of the form billingAccounts/{account_id},
         * specifying that usage from only this set of subaccounts should
         * be included in the budget. If a subaccount is set to the name of
         * the parent account, usage from the parent account will be included.
         * If the field is omitted, the report will include usage from the parent
         * account and all subaccounts, if they exist.
         * **Note:** If the field has a value in the config and needs to be removed, the field has to be an emtpy array in the config.
         */
        subaccounts: string[];
    }

    export interface BudgetBudgetFilterCustomPeriod {
        /**
         * Optional. The end date of the time period. Budgets with elapsed end date won't be processed.
         * If unset, specifies to track all usage incurred since the startDate.
         * Structure is documented below.
         */
        endDate?: outputs.billing.BudgetBudgetFilterCustomPeriodEndDate;
        /**
         * A start date is required. The start date must be after January 1, 2017.
         * Structure is documented below.
         */
        startDate: outputs.billing.BudgetBudgetFilterCustomPeriodStartDate;
    }

    export interface BudgetBudgetFilterCustomPeriodEndDate {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month.
         */
        day: number;
        /**
         * Month of a year. Must be from 1 to 12.
         */
        month: number;
        /**
         * Year of the date. Must be from 1 to 9999.
         */
        year: number;
    }

    export interface BudgetBudgetFilterCustomPeriodStartDate {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month.
         */
        day: number;
        /**
         * Month of a year. Must be from 1 to 12.
         */
        month: number;
        /**
         * Year of the date. Must be from 1 to 9999.
         */
        year: number;
    }

    export interface BudgetThresholdRule {
        /**
         * The type of basis used to determine if spend has passed
         * the threshold.
         * Default value is `CURRENT_SPEND`.
         * Possible values are: `CURRENT_SPEND`, `FORECASTED_SPEND`.
         */
        spendBasis?: string;
        /**
         * Send an alert when this threshold is exceeded. This is a
         * 1.0-based percentage, so 0.5 = 50%. Must be >= 0.
         */
        thresholdPercent: number;
    }

}

export namespace binaryauthorization {
    export interface AttestorAttestationAuthorityNote {
        /**
         * (Output)
         * This field will contain the service account email address that
         * this Attestor will use as the principal when querying Container
         * Analysis. Attestor administrators must grant this service account
         * the IAM role needed to read attestations from the noteReference in
         * Container Analysis (containeranalysis.notes.occurrences.viewer).
         * This email address is fixed for the lifetime of the Attestor, but
         * callers should not make any other assumptions about the service
         * account email; future versions may use an email based on a
         * different naming pattern.
         */
        delegationServiceAccountEmail: string;
        /**
         * The resource name of a ATTESTATION_AUTHORITY Note, created by the
         * user. If the Note is in a different project from the Attestor, it
         * should be specified in the format `projects/*&#47;notes/*` (or the legacy
         * `providers/*&#47;notes/*`). This field may not be updated.
         * An attestation by this attestor is stored as a Container Analysis
         * ATTESTATION_AUTHORITY Occurrence that names a container image
         * and that links to this Note.
         */
        noteReference: string;
        /**
         * Public keys that verify attestations signed by this attestor. This
         * field may be updated.
         * If this field is non-empty, one of the specified public keys must
         * verify that an attestation was signed by this attestor for the
         * image specified in the admission request.
         * If this field is empty, this attestor always returns that no valid
         * attestations exist.
         * Structure is documented below.
         */
        publicKeys?: outputs.binaryauthorization.AttestorAttestationAuthorityNotePublicKey[];
    }

    export interface AttestorAttestationAuthorityNotePublicKey {
        /**
         * ASCII-armored representation of a PGP public key, as the
         * entire output by the command
         * `gpg --export --armor foo@example.com` (either LF or CRLF
         * line endings). When using this field, id should be left
         * blank. The BinAuthz API handlers will calculate the ID
         * and fill it in automatically. BinAuthz computes this ID
         * as the OpenPGP RFC4880 V4 fingerprint, represented as
         * upper-case hex. If id is provided by the caller, it will
         * be overwritten by the API-calculated ID.
         */
        asciiArmoredPgpPublicKey?: string;
        /**
         * A descriptive comment. This field may be updated.
         */
        comment?: string;
        /**
         * The ID of this public key. Signatures verified by BinAuthz
         * must include the ID of the public key that can be used to
         * verify them, and that ID must match the contents of this
         * field exactly. Additional restrictions on this field can
         * be imposed based on which public key type is encapsulated.
         * See the documentation on publicKey cases below for details.
         */
        id: string;
        /**
         * A raw PKIX SubjectPublicKeyInfo format public key.
         * NOTE: id may be explicitly provided by the caller when using this
         * type of public key, but it MUST be a valid RFC3986 URI. If id is left
         * blank, a default one will be computed based on the digest of the DER
         * encoding of the public key.
         * Structure is documented below.
         */
        pkixPublicKey?: outputs.binaryauthorization.AttestorAttestationAuthorityNotePublicKeyPkixPublicKey;
    }

    export interface AttestorAttestationAuthorityNotePublicKeyPkixPublicKey {
        /**
         * A PEM-encoded public key, as described in
         * `https://tools.ietf.org/html/rfc7468#section-13`
         */
        publicKeyPem?: string;
        /**
         * The signature algorithm used to verify a message against
         * a signature using this key. These signature algorithm must
         * match the structure and any object identifiers encoded in
         * publicKeyPem (i.e. this algorithm must match that of the
         * public key).
         *
         * - - -
         */
        signatureAlgorithm?: string;
    }

    export interface AttestorIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AttestorIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface PolicyAdmissionWhitelistPattern {
        /**
         * An image name pattern to whitelist, in the form
         * `registry/path/to/image`. This supports a trailing * as a
         * wildcard, but this is allowed only in text after the registry/
         * part.
         */
        namePattern: string;
    }

    export interface PolicyClusterAdmissionRule {
        /**
         * The identifier for this object. Format specified above.
         */
        cluster: string;
        /**
         * The action when a pod creation is denied by the admission rule.
         * Possible values are: `ENFORCED_BLOCK_AND_AUDIT_LOG`, `DRYRUN_AUDIT_LOG_ONLY`.
         */
        enforcementMode: string;
        /**
         * How this admission rule will be evaluated.
         * Possible values are: `ALWAYS_ALLOW`, `REQUIRE_ATTESTATION`, `ALWAYS_DENY`.
         */
        evaluationMode: string;
        /**
         * The resource names of the attestors that must attest to a
         * container image. If the attestor is in a different project from the
         * policy, it should be specified in the format `projects/*&#47;attestors/*`.
         * Each attestor must exist before a policy can reference it. To add an
         * attestor to a policy the principal issuing the policy change
         * request must be able to read the attestor resource.
         * Note: this field must be non-empty when the evaluationMode field
         * specifies REQUIRE_ATTESTATION, otherwise it must be empty.
         */
        requireAttestationsBies?: string[];
    }

    export interface PolicyDefaultAdmissionRule {
        /**
         * The action when a pod creation is denied by the admission rule.
         * Possible values are: `ENFORCED_BLOCK_AND_AUDIT_LOG`, `DRYRUN_AUDIT_LOG_ONLY`.
         *
         * - - -
         */
        enforcementMode: string;
        /**
         * How this admission rule will be evaluated.
         * Possible values are: `ALWAYS_ALLOW`, `REQUIRE_ATTESTATION`, `ALWAYS_DENY`.
         */
        evaluationMode: string;
        /**
         * The resource names of the attestors that must attest to a
         * container image. If the attestor is in a different project from the
         * policy, it should be specified in the format `projects/*&#47;attestors/*`.
         * Each attestor must exist before a policy can reference it. To add an
         * attestor to a policy the principal issuing the policy change
         * request must be able to read the attestor resource.
         * Note: this field must be non-empty when the evaluationMode field
         * specifies REQUIRE_ATTESTATION, otherwise it must be empty.
         */
        requireAttestationsBies?: string[];
    }

}

export namespace certificateauthority {
    export interface AuthorityAccessUrl {
        /**
         * (Output)
         * The URL where this CertificateAuthority's CA certificate is published. This will only be
         * set for CAs that have been activated.
         */
        caCertificateAccessUrl: string;
        /**
         * (Output)
         * The URL where this CertificateAuthority's CRLs are published. This will only be set for
         * CAs that have been activated.
         */
        crlAccessUrls: string[];
    }

    export interface AuthorityConfig {
        /**
         * Specifies some of the values in a certificate that are related to the subject.
         * Structure is documented below.
         *
         *
         * <a name="nestedX509Config"></a>The `x509Config` block supports:
         */
        subjectConfig: outputs.certificateauthority.AuthorityConfigSubjectConfig;
        /**
         * Describes how some of the technical X.509 fields in a certificate should be populated.
         * Structure is documented below.
         */
        x509Config: outputs.certificateauthority.AuthorityConfigX509Config;
    }

    export interface AuthorityConfigSubjectConfig {
        /**
         * Contains distinguished name fields such as the location and organization.
         * Structure is documented below.
         */
        subject: outputs.certificateauthority.AuthorityConfigSubjectConfigSubject;
        /**
         * The subject alternative name fields.
         * Structure is documented below.
         */
        subjectAltName?: outputs.certificateauthority.AuthorityConfigSubjectConfigSubjectAltName;
    }

    export interface AuthorityConfigSubjectConfigSubject {
        /**
         * The common name of the distinguished name.
         */
        commonName: string;
        /**
         * The country code of the subject.
         */
        countryCode?: string;
        /**
         * The locality or city of the subject.
         */
        locality?: string;
        /**
         * The organization of the subject.
         */
        organization: string;
        /**
         * The organizational unit of the subject.
         */
        organizationalUnit?: string;
        /**
         * The postal code of the subject.
         */
        postalCode?: string;
        /**
         * The province, territory, or regional state of the subject.
         */
        province?: string;
        /**
         * The street address of the subject.
         */
        streetAddress?: string;
    }

    export interface AuthorityConfigSubjectConfigSubjectAltName {
        /**
         * Contains only valid, fully-qualified host names.
         */
        dnsNames?: string[];
        /**
         * Contains only valid RFC 2822 E-mail addresses.
         */
        emailAddresses?: string[];
        /**
         * Contains only valid 32-bit IPv4 addresses or RFC 4291 IPv6 addresses.
         */
        ipAddresses?: string[];
        /**
         * Contains only valid RFC 3986 URIs.
         */
        uris?: string[];
    }

    export interface AuthorityConfigX509Config {
        /**
         * Specifies an X.509 extension, which may be used in different parts of X.509 objects like certificates, CSRs, and CRLs.
         * Structure is documented below.
         */
        additionalExtensions?: outputs.certificateauthority.AuthorityConfigX509ConfigAdditionalExtension[];
        /**
         * Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the
         * "Authority Information Access" extension in the certificate.
         */
        aiaOcspServers?: string[];
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        caOptions: outputs.certificateauthority.AuthorityConfigX509ConfigCaOptions;
        /**
         * Indicates the intended use for keys that correspond to a certificate.
         * Structure is documented below.
         */
        keyUsage: outputs.certificateauthority.AuthorityConfigX509ConfigKeyUsage;
        /**
         * Describes the X.509 name constraints extension.
         * Structure is documented below.
         */
        nameConstraints?: outputs.certificateauthority.AuthorityConfigX509ConfigNameConstraints;
        /**
         * Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
         * Structure is documented below.
         */
        policyIds?: outputs.certificateauthority.AuthorityConfigX509ConfigPolicyId[];
    }

    export interface AuthorityConfigX509ConfigAdditionalExtension {
        /**
         * Indicates whether or not this extension is critical (i.e., if the client does not know how to
         * handle this extension, the client should consider this to be an error).
         */
        critical: boolean;
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        objectId: outputs.certificateauthority.AuthorityConfigX509ConfigAdditionalExtensionObjectId;
        /**
         * The value of this X.509 extension. A base64-encoded string.
         */
        value: string;
    }

    export interface AuthorityConfigX509ConfigAdditionalExtensionObjectId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface AuthorityConfigX509ConfigCaOptions {
        /**
         * When true, the "CA" in Basic Constraints extension will be set to true.
         */
        isCa: boolean;
        /**
         * Refers to the "path length constraint" in Basic Constraints extension. For a CA certificate, this value describes the depth of
         * subordinate CA certificates that are allowed. If this value is less than 0, the request will fail. Setting the value to 0
         * requires setting `zeroMaxIssuerPathLength = true`.
         */
        maxIssuerPathLength?: number;
        /**
         * When true, the "CA" in Basic Constraints extension will be set to false.
         * If both `isCa` and `nonCa` are unset, the extension will be omitted from the CA certificate.
         */
        nonCa?: boolean;
        /**
         * When true, the "path length constraint" in Basic Constraints extension will be set to 0.
         * If both `maxIssuerPathLength` and `zeroMaxIssuerPathLength` are unset,
         * the max path length will be omitted from the CA certificate.
         */
        zeroMaxIssuerPathLength?: boolean;
    }

    export interface AuthorityConfigX509ConfigKeyUsage {
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        baseKeyUsage: outputs.certificateauthority.AuthorityConfigX509ConfigKeyUsageBaseKeyUsage;
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        extendedKeyUsage: outputs.certificateauthority.AuthorityConfigX509ConfigKeyUsageExtendedKeyUsage;
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         * Structure is documented below.
         */
        unknownExtendedKeyUsages?: outputs.certificateauthority.AuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage[];
    }

    export interface AuthorityConfigX509ConfigKeyUsageBaseKeyUsage {
        /**
         * The key may be used to sign certificates.
         */
        certSign?: boolean;
        /**
         * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
         */
        contentCommitment?: boolean;
        /**
         * The key may be used sign certificate revocation lists.
         */
        crlSign?: boolean;
        /**
         * The key may be used to encipher data.
         */
        dataEncipherment?: boolean;
        /**
         * The key may be used to decipher only.
         */
        decipherOnly?: boolean;
        /**
         * The key may be used for digital signatures.
         */
        digitalSignature?: boolean;
        /**
         * The key may be used to encipher only.
         */
        encipherOnly?: boolean;
        /**
         * The key may be used in a key agreement protocol.
         */
        keyAgreement?: boolean;
        /**
         * The key may be used to encipher other keys.
         */
        keyEncipherment?: boolean;
    }

    export interface AuthorityConfigX509ConfigKeyUsageExtendedKeyUsage {
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
         */
        clientAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
         */
        codeSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
         */
        emailProtection?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
         */
        ocspSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
         */
        serverAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
         */
        timeStamping?: boolean;
    }

    export interface AuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface AuthorityConfigX509ConfigNameConstraints {
        /**
         * Indicates whether or not the name constraints are marked critical.
         */
        critical: boolean;
        /**
         * Contains excluded DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        excludedDnsNames?: string[];
        /**
         * Contains the excluded email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        excludedEmailAddresses?: string[];
        /**
         * Contains the excluded IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        excludedIpRanges?: string[];
        /**
         * Contains the excluded URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        excludedUris?: string[];
        /**
         * Contains permitted DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        permittedDnsNames?: string[];
        /**
         * Contains the permitted email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        permittedEmailAddresses?: string[];
        /**
         * Contains the permitted IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        permittedIpRanges?: string[];
        /**
         * Contains the permitted URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        permittedUris?: string[];
    }

    export interface AuthorityConfigX509ConfigPolicyId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface AuthorityKeySpec {
        /**
         * The algorithm to use for creating a managed Cloud KMS key for a for a simplified
         * experience. All managed keys will be have their ProtectionLevel as HSM.
         * Possible values are: `SIGN_HASH_ALGORITHM_UNSPECIFIED`, `RSA_PSS_2048_SHA256`, `RSA_PSS_3072_SHA256`, `RSA_PSS_4096_SHA256`, `RSA_PKCS1_2048_SHA256`, `RSA_PKCS1_3072_SHA256`, `RSA_PKCS1_4096_SHA256`, `EC_P256_SHA256`, `EC_P384_SHA384`.
         *
         * - - -
         */
        algorithm?: string;
        /**
         * The resource name for an existing Cloud KMS CryptoKeyVersion in the format
         * `projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*&#47;cryptoKeyVersions/*`.
         */
        cloudKmsKeyVersion?: string;
    }

    export interface AuthoritySubordinateConfig {
        /**
         * This can refer to a CertificateAuthority that was used to create a
         * subordinate CertificateAuthority. This field is used for information
         * and usability purposes only. The resource name is in the format
         * `projects/*&#47;locations/*&#47;caPools/*&#47;certificateAuthorities/*`.
         */
        certificateAuthority?: string;
        /**
         * Contains the PEM certificate chain for the issuers of this CertificateAuthority,
         * but not pem certificate for this CA itself.
         * Structure is documented below.
         */
        pemIssuerChain?: outputs.certificateauthority.AuthoritySubordinateConfigPemIssuerChain;
    }

    export interface AuthoritySubordinateConfigPemIssuerChain {
        /**
         * Expected to be in leaf-to-root order according to RFC 5246.
         */
        pemCertificates?: string[];
    }

    export interface CaPoolIamBindingCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface CaPoolIamMemberCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface CaPoolIssuancePolicy {
        /**
         * IssuanceModes specifies the allowed ways in which Certificates may be requested from this CaPool.
         * Structure is documented below.
         */
        allowedIssuanceModes?: outputs.certificateauthority.CaPoolIssuancePolicyAllowedIssuanceModes;
        /**
         * If any AllowedKeyType is specified, then the certificate request's public key must match one of the key types listed here.
         * Otherwise, any key may be used.
         * Structure is documented below.
         */
        allowedKeyTypes?: outputs.certificateauthority.CaPoolIssuancePolicyAllowedKeyType[];
        /**
         * A set of X.509 values that will be applied to all certificates issued through this CaPool. If a certificate request
         * includes conflicting values for the same properties, they will be overwritten by the values defined here. If a certificate
         * request uses a CertificateTemplate that defines conflicting predefinedValues for the same properties, the certificate
         * issuance request will fail.
         * Structure is documented below.
         */
        baselineValues?: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValues;
        /**
         * Describes constraints on identities that may appear in Certificates issued through this CaPool.
         * If this is omitted, then this CaPool will not add restrictions on a certificate's identity.
         * Structure is documented below.
         */
        identityConstraints?: outputs.certificateauthority.CaPoolIssuancePolicyIdentityConstraints;
        /**
         * The maximum lifetime allowed for issued Certificates. Note that if the issuing CertificateAuthority
         * expires before a Certificate's requested maximumLifetime, the effective lifetime will be explicitly truncated to match it.
         */
        maximumLifetime?: string;
    }

    export interface CaPoolIssuancePolicyAllowedIssuanceModes {
        /**
         * When true, allows callers to create Certificates by specifying a CertificateConfig.
         */
        allowConfigBasedIssuance: boolean;
        /**
         * When true, allows callers to create Certificates by specifying a CSR.
         */
        allowCsrBasedIssuance: boolean;
    }

    export interface CaPoolIssuancePolicyAllowedKeyType {
        /**
         * Represents an allowed Elliptic Curve key type.
         * Structure is documented below.
         */
        ellipticCurve?: outputs.certificateauthority.CaPoolIssuancePolicyAllowedKeyTypeEllipticCurve;
        /**
         * Describes an RSA key that may be used in a Certificate issued from a CaPool.
         * Structure is documented below.
         */
        rsa?: outputs.certificateauthority.CaPoolIssuancePolicyAllowedKeyTypeRsa;
    }

    export interface CaPoolIssuancePolicyAllowedKeyTypeEllipticCurve {
        /**
         * The algorithm used.
         * Possible values are: `ECDSA_P256`, `ECDSA_P384`, `EDDSA_25519`.
         */
        signatureAlgorithm: string;
    }

    export interface CaPoolIssuancePolicyAllowedKeyTypeRsa {
        /**
         * The maximum allowed RSA modulus size, in bits. If this is not set, or if set to zero, the
         * service will not enforce an explicit upper bound on RSA modulus sizes.
         */
        maxModulusSize?: string;
        /**
         * The minimum allowed RSA modulus size, in bits. If this is not set, or if set to zero, the
         * service-level min RSA modulus size will continue to apply.
         */
        minModulusSize?: string;
    }

    export interface CaPoolIssuancePolicyBaselineValues {
        /**
         * Specifies an X.509 extension, which may be used in different parts of X.509 objects like certificates, CSRs, and CRLs.
         * Structure is documented below.
         */
        additionalExtensions?: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesAdditionalExtension[];
        /**
         * Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the
         * "Authority Information Access" extension in the certificate.
         */
        aiaOcspServers?: string[];
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        caOptions: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesCaOptions;
        /**
         * Indicates the intended use for keys that correspond to a certificate.
         * Structure is documented below.
         */
        keyUsage: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesKeyUsage;
        /**
         * Describes the X.509 name constraints extension.
         * Structure is documented below.
         */
        nameConstraints?: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesNameConstraints;
        /**
         * Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
         * Structure is documented below.
         */
        policyIds?: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesPolicyId[];
    }

    export interface CaPoolIssuancePolicyBaselineValuesAdditionalExtension {
        /**
         * Indicates whether or not this extension is critical (i.e., if the client does not know how to
         * handle this extension, the client should consider this to be an error).
         */
        critical: boolean;
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        objectId: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesAdditionalExtensionObjectId;
        /**
         * The value of this X.509 extension. A base64-encoded string.
         */
        value: string;
    }

    export interface CaPoolIssuancePolicyBaselineValuesAdditionalExtensionObjectId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CaPoolIssuancePolicyBaselineValuesCaOptions {
        /**
         * When true, the "CA" in Basic Constraints extension will be set to true.
         */
        isCa?: boolean;
        /**
         * Refers to the "path length constraint" in Basic Constraints extension. For a CA certificate, this value describes the depth of
         * subordinate CA certificates that are allowed. If this value is less than 0, the request will fail.
         */
        maxIssuerPathLength?: number;
        /**
         * When true, the "CA" in Basic Constraints extension will be set to false.
         * If both `isCa` and `nonCa` are unset, the extension will be omitted from the CA certificate.
         */
        nonCa?: boolean;
        /**
         * When true, the "path length constraint" in Basic Constraints extension will be set to 0.
         * if both `maxIssuerPathLength` and `zeroMaxIssuerPathLength` are unset,
         * the max path length will be omitted from the CA certificate.
         */
        zeroMaxIssuerPathLength?: boolean;
    }

    export interface CaPoolIssuancePolicyBaselineValuesKeyUsage {
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        baseKeyUsage: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesKeyUsageBaseKeyUsage;
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        extendedKeyUsage: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesKeyUsageExtendedKeyUsage;
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         * Structure is documented below.
         */
        unknownExtendedKeyUsages?: outputs.certificateauthority.CaPoolIssuancePolicyBaselineValuesKeyUsageUnknownExtendedKeyUsage[];
    }

    export interface CaPoolIssuancePolicyBaselineValuesKeyUsageBaseKeyUsage {
        /**
         * The key may be used to sign certificates.
         */
        certSign?: boolean;
        /**
         * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
         */
        contentCommitment?: boolean;
        /**
         * The key may be used sign certificate revocation lists.
         */
        crlSign?: boolean;
        /**
         * The key may be used to encipher data.
         */
        dataEncipherment?: boolean;
        /**
         * The key may be used to decipher only.
         */
        decipherOnly?: boolean;
        /**
         * The key may be used for digital signatures.
         */
        digitalSignature?: boolean;
        /**
         * The key may be used to encipher only.
         */
        encipherOnly?: boolean;
        /**
         * The key may be used in a key agreement protocol.
         */
        keyAgreement?: boolean;
        /**
         * The key may be used to encipher other keys.
         */
        keyEncipherment?: boolean;
    }

    export interface CaPoolIssuancePolicyBaselineValuesKeyUsageExtendedKeyUsage {
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
         */
        clientAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
         */
        codeSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
         */
        emailProtection?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
         */
        ocspSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
         */
        serverAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
         */
        timeStamping?: boolean;
    }

    export interface CaPoolIssuancePolicyBaselineValuesKeyUsageUnknownExtendedKeyUsage {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CaPoolIssuancePolicyBaselineValuesNameConstraints {
        /**
         * Indicates whether or not the name constraints are marked critical.
         */
        critical: boolean;
        /**
         * Contains excluded DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        excludedDnsNames?: string[];
        /**
         * Contains the excluded email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        excludedEmailAddresses?: string[];
        /**
         * Contains the excluded IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        excludedIpRanges?: string[];
        /**
         * Contains the excluded URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        excludedUris?: string[];
        /**
         * Contains permitted DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        permittedDnsNames?: string[];
        /**
         * Contains the permitted email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        permittedEmailAddresses?: string[];
        /**
         * Contains the permitted IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        permittedIpRanges?: string[];
        /**
         * Contains the permitted URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        permittedUris?: string[];
    }

    export interface CaPoolIssuancePolicyBaselineValuesPolicyId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CaPoolIssuancePolicyIdentityConstraints {
        /**
         * If this is set, the SubjectAltNames extension may be copied from a certificate request into the signed certificate.
         * Otherwise, the requested SubjectAltNames will be discarded.
         */
        allowSubjectAltNamesPassthrough: boolean;
        /**
         * If this is set, the Subject field may be copied from a certificate request into the signed certificate.
         * Otherwise, the requested Subject will be discarded.
         */
        allowSubjectPassthrough: boolean;
        /**
         * A CEL expression that may be used to validate the resolved X.509 Subject and/or Subject Alternative Name before a
         * certificate is signed. To see the full allowed syntax and some examples,
         * see https://cloud.google.com/certificate-authority-service/docs/cel-guide
         * Structure is documented below.
         */
        celExpression?: outputs.certificateauthority.CaPoolIssuancePolicyIdentityConstraintsCelExpression;
    }

    export interface CaPoolIssuancePolicyIdentityConstraintsCelExpression {
        /**
         * Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface CaPoolPublishingOptions {
        /**
         * Specifies the encoding format of each CertificateAuthority's CA
         * certificate and CRLs. If this is omitted, CA certificates and CRLs
         * will be published in PEM.
         * Possible values are: `PEM`, `DER`.
         */
        encodingFormat?: string;
        /**
         * When true, publishes each CertificateAuthority's CA certificate and includes its URL in the "Authority Information Access"
         * X.509 extension in all issued Certificates. If this is false, the CA certificate will not be published and the corresponding
         * X.509 extension will not be written in issued certificates.
         */
        publishCaCert: boolean;
        /**
         * When true, publishes each CertificateAuthority's CRL and includes its URL in the "CRL Distribution Points" X.509 extension
         * in all issued Certificates. If this is false, CRLs will not be published and the corresponding X.509 extension will not
         * be written in issued certificates. CRLs will expire 7 days from their creation. However, we will rebuild daily. CRLs are
         * also rebuilt shortly after a certificate is revoked.
         */
        publishCrl: boolean;
    }

    export interface CertificateCertificateDescription {
        /**
         * (Output)
         * Describes lists of issuer CA certificate URLs that appear in the "Authority Information Access" extension in the certificate.
         */
        aiaIssuingCertificateUrls: string[];
        /**
         * (Output)
         * Identifies the subjectKeyId of the parent certificate, per https://tools.ietf.org/html/rfc5280#section-4.2.1.1
         * Structure is documented below.
         */
        authorityKeyIds: outputs.certificateauthority.CertificateCertificateDescriptionAuthorityKeyId[];
        /**
         * (Output)
         * The hash of the x.509 certificate.
         * Structure is documented below.
         */
        certFingerprints: outputs.certificateauthority.CertificateCertificateDescriptionCertFingerprint[];
        /**
         * (Output, Deprecated)
         * Describes some of the technical fields in a certificate.
         * Structure is documented below.
         *
         * @deprecated `config_values` is deprecated and will be removed in a future release. Use `x509_description` instead.
         */
        configValues: outputs.certificateauthority.CertificateCertificateDescriptionConfigValue[];
        /**
         * (Output)
         * Describes a list of locations to obtain CRL information, i.e. the DistributionPoint.fullName described by https://tools.ietf.org/html/rfc5280#section-4.2.1.13
         */
        crlDistributionPoints: string[];
        /**
         * A PublicKey describes a public key.
         * Structure is documented below.
         *
         *
         * <a name="nestedX509Config"></a>The `x509Config` block supports:
         */
        publicKeys: outputs.certificateauthority.CertificateCertificateDescriptionPublicKey[];
        /**
         * (Output)
         * Describes some of the values in a certificate that are related to the subject and lifetime.
         * Structure is documented below.
         */
        subjectDescriptions: outputs.certificateauthority.CertificateCertificateDescriptionSubjectDescription[];
        /**
         * (Output)
         * Provides a means of identifiying certificates that contain a particular public key, per https://tools.ietf.org/html/rfc5280#section-4.2.1.2.
         * Structure is documented below.
         */
        subjectKeyIds: outputs.certificateauthority.CertificateCertificateDescriptionSubjectKeyId[];
        /**
         * (Output)
         * A structured description of the issued X.509 certificate.
         * Structure is documented below.
         */
        x509Descriptions: outputs.certificateauthority.CertificateCertificateDescriptionX509Description[];
    }

    export interface CertificateCertificateDescriptionAuthorityKeyId {
        /**
         * (Output)
         * Optional. The value of this KeyId encoded in lowercase hexadecimal. This is most likely the 160 bit SHA-1 hash of the public key.
         */
        keyId: string;
    }

    export interface CertificateCertificateDescriptionCertFingerprint {
        /**
         * (Output)
         * The SHA 256 hash, encoded in hexadecimal, of the DER x509 certificate.
         */
        sha256Hash: string;
    }

    export interface CertificateCertificateDescriptionConfigValue {
        /**
         * Indicates the intended use for keys that correspond to a certificate.
         * Structure is documented below.
         */
        keyUsages: outputs.certificateauthority.CertificateCertificateDescriptionConfigValueKeyUsage[];
    }

    export interface CertificateCertificateDescriptionConfigValueKeyUsage {
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        baseKeyUsages: outputs.certificateauthority.CertificateCertificateDescriptionConfigValueKeyUsageBaseKeyUsage[];
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        extendedKeyUsages: outputs.certificateauthority.CertificateCertificateDescriptionConfigValueKeyUsageExtendedKeyUsage[];
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         * Structure is documented below.
         */
        unknownExtendedKeyUsages: outputs.certificateauthority.CertificateCertificateDescriptionConfigValueKeyUsageUnknownExtendedKeyUsage[];
    }

    export interface CertificateCertificateDescriptionConfigValueKeyUsageBaseKeyUsage {
        /**
         * (Output)
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        keyUsageOptions: outputs.certificateauthority.CertificateCertificateDescriptionConfigValueKeyUsageBaseKeyUsageKeyUsageOption[];
    }

    export interface CertificateCertificateDescriptionConfigValueKeyUsageBaseKeyUsageKeyUsageOption {
        /**
         * The key may be used to sign certificates.
         */
        certSign: boolean;
        /**
         * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
         */
        contentCommitment: boolean;
        /**
         * The key may be used sign certificate revocation lists.
         */
        crlSign: boolean;
        /**
         * The key may be used to encipher data.
         */
        dataEncipherment: boolean;
        /**
         * The key may be used to decipher only.
         */
        decipherOnly: boolean;
        /**
         * The key may be used for digital signatures.
         */
        digitalSignature: boolean;
        /**
         * The key may be used to encipher only.
         */
        encipherOnly: boolean;
        /**
         * The key may be used in a key agreement protocol.
         */
        keyAgreement: boolean;
        /**
         * The key may be used to encipher other keys.
         */
        keyEncipherment: boolean;
    }

    export interface CertificateCertificateDescriptionConfigValueKeyUsageExtendedKeyUsage {
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
         */
        clientAuth: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
         */
        codeSigning: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
         */
        emailProtection: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
         */
        ocspSigning: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
         */
        serverAuth: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
         */
        timeStamping: boolean;
    }

    export interface CertificateCertificateDescriptionConfigValueKeyUsageUnknownExtendedKeyUsage {
        /**
         * (Output)
         * Required. Describes how some of the technical fields in a certificate should be populated.
         * Structure is documented below.
         */
        obectIds: outputs.certificateauthority.CertificateCertificateDescriptionConfigValueKeyUsageUnknownExtendedKeyUsageObectId[];
    }

    export interface CertificateCertificateDescriptionConfigValueKeyUsageUnknownExtendedKeyUsageObectId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         *
         * (Required)
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         *
         * (Required)
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateCertificateDescriptionPublicKey {
        /**
         * The format of the public key. Currently, only PEM format is supported.
         * Possible values are: `KEY_TYPE_UNSPECIFIED`, `PEM`.
         */
        format: string;
        /**
         * Required. A public key. When this is specified in a request, the padding and encoding can be any of the options described by the respective 'KeyType' value. When this is generated by the service, it will always be an RFC 5280 SubjectPublicKeyInfo structure containing an algorithm identifier and a key. A base64-encoded string.
         */
        key: string;
    }

    export interface CertificateCertificateDescriptionSubjectDescription {
        /**
         * (Output)
         * The serial number encoded in lowercase hexadecimal.
         */
        hexSerialNumber: string;
        /**
         * The desired lifetime of the CA certificate. Used to create the "notBeforeTime" and
         * "notAfterTime" fields inside an X.509 certificate. A duration in seconds with up to nine
         * fractional digits, terminated by 's'. Example: "3.5s".
         */
        lifetime: string;
        /**
         * (Output)
         * The time at which the certificate expires.
         */
        notAfterTime: string;
        /**
         * (Output)
         * The time at which the certificate becomes valid.
         */
        notBeforeTime: string;
        /**
         * The subject alternative name fields.
         * Structure is documented below.
         */
        subjectAltNames: outputs.certificateauthority.CertificateCertificateDescriptionSubjectDescriptionSubjectAltName[];
        /**
         * Contains distinguished name fields such as the location and organization.
         * Structure is documented below.
         */
        subjects: outputs.certificateauthority.CertificateCertificateDescriptionSubjectDescriptionSubject[];
    }

    export interface CertificateCertificateDescriptionSubjectDescriptionSubject {
        /**
         * The common name of the distinguished name.
         */
        commonName: string;
        /**
         * The country code of the subject.
         */
        countryCode: string;
        /**
         * The locality or city of the subject.
         */
        locality: string;
        /**
         * The organization of the subject.
         */
        organization: string;
        /**
         * The organizational unit of the subject.
         */
        organizationalUnit: string;
        /**
         * The postal code of the subject.
         */
        postalCode: string;
        /**
         * The province, territory, or regional state of the subject.
         */
        province: string;
        /**
         * The street address of the subject.
         */
        streetAddress: string;
    }

    export interface CertificateCertificateDescriptionSubjectDescriptionSubjectAltName {
        /**
         * (Output)
         * Contains additional subject alternative name values.
         * Structure is documented below.
         */
        customSans: outputs.certificateauthority.CertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSan[];
        /**
         * Contains only valid, fully-qualified host names.
         */
        dnsNames: string[];
        /**
         * Contains only valid RFC 2822 E-mail addresses.
         */
        emailAddresses: string[];
        /**
         * Contains only valid 32-bit IPv4 addresses or RFC 4291 IPv6 addresses.
         */
        ipAddresses: string[];
        /**
         * Contains only valid RFC 3986 URIs.
         */
        uris: string[];
    }

    export interface CertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSan {
        /**
         * Indicates whether or not this extension is critical (i.e., if the client does not know how to
         * handle this extension, the client should consider this to be an error).
         *
         * (Required)
         * Indicates whether or not the name constraints are marked critical.
         */
        critical: boolean;
        /**
         * (Output)
         * Required. Describes how some of the technical fields in a certificate should be populated.
         * Structure is documented below.
         */
        obectIds: outputs.certificateauthority.CertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSanObectId[];
        /**
         * The value of this X.509 extension. A base64-encoded string.
         */
        value: string;
    }

    export interface CertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSanObectId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         *
         * (Required)
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         *
         * (Required)
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateCertificateDescriptionSubjectKeyId {
        /**
         * (Output)
         * Optional. The value of this KeyId encoded in lowercase hexadecimal. This is most likely the 160 bit SHA-1 hash of the public key.
         */
        keyId: string;
    }

    export interface CertificateCertificateDescriptionX509Description {
        /**
         * Specifies an X.509 extension, which may be used in different parts of X.509 objects like certificates, CSRs, and CRLs.
         * Structure is documented below.
         */
        additionalExtensions: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionAdditionalExtension[];
        /**
         * Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the
         * "Authority Information Access" extension in the certificate.
         */
        aiaOcspServers: string[];
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        caOptions: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionCaOption[];
        /**
         * Indicates the intended use for keys that correspond to a certificate.
         * Structure is documented below.
         */
        keyUsages: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionKeyUsage[];
        /**
         * Describes the X.509 name constraints extension.
         * Structure is documented below.
         */
        nameConstraints: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionNameConstraint[];
        /**
         * Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
         * Structure is documented below.
         */
        policyIds: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionPolicyId[];
    }

    export interface CertificateCertificateDescriptionX509DescriptionAdditionalExtension {
        /**
         * Indicates whether or not this extension is critical (i.e., if the client does not know how to
         * handle this extension, the client should consider this to be an error).
         */
        critical: boolean;
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        objectIds: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionAdditionalExtensionObjectId[];
        /**
         * The value of this X.509 extension. A base64-encoded string.
         */
        value?: string;
    }

    export interface CertificateCertificateDescriptionX509DescriptionAdditionalExtensionObjectId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateCertificateDescriptionX509DescriptionCaOption {
        /**
         * When true, the "CA" in Basic Constraints extension will be set to true.
         */
        isCa: boolean;
        /**
         * Refers to the "path length constraint" in Basic Constraints extension. For a CA certificate, this value describes the depth of
         * subordinate CA certificates that are allowed. If this value is less than 0, the request will fail.
         */
        maxIssuerPathLength: number;
    }

    export interface CertificateCertificateDescriptionX509DescriptionKeyUsage {
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        baseKeyUsages: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionKeyUsageBaseKeyUsage[];
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        extendedKeyUsages: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionKeyUsageExtendedKeyUsage[];
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         * Structure is documented below.
         */
        unknownExtendedKeyUsages: outputs.certificateauthority.CertificateCertificateDescriptionX509DescriptionKeyUsageUnknownExtendedKeyUsage[];
    }

    export interface CertificateCertificateDescriptionX509DescriptionKeyUsageBaseKeyUsage {
        /**
         * The key may be used to sign certificates.
         */
        certSign: boolean;
        /**
         * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
         */
        contentCommitment: boolean;
        /**
         * The key may be used sign certificate revocation lists.
         */
        crlSign: boolean;
        /**
         * The key may be used to encipher data.
         */
        dataEncipherment: boolean;
        /**
         * The key may be used to decipher only.
         */
        decipherOnly: boolean;
        /**
         * The key may be used for digital signatures.
         */
        digitalSignature: boolean;
        /**
         * The key may be used to encipher only.
         */
        encipherOnly: boolean;
        /**
         * The key may be used in a key agreement protocol.
         */
        keyAgreement: boolean;
        /**
         * The key may be used to encipher other keys.
         */
        keyEncipherment: boolean;
    }

    export interface CertificateCertificateDescriptionX509DescriptionKeyUsageExtendedKeyUsage {
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
         */
        clientAuth: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
         */
        codeSigning: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
         */
        emailProtection: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
         */
        ocspSigning: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
         */
        serverAuth: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
         */
        timeStamping: boolean;
    }

    export interface CertificateCertificateDescriptionX509DescriptionKeyUsageUnknownExtendedKeyUsage {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateCertificateDescriptionX509DescriptionNameConstraint {
        /**
         * Indicates whether or not the name constraints are marked critical.
         */
        critical: boolean;
        /**
         * Contains excluded DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        excludedDnsNames: string[];
        /**
         * Contains the excluded email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        excludedEmailAddresses: string[];
        /**
         * Contains the excluded IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        excludedIpRanges: string[];
        /**
         * Contains the excluded URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        excludedUris: string[];
        /**
         * Contains permitted DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        permittedDnsNames: string[];
        /**
         * Contains the permitted email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        permittedEmailAddresses: string[];
        /**
         * Contains the permitted IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        permittedIpRanges: string[];
        /**
         * Contains the permitted URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        permittedUris: string[];
    }

    export interface CertificateCertificateDescriptionX509DescriptionPolicyId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateConfig {
        /**
         * A PublicKey describes a public key.
         * Structure is documented below.
         *
         *
         * <a name="nestedX509Config"></a>The `x509Config` block supports:
         */
        publicKey: outputs.certificateauthority.CertificateConfigPublicKey;
        /**
         * Specifies some of the values in a certificate that are related to the subject.
         * Structure is documented below.
         */
        subjectConfig: outputs.certificateauthority.CertificateConfigSubjectConfig;
        /**
         * Describes how some of the technical X.509 fields in a certificate should be populated.
         * Structure is documented below.
         */
        x509Config: outputs.certificateauthority.CertificateConfigX509Config;
    }

    export interface CertificateConfigPublicKey {
        /**
         * The format of the public key. Currently, only PEM format is supported.
         * Possible values are: `KEY_TYPE_UNSPECIFIED`, `PEM`.
         */
        format: string;
        /**
         * Required. A public key. When this is specified in a request, the padding and encoding can be any of the options described by the respective 'KeyType' value. When this is generated by the service, it will always be an RFC 5280 SubjectPublicKeyInfo structure containing an algorithm identifier and a key. A base64-encoded string.
         */
        key?: string;
    }

    export interface CertificateConfigSubjectConfig {
        /**
         * Contains distinguished name fields such as the location and organization.
         * Structure is documented below.
         */
        subject: outputs.certificateauthority.CertificateConfigSubjectConfigSubject;
        /**
         * The subject alternative name fields.
         * Structure is documented below.
         */
        subjectAltName?: outputs.certificateauthority.CertificateConfigSubjectConfigSubjectAltName;
    }

    export interface CertificateConfigSubjectConfigSubject {
        /**
         * The common name of the distinguished name.
         */
        commonName: string;
        /**
         * The country code of the subject.
         */
        countryCode?: string;
        /**
         * The locality or city of the subject.
         */
        locality?: string;
        /**
         * The organization of the subject.
         */
        organization: string;
        /**
         * The organizational unit of the subject.
         */
        organizationalUnit?: string;
        /**
         * The postal code of the subject.
         */
        postalCode?: string;
        /**
         * The province, territory, or regional state of the subject.
         */
        province?: string;
        /**
         * The street address of the subject.
         */
        streetAddress?: string;
    }

    export interface CertificateConfigSubjectConfigSubjectAltName {
        /**
         * Contains only valid, fully-qualified host names.
         */
        dnsNames?: string[];
        /**
         * Contains only valid RFC 2822 E-mail addresses.
         */
        emailAddresses?: string[];
        /**
         * Contains only valid 32-bit IPv4 addresses or RFC 4291 IPv6 addresses.
         */
        ipAddresses?: string[];
        /**
         * Contains only valid RFC 3986 URIs.
         */
        uris?: string[];
    }

    export interface CertificateConfigX509Config {
        /**
         * Specifies an X.509 extension, which may be used in different parts of X.509 objects like certificates, CSRs, and CRLs.
         * Structure is documented below.
         */
        additionalExtensions?: outputs.certificateauthority.CertificateConfigX509ConfigAdditionalExtension[];
        /**
         * Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the
         * "Authority Information Access" extension in the certificate.
         */
        aiaOcspServers?: string[];
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        caOptions?: outputs.certificateauthority.CertificateConfigX509ConfigCaOptions;
        /**
         * Indicates the intended use for keys that correspond to a certificate.
         * Structure is documented below.
         */
        keyUsage: outputs.certificateauthority.CertificateConfigX509ConfigKeyUsage;
        /**
         * Describes the X.509 name constraints extension.
         * Structure is documented below.
         */
        nameConstraints?: outputs.certificateauthority.CertificateConfigX509ConfigNameConstraints;
        /**
         * Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
         * Structure is documented below.
         */
        policyIds?: outputs.certificateauthority.CertificateConfigX509ConfigPolicyId[];
    }

    export interface CertificateConfigX509ConfigAdditionalExtension {
        /**
         * Indicates whether or not this extension is critical (i.e., if the client does not know how to
         * handle this extension, the client should consider this to be an error).
         */
        critical: boolean;
        /**
         * Describes values that are relevant in a CA certificate.
         * Structure is documented below.
         */
        objectId: outputs.certificateauthority.CertificateConfigX509ConfigAdditionalExtensionObjectId;
        /**
         * The value of this X.509 extension. A base64-encoded string.
         */
        value: string;
    }

    export interface CertificateConfigX509ConfigAdditionalExtensionObjectId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateConfigX509ConfigCaOptions {
        /**
         * When true, the "CA" in Basic Constraints extension will be set to true.
         */
        isCa?: boolean;
        /**
         * Refers to the "path length constraint" in Basic Constraints extension. For a CA certificate, this value describes the depth of
         * subordinate CA certificates that are allowed. If this value is less than 0, the request will fail.
         */
        maxIssuerPathLength?: number;
        /**
         * When true, the "CA" in Basic Constraints extension will be set to false.
         * If both `isCa` and `nonCa` are unset, the extension will be omitted from the CA certificate.
         */
        nonCa?: boolean;
        /**
         * When true, the "path length constraint" in Basic Constraints extension will be set to 0.
         * if both `maxIssuerPathLength` and `zeroMaxIssuerPathLength` are unset,
         * the max path length will be omitted from the CA certificate.
         */
        zeroMaxIssuerPathLength?: boolean;
    }

    export interface CertificateConfigX509ConfigKeyUsage {
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        baseKeyUsage: outputs.certificateauthority.CertificateConfigX509ConfigKeyUsageBaseKeyUsage;
        /**
         * Describes high-level ways in which a key may be used.
         * Structure is documented below.
         */
        extendedKeyUsage: outputs.certificateauthority.CertificateConfigX509ConfigKeyUsageExtendedKeyUsage;
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         * Structure is documented below.
         */
        unknownExtendedKeyUsages?: outputs.certificateauthority.CertificateConfigX509ConfigKeyUsageUnknownExtendedKeyUsage[];
    }

    export interface CertificateConfigX509ConfigKeyUsageBaseKeyUsage {
        /**
         * The key may be used to sign certificates.
         */
        certSign?: boolean;
        /**
         * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
         */
        contentCommitment?: boolean;
        /**
         * The key may be used sign certificate revocation lists.
         */
        crlSign?: boolean;
        /**
         * The key may be used to encipher data.
         */
        dataEncipherment?: boolean;
        /**
         * The key may be used to decipher only.
         */
        decipherOnly?: boolean;
        /**
         * The key may be used for digital signatures.
         */
        digitalSignature?: boolean;
        /**
         * The key may be used to encipher only.
         */
        encipherOnly?: boolean;
        /**
         * The key may be used in a key agreement protocol.
         */
        keyAgreement?: boolean;
        /**
         * The key may be used to encipher other keys.
         */
        keyEncipherment?: boolean;
    }

    export interface CertificateConfigX509ConfigKeyUsageExtendedKeyUsage {
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
         */
        clientAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
         */
        codeSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
         */
        emailProtection?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
         */
        ocspSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
         */
        serverAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
         */
        timeStamping?: boolean;
    }

    export interface CertificateConfigX509ConfigKeyUsageUnknownExtendedKeyUsage {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateConfigX509ConfigNameConstraints {
        /**
         * Indicates whether or not the name constraints are marked critical.
         */
        critical: boolean;
        /**
         * Contains excluded DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        excludedDnsNames?: string[];
        /**
         * Contains the excluded email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        excludedEmailAddresses?: string[];
        /**
         * Contains the excluded IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        excludedIpRanges?: string[];
        /**
         * Contains the excluded URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        excludedUris?: string[];
        /**
         * Contains permitted DNS names. Any DNS name that can be
         * constructed by simply adding zero or more labels to
         * the left-hand side of the name satisfies the name constraint.
         * For example, `example.com`, `www.example.com`, `www.sub.example.com`
         * would satisfy `example.com` while `example1.com` does not.
         */
        permittedDnsNames?: string[];
        /**
         * Contains the permitted email addresses. The value can be a particular
         * email address, a hostname to indicate all email addresses on that host or
         * a domain with a leading period (e.g. `.example.com`) to indicate
         * all email addresses in that domain.
         */
        permittedEmailAddresses?: string[];
        /**
         * Contains the permitted IP ranges. For IPv4 addresses, the ranges
         * are expressed using CIDR notation as specified in RFC 4632.
         * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
         * addresses.
         */
        permittedIpRanges?: string[];
        /**
         * Contains the permitted URIs that apply to the host part of the name.
         * The value can be a hostname or a domain with a
         * leading period (like `.example.com`)
         */
        permittedUris?: string[];
    }

    export interface CertificateConfigX509ConfigPolicyId {
        /**
         * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
         */
        objectIdPaths: number[];
    }

    export interface CertificateRevocationDetail {
        /**
         * (Output)
         * Indicates why a Certificate was revoked.
         */
        revocationState: string;
        /**
         * (Output)
         * The time at which this Certificate was revoked.
         */
        revocationTime: string;
    }

    export interface CertificateTemplateIamBindingCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface CertificateTemplateIamMemberCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface CertificateTemplateIdentityConstraints {
        /**
         * Required. If this is true, the SubjectAltNames extension may be copied from a certificate request into the signed certificate. Otherwise, the requested SubjectAltNames will be discarded.
         */
        allowSubjectAltNamesPassthrough: boolean;
        /**
         * Required. If this is true, the Subject field may be copied from a certificate request into the signed certificate. Otherwise, the requested Subject will be discarded.
         */
        allowSubjectPassthrough: boolean;
        /**
         * Optional. A CEL expression that may be used to validate the resolved X.509 Subject and/or Subject Alternative Name before a certificate is signed. To see the full allowed syntax and some examples, see https://cloud.google.com/certificate-authority-service/docs/using-cel
         */
        celExpression?: outputs.certificateauthority.CertificateTemplateIdentityConstraintsCelExpression;
    }

    export interface CertificateTemplateIdentityConstraintsCelExpression {
        /**
         * Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression?: string;
        /**
         * Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
         */
        location?: string;
        /**
         * Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface CertificateTemplatePassthroughExtensions {
        /**
         * Optional. A set of ObjectIds identifying custom X.509 extensions. Will be combined with knownExtensions to determine the full set of X.509 extensions.
         */
        additionalExtensions?: outputs.certificateauthority.CertificateTemplatePassthroughExtensionsAdditionalExtension[];
        /**
         * Optional. A set of named X.509 extensions. Will be combined with additionalExtensions to determine the full set of X.509 extensions.
         */
        knownExtensions?: string[];
    }

    export interface CertificateTemplatePassthroughExtensionsAdditionalExtension {
        /**
         * Required. The parts of an OID path. The most significant parts of the path come first.
         */
        objectIdPaths: number[];
    }

    export interface CertificateTemplatePredefinedValues {
        /**
         * Optional. Describes custom X.509 extensions.
         */
        additionalExtensions?: outputs.certificateauthority.CertificateTemplatePredefinedValuesAdditionalExtension[];
        /**
         * Optional. Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the "Authority Information Access" extension in the certificate.
         */
        aiaOcspServers?: string[];
        /**
         * Optional. Describes options in this X509Parameters that are relevant in a CA certificate.
         */
        caOptions?: outputs.certificateauthority.CertificateTemplatePredefinedValuesCaOptions;
        /**
         * Optional. Indicates the intended use for keys that correspond to a certificate.
         */
        keyUsage?: outputs.certificateauthority.CertificateTemplatePredefinedValuesKeyUsage;
        /**
         * Optional. Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
         */
        policyIds?: outputs.certificateauthority.CertificateTemplatePredefinedValuesPolicyId[];
    }

    export interface CertificateTemplatePredefinedValuesAdditionalExtension {
        /**
         * Optional. Indicates whether or not this extension is critical (i.e., if the client does not know how to handle this extension, the client should consider this to be an error).
         */
        critical?: boolean;
        /**
         * Required. The OID for this X.509 extension.
         */
        objectId: outputs.certificateauthority.CertificateTemplatePredefinedValuesAdditionalExtensionObjectId;
        /**
         * Required. The value of this X.509 extension.
         */
        value: string;
    }

    export interface CertificateTemplatePredefinedValuesAdditionalExtensionObjectId {
        /**
         * Required. The parts of an OID path. The most significant parts of the path come first.
         *
         * - - -
         */
        objectIdPaths: number[];
    }

    export interface CertificateTemplatePredefinedValuesCaOptions {
        /**
         * Optional. Refers to the "CA" X.509 extension, which is a boolean value. When this value is missing, the extension will be omitted from the CA certificate.
         */
        isCa?: boolean;
        /**
         * Optional. Refers to the path length restriction X.509 extension. For a CA certificate, this value describes the depth of subordinate CA certificates that are allowed. If this value is less than 0, the request will fail. If this value is missing, the max path length will be omitted from the CA certificate.
         */
        maxIssuerPathLength?: number;
    }

    export interface CertificateTemplatePredefinedValuesKeyUsage {
        /**
         * Describes high-level ways in which a key may be used.
         */
        baseKeyUsage?: outputs.certificateauthority.CertificateTemplatePredefinedValuesKeyUsageBaseKeyUsage;
        /**
         * Detailed scenarios in which a key may be used.
         */
        extendedKeyUsage?: outputs.certificateauthority.CertificateTemplatePredefinedValuesKeyUsageExtendedKeyUsage;
        /**
         * Used to describe extended key usages that are not listed in the KeyUsage.ExtendedKeyUsageOptions message.
         */
        unknownExtendedKeyUsages?: outputs.certificateauthority.CertificateTemplatePredefinedValuesKeyUsageUnknownExtendedKeyUsage[];
    }

    export interface CertificateTemplatePredefinedValuesKeyUsageBaseKeyUsage {
        /**
         * The key may be used to sign certificates.
         */
        certSign?: boolean;
        /**
         * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
         */
        contentCommitment?: boolean;
        /**
         * The key may be used sign certificate revocation lists.
         */
        crlSign?: boolean;
        /**
         * The key may be used to encipher data.
         */
        dataEncipherment?: boolean;
        /**
         * The key may be used to decipher only.
         */
        decipherOnly?: boolean;
        /**
         * The key may be used for digital signatures.
         */
        digitalSignature?: boolean;
        /**
         * The key may be used to encipher only.
         */
        encipherOnly?: boolean;
        /**
         * The key may be used in a key agreement protocol.
         */
        keyAgreement?: boolean;
        /**
         * The key may be used to encipher other keys.
         */
        keyEncipherment?: boolean;
    }

    export interface CertificateTemplatePredefinedValuesKeyUsageExtendedKeyUsage {
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
         */
        clientAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
         */
        codeSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
         */
        emailProtection?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
         */
        ocspSigning?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
         */
        serverAuth?: boolean;
        /**
         * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
         */
        timeStamping?: boolean;
    }

    export interface CertificateTemplatePredefinedValuesKeyUsageUnknownExtendedKeyUsage {
        /**
         * Required. The parts of an OID path. The most significant parts of the path come first.
         */
        objectIdPaths: number[];
    }

    export interface CertificateTemplatePredefinedValuesPolicyId {
        /**
         * Required. The parts of an OID path. The most significant parts of the path come first.
         */
        objectIdPaths: number[];
    }

    export interface GetAuthorityAccessUrl {
        caCertificateAccessUrl: string;
        crlAccessUrls: string[];
    }

    export interface GetAuthorityConfig {
        subjectConfigs: outputs.certificateauthority.GetAuthorityConfigSubjectConfig[];
        x509Configs: outputs.certificateauthority.GetAuthorityConfigX509Config[];
    }

    export interface GetAuthorityConfigSubjectConfig {
        subjectAltNames: outputs.certificateauthority.GetAuthorityConfigSubjectConfigSubjectAltName[];
        subjects: outputs.certificateauthority.GetAuthorityConfigSubjectConfigSubject[];
    }

    export interface GetAuthorityConfigSubjectConfigSubject {
        commonName: string;
        countryCode: string;
        locality: string;
        organization: string;
        organizationalUnit: string;
        postalCode: string;
        province: string;
        streetAddress: string;
    }

    export interface GetAuthorityConfigSubjectConfigSubjectAltName {
        dnsNames: string[];
        emailAddresses: string[];
        ipAddresses: string[];
        uris: string[];
    }

    export interface GetAuthorityConfigX509Config {
        additionalExtensions: outputs.certificateauthority.GetAuthorityConfigX509ConfigAdditionalExtension[];
        aiaOcspServers: string[];
        caOptions: outputs.certificateauthority.GetAuthorityConfigX509ConfigCaOption[];
        keyUsages: outputs.certificateauthority.GetAuthorityConfigX509ConfigKeyUsage[];
        nameConstraints: outputs.certificateauthority.GetAuthorityConfigX509ConfigNameConstraint[];
        policyIds: outputs.certificateauthority.GetAuthorityConfigX509ConfigPolicyId[];
    }

    export interface GetAuthorityConfigX509ConfigAdditionalExtension {
        critical: boolean;
        objectIds: outputs.certificateauthority.GetAuthorityConfigX509ConfigAdditionalExtensionObjectId[];
        value: string;
    }

    export interface GetAuthorityConfigX509ConfigAdditionalExtensionObjectId {
        objectIdPaths: number[];
    }

    export interface GetAuthorityConfigX509ConfigCaOption {
        isCa: boolean;
        maxIssuerPathLength: number;
        nonCa: boolean;
        zeroMaxIssuerPathLength: boolean;
    }

    export interface GetAuthorityConfigX509ConfigKeyUsage {
        baseKeyUsages: outputs.certificateauthority.GetAuthorityConfigX509ConfigKeyUsageBaseKeyUsage[];
        extendedKeyUsages: outputs.certificateauthority.GetAuthorityConfigX509ConfigKeyUsageExtendedKeyUsage[];
        unknownExtendedKeyUsages: outputs.certificateauthority.GetAuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage[];
    }

    export interface GetAuthorityConfigX509ConfigKeyUsageBaseKeyUsage {
        certSign: boolean;
        contentCommitment: boolean;
        crlSign: boolean;
        dataEncipherment: boolean;
        decipherOnly: boolean;
        digitalSignature: boolean;
        encipherOnly: boolean;
        keyAgreement: boolean;
        keyEncipherment: boolean;
    }

    export interface GetAuthorityConfigX509ConfigKeyUsageExtendedKeyUsage {
        clientAuth: boolean;
        codeSigning: boolean;
        emailProtection: boolean;
        ocspSigning: boolean;
        serverAuth: boolean;
        timeStamping: boolean;
    }

    export interface GetAuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage {
        objectIdPaths: number[];
    }

    export interface GetAuthorityConfigX509ConfigNameConstraint {
        critical: boolean;
        excludedDnsNames: string[];
        excludedEmailAddresses: string[];
        excludedIpRanges: string[];
        excludedUris: string[];
        permittedDnsNames: string[];
        permittedEmailAddresses: string[];
        permittedIpRanges: string[];
        permittedUris: string[];
    }

    export interface GetAuthorityConfigX509ConfigPolicyId {
        objectIdPaths: number[];
    }

    export interface GetAuthorityKeySpec {
        algorithm: string;
        cloudKmsKeyVersion: string;
    }

    export interface GetAuthoritySubordinateConfig {
        certificateAuthority: string;
        pemIssuerChains: outputs.certificateauthority.GetAuthoritySubordinateConfigPemIssuerChain[];
    }

    export interface GetAuthoritySubordinateConfigPemIssuerChain {
        pemCertificates: string[];
    }

}

export namespace certificatemanager {
    export interface CertificateIssuanceConfigCertificateAuthorityConfig {
        /**
         * Defines a CertificateAuthorityServiceConfig.
         * Structure is documented below.
         */
        certificateAuthorityServiceConfig?: outputs.certificatemanager.CertificateIssuanceConfigCertificateAuthorityConfigCertificateAuthorityServiceConfig;
    }

    export interface CertificateIssuanceConfigCertificateAuthorityConfigCertificateAuthorityServiceConfig {
        /**
         * A CA pool resource used to issue a certificate.
         * The CA pool string has a relative resource path following the form
         * "projects/{project}/locations/{location}/caPools/{caPool}".
         *
         * - - -
         */
        caPool: string;
    }

    export interface CertificateManaged {
        /**
         * (Output)
         * Detailed state of the latest authorization attempt for each domain
         * specified for this Managed Certificate.
         * Structure is documented below.
         *
         *
         * <a name="nestedProvisioningIssue"></a>The `provisioningIssue` block contains:
         */
        authorizationAttemptInfos: outputs.certificatemanager.CertificateManagedAuthorizationAttemptInfo[];
        /**
         * Authorizations that will be used for performing domain authorization. Either issuanceConfig or dnsAuthorizations should be specificed, but not both.
         */
        dnsAuthorizations?: string[];
        /**
         * The domains for which a managed SSL certificate will be generated.
         * Wildcard domains are only supported with DNS challenge resolution
         */
        domains?: string[];
        /**
         * The resource name for a CertificateIssuanceConfig used to configure private PKI certificates in the format projects/*&#47;locations/*&#47;certificateIssuanceConfigs/*.
         * If this field is not set, the certificates will instead be publicly signed as documented at https://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs#caa.
         * Either issuanceConfig or dnsAuthorizations should be specificed, but not both.
         */
        issuanceConfig?: string;
        /**
         * (Output)
         * Information about issues with provisioning this Managed Certificate.
         * Structure is documented below.
         */
        provisioningIssues: outputs.certificatemanager.CertificateManagedProvisioningIssue[];
        /**
         * (Output)
         * State of the domain for managed certificate issuance.
         */
        state: string;
    }

    export interface CertificateManagedAuthorizationAttemptInfo {
        /**
         * (Output)
         * Human readable explanation about the issue. Provided to help address
         * the configuration issues.
         * Not guaranteed to be stable. For programmatic access use `reason` field.
         *
         * <a name="nestedAuthorizationAttemptInfo"></a>The `authorizationAttemptInfo` block contains:
         *
         * (Output)
         * Human readable explanation for reaching the state. Provided to help
         * address the configuration issues.
         * Not guaranteed to be stable. For programmatic access use `failureReason` field.
         */
        details: string;
        /**
         * (Output)
         * Domain name of the authorization attempt.
         */
        domain: string;
        /**
         * (Output)
         * Reason for failure of the authorization attempt for the domain.
         */
        failureReason: string;
        /**
         * (Output)
         * A state of this Managed Certificate.
         *
         * (Output)
         * State of the domain for managed certificate issuance.
         */
        state: string;
    }

    export interface CertificateManagedProvisioningIssue {
        /**
         * (Output)
         * Human readable explanation about the issue. Provided to help address
         * the configuration issues.
         * Not guaranteed to be stable. For programmatic access use `reason` field.
         *
         * <a name="nestedAuthorizationAttemptInfo"></a>The `authorizationAttemptInfo` block contains:
         *
         * (Output)
         * Human readable explanation for reaching the state. Provided to help
         * address the configuration issues.
         * Not guaranteed to be stable. For programmatic access use `failureReason` field.
         */
        details: string;
        /**
         * (Output)
         * Reason for provisioning failures.
         */
        reason: string;
    }

    export interface CertificateMapGclbTarget {
        /**
         * An IP configuration where this Certificate Map is serving
         * Structure is documented below.
         */
        ipConfigs?: outputs.certificatemanager.CertificateMapGclbTargetIpConfig[];
        /**
         * Proxy name must be in the format projects/*&#47;locations/*&#47;targetHttpsProxies/*.
         * This field is part of a union field `targetProxy`: Only one of `targetHttpsProxy` or
         * `targetSslProxy` may be set.
         */
        targetHttpsProxy?: string;
        /**
         * Proxy name must be in the format projects/*&#47;locations/*&#47;targetSslProxies/*.
         * This field is part of a union field `targetProxy`: Only one of `targetHttpsProxy` or
         * `targetSslProxy` may be set.
         */
        targetSslProxy?: string;
    }

    export interface CertificateMapGclbTargetIpConfig {
        /**
         * An external IP address
         */
        ipAddress?: string;
        /**
         * A list of ports
         */
        ports?: number[];
    }

    export interface CertificateSelfManaged {
        /**
         * (Optional, Deprecated)
         * The certificate chain in PEM-encoded form.
         * Leaf certificate comes first, followed by intermediate ones if any.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         *
         * > **Warning:** `certificatePem` is deprecated and will be removed in a future major release. Use `pemCertificate` instead.
         *
         * @deprecated `certificate_pem` is deprecated and will be removed in a future major release. Use `pem_certificate` instead.
         */
        certificatePem?: string;
        /**
         * The certificate chain in PEM-encoded form.
         * Leaf certificate comes first, followed by intermediate ones if any.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        pemCertificate?: string;
        /**
         * The private key of the leaf certificate in PEM-encoded form.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        pemPrivateKey?: string;
        /**
         * (Optional, Deprecated)
         * The private key of the leaf certificate in PEM-encoded form.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         *
         * > **Warning:** `privateKeyPem` is deprecated and will be removed in a future major release. Use `pemPrivateKey` instead.
         *
         * @deprecated `private_key_pem` is deprecated and will be removed in a future major release. Use `pem_private_key` instead.
         */
        privateKeyPem?: string;
    }

    export interface DnsAuthorizationDnsResourceRecord {
        /**
         * (Output)
         * Data of the DNS Resource Record.
         */
        data: string;
        /**
         * Name of the resource; provided by the client when the resource is created.
         * The name must be 1-64 characters long, and match the regular expression [a-zA-Z][a-zA-Z0-9_-]* which means the first character must be a letter,
         * and all following characters must be a dash, underscore, letter or digit.
         *
         *
         * - - -
         */
        name: string;
        /**
         * (Output)
         * Type of the DNS Resource Record.
         */
        type: string;
    }

    export interface GetCertificateMapGclbTarget {
        ipConfigs: outputs.certificatemanager.GetCertificateMapGclbTargetIpConfig[];
        targetHttpsProxy: string;
        targetSslProxy: string;
    }

    export interface GetCertificateMapGclbTargetIpConfig {
        ipAddress: string;
        ports: number[];
    }

    export interface TrustConfigTrustStore {
        /**
         * Set of intermediate CA certificates used for the path building phase of chain validation.
         * The field is currently not supported if trust config is used for the workload certificate feature.
         * Structure is documented below.
         */
        intermediateCas?: outputs.certificatemanager.TrustConfigTrustStoreIntermediateCa[];
        /**
         * List of Trust Anchors to be used while performing validation against a given TrustStore.
         * Structure is documented below.
         */
        trustAnchors?: outputs.certificatemanager.TrustConfigTrustStoreTrustAnchor[];
    }

    export interface TrustConfigTrustStoreIntermediateCa {
        /**
         * PEM intermediate certificate used for building up paths for validation.
         * Each certificate provided in PEM format may occupy up to 5kB.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        pemCertificate?: string;
    }

    export interface TrustConfigTrustStoreTrustAnchor {
        /**
         * PEM root certificate of the PKI used for validation.
         * Each certificate provided in PEM format may occupy up to 5kB.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        pemCertificate?: string;
    }

}

export namespace cloudasset {
    export interface FolderFeedCondition {
        /**
         * Description of the expression. This is a longer text which describes the expression,
         * e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a file
         * name and a position in the file.
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose.
         * This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface FolderFeedFeedOutputConfig {
        /**
         * Destination on Cloud Pubsub.
         * Structure is documented below.
         */
        pubsubDestination: outputs.cloudasset.FolderFeedFeedOutputConfigPubsubDestination;
    }

    export interface FolderFeedFeedOutputConfigPubsubDestination {
        /**
         * Destination on Cloud Pubsub topic.
         *
         * - - -
         */
        topic: string;
    }

    export interface GetResourcesSearchAllResult {
        /**
         * Additional searchable attributes of this resource. Informational only. The exact set of attributes is subject to change. For example: project id, DNS name etc.
         */
        additionalAttributes: string[];
        /**
         * The type of this resource.
         */
        assetType: string;
        /**
         * One or more paragraphs of text description of this resource. Maximum length could be up to 1M bytes.
         */
        description: string;
        /**
         * The display name of this resource.
         */
        displayName: string;
        /**
         * Labels associated with this resource.
         */
        labels: {[key: string]: string};
        /**
         * Location can be `global`, regional like `us-east1`, or zonal like `us-west1-b`.
         */
        location: string;
        /**
         * The full resource name. See [Resource Names](https://cloud.google.com/apis/design/resource_names#full_resource_name) for more information.
         */
        name: string;
        /**
         * Network tags associated with this resource.
         */
        networkTags: string[];
        /**
         * The project that this resource belongs to, in the form of `projects/{project_number}`.
         */
        project: string;
    }

    export interface OrganizationFeedCondition {
        /**
         * Description of the expression. This is a longer text which describes the expression,
         * e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a file
         * name and a position in the file.
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose.
         * This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface OrganizationFeedFeedOutputConfig {
        /**
         * Destination on Cloud Pubsub.
         * Structure is documented below.
         */
        pubsubDestination: outputs.cloudasset.OrganizationFeedFeedOutputConfigPubsubDestination;
    }

    export interface OrganizationFeedFeedOutputConfigPubsubDestination {
        /**
         * Destination on Cloud Pubsub topic.
         *
         * - - -
         */
        topic: string;
    }

    export interface ProjectFeedCondition {
        /**
         * Description of the expression. This is a longer text which describes the expression,
         * e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a file
         * name and a position in the file.
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose.
         * This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface ProjectFeedFeedOutputConfig {
        /**
         * Destination on Cloud Pubsub.
         * Structure is documented below.
         */
        pubsubDestination: outputs.cloudasset.ProjectFeedFeedOutputConfigPubsubDestination;
    }

    export interface ProjectFeedFeedOutputConfigPubsubDestination {
        /**
         * Destination on Cloud Pubsub topic.
         *
         * - - -
         */
        topic: string;
    }

}

export namespace cloudbuild {
    export interface BitbucketServerConfigConnectedRepository {
        /**
         * Identifier for the project storing the repository.
         */
        projectKey: string;
        /**
         * Identifier for the repository.
         */
        repoSlug: string;
    }

    export interface BitbucketServerConfigSecrets {
        /**
         * The resource name for the admin access token's secret version.
         */
        adminAccessTokenVersionName: string;
        /**
         * The resource name for the read access token's secret version.
         */
        readAccessTokenVersionName: string;
        /**
         * Immutable. The resource name for the webhook secret's secret version. Once this field has been set, it cannot be changed.
         * Changing this field will result in deleting/ recreating the resource.
         *
         * - - -
         */
        webhookSecretVersionName: string;
    }

    export interface GetTriggerApprovalConfig {
        approvalRequired: boolean;
    }

    export interface GetTriggerBitbucketServerTriggerConfig {
        bitbucketServerConfigResource: string;
        projectKey: string;
        pullRequests: outputs.cloudbuild.GetTriggerBitbucketServerTriggerConfigPullRequest[];
        pushes: outputs.cloudbuild.GetTriggerBitbucketServerTriggerConfigPush[];
        repoSlug: string;
    }

    export interface GetTriggerBitbucketServerTriggerConfigPullRequest {
        branch: string;
        commentControl: string;
        invertRegex: boolean;
    }

    export interface GetTriggerBitbucketServerTriggerConfigPush {
        branch: string;
        invertRegex: boolean;
        tag: string;
    }

    export interface GetTriggerBuild {
        artifacts: outputs.cloudbuild.GetTriggerBuildArtifact[];
        availableSecrets: outputs.cloudbuild.GetTriggerBuildAvailableSecret[];
        images: string[];
        logsBucket: string;
        options: outputs.cloudbuild.GetTriggerBuildOption[];
        queueTtl: string;
        secrets: outputs.cloudbuild.GetTriggerBuildSecret[];
        sources: outputs.cloudbuild.GetTriggerBuildSource[];
        steps: outputs.cloudbuild.GetTriggerBuildStep[];
        substitutions: {[key: string]: string};
        tags: string[];
        timeout: string;
    }

    export interface GetTriggerBuildArtifact {
        images: string[];
        objects: outputs.cloudbuild.GetTriggerBuildArtifactObject[];
    }

    export interface GetTriggerBuildArtifactObject {
        /**
         * The Cloud Build location for the trigger.
         *
         * - - -
         */
        location: string;
        paths: string[];
        timings: outputs.cloudbuild.GetTriggerBuildArtifactObjectTiming[];
    }

    export interface GetTriggerBuildArtifactObjectTiming {
        endTime: string;
        startTime: string;
    }

    export interface GetTriggerBuildAvailableSecret {
        secretManagers: outputs.cloudbuild.GetTriggerBuildAvailableSecretSecretManager[];
    }

    export interface GetTriggerBuildAvailableSecretSecretManager {
        env: string;
        versionName: string;
    }

    export interface GetTriggerBuildOption {
        diskSizeGb: number;
        dynamicSubstitutions: boolean;
        envs: string[];
        logStreamingOption: string;
        logging: string;
        machineType: string;
        requestedVerifyOption: string;
        secretEnvs: string[];
        sourceProvenanceHashes: string[];
        substitutionOption: string;
        volumes: outputs.cloudbuild.GetTriggerBuildOptionVolume[];
        workerPool: string;
    }

    export interface GetTriggerBuildOptionVolume {
        name: string;
        path: string;
    }

    export interface GetTriggerBuildSecret {
        kmsKeyName: string;
        secretEnv: {[key: string]: string};
    }

    export interface GetTriggerBuildSource {
        repoSources: outputs.cloudbuild.GetTriggerBuildSourceRepoSource[];
        storageSources: outputs.cloudbuild.GetTriggerBuildSourceStorageSource[];
    }

    export interface GetTriggerBuildSourceRepoSource {
        branchName: string;
        commitSha: string;
        dir: string;
        invertRegex: boolean;
        projectId: string;
        repoName: string;
        substitutions: {[key: string]: string};
        tagName: string;
    }

    export interface GetTriggerBuildSourceStorageSource {
        bucket: string;
        generation: string;
        object: string;
    }

    export interface GetTriggerBuildStep {
        allowExitCodes: number[];
        allowFailure: boolean;
        args: string[];
        dir: string;
        entrypoint: string;
        envs: string[];
        id: string;
        name: string;
        script: string;
        secretEnvs: string[];
        timeout: string;
        timing: string;
        volumes: outputs.cloudbuild.GetTriggerBuildStepVolume[];
        waitFors: string[];
    }

    export interface GetTriggerBuildStepVolume {
        name: string;
        path: string;
    }

    export interface GetTriggerGitFileSource {
        bitbucketServerConfig: string;
        githubEnterpriseConfig: string;
        path: string;
        repoType: string;
        repository: string;
        revision: string;
        uri: string;
    }

    export interface GetTriggerGithub {
        enterpriseConfigResourceName: string;
        name: string;
        owner: string;
        pullRequests: outputs.cloudbuild.GetTriggerGithubPullRequest[];
        pushes: outputs.cloudbuild.GetTriggerGithubPush[];
    }

    export interface GetTriggerGithubPullRequest {
        branch: string;
        commentControl: string;
        invertRegex: boolean;
    }

    export interface GetTriggerGithubPush {
        branch: string;
        invertRegex: boolean;
        tag: string;
    }

    export interface GetTriggerPubsubConfig {
        serviceAccountEmail: string;
        state: string;
        subscription: string;
        topic: string;
    }

    export interface GetTriggerRepositoryEventConfig {
        pullRequests: outputs.cloudbuild.GetTriggerRepositoryEventConfigPullRequest[];
        pushes: outputs.cloudbuild.GetTriggerRepositoryEventConfigPush[];
        repository: string;
    }

    export interface GetTriggerRepositoryEventConfigPullRequest {
        branch: string;
        commentControl: string;
        invertRegex: boolean;
    }

    export interface GetTriggerRepositoryEventConfigPush {
        branch: string;
        invertRegex: boolean;
        tag: string;
    }

    export interface GetTriggerSourceToBuild {
        bitbucketServerConfig: string;
        githubEnterpriseConfig: string;
        ref: string;
        repoType: string;
        repository: string;
        uri: string;
    }

    export interface GetTriggerTriggerTemplate {
        branchName: string;
        commitSha: string;
        dir: string;
        invertRegex: boolean;
        projectId: string;
        repoName: string;
        tagName: string;
    }

    export interface GetTriggerWebhookConfig {
        secret: string;
        state: string;
    }

    export interface TriggerApprovalConfig {
        /**
         * Whether or not approval is needed. If this is set on a build, it will become pending when run,
         * and will need to be explicitly approved to start.
         */
        approvalRequired?: boolean;
    }

    export interface TriggerBitbucketServerTriggerConfig {
        /**
         * The Bitbucket server config resource that this trigger config maps to.
         */
        bitbucketServerConfigResource: string;
        /**
         * Key of the project that the repo is in. For example: The key for https://mybitbucket.server/projects/TEST/repos/test-repo is "TEST".
         */
        projectKey: string;
        /**
         * Filter to match changes in pull requests.
         * Structure is documented below.
         */
        pullRequest?: outputs.cloudbuild.TriggerBitbucketServerTriggerConfigPullRequest;
        /**
         * Filter to match changes in refs like branches, tags.
         * Structure is documented below.
         */
        push?: outputs.cloudbuild.TriggerBitbucketServerTriggerConfigPush;
        /**
         * Slug of the repository. A repository slug is a URL-friendly version of a repository name, automatically generated by Bitbucket for use in the URL.
         * For example, if the repository name is 'test repo', in the URL it would become 'test-repo' as in https://mybitbucket.server/projects/TEST/repos/test-repo.
         */
        repoSlug: string;
    }

    export interface TriggerBitbucketServerTriggerConfigPullRequest {
        /**
         * Regex of branches to match.
         * The syntax of the regular expressions accepted is the syntax accepted by RE2 and described at https://github.com/google/re2/wiki/Syntax
         */
        branch: string;
        /**
         * Configure builds to run whether a repository owner or collaborator need to comment /gcbrun.
         * Possible values are: `COMMENTS_DISABLED`, `COMMENTS_ENABLED`, `COMMENTS_ENABLED_FOR_EXTERNAL_CONTRIBUTORS_ONLY`.
         */
        commentControl?: string;
        /**
         * If true, branches that do NOT match the gitRef will trigger a build.
         */
        invertRegex?: boolean;
    }

    export interface TriggerBitbucketServerTriggerConfigPush {
        /**
         * Regex of branches to match.  Specify only one of branch or tag.
         */
        branch?: string;
        /**
         * When true, only trigger a build if the revision regex does NOT match the gitRef regex.
         */
        invertRegex?: boolean;
        /**
         * Regex of tags to match.  Specify only one of branch or tag.
         */
        tag?: string;
    }

    export interface TriggerBuild {
        /**
         * Artifacts produced by the build that should be uploaded upon successful completion of all build steps.
         * Structure is documented below.
         */
        artifacts?: outputs.cloudbuild.TriggerBuildArtifacts;
        /**
         * Secrets and secret environment variables.
         * Structure is documented below.
         */
        availableSecrets?: outputs.cloudbuild.TriggerBuildAvailableSecrets;
        /**
         * A list of images to be pushed upon the successful completion of all build steps.
         * The images are pushed using the builder service account's credentials.
         * The digests of the pushed images will be stored in the Build resource's results field.
         * If any of the images fail to be pushed, the build status is marked FAILURE.
         */
        images?: string[];
        /**
         * Google Cloud Storage bucket where logs should be written.
         * Logs file names will be of the format ${logsBucket}/log-${build_id}.txt.
         */
        logsBucket?: string;
        /**
         * Special options for this build.
         * Structure is documented below.
         */
        options?: outputs.cloudbuild.TriggerBuildOptions;
        /**
         * TTL in queue for this build. If provided and the build is enqueued longer than this value,
         * the build will expire and the build status will be EXPIRED.
         * The TTL starts ticking from createTime.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        queueTtl?: string;
        /**
         * Secrets to decrypt using Cloud Key Management Service.
         * Structure is documented below.
         */
        secrets?: outputs.cloudbuild.TriggerBuildSecret[];
        /**
         * The location of the source files to build.
         * One of `storageSource` or `repoSource` must be provided.
         * Structure is documented below.
         */
        source?: outputs.cloudbuild.TriggerBuildSource;
        /**
         * The operations to be performed on the workspace.
         * Structure is documented below.
         */
        steps: outputs.cloudbuild.TriggerBuildStep[];
        /**
         * Substitutions data for Build resource.
         */
        substitutions?: {[key: string]: string};
        /**
         * Tags for annotation of a Build. These are not docker tags.
         */
        tags?: string[];
        /**
         * Amount of time that this build should be allowed to run, to second granularity.
         * If this amount of time elapses, work on the build will cease and the build status will be TIMEOUT.
         * This timeout must be equal to or greater than the sum of the timeouts for build steps within the build.
         * The expected format is the number of seconds followed by s.
         * Default time is ten minutes (600s).
         */
        timeout?: string;
    }

    export interface TriggerBuildArtifacts {
        /**
         * A list of images to be pushed upon the successful completion of all build steps.
         * The images will be pushed using the builder service account's credentials.
         * The digests of the pushed images will be stored in the Build resource's results field.
         * If any of the images fail to be pushed, the build is marked FAILURE.
         */
        images?: string[];
        /**
         * A list of objects to be uploaded to Cloud Storage upon successful completion of all build steps.
         * Files in the workspace matching specified paths globs will be uploaded to the
         * Cloud Storage location using the builder service account's credentials.
         * The location and generation of the uploaded objects will be stored in the Build resource's results field.
         * If any objects fail to be pushed, the build is marked FAILURE.
         * Structure is documented below.
         */
        objects?: outputs.cloudbuild.TriggerBuildArtifactsObjects;
    }

    export interface TriggerBuildArtifactsObjects {
        /**
         * Cloud Storage bucket and optional object path, in the form "gs://bucket/path/to/somewhere/".
         * Files in the workspace matching any path pattern will be uploaded to Cloud Storage with
         * this location as a prefix.
         */
        location?: string;
        /**
         * Path globs used to match files in the build's workspace.
         */
        paths?: string[];
        /**
         * (Output)
         * Output only. Stores timing information for pushing all artifact objects.
         * Structure is documented below.
         *
         *
         * <a name="nestedTiming"></a>The `timing` block contains:
         */
        timings: outputs.cloudbuild.TriggerBuildArtifactsObjectsTiming[];
    }

    export interface TriggerBuildArtifactsObjectsTiming {
        /**
         * End of time span.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to
         * nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        endTime?: string;
        /**
         * Start of time span.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to
         * nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        startTime?: string;
    }

    export interface TriggerBuildAvailableSecrets {
        /**
         * Pairs a secret environment variable with a SecretVersion in Secret Manager.
         * Structure is documented below.
         */
        secretManagers: outputs.cloudbuild.TriggerBuildAvailableSecretsSecretManager[];
    }

    export interface TriggerBuildAvailableSecretsSecretManager {
        /**
         * Environment variable name to associate with the secret. Secret environment
         * variables must be unique across all of a build's secrets, and must be used
         * by at least one build step.
         */
        env: string;
        /**
         * Resource name of the SecretVersion. In format: projects/*&#47;secrets/*&#47;versions/*
         */
        versionName: string;
    }

    export interface TriggerBuildOptions {
        /**
         * Requested disk size for the VM that runs the build. Note that this is NOT "disk free";
         * some of the space will be used by the operating system and build utilities.
         * Also note that this is the minimum disk size that will be allocated for the build --
         * the build may run with a larger disk than requested. At present, the maximum disk size
         * is 1000GB; builds that request more than the maximum are rejected with an error.
         */
        diskSizeGb?: number;
        /**
         * Option to specify whether or not to apply bash style string operations to the substitutions.
         * NOTE this is always enabled for triggered builds and cannot be overridden in the build configuration file.
         */
        dynamicSubstitutions?: boolean;
        /**
         * A list of global environment variable definitions that will exist for all build steps
         * in this build. If a variable is defined in both globally and in a build step,
         * the variable will use the build step value.
         * The elements are of the form "KEY=VALUE" for the environment variable "KEY" being given the value "VALUE".
         */
        envs?: string[];
        /**
         * Option to define build log streaming behavior to Google Cloud Storage.
         * Possible values are: `STREAM_DEFAULT`, `STREAM_ON`, `STREAM_OFF`.
         */
        logStreamingOption?: string;
        /**
         * Option to specify the logging mode, which determines if and where build logs are stored.
         * Possible values are: `LOGGING_UNSPECIFIED`, `LEGACY`, `GCS_ONLY`, `STACKDRIVER_ONLY`, `CLOUD_LOGGING_ONLY`, `NONE`.
         */
        logging?: string;
        /**
         * Compute Engine machine type on which to run the build.
         */
        machineType?: string;
        /**
         * Requested verifiability options.
         * Possible values are: `NOT_VERIFIED`, `VERIFIED`.
         */
        requestedVerifyOption?: string;
        /**
         * A list of global environment variables, which are encrypted using a Cloud Key Management
         * Service crypto key. These values must be specified in the build's Secret. These variables
         * will be available to all build steps in this build.
         */
        secretEnvs?: string[];
        /**
         * Requested hash for SourceProvenance.
         * Each value may be one of: `NONE`, `SHA256`, `MD5`.
         */
        sourceProvenanceHashes?: string[];
        /**
         * Option to specify behavior when there is an error in the substitution checks.
         * NOTE this is always set to ALLOW_LOOSE for triggered builds and cannot be overridden
         * in the build configuration file.
         * Possible values are: `MUST_MATCH`, `ALLOW_LOOSE`.
         */
        substitutionOption?: string;
        /**
         * Global list of volumes to mount for ALL build steps
         * Each volume is created as an empty volume prior to starting the build process.
         * Upon completion of the build, volumes and their contents are discarded. Global
         * volume names and paths cannot conflict with the volumes defined a build step.
         * Using a global volume in a build with only one step is not valid as it is indicative
         * of a build request with an incorrect configuration.
         * Structure is documented below.
         */
        volumes?: outputs.cloudbuild.TriggerBuildOptionsVolume[];
        /**
         * Option to specify a WorkerPool for the build. Format projects/{project}/workerPools/{workerPool}
         * This field is experimental.
         */
        workerPool?: string;
    }

    export interface TriggerBuildOptionsVolume {
        /**
         * Name of the volume to mount.
         * Volume names must be unique per build step and must be valid names for Docker volumes.
         * Each named volume must be used by at least two build steps.
         */
        name?: string;
        /**
         * Path at which to mount the volume.
         * Paths must be absolute and cannot conflict with other volume paths on the same
         * build step or with certain reserved volume paths.
         */
        path?: string;
    }

    export interface TriggerBuildSecret {
        /**
         * Cloud KMS key name to use to decrypt these envs.
         */
        kmsKeyName: string;
        /**
         * Map of environment variable name to its encrypted value.
         * Secret environment variables must be unique across all of a build's secrets,
         * and must be used by at least one build step. Values can be at most 64 KB in size.
         * There can be at most 100 secret values across all of a build's secrets.
         */
        secretEnv?: {[key: string]: string};
    }

    export interface TriggerBuildSource {
        /**
         * Location of the source in a Google Cloud Source Repository.
         * Structure is documented below.
         */
        repoSource?: outputs.cloudbuild.TriggerBuildSourceRepoSource;
        /**
         * Location of the source in an archive file in Google Cloud Storage.
         * Structure is documented below.
         */
        storageSource?: outputs.cloudbuild.TriggerBuildSourceStorageSource;
    }

    export interface TriggerBuildSourceRepoSource {
        /**
         * Regex matching branches to build. Exactly one a of branch name, tag, or commit SHA must be provided.
         * The syntax of the regular expressions accepted is the syntax accepted by RE2 and
         * described at https://github.com/google/re2/wiki/Syntax
         */
        branchName?: string;
        /**
         * Explicit commit SHA to build. Exactly one a of branch name, tag, or commit SHA must be provided.
         */
        commitSha?: string;
        /**
         * Directory, relative to the source root, in which to run the build.
         * This must be a relative path. If a step's dir is specified and is an absolute path,
         * this value is ignored for that step's execution.
         */
        dir?: string;
        /**
         * Only trigger a build if the revision regex does NOT match the revision regex.
         */
        invertRegex?: boolean;
        /**
         * ID of the project that owns the Cloud Source Repository.
         * If omitted, the project ID requesting the build is assumed.
         */
        projectId?: string;
        /**
         * Name of the Cloud Source Repository.
         */
        repoName: string;
        /**
         * Substitutions to use in a triggered build. Should only be used with triggers.run
         */
        substitutions?: {[key: string]: string};
        /**
         * Regex matching tags to build. Exactly one a of branch name, tag, or commit SHA must be provided.
         * The syntax of the regular expressions accepted is the syntax accepted by RE2 and
         * described at https://github.com/google/re2/wiki/Syntax
         */
        tagName?: string;
    }

    export interface TriggerBuildSourceStorageSource {
        /**
         * Google Cloud Storage bucket containing the source.
         */
        bucket: string;
        /**
         * Google Cloud Storage generation for the object.
         * If the generation is omitted, the latest generation will be used
         */
        generation?: string;
        /**
         * Google Cloud Storage object containing the source.
         * This object must be a gzipped archive file (.tar.gz) containing source to build.
         */
        object: string;
    }

    export interface TriggerBuildStep {
        /**
         * Allow this build step to fail without failing the entire build if and
         * only if the exit code is one of the specified codes.
         * If `allowFailure` is also specified, this field will take precedence.
         */
        allowExitCodes?: number[];
        /**
         * Allow this build step to fail without failing the entire build.
         * If false, the entire build will fail if this step fails. Otherwise, the
         * build will succeed, but this step will still have a failure status.
         * Error information will be reported in the `failureDetail` field.
         * `allowExitCodes` takes precedence over this field.
         */
        allowFailure?: boolean;
        /**
         * A list of arguments that will be presented to the step when it is started.
         * If the image used to run the step's container has an entrypoint, the args
         * are used as arguments to that entrypoint. If the image does not define an
         * entrypoint, the first element in args is used as the entrypoint, and the
         * remainder will be used as arguments.
         */
        args?: string[];
        /**
         * Working directory to use when running this step's container.
         * If this value is a relative path, it is relative to the build's working
         * directory. If this value is absolute, it may be outside the build's working
         * directory, in which case the contents of the path may not be persisted
         * across build step executions, unless a `volume` for that path is specified.
         * If the build specifies a `RepoSource` with `dir` and a step with a
         * `dir`,
         * which specifies an absolute path, the `RepoSource` `dir` is ignored
         * for the step's execution.
         */
        dir?: string;
        /**
         * Entrypoint to be used instead of the build step image's
         * default entrypoint.
         * If unset, the image's default entrypoint is used
         */
        entrypoint?: string;
        /**
         * A list of environment variable definitions to be used when
         * running a step.
         * The elements are of the form "KEY=VALUE" for the environment variable
         * "KEY" being given the value "VALUE".
         */
        envs?: string[];
        /**
         * Unique identifier for this build step, used in `waitFor` to
         * reference this build step as a dependency.
         */
        id?: string;
        /**
         * The name of the container image that will run this particular build step.
         * If the image is available in the host's Docker daemon's cache, it will be
         * run directly. If not, the host will attempt to pull the image first, using
         * the builder service account's credentials if necessary.
         * The Docker daemon's cache will already have the latest versions of all of
         * the officially supported build steps (see https://github.com/GoogleCloudPlatform/cloud-builders
         * for images and examples).
         * The Docker daemon will also have cached many of the layers for some popular
         * images, like "ubuntu", "debian", but they will be refreshed at the time
         * you attempt to use them.
         * If you built an image in a previous build step, it will be stored in the
         * host's Docker daemon's cache and is available to use as the name for a
         * later build step.
         */
        name: string;
        /**
         * A shell script to be executed in the step.
         * When script is provided, the user cannot specify the entrypoint or args.
         */
        script?: string;
        /**
         * A list of environment variables which are encrypted using
         * a Cloud Key
         * Management Service crypto key. These values must be specified in
         * the build's `Secret`.
         */
        secretEnvs?: string[];
        /**
         * Time limit for executing this build step. If not defined,
         * the step has no
         * time limit and will be allowed to continue to run until either it
         * completes or the build itself times out.
         */
        timeout?: string;
        /**
         * Output only. Stores timing information for executing this
         * build step.
         */
        timing?: string;
        /**
         * List of volumes to mount into the build step.
         * Each volume is created as an empty volume prior to execution of the
         * build step. Upon completion of the build, volumes and their contents
         * are discarded.
         * Using a named volume in only one step is not valid as it is
         * indicative of a build request with an incorrect configuration.
         * Structure is documented below.
         */
        volumes?: outputs.cloudbuild.TriggerBuildStepVolume[];
        /**
         * The ID(s) of the step(s) that this build step depends on.
         * This build step will not start until all the build steps in `waitFor`
         * have completed successfully. If `waitFor` is empty, this build step
         * will start when all previous build steps in the `Build.Steps` list
         * have completed successfully.
         */
        waitFors?: string[];
    }

    export interface TriggerBuildStepVolume {
        /**
         * Name of the volume to mount.
         * Volume names must be unique per build step and must be valid names for Docker volumes.
         * Each named volume must be used by at least two build steps.
         */
        name: string;
        /**
         * Path at which to mount the volume.
         * Paths must be absolute and cannot conflict with other volume paths on the same
         * build step or with certain reserved volume paths.
         */
        path: string;
    }

    export interface TriggerGitFileSource {
        /**
         * The full resource name of the bitbucket server config.
         * Format: projects/{project}/locations/{location}/bitbucketServerConfigs/{id}.
         */
        bitbucketServerConfig?: string;
        /**
         * The full resource name of the github enterprise config.
         * Format: projects/{project}/locations/{location}/githubEnterpriseConfigs/{id}. projects/{project}/githubEnterpriseConfigs/{id}.
         */
        githubEnterpriseConfig?: string;
        /**
         * The path of the file, with the repo root as the root of the path.
         */
        path: string;
        /**
         * The type of the repo, since it may not be explicit from the repo field (e.g from a URL).
         * Values can be UNKNOWN, CLOUD_SOURCE_REPOSITORIES, GITHUB, BITBUCKET_SERVER
         * Possible values are: `UNKNOWN`, `CLOUD_SOURCE_REPOSITORIES`, `GITHUB`, `BITBUCKET_SERVER`.
         */
        repoType: string;
        /**
         * The fully qualified resource name of the Repo API repository. The fully qualified resource name of the Repo API repository.
         * If unspecified, the repo from which the trigger invocation originated is assumed to be the repo from which to read the specified path.
         */
        repository?: string;
        /**
         * The branch, tag, arbitrary ref, or SHA version of the repo to use when resolving the
         * filename (optional). This field respects the same syntax/resolution as described here: https://git-scm.com/docs/gitrevisions
         * If unspecified, the revision from which the trigger invocation originated is assumed to be the revision from which to read the specified path.
         */
        revision?: string;
        /**
         * The URI of the repo (optional). If unspecified, the repo from which the trigger
         * invocation originated is assumed to be the repo from which to read the specified path.
         */
        uri?: string;
    }

    export interface TriggerGithub {
        /**
         * The resource name of the github enterprise config that should be applied to this installation.
         * For example: "projects/{$projectId}/locations/{$locationId}/githubEnterpriseConfigs/{$configId}"
         */
        enterpriseConfigResourceName?: string;
        /**
         * Name of the repository. For example: The name for
         * https://github.com/googlecloudplatform/cloud-builders is "cloud-builders".
         */
        name?: string;
        /**
         * Owner of the repository. For example: The owner for
         * https://github.com/googlecloudplatform/cloud-builders is "googlecloudplatform".
         */
        owner?: string;
        /**
         * filter to match changes in pull requests. Specify only one of `pullRequest` or `push`.
         * Structure is documented below.
         */
        pullRequest?: outputs.cloudbuild.TriggerGithubPullRequest;
        /**
         * filter to match changes in refs, like branches or tags. Specify only one of `pullRequest` or `push`.
         * Structure is documented below.
         */
        push?: outputs.cloudbuild.TriggerGithubPush;
    }

    export interface TriggerGithubPullRequest {
        /**
         * Regex of branches to match.
         * The syntax of the regular expressions accepted is the syntax accepted by RE2 and described at https://github.com/google/re2/wiki/Syntax
         */
        branch: string;
        /**
         * Configure builds to run whether a repository owner or collaborator need to comment /gcbrun.
         * Possible values are: `COMMENTS_DISABLED`, `COMMENTS_ENABLED`, `COMMENTS_ENABLED_FOR_EXTERNAL_CONTRIBUTORS_ONLY`.
         */
        commentControl?: string;
        /**
         * If true, branches that do NOT match the gitRef will trigger a build.
         */
        invertRegex?: boolean;
    }

    export interface TriggerGithubPush {
        /**
         * Regex of branches to match.  Specify only one of branch or tag.
         */
        branch?: string;
        /**
         * When true, only trigger a build if the revision regex does NOT match the gitRef regex.
         */
        invertRegex?: boolean;
        /**
         * Regex of tags to match.  Specify only one of branch or tag.
         */
        tag?: string;
    }

    export interface TriggerPubsubConfig {
        /**
         * Service account that will make the push request.
         */
        serviceAccountEmail?: string;
        /**
         * (Output)
         * Potential issues with the underlying Pub/Sub subscription configuration.
         * Only populated on get requests.
         */
        state: string;
        /**
         * (Output)
         * Output only. Name of the subscription.
         */
        subscription: string;
        /**
         * The name of the topic from which this subscription is receiving messages.
         */
        topic: string;
    }

    export interface TriggerRepositoryEventConfig {
        /**
         * Contains filter properties for matching Pull Requests.
         * Structure is documented below.
         */
        pullRequest?: outputs.cloudbuild.TriggerRepositoryEventConfigPullRequest;
        /**
         * Contains filter properties for matching git pushes.
         * Structure is documented below.
         */
        push?: outputs.cloudbuild.TriggerRepositoryEventConfigPush;
        /**
         * The resource name of the Repo API resource.
         */
        repository?: string;
    }

    export interface TriggerRepositoryEventConfigPullRequest {
        /**
         * Regex of branches to match.
         * The syntax of the regular expressions accepted is the syntax accepted by RE2 and described at https://github.com/google/re2/wiki/Syntax
         */
        branch?: string;
        /**
         * Configure builds to run whether a repository owner or collaborator need to comment /gcbrun.
         * Possible values are: `COMMENTS_DISABLED`, `COMMENTS_ENABLED`, `COMMENTS_ENABLED_FOR_EXTERNAL_CONTRIBUTORS_ONLY`.
         */
        commentControl?: string;
        /**
         * If true, branches that do NOT match the gitRef will trigger a build.
         */
        invertRegex?: boolean;
    }

    export interface TriggerRepositoryEventConfigPush {
        /**
         * Regex of branches to match.  Specify only one of branch or tag.
         */
        branch?: string;
        /**
         * When true, only trigger a build if the revision regex does NOT match the gitRef regex.
         */
        invertRegex?: boolean;
        /**
         * Regex of tags to match.  Specify only one of branch or tag.
         */
        tag?: string;
    }

    export interface TriggerSourceToBuild {
        /**
         * The full resource name of the bitbucket server config.
         * Format: projects/{project}/locations/{location}/bitbucketServerConfigs/{id}.
         */
        bitbucketServerConfig?: string;
        /**
         * The full resource name of the github enterprise config.
         * Format: projects/{project}/locations/{location}/githubEnterpriseConfigs/{id}. projects/{project}/githubEnterpriseConfigs/{id}.
         */
        githubEnterpriseConfig?: string;
        /**
         * The branch or tag to use. Must start with "refs/" (required).
         */
        ref: string;
        /**
         * The type of the repo, since it may not be explicit from the repo field (e.g from a URL).
         * Values can be UNKNOWN, CLOUD_SOURCE_REPOSITORIES, GITHUB, BITBUCKET_SERVER
         * Possible values are: `UNKNOWN`, `CLOUD_SOURCE_REPOSITORIES`, `GITHUB`, `BITBUCKET_SERVER`.
         */
        repoType: string;
        /**
         * The qualified resource name of the Repo API repository.
         * Either uri or repository can be specified and is required.
         */
        repository?: string;
        /**
         * The URI of the repo.
         */
        uri?: string;
    }

    export interface TriggerTriggerTemplate {
        /**
         * Name of the branch to build. Exactly one a of branch name, tag, or commit SHA must be provided.
         * This field is a regular expression.
         */
        branchName?: string;
        /**
         * Explicit commit SHA to build. Exactly one of a branch name, tag, or commit SHA must be provided.
         */
        commitSha?: string;
        /**
         * Directory, relative to the source root, in which to run the build.
         * This must be a relative path. If a step's dir is specified and
         * is an absolute path, this value is ignored for that step's
         * execution.
         */
        dir?: string;
        /**
         * Only trigger a build if the revision regex does NOT match the revision regex.
         */
        invertRegex?: boolean;
        /**
         * ID of the project that owns the Cloud Source Repository. If
         * omitted, the project ID requesting the build is assumed.
         */
        projectId: string;
        /**
         * Name of the Cloud Source Repository. If omitted, the name "default" is assumed.
         */
        repoName?: string;
        /**
         * Name of the tag to build. Exactly one of a branch name, tag, or commit SHA must be provided.
         * This field is a regular expression.
         */
        tagName?: string;
    }

    export interface TriggerWebhookConfig {
        /**
         * Resource name for the secret required as a URL parameter.
         */
        secret: string;
        /**
         * (Output)
         * Potential issues with the underlying Pub/Sub subscription configuration.
         * Only populated on get requests.
         */
        state: string;
    }

    export interface WorkerPoolNetworkConfig {
        /**
         * Immutable. The network definition that the workers are peered to. If this section is left empty, the workers will be peered to `WorkerPool.project_id` on the service producer network. Must be in the format `projects/{project}/global/networks/{network}`, where `{project}` is a project number, such as `12345`, and `{network}` is the name of a VPC network in the project. See (https://cloud.google.com/cloud-build/docs/custom-workers/set-up-custom-worker-pool-environment#understanding_the_network_configuration_options)
         */
        peeredNetwork: string;
        /**
         * Immutable. Subnet IP range within the peered network. This is specified in CIDR notation with a slash and the subnet prefix size. You can optionally specify an IP address before the subnet prefix value. e.g. `192.168.0.0/29` would specify an IP range starting at 192.168.0.0 with a prefix size of 29 bits. `/16` would specify a prefix size of 16 bits, with an automatically determined IP within the peered VPC. If unspecified, a value of `/24` will be used.
         */
        peeredNetworkIpRange?: string;
    }

    export interface WorkerPoolWorkerConfig {
        /**
         * Size of the disk attached to the worker, in GB. See (https://cloud.google.com/cloud-build/docs/custom-workers/worker-pool-config-file). Specify a value of up to 1000. If `0` is specified, Cloud Build will use a standard disk size.
         */
        diskSizeGb?: number;
        /**
         * Machine type of a worker, such as `n1-standard-1`. See (https://cloud.google.com/cloud-build/docs/custom-workers/worker-pool-config-file). If left blank, Cloud Build will use `n1-standard-1`.
         */
        machineType?: string;
        /**
         * If true, workers are created without any public address, which prevents network egress to public IPs.
         */
        noExternalIp: boolean;
    }

}

export namespace cloudbuildv2 {
    export interface ConnectionGithubConfig {
        /**
         * GitHub App installation id.
         */
        appInstallationId?: number;
        /**
         * OAuth credential of the account that authorized the Cloud Build GitHub App. It is recommended to use a robot account instead of a human user account. The OAuth token must be tied to the Cloud Build GitHub App.
         */
        authorizerCredential?: outputs.cloudbuildv2.ConnectionGithubConfigAuthorizerCredential;
    }

    export interface ConnectionGithubConfigAuthorizerCredential {
        /**
         * A SecretManager resource containing the OAuth token that authorizes the Cloud Build connection. Format: `projects/*&#47;secrets/*&#47;versions/*`.
         */
        oauthTokenSecretVersion?: string;
        /**
         * Output only. The username associated to this token.
         */
        username: string;
    }

    export interface ConnectionGithubEnterpriseConfig {
        /**
         * Id of the GitHub App created from the manifest.
         */
        appId?: number;
        /**
         * ID of the installation of the GitHub App.
         */
        appInstallationId?: number;
        /**
         * The URL-friendly name of the GitHub App.
         */
        appSlug?: string;
        /**
         * Required. The URI of the GitHub Enterprise host this connection is for.
         */
        hostUri: string;
        /**
         * SecretManager resource containing the private key of the GitHub App, formatted as `projects/*&#47;secrets/*&#47;versions/*`.
         */
        privateKeySecretVersion?: string;
        /**
         * Configuration for using Service Directory to privately connect to a GitHub Enterprise server. This should only be set if the GitHub Enterprise server is hosted on-premises and not reachable by public internet. If this field is left empty, calls to the GitHub Enterprise server will be made over the public internet.
         */
        serviceDirectoryConfig?: outputs.cloudbuildv2.ConnectionGithubEnterpriseConfigServiceDirectoryConfig;
        /**
         * SSL certificate to use for requests to GitHub Enterprise.
         */
        sslCa?: string;
        /**
         * SecretManager resource containing the webhook secret of the GitHub App, formatted as `projects/*&#47;secrets/*&#47;versions/*`.
         */
        webhookSecretSecretVersion?: string;
    }

    export interface ConnectionGithubEnterpriseConfigServiceDirectoryConfig {
        /**
         * Required. The Service Directory service name. Format: projects/{project}/locations/{location}/namespaces/{namespace}/services/{service}.
         */
        service: string;
    }

    export interface ConnectionGitlabConfig {
        /**
         * Required. A GitLab personal access token with the `api` scope access.
         */
        authorizerCredential: outputs.cloudbuildv2.ConnectionGitlabConfigAuthorizerCredential;
        /**
         * The URI of the GitLab Enterprise host this connection is for. If not specified, the default value is https://gitlab.com.
         */
        hostUri: string;
        /**
         * Required. A GitLab personal access token with the minimum `readApi` scope access.
         */
        readAuthorizerCredential: outputs.cloudbuildv2.ConnectionGitlabConfigReadAuthorizerCredential;
        /**
         * Output only. Version of the GitLab Enterprise server running on the `hostUri`.
         */
        serverVersion: string;
        /**
         * Configuration for using Service Directory to privately connect to a GitLab Enterprise server. This should only be set if the GitLab Enterprise server is hosted on-premises and not reachable by public internet. If this field is left empty, calls to the GitLab Enterprise server will be made over the public internet.
         */
        serviceDirectoryConfig?: outputs.cloudbuildv2.ConnectionGitlabConfigServiceDirectoryConfig;
        /**
         * SSL certificate to use for requests to GitLab Enterprise.
         */
        sslCa?: string;
        /**
         * Required. Immutable. SecretManager resource containing the webhook secret of a GitLab Enterprise project, formatted as `projects/*&#47;secrets/*&#47;versions/*`.
         */
        webhookSecretSecretVersion: string;
    }

    export interface ConnectionGitlabConfigAuthorizerCredential {
        /**
         * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: `projects/*&#47;secrets/*&#47;versions/*`.
         */
        userTokenSecretVersion: string;
        /**
         * Output only. The username associated to this token.
         */
        username: string;
    }

    export interface ConnectionGitlabConfigReadAuthorizerCredential {
        /**
         * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: `projects/*&#47;secrets/*&#47;versions/*`.
         */
        userTokenSecretVersion: string;
        /**
         * Output only. The username associated to this token.
         *
         * - - -
         */
        username: string;
    }

    export interface ConnectionGitlabConfigServiceDirectoryConfig {
        /**
         * Required. The Service Directory service name. Format: projects/{project}/locations/{location}/namespaces/{namespace}/services/{service}.
         */
        service: string;
    }

    export interface ConnectionIAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ConnectionIAMMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ConnectionInstallationState {
        actionUri: string;
        message: string;
        stage: string;
    }

}

export namespace clouddeploy {
    export interface DeliveryPipelineCondition {
        pipelineReadyConditions: outputs.clouddeploy.DeliveryPipelineConditionPipelineReadyCondition[];
        targetsPresentConditions: outputs.clouddeploy.DeliveryPipelineConditionTargetsPresentCondition[];
        targetsTypeConditions: outputs.clouddeploy.DeliveryPipelineConditionTargetsTypeCondition[];
    }

    export interface DeliveryPipelineConditionPipelineReadyCondition {
        status: boolean;
        /**
         * Output only. Most recent time at which the pipeline was updated.
         */
        updateTime: string;
    }

    export interface DeliveryPipelineConditionTargetsPresentCondition {
        missingTargets: string[];
        status: boolean;
        /**
         * Output only. Most recent time at which the pipeline was updated.
         */
        updateTime: string;
    }

    export interface DeliveryPipelineConditionTargetsTypeCondition {
        errorDetails: string;
        status: boolean;
    }

    export interface DeliveryPipelineSerialPipeline {
        /**
         * Each stage specifies configuration for a `Target`. The ordering of this list defines the promotion flow.
         */
        stages?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStage[];
    }

    export interface DeliveryPipelineSerialPipelineStage {
        /**
         * Optional. The deploy parameters to use for the target in this stage.
         */
        deployParameters?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageDeployParameter[];
        /**
         * Skaffold profiles to use when rendering the manifest for this stage's `Target`.
         */
        profiles?: string[];
        /**
         * Optional. The strategy to use for a `Rollout` to this stage.
         */
        strategy?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategy;
        /**
         * The targetId to which this stage points. This field refers exclusively to the last segment of a target name. For example, this field would just be `my-target` (rather than `projects/project/locations/location/targets/my-target`). The location of the `Target` is inferred to be the same as the location of the `DeliveryPipeline` that contains this `Stage`.
         */
        targetId?: string;
    }

    export interface DeliveryPipelineSerialPipelineStageDeployParameter {
        /**
         * Optional. Deploy parameters are applied to targets with match labels. If unspecified, deploy parameters are applied to all targets (including child targets of a multi-target).
         */
        matchTargetLabels?: {[key: string]: string};
        /**
         * Required. Values are deploy parameters in key-value pairs.
         */
        values: {[key: string]: string};
    }

    export interface DeliveryPipelineSerialPipelineStageStrategy {
        /**
         * Canary deployment strategy provides progressive percentage based deployments to a Target.
         */
        canary?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanary;
        /**
         * Standard deployment strategy executes a single deploy and allows verifying the deployment.
         */
        standard?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyStandard;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanary {
        /**
         * Configures the progressive based deployment for a Target.
         */
        canaryDeployment?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeployment;
        /**
         * Configures the progressive based deployment for a Target, but allows customizing at the phase level where a phase represents each of the percentage deployments.
         */
        customCanaryDeployment?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeployment;
        /**
         * Optional. Runtime specific configurations for the deployment strategy. The runtime configuration is used to determine how Cloud Deploy will split traffic to enable a progressive deployment.
         */
        runtimeConfig?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfig;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeployment {
        /**
         * Required. The percentage based deployments that will occur as a part of a `Rollout`. List is expected in ascending order and each integer n is 0 <= n < 100.
         */
        percentages: number[];
        /**
         * (Beta only) Optional. Configuration for the postdeploy job of the last phase. If this is not configured, postdeploy job will not be present.
         */
        postdeploy?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPostdeploy;
        /**
         * (Beta only) Optional. Configuration for the predeploy job of the first phase. If this is not configured, predeploy job will not be present.
         */
        predeploy?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPredeploy;
        /**
         * Whether to run verify tests after each percentage deployment.
         */
        verify?: boolean;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPostdeploy {
        /**
         * Optional. A sequence of skaffold custom actions to invoke during execution of the postdeploy job.
         */
        actions?: string[];
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPredeploy {
        /**
         * Optional. A sequence of skaffold custom actions to invoke during execution of the predeploy job.
         */
        actions?: string[];
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeployment {
        /**
         * Required. Configuration for each phase in the canary deployment in the order executed.
         */
        phaseConfigs: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfig[];
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfig {
        /**
         * Required. Percentage deployment for the phase.
         */
        percentage: number;
        /**
         * Required. The ID to assign to the `Rollout` phase. This value must consist of lower-case letters, numbers, and hyphens, start with a letter and end with a letter or a number, and have a max length of 63 characters. In other words, it must match the following regex: `^a-z?$`.
         */
        phaseId: string;
        /**
         * (Beta only) Optional. Configuration for the postdeploy job of this phase. If this is not configured, postdeploy job will not be present for this phase.
         */
        postdeploy?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPostdeploy;
        /**
         * (Beta only) Optional. Configuration for the predeploy job of this phase. If this is not configured, predeploy job will not be present for this phase.
         */
        predeploy?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPredeploy;
        /**
         * Skaffold profiles to use when rendering the manifest for this phase. These are in addition to the profiles list specified in the `DeliveryPipeline` stage.
         */
        profiles?: string[];
        /**
         * Whether to run verify tests after the deployment.
         *
         * - - -
         */
        verify?: boolean;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPostdeploy {
        /**
         * Optional. A sequence of skaffold custom actions to invoke during execution of the postdeploy job.
         */
        actions?: string[];
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPredeploy {
        /**
         * Optional. A sequence of skaffold custom actions to invoke during execution of the predeploy job.
         */
        actions?: string[];
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfig {
        /**
         * Cloud Run runtime configuration.
         */
        cloudRun?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigCloudRun;
        /**
         * Kubernetes runtime configuration.
         */
        kubernetes?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetes;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigCloudRun {
        /**
         * Whether Cloud Deploy should update the traffic stanza in a Cloud Run Service on the user's behalf to facilitate traffic splitting. This is required to be true for CanaryDeployments, but optional for CustomCanaryDeployments.
         */
        automaticTrafficControl?: boolean;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetes {
        /**
         * Kubernetes Gateway API service mesh configuration.
         */
        gatewayServiceMesh?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesGatewayServiceMesh;
        /**
         * Kubernetes Service networking configuration.
         */
        serviceNetworking?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesServiceNetworking;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesGatewayServiceMesh {
        /**
         * Required. Name of the Kubernetes Deployment whose traffic is managed by the specified HTTPRoute and Service.
         */
        deployment: string;
        /**
         * Required. Name of the Gateway API HTTPRoute.
         */
        httpRoute: string;
        /**
         * Optional. The time to wait for route updates to propagate. The maximum configurable time is 3 hours, in seconds format. If unspecified, there is no wait time.
         */
        routeUpdateWaitTime?: string;
        /**
         * Required. Name of the Kubernetes Service.
         */
        service: string;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesServiceNetworking {
        /**
         * Required. Name of the Kubernetes Deployment whose traffic is managed by the specified Service.
         */
        deployment: string;
        /**
         * Optional. Whether to disable Pod overprovisioning. If Pod overprovisioning is disabled then Cloud Deploy will limit the number of total Pods used for the deployment strategy to the number of Pods the Deployment has on the cluster.
         */
        disablePodOverprovisioning?: boolean;
        /**
         * Required. Name of the Kubernetes Service.
         */
        service: string;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyStandard {
        /**
         * (Beta only) Optional. Configuration for the postdeploy job. If this is not configured, postdeploy job will not be present.
         */
        postdeploy?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyStandardPostdeploy;
        /**
         * (Beta only) Optional. Configuration for the predeploy job. If this is not configured, predeploy job will not be present.
         */
        predeploy?: outputs.clouddeploy.DeliveryPipelineSerialPipelineStageStrategyStandardPredeploy;
        /**
         * Whether to verify a deployment.
         */
        verify?: boolean;
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyStandardPostdeploy {
        /**
         * Optional. A sequence of skaffold custom actions to invoke during execution of the postdeploy job.
         */
        actions?: string[];
    }

    export interface DeliveryPipelineSerialPipelineStageStrategyStandardPredeploy {
        /**
         * Optional. A sequence of skaffold custom actions to invoke during execution of the predeploy job.
         */
        actions?: string[];
    }

    export interface TargetAnthosCluster {
        /**
         * Membership of the GKE Hub-registered cluster to which to apply the Skaffold configuration. Format is `projects/{project}/locations/{location}/memberships/{membership_name}`.
         */
        membership?: string;
    }

    export interface TargetExecutionConfig {
        /**
         * Optional. Cloud Storage location in which to store execution outputs. This can either be a bucket ("gs://my-bucket") or a path within a bucket ("gs://my-bucket/my-dir"). If unspecified, a default bucket located in the same region will be used.
         */
        artifactStorage: string;
        /**
         * Optional. Execution timeout for a Cloud Build Execution. This must be between 10m and 24h in seconds format. If unspecified, a default timeout of 1h is used.
         */
        executionTimeout: string;
        /**
         * Optional. Google service account to use for execution. If unspecified, the project execution service account (-compute@developer.gserviceaccount.com) is used.
         */
        serviceAccount: string;
        /**
         * Required. Usages when this configuration should be applied.
         */
        usages: string[];
        /**
         * Optional. The resource name of the `WorkerPool`, with the format `projects/{project}/locations/{location}/workerPools/{worker_pool}`. If this optional field is unspecified, the default Cloud Build pool will be used.
         */
        workerPool?: string;
    }

    export interface TargetGke {
        /**
         * Information specifying a GKE Cluster. Format is `projects/{project_id}/locations/{location_id}/clusters/{cluster_id}.
         */
        cluster?: string;
        /**
         * Optional. If true, `cluster` is accessed using the private IP address of the control plane endpoint. Otherwise, the default IP address of the control plane endpoint is used. The default IP address is the private IP address for clusters with private control-plane endpoints and the public IP address otherwise. Only specify this option when `cluster` is a [private GKE cluster](https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept).
         */
        internalIp?: boolean;
    }

    export interface TargetMultiTarget {
        /**
         * Required. The targetIds of this multiTarget.
         */
        targetIds: string[];
    }

    export interface TargetRun {
        /**
         * Required. The location where the Cloud Run Service should be located. Format is `projects/{project}/locations/{location}`.
         */
        location: string;
    }

}

export namespace cloudfunctions {
    export interface FunctionEventTrigger {
        /**
         * The type of event to observe. For example: `"google.storage.object.finalize"`.
         * See the documentation on [calling Cloud Functions](https://cloud.google.com/functions/docs/calling/) for a
         * full reference of accepted triggers.
         */
        eventType: string;
        /**
         * Specifies policy for failed executions. Structure is documented below.
         */
        failurePolicy: outputs.cloudfunctions.FunctionEventTriggerFailurePolicy;
        /**
         * Required. The name or partial URI of the resource from
         * which to observe events. For example, `"myBucket"` or `"projects/my-project/topics/my-topic"`
         */
        resource: string;
    }

    export interface FunctionEventTriggerFailurePolicy {
        /**
         * Whether the function should be retried on failure. Defaults to `false`.
         */
        retry: boolean;
    }

    export interface FunctionIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FunctionIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FunctionSecretEnvironmentVariable {
        /**
         * Name of the environment variable.
         */
        key: string;
        /**
         * Project identifier (due to a known limitation, only project number is supported by this field) of the project that contains the secret. If not set, it will be populated with the function's project, assuming that the secret exists in the same project as of the function.
         */
        projectId: string;
        /**
         * ID of the secret in secret manager (not the full resource name).
         */
        secret: string;
        /**
         * Version of the secret (version number or the string "latest"). It is recommended to use a numeric version for secret environment variables as any updates to the secret value is not reflected until new clones start.
         */
        version: string;
    }

    export interface FunctionSecretVolume {
        /**
         * The path within the container to mount the secret volume. For example, setting the mountPath as "/etc/secrets" would mount the secret value files under the "/etc/secrets" directory. This directory will also be completely shadowed and unavailable to mount any other secrets. Recommended mount paths: "/etc/secrets" Restricted mount paths: "/cloudsql", "/dev/log", "/pod", "/proc", "/var/log".
         */
        mountPath: string;
        /**
         * Project identifier (due to a known limitation, only project number is supported by this field) of the project that contains the secret. If not set, it will be populated with the function's project, assuming that the secret exists in the same project as of the function.
         */
        projectId: string;
        /**
         * ID of the secret in secret manager (not the full resource name).
         */
        secret: string;
        /**
         * List of secret versions to mount for this secret. If empty, the "latest" version of the secret will be made available in a file named after the secret under the mount point. Structure is documented below.
         */
        versions?: outputs.cloudfunctions.FunctionSecretVolumeVersion[];
    }

    export interface FunctionSecretVolumeVersion {
        /**
         * Relative path of the file under the mount path where the secret value for this version will be fetched and made available. For example, setting the mountPath as "/etc/secrets" and path as "/secret_foo" would mount the secret value file at "/etc/secrets/secret_foo".
         */
        path: string;
        /**
         * Version of the secret (version number or the string "latest"). It is preferable to use "latest" version with secret volumes as secret value changes are reflected immediately.
         */
        version: string;
    }

    export interface FunctionSourceRepository {
        deployedUrl: string;
        /**
         * The URL pointing to the hosted repository where the function is defined. There are supported Cloud Source Repository URLs in the following formats:
         *
         * * To refer to a specific commit: `https://source.developers.google.com/projects/*&#47;repos/*&#47;revisions/*&#47;paths/*`
         * * To refer to a moveable alias (branch): `https://source.developers.google.com/projects/*&#47;repos/*&#47;moveable-aliases/*&#47;paths/*`. To refer to HEAD, use the `master` moveable alias.
         * * To refer to a specific fixed alias (tag): `https://source.developers.google.com/projects/*&#47;repos/*&#47;fixed-aliases/*&#47;paths/*`
         */
        url: string;
    }

    export interface GetFunctionEventTrigger {
        /**
         * The type of event to observe. For example: `"google.storage.object.finalize"`.
         * See the documentation on [calling Cloud Functions](https://cloud.google.com/functions/docs/calling/)
         * for a full reference of accepted triggers.
         */
        eventType: string;
        /**
         * Policy for failed executions. Structure is documented below.
         */
        failurePolicies: outputs.cloudfunctions.GetFunctionEventTriggerFailurePolicy[];
        /**
         * The name of the resource whose events are being observed, for example, `"myBucket"`
         */
        resource: string;
    }

    export interface GetFunctionEventTriggerFailurePolicy {
        /**
         * Whether the function should be retried on failure.
         */
        retry: boolean;
    }

    export interface GetFunctionSecretEnvironmentVariable {
        key: string;
        projectId: string;
        secret: string;
        version: string;
    }

    export interface GetFunctionSecretVolume {
        mountPath: string;
        projectId: string;
        secret: string;
        versions: outputs.cloudfunctions.GetFunctionSecretVolumeVersion[];
    }

    export interface GetFunctionSecretVolumeVersion {
        path: string;
        version: string;
    }

    export interface GetFunctionSourceRepository {
        deployedUrl: string;
        /**
         * The URL pointing to the hosted repository where the function is defined.
         */
        url: string;
    }

}

export namespace cloudfunctionsv2 {
    export interface FunctionBuildConfig {
        /**
         * (Output)
         * The Cloud Build name of the latest successful
         * deployment of the function.
         */
        build: string;
        /**
         * User managed repository created in Artifact Registry optionally with a customer managed encryption key.
         */
        dockerRepository?: string;
        /**
         * The name of the function (as defined in source code) that will be executed.
         * Defaults to the resource name suffix, if not specified. For backward
         * compatibility, if function with given name is not found, then the system
         * will try to use function named "function". For Node.js this is name of a
         * function exported by the module specified in source_location.
         */
        entryPoint?: string;
        /**
         * User-provided build-time environment variables for the function.
         */
        environmentVariables: {[key: string]: string};
        /**
         * The runtime in which to run the function. Required when deploying a new
         * function, optional when updating an existing function.
         */
        runtime?: string;
        /**
         * The location of the function source code.
         * Structure is documented below.
         */
        source?: outputs.cloudfunctionsv2.FunctionBuildConfigSource;
        /**
         * Name of the Cloud Build Custom Worker Pool that should be used to build the function.
         */
        workerPool?: string;
    }

    export interface FunctionBuildConfigSource {
        /**
         * If provided, get the source from this location in a Cloud Source Repository.
         * Structure is documented below.
         */
        repoSource?: outputs.cloudfunctionsv2.FunctionBuildConfigSourceRepoSource;
        /**
         * If provided, get the source from this location in Google Cloud Storage.
         * Structure is documented below.
         */
        storageSource?: outputs.cloudfunctionsv2.FunctionBuildConfigSourceStorageSource;
    }

    export interface FunctionBuildConfigSourceRepoSource {
        /**
         * Regex matching branches to build.
         */
        branchName?: string;
        /**
         * Regex matching tags to build.
         */
        commitSha?: string;
        /**
         * Directory, relative to the source root, in which to run the build.
         */
        dir?: string;
        /**
         * Only trigger a build if the revision regex does
         * NOT match the revision regex.
         */
        invertRegex?: boolean;
        /**
         * ID of the project that owns the Cloud Source Repository. If omitted, the
         * project ID requesting the build is assumed.
         */
        projectId?: string;
        /**
         * Name of the Cloud Source Repository.
         */
        repoName?: string;
        /**
         * Regex matching tags to build.
         */
        tagName?: string;
    }

    export interface FunctionBuildConfigSourceStorageSource {
        /**
         * Google Cloud Storage bucket containing the source
         */
        bucket?: string;
        /**
         * Google Cloud Storage generation for the object. If the generation
         * is omitted, the latest generation will be used.
         */
        generation: number;
        /**
         * Google Cloud Storage object containing the source.
         */
        object?: string;
    }

    export interface FunctionEventTrigger {
        /**
         * Criteria used to filter events.
         * Structure is documented below.
         */
        eventFilters?: outputs.cloudfunctionsv2.FunctionEventTriggerEventFilter[];
        /**
         * Required. The type of event to observe.
         */
        eventType?: string;
        /**
         * The name of a Pub/Sub topic in the same project that will be used
         * as the transport topic for the event delivery.
         */
        pubsubTopic: string;
        /**
         * Describes the retry policy in case of function's execution failure.
         * Retried execution is charged as any other execution.
         * Possible values are: `RETRY_POLICY_UNSPECIFIED`, `RETRY_POLICY_DO_NOT_RETRY`, `RETRY_POLICY_RETRY`.
         */
        retryPolicy?: string;
        /**
         * The email of the service account for this function.
         */
        serviceAccountEmail: string;
        /**
         * (Output)
         * Output only. The resource name of the Eventarc trigger.
         */
        trigger: string;
        /**
         * The region that the trigger will be in. The trigger will only receive
         * events originating in this region. It can be the same
         * region as the function, a different region or multi-region, or the global
         * region. If not provided, defaults to the same region as the function.
         */
        triggerRegion?: string;
    }

    export interface FunctionEventTriggerEventFilter {
        /**
         * 'Required. The name of a CloudEvents attribute.
         * Currently, only a subset of attributes are supported for filtering. Use the `gcloud eventarc providers describe` command to learn more about events and their attributes.
         * Do not filter for the 'type' attribute here, as this is already achieved by the resource's `eventType` attribute.
         */
        attribute: string;
        /**
         * Optional. The operator used for matching the events with the value of
         * the filter. If not specified, only events that have an exact key-value
         * pair specified in the filter are matched.
         * The only allowed value is `match-path-pattern`.
         * [See documentation on path patterns here](https://cloud.google.com/eventarc/docs/path-patterns)'
         */
        operator?: string;
        /**
         * Required. The value for the attribute.
         * If the operator field is set as `match-path-pattern`, this value can be a path pattern instead of an exact value.
         */
        value: string;
    }

    export interface FunctionIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FunctionIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FunctionServiceConfig {
        /**
         * Whether 100% of traffic is routed to the latest revision. Defaults to true.
         */
        allTrafficOnLatestRevision?: boolean;
        /**
         * The number of CPUs used in a single container instance. Default value is calculated from available memory.
         */
        availableCpu: string;
        /**
         * The amount of memory available for a function.
         * Defaults to 256M. Supported units are k, M, G, Mi, Gi. If no unit is
         * supplied the value is interpreted as bytes.
         */
        availableMemory: string;
        /**
         * Environment variables that shall be available during function execution.
         */
        environmentVariables?: {[key: string]: string};
        /**
         * (Output)
         * URIs of the Service deployed
         */
        gcfUri: string;
        /**
         * Available ingress settings. Defaults to "ALLOW_ALL" if unspecified.
         * Default value is `ALLOW_ALL`.
         * Possible values are: `ALLOW_ALL`, `ALLOW_INTERNAL_ONLY`, `ALLOW_INTERNAL_AND_GCLB`.
         */
        ingressSettings?: string;
        /**
         * The limit on the maximum number of function instances that may coexist at a
         * given time.
         */
        maxInstanceCount: number;
        /**
         * Sets the maximum number of concurrent requests that each instance can receive. Defaults to 1.
         */
        maxInstanceRequestConcurrency: number;
        /**
         * The limit on the minimum number of function instances that may coexist at a
         * given time.
         */
        minInstanceCount?: number;
        /**
         * Secret environment variables configuration.
         * Structure is documented below.
         */
        secretEnvironmentVariables?: outputs.cloudfunctionsv2.FunctionServiceConfigSecretEnvironmentVariable[];
        /**
         * Secret volumes configuration.
         * Structure is documented below.
         */
        secretVolumes?: outputs.cloudfunctionsv2.FunctionServiceConfigSecretVolume[];
        /**
         * Name of the service associated with a Function.
         */
        service: string;
        /**
         * The email of the service account for this function.
         */
        serviceAccountEmail: string;
        /**
         * The function execution timeout. Execution is considered failed and
         * can be terminated if the function is not completed at the end of the
         * timeout period. Defaults to 60 seconds.
         */
        timeoutSeconds: number;
        /**
         * (Output)
         * URI of the Service deployed.
         */
        uri: string;
        /**
         * The Serverless VPC Access connector that this cloud function can connect to.
         */
        vpcConnector?: string;
        /**
         * Available egress settings.
         * Possible values are: `VPC_CONNECTOR_EGRESS_SETTINGS_UNSPECIFIED`, `PRIVATE_RANGES_ONLY`, `ALL_TRAFFIC`.
         */
        vpcConnectorEgressSettings?: string;
    }

    export interface FunctionServiceConfigSecretEnvironmentVariable {
        /**
         * Name of the environment variable.
         */
        key: string;
        /**
         * Project identifier (preferrably project number but can also be the project ID) of the project that contains the secret. If not set, it will be populated with the function's project assuming that the secret exists in the same project as of the function.
         */
        projectId: string;
        /**
         * Name of the secret in secret manager (not the full resource name).
         */
        secret: string;
        /**
         * Version of the secret (version number or the string 'latest'). It is recommended to use a numeric version for secret environment variables as any updates to the secret value is not reflected until new instances start.
         */
        version: string;
    }

    export interface FunctionServiceConfigSecretVolume {
        /**
         * The path within the container to mount the secret volume. For example, setting the mountPath as /etc/secrets would mount the secret value files under the /etc/secrets directory. This directory will also be completely shadowed and unavailable to mount any other secrets. Recommended mount path: /etc/secrets
         */
        mountPath: string;
        /**
         * Project identifier (preferrably project number but can also be the project ID) of the project that contains the secret. If not set, it will be populated with the function's project assuming that the secret exists in the same project as of the function.
         */
        projectId: string;
        /**
         * Name of the secret in secret manager (not the full resource name).
         */
        secret: string;
        /**
         * List of secret versions to mount for this secret. If empty, the latest version of the secret will be made available in a file named after the secret under the mount point.'
         * Structure is documented below.
         */
        versions: outputs.cloudfunctionsv2.FunctionServiceConfigSecretVolumeVersion[];
    }

    export interface FunctionServiceConfigSecretVolumeVersion {
        /**
         * Relative path of the file under the mount path where the secret value for this version will be fetched and made available. For example, setting the mountPath as '/etc/secrets' and path as secretFoo would mount the secret value file at /etc/secrets/secret_foo.
         */
        path: string;
        /**
         * Version of the secret (version number or the string 'latest'). It is preferable to use latest version with secret volumes as secret value changes are reflected immediately.
         */
        version: string;
    }

    export interface GetFunctionBuildConfig {
        build: string;
        dockerRepository: string;
        entryPoint: string;
        environmentVariables: {[key: string]: string};
        runtime: string;
        sources: outputs.cloudfunctionsv2.GetFunctionBuildConfigSource[];
        workerPool: string;
    }

    export interface GetFunctionBuildConfigSource {
        repoSources: outputs.cloudfunctionsv2.GetFunctionBuildConfigSourceRepoSource[];
        storageSources: outputs.cloudfunctionsv2.GetFunctionBuildConfigSourceStorageSource[];
    }

    export interface GetFunctionBuildConfigSourceRepoSource {
        branchName: string;
        commitSha: string;
        dir: string;
        invertRegex: boolean;
        projectId: string;
        repoName: string;
        tagName: string;
    }

    export interface GetFunctionBuildConfigSourceStorageSource {
        bucket: string;
        generation: number;
        object: string;
    }

    export interface GetFunctionEventTrigger {
        eventFilters: outputs.cloudfunctionsv2.GetFunctionEventTriggerEventFilter[];
        eventType: string;
        pubsubTopic: string;
        retryPolicy: string;
        serviceAccountEmail: string;
        trigger: string;
        triggerRegion: string;
    }

    export interface GetFunctionEventTriggerEventFilter {
        attribute: string;
        operator: string;
        value: string;
    }

    export interface GetFunctionServiceConfig {
        allTrafficOnLatestRevision: boolean;
        availableCpu: string;
        availableMemory: string;
        environmentVariables: {[key: string]: string};
        gcfUri: string;
        ingressSettings: string;
        maxInstanceCount: number;
        maxInstanceRequestConcurrency: number;
        minInstanceCount: number;
        secretEnvironmentVariables: outputs.cloudfunctionsv2.GetFunctionServiceConfigSecretEnvironmentVariable[];
        secretVolumes: outputs.cloudfunctionsv2.GetFunctionServiceConfigSecretVolume[];
        service: string;
        serviceAccountEmail: string;
        timeoutSeconds: number;
        uri: string;
        vpcConnector: string;
        vpcConnectorEgressSettings: string;
    }

    export interface GetFunctionServiceConfigSecretEnvironmentVariable {
        key: string;
        projectId: string;
        secret: string;
        version: string;
    }

    export interface GetFunctionServiceConfigSecretVolume {
        mountPath: string;
        projectId: string;
        secret: string;
        versions: outputs.cloudfunctionsv2.GetFunctionServiceConfigSecretVolumeVersion[];
    }

    export interface GetFunctionServiceConfigSecretVolumeVersion {
        path: string;
        version: string;
    }

}

export namespace cloudidentity {
    export interface GetGroupMembershipsMembership {
        createTime: string;
        /**
         * The parent Group resource under which to lookup the Membership names. Must be of the form groups/{group_id}.
         */
        group: string;
        /**
         * EntityKey of the member.  Structure is documented below.
         */
        memberKeys: outputs.cloudidentity.GetGroupMembershipsMembershipMemberKey[];
        /**
         * The name of the MembershipRole. One of OWNER, MANAGER, MEMBER.
         */
        name: string;
        /**
         * EntityKey of the member.  Structure is documented below.
         */
        preferredMemberKeys: outputs.cloudidentity.GetGroupMembershipsMembershipPreferredMemberKey[];
        /**
         * The MembershipRoles that apply to the Membership. Structure is documented below.
         */
        roles: outputs.cloudidentity.GetGroupMembershipsMembershipRole[];
        /**
         * The type of the membership.
         */
        type: string;
        updateTime: string;
    }

    export interface GetGroupMembershipsMembershipMemberKey {
        /**
         * The ID of the entity. For Google-managed entities, the id is the email address of an existing
         * group or user. For external-identity-mapped entities, the id is a string conforming
         * to the Identity Source's requirements.
         */
        id: string;
        /**
         * The namespace in which the entity exists.
         * If not populated, the EntityKey represents a Google-managed entity
         * such as a Google user or a Google Group.
         * If populated, the EntityKey represents an external-identity-mapped group.
         */
        namespace: string;
    }

    export interface GetGroupMembershipsMembershipPreferredMemberKey {
        /**
         * The ID of the entity. For Google-managed entities, the id is the email address of an existing
         * group or user. For external-identity-mapped entities, the id is a string conforming
         * to the Identity Source's requirements.
         */
        id: string;
        /**
         * The namespace in which the entity exists.
         * If not populated, the EntityKey represents a Google-managed entity
         * such as a Google user or a Google Group.
         * If populated, the EntityKey represents an external-identity-mapped group.
         */
        namespace: string;
    }

    export interface GetGroupMembershipsMembershipRole {
        /**
         * The name of the MembershipRole. One of OWNER, MANAGER, MEMBER.
         */
        name: string;
    }

    export interface GetGroupsGroup {
        createTime: string;
        /**
         * An extended description to help users determine the purpose of a Group.
         */
        description: string;
        /**
         * The display name of the Group.
         */
        displayName: string;
        /**
         * EntityKey of the Group.  Structure is documented below.
         */
        groupKeys: outputs.cloudidentity.GetGroupsGroupGroupKey[];
        initialGroupConfig: string;
        /**
         * The labels that apply to the Group.
         * Contains 'cloudidentity.googleapis.com/groups.discussion_forum': '' if the Group is a Google Group or
         * 'system/groups/external': '' if the Group is an external-identity-mapped group.
         */
        labels: {[key: string]: string};
        /**
         * Resource name of the Group in the format: groups/{group_id}, where `groupId` is the unique ID assigned to the Group.
         */
        name: string;
        /**
         * The parent resource under which to list all Groups. Must be of the form identitysources/{identity_source_id} for external- identity-mapped groups or customers/{customer_id} for Google Groups.
         */
        parent: string;
        updateTime: string;
    }

    export interface GetGroupsGroupGroupKey {
        /**
         * The ID of the entity.
         * For Google-managed entities, the id is the email address of an existing group or user.
         * For external-identity-mapped entities, the id is a string conforming
         * to the Identity Source's requirements.
         */
        id: string;
        /**
         * The namespace in which the entity exists.
         * If not populated, the EntityKey represents a Google-managed entity
         * such as a Google user or a Google Group.
         * If populated, the EntityKey represents an external-identity-mapped group.
         * The namespace must correspond to an identity source created in Admin Console
         * and must be in the form of `identitysources/{identity_source_id}`.
         */
        namespace: string;
    }

    export interface GroupGroupKey {
        /**
         * The ID of the entity.
         * For Google-managed entities, the id must be the email address of an existing
         * group or user.
         * For external-identity-mapped entities, the id must be a string conforming
         * to the Identity Source's requirements.
         * Must be unique within a namespace.
         */
        id: string;
        /**
         * The namespace in which the entity exists.
         * If not specified, the EntityKey represents a Google-managed entity
         * such as a Google user or a Google Group.
         * If specified, the EntityKey represents an external-identity-mapped group.
         * The namespace must correspond to an identity source created in Admin Console
         * and must be in the form of `identitysources/{identity_source_id}`.
         *
         * - - -
         */
        namespace?: string;
    }

    export interface GroupMembershipMemberKey {
        /**
         * The ID of the entity.
         * For Google-managed entities, the id must be the email address of an existing
         * group or user.
         * For external-identity-mapped entities, the id must be a string conforming
         * to the Identity Source's requirements.
         * Must be unique within a namespace.
         */
        id: string;
        /**
         * The namespace in which the entity exists.
         * If not specified, the EntityKey represents a Google-managed entity
         * such as a Google user or a Google Group.
         * If specified, the EntityKey represents an external-identity-mapped group.
         * The namespace must correspond to an identity source created in Admin Console
         * and must be in the form of `identitysources/{identity_source_id}`.
         */
        namespace?: string;
    }

    export interface GroupMembershipPreferredMemberKey {
        /**
         * The ID of the entity.
         * For Google-managed entities, the id must be the email address of an existing
         * group or user.
         * For external-identity-mapped entities, the id must be a string conforming
         * to the Identity Source's requirements.
         * Must be unique within a namespace.
         */
        id: string;
        /**
         * The namespace in which the entity exists.
         * If not specified, the EntityKey represents a Google-managed entity
         * such as a Google user or a Google Group.
         * If specified, the EntityKey represents an external-identity-mapped group.
         * The namespace must correspond to an identity source created in Admin Console
         * and must be in the form of `identitysources/{identity_source_id}`.
         */
        namespace?: string;
    }

    export interface GroupMembershipRole {
        /**
         * The name of the MembershipRole. Must be one of OWNER, MANAGER, MEMBER.
         * Possible values are: `OWNER`, `MANAGER`, `MEMBER`.
         *
         * - - -
         */
        name: string;
    }

}

export namespace cloudrun {
    export interface DomainMappingMetadata {
        /**
         * Annotations is a key value map stored with a resource that
         * may be set by external tools to store and retrieve arbitrary metadata. More
         * info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations
         * **Note**: The Cloud Run API may add additional annotations that were not provided in your config.
         * If the provider plan shows a diff where a server-side annotation is added, you can add it to your config
         * or apply the lifecycle.ignore_changes rule to the metadata.0.annotations field.
         *
         * - - -
         */
        annotations: {[key: string]: string};
        /**
         * (Output)
         * A sequence number representing a specific generation of the desired state.
         */
        generation: number;
        /**
         * Map of string keys and values that can be used to organize and categorize
         * (scope and select) objects. May match selectors of replication controllers
         * and routes.
         * More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels
         */
        labels: {[key: string]: string};
        /**
         * In Cloud Run the namespace must be equal to either the
         * project ID or project number.
         */
        namespace: string;
        /**
         * (Output)
         * An opaque value that represents the internal version of this object that
         * can be used by clients to determine when objects have changed. May be used
         * for optimistic concurrency, change detection, and the watch operation on a
         * resource or set of resources. They may only be valid for a
         * particular resource or set of resources.
         * More info:
         * https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency
         */
        resourceVersion: string;
        /**
         * (Output)
         * SelfLink is a URL representing this object.
         */
        selfLink: string;
        /**
         * (Output)
         * UID is a unique id generated by the server on successful creation of a resource and is not
         * allowed to change on PUT operations.
         * More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids
         */
        uid: string;
    }

    export interface DomainMappingSpec {
        /**
         * The mode of the certificate.
         * Default value is `AUTOMATIC`.
         * Possible values are: `NONE`, `AUTOMATIC`.
         */
        certificateMode?: string;
        /**
         * If set, the mapping will override any mapping set before this spec was set.
         * It is recommended that the user leaves this empty to receive an error
         * warning about a potential conflict and only set it once the respective UI
         * has given such a warning.
         */
        forceOverride?: boolean;
        /**
         * The name of the Cloud Run Service that this DomainMapping applies to.
         * The route must exist.
         */
        routeName: string;
    }

    export interface DomainMappingStatus {
        /**
         * (Output)
         * Array of observed DomainMappingConditions, indicating the current state
         * of the DomainMapping.
         * Structure is documented below.
         */
        conditions: outputs.cloudrun.DomainMappingStatusCondition[];
        /**
         * (Output)
         * The name of the route that the mapping currently points to.
         */
        mappedRouteName: string;
        /**
         * (Output)
         * ObservedGeneration is the 'Generation' of the DomainMapping that
         * was last processed by the controller.
         */
        observedGeneration: number;
        /**
         * The resource records required to configure this domain mapping. These
         * records must be added to the domain's DNS configuration in order to
         * serve the application via this domain mapping.
         * Structure is documented below.
         */
        resourceRecords?: outputs.cloudrun.DomainMappingStatusResourceRecord[];
    }

    export interface DomainMappingStatusCondition {
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
        /**
         * (Output)
         * One-word CamelCase reason for the condition's current status.
         */
        reason: string;
        /**
         * (Output)
         * Status of the condition, one of True, False, Unknown.
         */
        status: string;
        /**
         * Resource record type. Example: `AAAA`.
         * Possible values are: `A`, `AAAA`, `CNAME`.
         */
        type: string;
    }

    export interface DomainMappingStatusResourceRecord {
        /**
         * Name should be a [verified](https://support.google.com/webmasters/answer/9008080) domain
         */
        name: string;
        /**
         * (Output)
         * Data for this record. Values vary by record type, as defined in RFC 1035
         * (section 5) and RFC 1034 (section 3.6.1).
         */
        rrdata: string;
        /**
         * Resource record type. Example: `AAAA`.
         * Possible values are: `A`, `AAAA`, `CNAME`.
         */
        type?: string;
    }

    export interface GetServiceMetadata {
        annotations: {[key: string]: string};
        generation: number;
        labels: {[key: string]: string};
        namespace: string;
        resourceVersion: string;
        selfLink: string;
        uid: string;
    }

    export interface GetServiceStatus {
        conditions: outputs.cloudrun.GetServiceStatusCondition[];
        latestCreatedRevisionName: string;
        latestReadyRevisionName: string;
        observedGeneration: number;
        traffics: outputs.cloudrun.GetServiceStatusTraffic[];
        url: string;
    }

    export interface GetServiceStatusCondition {
        message: string;
        reason: string;
        status: string;
        type: string;
    }

    export interface GetServiceStatusTraffic {
        latestRevision: boolean;
        percent: number;
        revisionName: string;
        tag: string;
        url: string;
    }

    export interface GetServiceTemplate {
        metadatas: outputs.cloudrun.GetServiceTemplateMetadata[];
        specs: outputs.cloudrun.GetServiceTemplateSpec[];
    }

    export interface GetServiceTemplateMetadata {
        annotations: {[key: string]: string};
        generation: number;
        labels: {[key: string]: string};
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
        namespace: string;
        resourceVersion: string;
        selfLink: string;
        uid: string;
    }

    export interface GetServiceTemplateSpec {
        containerConcurrency: number;
        containers: outputs.cloudrun.GetServiceTemplateSpecContainer[];
        serviceAccountName: string;
        servingState: string;
        timeoutSeconds: number;
        volumes: outputs.cloudrun.GetServiceTemplateSpecVolume[];
    }

    export interface GetServiceTemplateSpecContainer {
        args: string[];
        commands: string[];
        envFroms: outputs.cloudrun.GetServiceTemplateSpecContainerEnvFrom[];
        envs: outputs.cloudrun.GetServiceTemplateSpecContainerEnv[];
        image: string;
        livenessProbes: outputs.cloudrun.GetServiceTemplateSpecContainerLivenessProbe[];
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
        ports: outputs.cloudrun.GetServiceTemplateSpecContainerPort[];
        resources: outputs.cloudrun.GetServiceTemplateSpecContainerResource[];
        startupProbes: outputs.cloudrun.GetServiceTemplateSpecContainerStartupProbe[];
        volumeMounts: outputs.cloudrun.GetServiceTemplateSpecContainerVolumeMount[];
        workingDir: string;
    }

    export interface GetServiceTemplateSpecContainerEnv {
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
        value: string;
        valueFroms: outputs.cloudrun.GetServiceTemplateSpecContainerEnvValueFrom[];
    }

    export interface GetServiceTemplateSpecContainerEnvFrom {
        configMapReves: outputs.cloudrun.GetServiceTemplateSpecContainerEnvFromConfigMapRef[];
        prefix: string;
        secretReves: outputs.cloudrun.GetServiceTemplateSpecContainerEnvFromSecretRef[];
    }

    export interface GetServiceTemplateSpecContainerEnvFromConfigMapRef {
        localObjectReferences: outputs.cloudrun.GetServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference[];
        optional: boolean;
    }

    export interface GetServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference {
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
    }

    export interface GetServiceTemplateSpecContainerEnvFromSecretRef {
        localObjectReferences: outputs.cloudrun.GetServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference[];
        optional: boolean;
    }

    export interface GetServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference {
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
    }

    export interface GetServiceTemplateSpecContainerEnvValueFrom {
        secretKeyReves: outputs.cloudrun.GetServiceTemplateSpecContainerEnvValueFromSecretKeyRef[];
    }

    export interface GetServiceTemplateSpecContainerEnvValueFromSecretKeyRef {
        key: string;
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
    }

    export interface GetServiceTemplateSpecContainerLivenessProbe {
        failureThreshold: number;
        grpcs: outputs.cloudrun.GetServiceTemplateSpecContainerLivenessProbeGrpc[];
        httpGets: outputs.cloudrun.GetServiceTemplateSpecContainerLivenessProbeHttpGet[];
        initialDelaySeconds: number;
        periodSeconds: number;
        timeoutSeconds: number;
    }

    export interface GetServiceTemplateSpecContainerLivenessProbeGrpc {
        port: number;
        service: string;
    }

    export interface GetServiceTemplateSpecContainerLivenessProbeHttpGet {
        httpHeaders: outputs.cloudrun.GetServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader[];
        path: string;
        port: number;
    }

    export interface GetServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader {
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
        value: string;
    }

    export interface GetServiceTemplateSpecContainerPort {
        containerPort: number;
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
        protocol: string;
    }

    export interface GetServiceTemplateSpecContainerResource {
        limits: {[key: string]: string};
        requests: {[key: string]: string};
    }

    export interface GetServiceTemplateSpecContainerStartupProbe {
        failureThreshold: number;
        grpcs: outputs.cloudrun.GetServiceTemplateSpecContainerStartupProbeGrpc[];
        httpGets: outputs.cloudrun.GetServiceTemplateSpecContainerStartupProbeHttpGet[];
        initialDelaySeconds: number;
        periodSeconds: number;
        tcpSockets: outputs.cloudrun.GetServiceTemplateSpecContainerStartupProbeTcpSocket[];
        timeoutSeconds: number;
    }

    export interface GetServiceTemplateSpecContainerStartupProbeGrpc {
        port: number;
        service: string;
    }

    export interface GetServiceTemplateSpecContainerStartupProbeHttpGet {
        httpHeaders: outputs.cloudrun.GetServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader[];
        path: string;
        port: number;
    }

    export interface GetServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader {
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
        value: string;
    }

    export interface GetServiceTemplateSpecContainerStartupProbeTcpSocket {
        port: number;
    }

    export interface GetServiceTemplateSpecContainerVolumeMount {
        mountPath: string;
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
    }

    export interface GetServiceTemplateSpecVolume {
        emptyDirs: outputs.cloudrun.GetServiceTemplateSpecVolumeEmptyDir[];
        /**
         * The name of the Cloud Run Service.
         */
        name: string;
        secrets: outputs.cloudrun.GetServiceTemplateSpecVolumeSecret[];
    }

    export interface GetServiceTemplateSpecVolumeEmptyDir {
        medium: string;
        sizeLimit: string;
    }

    export interface GetServiceTemplateSpecVolumeSecret {
        defaultMode: number;
        items: outputs.cloudrun.GetServiceTemplateSpecVolumeSecretItem[];
        secretName: string;
    }

    export interface GetServiceTemplateSpecVolumeSecretItem {
        key: string;
        mode: number;
        path: string;
    }

    export interface GetServiceTraffic {
        latestRevision: boolean;
        percent: number;
        revisionName: string;
        tag: string;
        url: string;
    }

    export interface IamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface IamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServiceMetadata {
        /**
         * Annotations is a key value map stored with a resource that
         * may be set by external tools to store and retrieve arbitrary metadata. More
         * info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations
         * **Note**: The Cloud Run API may add additional annotations that were not provided in your config.
         * If the provider plan shows a diff where a server-side annotation is added, you can add it to your config
         * or apply the lifecycle.ignore_changes rule to the metadata.0.annotations field.
         * Annotations with `run.googleapis.com/` and `autoscaling.knative.dev` are restricted. Use the following annotation
         * keys to configure features on a Service:
         * - `run.googleapis.com/binary-authorization-breakglass` sets the [Binary Authorization breakglass](https://cloud.google.com/sdk/gcloud/reference/run/deploy#--breakglass).
         * - `run.googleapis.com/binary-authorization` sets the [Binary Authorization](https://cloud.google.com/sdk/gcloud/reference/run/deploy#--binary-authorization).
         * - `run.googleapis.com/client-name` sets the client name calling the Cloud Run API.
         * - `run.googleapis.com/custom-audiences` sets the [custom audiences](https://cloud.google.com/sdk/gcloud/reference/alpha/run/deploy#--add-custom-audiences)
         * that can be used in the audience field of ID token for authenticated requests.
         * - `run.googleapis.com/description` sets a user defined description for the Service.
         * - `run.googleapis.com/ingress` sets the [ingress settings](https://cloud.google.com/sdk/gcloud/reference/run/deploy#--ingress)
         * for the Service. For example, `"run.googleapis.com/ingress" = "all"`.
         * - `run.googleapis.com/launch-stage` sets the [launch stage](https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation)
         * when a preview feature is used. For example, `"run.googleapis.com/launch-stage": "BETA"`
         */
        annotations: {[key: string]: string};
        /**
         * (Output)
         * A sequence number representing a specific generation of the desired state.
         */
        generation: number;
        /**
         * Map of string keys and values that can be used to organize and categorize
         * (scope and select) objects. May match selectors of replication controllers
         * and routes.
         */
        labels: {[key: string]: string};
        /**
         * In Cloud Run the namespace must be equal to either the
         * project ID or project number.
         */
        namespace: string;
        /**
         * (Output)
         * An opaque value that represents the internal version of this object that
         * can be used by clients to determine when objects have changed. May be used
         * for optimistic concurrency, change detection, and the watch operation on a
         * resource or set of resources. They may only be valid for a
         * particular resource or set of resources.
         */
        resourceVersion: string;
        /**
         * (Output)
         * SelfLink is a URL representing this object.
         */
        selfLink: string;
        /**
         * (Output)
         * UID is a unique id generated by the server on successful creation of a resource and is not
         * allowed to change on PUT operations.
         */
        uid: string;
    }

    export interface ServiceStatus {
        /**
         * (Output)
         * Array of observed Service Conditions, indicating the current ready state of the service.
         * Structure is documented below.
         */
        conditions: outputs.cloudrun.ServiceStatusCondition[];
        /**
         * (Output)
         * From ConfigurationStatus. LatestCreatedRevisionName is the last revision that was created
         * from this Service's Configuration. It might not be ready yet, for that use
         * LatestReadyRevisionName.
         */
        latestCreatedRevisionName: string;
        /**
         * (Output)
         * From ConfigurationStatus. LatestReadyRevisionName holds the name of the latest Revision
         * stamped out from this Service's Configuration that has had its "Ready" condition become
         * "True".
         */
        latestReadyRevisionName: string;
        /**
         * (Output)
         * ObservedGeneration is the 'Generation' of the Route that was last processed by the
         * controller.
         * Clients polling for completed reconciliation should poll until observedGeneration =
         * metadata.generation and the Ready condition's status is True or False.
         */
        observedGeneration: number;
        /**
         * (Output)
         * Traffic specifies how to distribute traffic over a collection of Knative Revisions
         * and Configurations
         * Structure is documented below.
         */
        traffics: outputs.cloudrun.ServiceStatusTraffic[];
        /**
         * (Output)
         * URL displays the URL for accessing tagged traffic targets. URL is displayed in status,
         * and is disallowed on spec. URL must contain a scheme (e.g. http://) and a hostname,
         * but may not contain anything else (e.g. basic auth, url path, etc.)
         */
        url: string;
    }

    export interface ServiceStatusCondition {
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
        /**
         * (Output)
         * One-word CamelCase reason for the condition's current status.
         */
        reason: string;
        /**
         * (Output)
         * Status of the condition, one of True, False, Unknown.
         */
        status: string;
        /**
         * (Output)
         * Type of domain mapping condition.
         */
        type: string;
    }

    export interface ServiceStatusTraffic {
        /**
         * LatestRevision may be optionally provided to indicate that the latest ready
         * Revision of the Configuration should be used for this traffic target. When
         * provided LatestRevision must be true if RevisionName is empty; it must be
         * false when RevisionName is non-empty.
         */
        latestRevision: boolean;
        /**
         * Percent specifies percent of the traffic to this Revision or Configuration.
         */
        percent: number;
        /**
         * RevisionName of a specific revision to which to send this portion of traffic.
         */
        revisionName: string;
        /**
         * Tag is optionally used to expose a dedicated url for referencing this target exclusively.
         */
        tag: string;
        /**
         * (Output)
         * URL displays the URL for accessing tagged traffic targets. URL is displayed in status,
         * and is disallowed on spec. URL must contain a scheme (e.g. http://) and a hostname,
         * but may not contain anything else (e.g. basic auth, url path, etc.)
         */
        url: string;
    }

    export interface ServiceTemplate {
        /**
         * Optional metadata for this Revision, including labels and annotations.
         * Name will be generated by the Configuration. To set minimum instances
         * for this revision, use the "autoscaling.knative.dev/minScale" annotation
         * key. To set maximum instances for this revision, use the
         * "autoscaling.knative.dev/maxScale" annotation key. To set Cloud SQL
         * connections for the revision, use the "run.googleapis.com/cloudsql-instances"
         * annotation key.
         * Structure is documented below.
         */
        metadata: outputs.cloudrun.ServiceTemplateMetadata;
        /**
         * RevisionSpec holds the desired state of the Revision (from the client).
         * Structure is documented below.
         */
        spec: outputs.cloudrun.ServiceTemplateSpec;
    }

    export interface ServiceTemplateMetadata {
        /**
         * Annotations is a key value map stored with a resource that
         * may be set by external tools to store and retrieve arbitrary metadata. More
         * info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations
         * **Note**: The Cloud Run API may add additional annotations that were not provided in your config.
         * If the provider plan shows a diff where a server-side annotation is added, you can add it to your config
         * or apply the lifecycle.ignore_changes rule to the metadata.0.annotations field.
         * Annotations with `run.googleapis.com/` and `autoscaling.knative.dev` are restricted. Use the following annotation
         * keys to configure features on a Service:
         * - `run.googleapis.com/binary-authorization-breakglass` sets the [Binary Authorization breakglass](https://cloud.google.com/sdk/gcloud/reference/run/deploy#--breakglass).
         * - `run.googleapis.com/binary-authorization` sets the [Binary Authorization](https://cloud.google.com/sdk/gcloud/reference/run/deploy#--binary-authorization).
         * - `run.googleapis.com/client-name` sets the client name calling the Cloud Run API.
         * - `run.googleapis.com/custom-audiences` sets the [custom audiences](https://cloud.google.com/sdk/gcloud/reference/alpha/run/deploy#--add-custom-audiences)
         * that can be used in the audience field of ID token for authenticated requests.
         * - `run.googleapis.com/description` sets a user defined description for the Service.
         * - `run.googleapis.com/ingress` sets the [ingress settings](https://cloud.google.com/sdk/gcloud/reference/run/deploy#--ingress)
         * for the Service. For example, `"run.googleapis.com/ingress" = "all"`.
         * - `run.googleapis.com/launch-stage` sets the [launch stage](https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation)
         * when a preview feature is used. For example, `"run.googleapis.com/launch-stage": "BETA"`
         */
        annotations: {[key: string]: string};
        /**
         * (Output)
         * A sequence number representing a specific generation of the desired state.
         */
        generation: number;
        /**
         * Map of string keys and values that can be used to organize and categorize
         * (scope and select) objects. May match selectors of replication controllers
         * and routes.
         */
        labels: {[key: string]: string};
        /**
         * Name must be unique within a Google Cloud project and region.
         * Is required when creating resources. Name is primarily intended
         * for creation idempotence and configuration definition. Cannot be updated.
         */
        name: string;
        /**
         * In Cloud Run the namespace must be equal to either the
         * project ID or project number.
         */
        namespace: string;
        /**
         * (Output)
         * An opaque value that represents the internal version of this object that
         * can be used by clients to determine when objects have changed. May be used
         * for optimistic concurrency, change detection, and the watch operation on a
         * resource or set of resources. They may only be valid for a
         * particular resource or set of resources.
         */
        resourceVersion: string;
        /**
         * (Output)
         * SelfLink is a URL representing this object.
         */
        selfLink: string;
        /**
         * (Output)
         * UID is a unique id generated by the server on successful creation of a resource and is not
         * allowed to change on PUT operations.
         */
        uid: string;
    }

    export interface ServiceTemplateSpec {
        /**
         * ContainerConcurrency specifies the maximum allowed in-flight (concurrent)
         * requests per container of the Revision. Values are:
         */
        containerConcurrency: number;
        /**
         * Containers defines the unit of execution for this Revision.
         * Structure is documented below.
         */
        containers: outputs.cloudrun.ServiceTemplateSpecContainer[];
        /**
         * Email address of the IAM service account associated with the revision of the
         * service. The service account represents the identity of the running revision,
         * and determines what permissions the revision has. If not provided, the revision
         * will use the project's default service account.
         */
        serviceAccountName: string;
        /**
         * (Output, Deprecated)
         * ServingState holds a value describing the state the resources
         * are in for this Revision.
         * It is expected
         * that the system will manipulate this based on routability and load.
         *
         * > **Warning:** `servingState` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         *
         * @deprecated `serving_state` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         */
        servingState: string;
        /**
         * TimeoutSeconds holds the max duration the instance is allowed for responding to a request.
         */
        timeoutSeconds: number;
        /**
         * Volume represents a named volume in a container.
         * Structure is documented below.
         */
        volumes?: outputs.cloudrun.ServiceTemplateSpecVolume[];
    }

    export interface ServiceTemplateSpecContainer {
        /**
         * Arguments to the entrypoint.
         * The docker image's CMD is used if this is not provided.
         */
        args?: string[];
        /**
         * Entrypoint array. Not executed within a shell.
         * The docker image's ENTRYPOINT is used if this is not provided.
         */
        commands?: string[];
        /**
         * (Optional, Deprecated)
         * List of sources to populate environment variables in the container.
         * All invalid keys will be reported as an event when the container is starting.
         * When a key exists in multiple sources, the value associated with the last source will
         * take precedence. Values defined by an Env with a duplicate key will take
         * precedence.
         * Structure is documented below.
         *
         * > **Warning:** `envFrom` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         *
         * @deprecated `env_from` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         */
        envFroms?: outputs.cloudrun.ServiceTemplateSpecContainerEnvFrom[];
        /**
         * List of environment variables to set in the container.
         * Structure is documented below.
         */
        envs?: outputs.cloudrun.ServiceTemplateSpecContainerEnv[];
        /**
         * Docker image name. This is most often a reference to a container located
         * in the container registry, such as gcr.io/cloudrun/hello
         */
        image: string;
        /**
         * Periodic probe of container liveness. Container will be restarted if the probe fails. More info:
         * https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         * Structure is documented below.
         */
        livenessProbe?: outputs.cloudrun.ServiceTemplateSpecContainerLivenessProbe;
        /**
         * Name of the container
         */
        name: string;
        /**
         * List of open ports in the container.
         * Structure is documented below.
         */
        ports: outputs.cloudrun.ServiceTemplateSpecContainerPort[];
        /**
         * Compute Resources required by this container. Used to set values such as max memory
         * Structure is documented below.
         */
        resources: outputs.cloudrun.ServiceTemplateSpecContainerResources;
        /**
         * Startup probe of application within the container.
         * All other probes are disabled if a startup probe is provided, until it
         * succeeds. Container will not be added to service endpoints if the probe fails.
         * Structure is documented below.
         */
        startupProbe: outputs.cloudrun.ServiceTemplateSpecContainerStartupProbe;
        /**
         * Volume to mount into the container's filesystem.
         * Only supports SecretVolumeSources.
         * Structure is documented below.
         */
        volumeMounts?: outputs.cloudrun.ServiceTemplateSpecContainerVolumeMount[];
        /**
         * (Optional, Deprecated)
         * Container's working directory.
         * If not specified, the container runtime's default will be used, which
         * might be configured in the container image.
         *
         * > **Warning:** `workingDir` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         *
         * @deprecated `working_dir` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         */
        workingDir?: string;
    }

    export interface ServiceTemplateSpecContainerEnv {
        /**
         * Name of the environment variable.
         */
        name?: string;
        /**
         * Defaults to "".
         */
        value?: string;
        /**
         * Source for the environment variable's value. Only supports secret_key_ref.
         * Structure is documented below.
         */
        valueFrom?: outputs.cloudrun.ServiceTemplateSpecContainerEnvValueFrom;
    }

    export interface ServiceTemplateSpecContainerEnvFrom {
        /**
         * The ConfigMap to select from.
         * Structure is documented below.
         */
        configMapRef?: outputs.cloudrun.ServiceTemplateSpecContainerEnvFromConfigMapRef;
        /**
         * An optional identifier to prepend to each key in the ConfigMap.
         */
        prefix?: string;
        /**
         * The Secret to select from.
         * Structure is documented below.
         */
        secretRef?: outputs.cloudrun.ServiceTemplateSpecContainerEnvFromSecretRef;
    }

    export interface ServiceTemplateSpecContainerEnvFromConfigMapRef {
        /**
         * The ConfigMap to select from.
         * Structure is documented below.
         */
        localObjectReference?: outputs.cloudrun.ServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference;
        /**
         * Specify whether the ConfigMap must be defined
         */
        optional?: boolean;
    }

    export interface ServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference {
        /**
         * Name of the referent.
         */
        name: string;
    }

    export interface ServiceTemplateSpecContainerEnvFromSecretRef {
        /**
         * The Secret to select from.
         * Structure is documented below.
         */
        localObjectReference?: outputs.cloudrun.ServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference;
        /**
         * Specify whether the Secret must be defined
         */
        optional?: boolean;
    }

    export interface ServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference {
        /**
         * Name of the referent.
         */
        name: string;
    }

    export interface ServiceTemplateSpecContainerEnvValueFrom {
        /**
         * Selects a key (version) of a secret in Secret Manager.
         * Structure is documented below.
         */
        secretKeyRef: outputs.cloudrun.ServiceTemplateSpecContainerEnvValueFromSecretKeyRef;
    }

    export interface ServiceTemplateSpecContainerEnvValueFromSecretKeyRef {
        /**
         * A Cloud Secret Manager secret version. Must be 'latest' for the latest
         * version or an integer for a specific version.
         */
        key: string;
        /**
         * The name of the secret in Cloud Secret Manager. By default, the secret is assumed to be in the same project.
         * If the secret is in another project, you must define an alias.
         * An alias definition has the form: :projects/{project-id|project-number}/secrets/.
         * If multiple alias definitions are needed, they must be separated by commas.
         * The alias definitions must be set on the run.googleapis.com/secrets annotation.
         */
        name: string;
    }

    export interface ServiceTemplateSpecContainerLivenessProbe {
        /**
         * Minimum consecutive failures for the probe to be considered failed after
         * having succeeded. Defaults to 3. Minimum value is 1.
         */
        failureThreshold?: number;
        /**
         * GRPC specifies an action involving a GRPC port.
         * Structure is documented below.
         */
        grpc?: outputs.cloudrun.ServiceTemplateSpecContainerLivenessProbeGrpc;
        /**
         * HttpGet specifies the http request to perform.
         * Structure is documented below.
         */
        httpGet?: outputs.cloudrun.ServiceTemplateSpecContainerLivenessProbeHttpGet;
        /**
         * Number of seconds after the container has started before the probe is
         * initiated.
         * Defaults to 0 seconds. Minimum value is 0. Maximum value is 3600.
         */
        initialDelaySeconds?: number;
        /**
         * How often (in seconds) to perform the probe.
         * Default to 10 seconds. Minimum value is 1. Maximum value is 3600.
         */
        periodSeconds?: number;
        /**
         * Number of seconds after which the probe times out.
         * Defaults to 1 second. Minimum value is 1. Maximum value is 3600.
         * Must be smaller than period_seconds.
         */
        timeoutSeconds?: number;
    }

    export interface ServiceTemplateSpecContainerLivenessProbeGrpc {
        /**
         * Port number to access on the container. Number must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
        /**
         * The name of the service to place in the gRPC HealthCheckRequest
         * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
         * If this is not specified, the default behavior is defined by gRPC.
         */
        service?: string;
    }

    export interface ServiceTemplateSpecContainerLivenessProbeHttpGet {
        /**
         * Custom headers to set in the request. HTTP allows repeated headers.
         * Structure is documented below.
         */
        httpHeaders?: outputs.cloudrun.ServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader[];
        /**
         * Path to access on the HTTP server. If set, it should not be empty string.
         */
        path?: string;
        /**
         * Port number to access on the container. Number must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
    }

    export interface ServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader {
        /**
         * The header field name.
         */
        name: string;
        /**
         * The header field value.
         */
        value?: string;
    }

    export interface ServiceTemplateSpecContainerPort {
        /**
         * Port number the container listens on. This must be a valid port number (between 1 and 65535). Defaults to "8080".
         */
        containerPort?: number;
        /**
         * If specified, used to specify which protocol to use. Allowed values are "http1" (HTTP/1) and "h2c" (HTTP/2 end-to-end). Defaults to "http1".
         */
        name: string;
        /**
         * Protocol for port. Must be "TCP". Defaults to "TCP".
         */
        protocol?: string;
    }

    export interface ServiceTemplateSpecContainerResources {
        /**
         * Limits describes the maximum amount of compute resources allowed.
         * The values of the map is string form of the 'quantity' k8s type:
         * https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
         */
        limits: {[key: string]: string};
        /**
         * Requests describes the minimum amount of compute resources required.
         * If Requests is omitted for a container, it defaults to Limits if that is
         * explicitly specified, otherwise to an implementation-defined value.
         * The values of the map is string form of the 'quantity' k8s type:
         * https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
         */
        requests?: {[key: string]: string};
    }

    export interface ServiceTemplateSpecContainerStartupProbe {
        /**
         * Minimum consecutive failures for the probe to be considered failed after
         * having succeeded. Defaults to 3. Minimum value is 1.
         */
        failureThreshold?: number;
        /**
         * GRPC specifies an action involving a GRPC port.
         * Structure is documented below.
         */
        grpc?: outputs.cloudrun.ServiceTemplateSpecContainerStartupProbeGrpc;
        /**
         * HttpGet specifies the http request to perform.
         * Structure is documented below.
         */
        httpGet?: outputs.cloudrun.ServiceTemplateSpecContainerStartupProbeHttpGet;
        /**
         * Number of seconds after the container has started before the probe is
         * initiated.
         * Defaults to 0 seconds. Minimum value is 0. Maximum value is 240.
         */
        initialDelaySeconds?: number;
        /**
         * How often (in seconds) to perform the probe.
         * Default to 10 seconds. Minimum value is 1. Maximum value is 240.
         */
        periodSeconds?: number;
        /**
         * TcpSocket specifies an action involving a TCP port.
         * Structure is documented below.
         */
        tcpSocket?: outputs.cloudrun.ServiceTemplateSpecContainerStartupProbeTcpSocket;
        /**
         * Number of seconds after which the probe times out.
         * Defaults to 1 second. Minimum value is 1. Maximum value is 3600.
         * Must be smaller than periodSeconds.
         */
        timeoutSeconds?: number;
    }

    export interface ServiceTemplateSpecContainerStartupProbeGrpc {
        /**
         * Port number to access on the container. Number must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
        /**
         * The name of the service to place in the gRPC HealthCheckRequest
         * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
         * If this is not specified, the default behavior is defined by gRPC.
         */
        service?: string;
    }

    export interface ServiceTemplateSpecContainerStartupProbeHttpGet {
        /**
         * Custom headers to set in the request. HTTP allows repeated headers.
         * Structure is documented below.
         */
        httpHeaders?: outputs.cloudrun.ServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader[];
        /**
         * Path to access on the HTTP server. If set, it should not be empty string.
         */
        path?: string;
        /**
         * Port number to access on the container. Number must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
    }

    export interface ServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader {
        /**
         * The header field name.
         */
        name: string;
        /**
         * The header field value.
         */
        value?: string;
    }

    export interface ServiceTemplateSpecContainerStartupProbeTcpSocket {
        /**
         * Port number to access on the container. Number must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
    }

    export interface ServiceTemplateSpecContainerVolumeMount {
        /**
         * Path within the container at which the volume should be mounted.  Must
         * not contain ':'.
         */
        mountPath: string;
        /**
         * This must match the Name of a Volume.
         */
        name: string;
    }

    export interface ServiceTemplateSpecVolume {
        emptyDir?: outputs.cloudrun.ServiceTemplateSpecVolumeEmptyDir;
        /**
         * Volume's name.
         */
        name: string;
        /**
         * The secret's value will be presented as the content of a file whose
         * name is defined in the item path. If no items are defined, the name of
         * the file is the secret_name.
         * Structure is documented below.
         */
        secret?: outputs.cloudrun.ServiceTemplateSpecVolumeSecret;
    }

    export interface ServiceTemplateSpecVolumeEmptyDir {
        /**
         * The medium on which the data is stored. The default is "" which means to use the node's default medium. Must be an empty string (default) or Memory.
         */
        medium?: string;
        /**
         * Limit on the storage usable by this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. This field's values are of the 'Quantity' k8s type: https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/. The default is nil which means that the limit is undefined. More info: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir.
         *
         * - - -
         */
        sizeLimit?: string;
    }

    export interface ServiceTemplateSpecVolumeSecret {
        /**
         * Mode bits to use on created files by default. Must be a value between 0000
         * and 0777. Defaults to 0644. Directories within the path are not affected by
         * this setting. This might be in conflict with other options that affect the
         * file mode, like fsGroup, and the result can be other mode bits set.
         */
        defaultMode?: number;
        /**
         * If unspecified, the volume will expose a file whose name is the
         * secret_name.
         * If specified, the key will be used as the version to fetch from Cloud
         * Secret Manager and the path will be the name of the file exposed in the
         * volume. When items are defined, they must specify a key and a path.
         * Structure is documented below.
         */
        items?: outputs.cloudrun.ServiceTemplateSpecVolumeSecretItem[];
        /**
         * The name of the secret in Cloud Secret Manager. By default, the secret
         * is assumed to be in the same project.
         * If the secret is in another project, you must define an alias.
         * An alias definition has the form:
         * {alias}:projects/{project-id|project-number}/secrets/{secret-name}.
         * If multiple alias definitions are needed, they must be separated by
         * commas.
         * The alias definitions must be set on the run.googleapis.com/secrets
         * annotation.
         */
        secretName: string;
    }

    export interface ServiceTemplateSpecVolumeSecretItem {
        /**
         * The Cloud Secret Manager secret version.
         * Can be 'latest' for the latest value or an integer for a specific version.
         */
        key: string;
        /**
         * Mode bits to use on this file, must be a value between 0000 and 0777. If
         * not specified, the volume defaultMode will be used. This might be in
         * conflict with other options that affect the file mode, like fsGroup, and
         * the result can be other mode bits set.
         */
        mode?: number;
        /**
         * The relative path of the file to map the key to.
         * May not be an absolute path.
         * May not contain the path element '..'.
         * May not start with the string '..'.
         */
        path: string;
    }

    export interface ServiceTraffic {
        /**
         * LatestRevision may be optionally provided to indicate that the latest ready
         * Revision of the Configuration should be used for this traffic target. When
         * provided LatestRevision must be true if RevisionName is empty; it must be
         * false when RevisionName is non-empty.
         */
        latestRevision?: boolean;
        /**
         * Percent specifies percent of the traffic to this Revision or Configuration.
         */
        percent: number;
        /**
         * RevisionName of a specific revision to which to send this portion of traffic.
         */
        revisionName?: string;
        /**
         * Tag is optionally used to expose a dedicated url for referencing this target exclusively.
         */
        tag?: string;
        /**
         * (Output)
         * URL displays the URL for accessing tagged traffic targets. URL is displayed in status,
         * and is disallowed on spec. URL must contain a scheme (e.g. http://) and a hostname,
         * but may not contain anything else (e.g. basic auth, url path, etc.)
         */
        url: string;
    }

}

export namespace cloudrunv2 {
    export interface JobBinaryAuthorization {
        /**
         * If present, indicates to use Breakglass using this justification. If useDefault is False, then it must be empty. For more information on breakglass, see https://cloud.google.com/binary-authorization/docs/using-breakglass
         */
        breakglassJustification?: string;
        /**
         * If True, indicates to use the default project's binary authorization policy. If False, binary authorization will be disabled.
         */
        useDefault?: boolean;
    }

    export interface JobCondition {
        /**
         * (Output)
         * A reason for the execution condition.
         */
        executionReason: string;
        /**
         * (Output)
         * Last time the condition transitioned from one status to another.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        lastTransitionTime: string;
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
        /**
         * (Output)
         * A common (service-level) reason for this condition.
         */
        reason: string;
        /**
         * (Output)
         * A reason for the revision condition.
         */
        revisionReason: string;
        /**
         * (Output)
         * How to interpret failures of this condition, one of Error, Warning, Info
         */
        severity: string;
        /**
         * (Output)
         * State of the condition.
         */
        state: string;
        /**
         * (Output)
         * type is used to communicate the status of the reconciliation process. See also: https://github.com/knative/serving/blob/main/docs/spec/errors.md#error-conditions-and-reporting Types common to all resources include: * "Ready": True when the Resource is ready.
         */
        type: string;
    }

    export interface JobIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface JobIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface JobLatestCreatedExecution {
        /**
         * (Output)
         * Completion timestamp of the execution.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        completionTime: string;
        /**
         * (Output)
         * Creation timestamp of the execution.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        createTime: string;
        /**
         * Name of the Job.
         */
        name: string;
    }

    export interface JobTemplate {
        /**
         * Unstructured key value map that may be set by external tools to store and arbitrary metadata. They are not queryable and should be preserved when modifying objects.
         * Cloud Run API v2 does not support annotations with `run.googleapis.com`, `cloud.googleapis.com`, `serving.knative.dev`, or `autoscaling.knative.dev` namespaces, and they will be rejected.
         * All system annotations in v1 now have a corresponding field in v2 ExecutionTemplate.
         * This field follows Kubernetes annotations' namespacing, limits, and rules.
         */
        annotations?: {[key: string]: string};
        /**
         * Unstructured key value map that can be used to organize and categorize objects. User-provided labels are shared with Google's billing system, so they can be used to filter,
         * or break down billing charges by team, component, environment, state, etc. For more information, visit https://cloud.google.com/resource-manager/docs/creating-managing-labels or
         * https://cloud.google.com/run/docs/configuring/labels.
         * Cloud Run API v2 does not support labels with `run.googleapis.com`, `cloud.googleapis.com`, `serving.knative.dev`, or `autoscaling.knative.dev` namespaces, and they will be rejected.
         * All system labels in v1 now have a corresponding field in v2 ExecutionTemplate.
         */
        labels?: {[key: string]: string};
        /**
         * Specifies the maximum desired number of tasks the execution should run at given time. Must be <= taskCount. When the job is run, if this field is 0 or unset, the maximum possible value will be used for that execution. The actual number of tasks running in steady state will be less than this number when there are fewer tasks waiting to be completed remaining, i.e. when the work left to do is less than max parallelism.
         */
        parallelism: number;
        /**
         * Specifies the desired number of tasks the execution should run. Setting to 1 means that parallelism is limited to 1 and the success of that task signals the success of the execution. More info: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/
         */
        taskCount: number;
        /**
         * Describes the task(s) that will be created when executing an execution
         * Structure is documented below.
         */
        template: outputs.cloudrunv2.JobTemplateTemplate;
    }

    export interface JobTemplateTemplate {
        /**
         * Holds the single container that defines the unit of execution for this task.
         * Structure is documented below.
         */
        containers: outputs.cloudrunv2.JobTemplateTemplateContainer[];
        /**
         * A reference to a customer managed encryption key (CMEK) to use to encrypt this container image. For more information, go to https://cloud.google.com/run/docs/securing/using-cmek
         */
        encryptionKey?: string;
        /**
         * The execution environment being used to host this Task.
         * Possible values are: `EXECUTION_ENVIRONMENT_GEN1`, `EXECUTION_ENVIRONMENT_GEN2`.
         */
        executionEnvironment: string;
        /**
         * Number of retries allowed per Task, before marking this Task failed.
         */
        maxRetries?: number;
        /**
         * Email address of the IAM service account associated with the Task of a Job. The service account represents the identity of the running task, and determines what permissions the task has. If not provided, the task will use the project's default service account.
         */
        serviceAccount: string;
        /**
         * Max allowed time duration the Task may be active before the system will actively try to mark it failed and kill associated containers. This applies per attempt of a task, meaning each retry can run for the full timeout.
         * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
         */
        timeout: string;
        /**
         * A list of Volumes to make available to containers.
         * Structure is documented below.
         */
        volumes?: outputs.cloudrunv2.JobTemplateTemplateVolume[];
        /**
         * VPC Access configuration to use for this Task. For more information, visit https://cloud.google.com/run/docs/configuring/connecting-vpc.
         * Structure is documented below.
         */
        vpcAccess?: outputs.cloudrunv2.JobTemplateTemplateVpcAccess;
    }

    export interface JobTemplateTemplateContainer {
        /**
         * Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
         */
        args?: string[];
        /**
         * Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
         */
        commands?: string[];
        /**
         * List of environment variables to set in the container.
         * Structure is documented below.
         */
        envs?: outputs.cloudrunv2.JobTemplateTemplateContainerEnv[];
        /**
         * URL of the Container image in Google Container Registry or Google Artifact Registry. More info: https://kubernetes.io/docs/concepts/containers/images
         */
        image: string;
        /**
         * (Optional, Deprecated)
         * Periodic probe of container liveness. Container will be restarted if the probe fails. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         * This field is not supported in Cloud Run Job currently.
         * Structure is documented below.
         *
         * > **Warning:** `livenessProbe` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         *
         * @deprecated `liveness_probe` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         */
        livenessProbe?: outputs.cloudrunv2.JobTemplateTemplateContainerLivenessProbe;
        /**
         * Name of the container specified as a DNS_LABEL.
         */
        name?: string;
        /**
         * List of ports to expose from the container. Only a single port can be specified. The specified ports must be listening on all interfaces (0.0.0.0) within the container to be accessible.
         * If omitted, a port number will be chosen and passed to the container through the PORT environment variable for the container to listen on
         * Structure is documented below.
         */
        ports?: outputs.cloudrunv2.JobTemplateTemplateContainerPort[];
        /**
         * Compute Resource requirements by this container. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
         * Structure is documented below.
         */
        resources: outputs.cloudrunv2.JobTemplateTemplateContainerResources;
        /**
         * (Optional, Deprecated)
         * Startup probe of application within the container. All other probes are disabled if a startup probe is provided, until it succeeds. Container will not be added to service endpoints if the probe fails. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         * This field is not supported in Cloud Run Job currently.
         * Structure is documented below.
         *
         * > **Warning:** `startupProbe` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         *
         * @deprecated `startup_probe` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         */
        startupProbe: outputs.cloudrunv2.JobTemplateTemplateContainerStartupProbe;
        /**
         * Volume to mount into the container's filesystem.
         * Structure is documented below.
         */
        volumeMounts?: outputs.cloudrunv2.JobTemplateTemplateContainerVolumeMount[];
        /**
         * Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image.
         */
        workingDir?: string;
    }

    export interface JobTemplateTemplateContainerEnv {
        /**
         * Name of the environment variable. Must be a C_IDENTIFIER, and mnay not exceed 32768 characters.
         */
        name: string;
        /**
         * Variable references $(VAR_NAME) are expanded using the previous defined environment variables in the container and any route environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to "", and the maximum length is 32768 bytes
         */
        value?: string;
        /**
         * Source for the environment variable's value.
         * Structure is documented below.
         */
        valueSource?: outputs.cloudrunv2.JobTemplateTemplateContainerEnvValueSource;
    }

    export interface JobTemplateTemplateContainerEnvValueSource {
        /**
         * Selects a secret and a specific version from Cloud Secret Manager.
         * Structure is documented below.
         */
        secretKeyRef?: outputs.cloudrunv2.JobTemplateTemplateContainerEnvValueSourceSecretKeyRef;
    }

    export interface JobTemplateTemplateContainerEnvValueSourceSecretKeyRef {
        /**
         * The name of the secret in Cloud Secret Manager. Format: {secretName} if the secret is in the same project. projects/{project}/secrets/{secretName} if the secret is in a different project.
         */
        secret: string;
        /**
         * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version.
         */
        version: string;
    }

    export interface JobTemplateTemplateContainerLivenessProbe {
        /**
         * Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1.
         */
        failureThreshold?: number;
        /**
         * HTTPGet specifies the http request to perform. Exactly one of HTTPGet or TCPSocket must be specified.
         * Structure is documented below.
         */
        httpGet?: outputs.cloudrunv2.JobTemplateTemplateContainerLivenessProbeHttpGet;
        /**
         * Number of seconds after the container has started before the probe is initiated. Defaults to 0 seconds. Minimum value is 0. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        initialDelaySeconds?: number;
        /**
         * How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. Must be greater or equal than timeoutSeconds
         */
        periodSeconds?: number;
        /**
         * TCPSocket specifies an action involving a TCP port. Exactly one of HTTPGet or TCPSocket must be specified.
         * Structure is documented below.
         */
        tcpSocket?: outputs.cloudrunv2.JobTemplateTemplateContainerLivenessProbeTcpSocket;
        /**
         * Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Maximum value is 3600. Must be smaller than periodSeconds. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        timeoutSeconds?: number;
    }

    export interface JobTemplateTemplateContainerLivenessProbeHttpGet {
        /**
         * Custom headers to set in the request. HTTP allows repeated headers.
         * Structure is documented below.
         */
        httpHeaders?: outputs.cloudrunv2.JobTemplateTemplateContainerLivenessProbeHttpGetHttpHeader[];
        /**
         * Path to access on the HTTP server. Defaults to '/'.
         */
        path?: string;
    }

    export interface JobTemplateTemplateContainerLivenessProbeHttpGetHttpHeader {
        /**
         * The header field name
         */
        name: string;
        /**
         * The header field value
         */
        value?: string;
    }

    export interface JobTemplateTemplateContainerLivenessProbeTcpSocket {
        /**
         * Port number to access on the container. Must be in the range 1 to 65535. If not specified, defaults to 8080.
         */
        port?: number;
    }

    export interface JobTemplateTemplateContainerPort {
        /**
         * Port number the container listens on. This must be a valid TCP port number, 0 < containerPort < 65536.
         */
        containerPort?: number;
        /**
         * If specified, used to specify which protocol to use. Allowed values are "http1" and "h2c".
         */
        name?: string;
    }

    export interface JobTemplateTemplateContainerResources {
        /**
         * Only memory and CPU are supported. Note: The only supported values for CPU are '1', '2', '4', and '8'. Setting 4 CPU requires at least 2Gi of memory. The values of the map is string form of the 'quantity' k8s type: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
         */
        limits: {[key: string]: string};
    }

    export interface JobTemplateTemplateContainerStartupProbe {
        /**
         * Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1.
         */
        failureThreshold?: number;
        /**
         * HTTPGet specifies the http request to perform. Exactly one of HTTPGet or TCPSocket must be specified.
         * Structure is documented below.
         */
        httpGet?: outputs.cloudrunv2.JobTemplateTemplateContainerStartupProbeHttpGet;
        /**
         * Number of seconds after the container has started before the probe is initiated. Defaults to 0 seconds. Minimum value is 0. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        initialDelaySeconds?: number;
        /**
         * How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. Must be greater or equal than timeoutSeconds
         */
        periodSeconds?: number;
        /**
         * TCPSocket specifies an action involving a TCP port. Exactly one of HTTPGet or TCPSocket must be specified.
         * Structure is documented below.
         */
        tcpSocket?: outputs.cloudrunv2.JobTemplateTemplateContainerStartupProbeTcpSocket;
        /**
         * Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Maximum value is 3600. Must be smaller than periodSeconds. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        timeoutSeconds?: number;
    }

    export interface JobTemplateTemplateContainerStartupProbeHttpGet {
        /**
         * Custom headers to set in the request. HTTP allows repeated headers.
         * Structure is documented below.
         */
        httpHeaders?: outputs.cloudrunv2.JobTemplateTemplateContainerStartupProbeHttpGetHttpHeader[];
        /**
         * Path to access on the HTTP server. Defaults to '/'.
         */
        path?: string;
    }

    export interface JobTemplateTemplateContainerStartupProbeHttpGetHttpHeader {
        /**
         * The header field name
         */
        name: string;
        /**
         * The header field value
         */
        value?: string;
    }

    export interface JobTemplateTemplateContainerStartupProbeTcpSocket {
        /**
         * Port number to access on the container. Must be in the range 1 to 65535. If not specified, defaults to 8080.
         */
        port: number;
    }

    export interface JobTemplateTemplateContainerVolumeMount {
        /**
         * Path within the container at which the volume should be mounted. Must not contain ':'. For Cloud SQL volumes, it can be left empty, or must otherwise be /cloudsql. All instances defined in the Volume will be available as /cloudsql/[instance]. For more information on Cloud SQL volumes, visit https://cloud.google.com/sql/docs/mysql/connect-run
         */
        mountPath: string;
        /**
         * This must match the Name of a Volume.
         */
        name: string;
    }

    export interface JobTemplateTemplateVolume {
        /**
         * For Cloud SQL volumes, contains the specific instances that should be mounted. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run.
         * Structure is documented below.
         */
        cloudSqlInstance?: outputs.cloudrunv2.JobTemplateTemplateVolumeCloudSqlInstance;
        emptyDir?: outputs.cloudrunv2.JobTemplateTemplateVolumeEmptyDir;
        /**
         * Volume's name.
         */
        name: string;
        /**
         * Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
         * Structure is documented below.
         */
        secret?: outputs.cloudrunv2.JobTemplateTemplateVolumeSecret;
    }

    export interface JobTemplateTemplateVolumeCloudSqlInstance {
        /**
         * The Cloud SQL instance connection names, as can be found in https://console.cloud.google.com/sql/instances. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run. Format: {project}:{location}:{instance}
         */
        instances?: string[];
    }

    export interface JobTemplateTemplateVolumeEmptyDir {
        /**
         * The different types of medium supported for EmptyDir.
         * Default value is `MEMORY`.
         * Possible values are: `MEMORY`.
         */
        medium?: string;
        /**
         * Limit on the storage usable by this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. This field's values are of the 'Quantity' k8s type: https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/. The default is nil which means that the limit is undefined. More info: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir.
         */
        sizeLimit?: string;
    }

    export interface JobTemplateTemplateVolumeSecret {
        /**
         * Integer representation of mode bits to use on created files by default. Must be a value between 0000 and 0777 (octal), defaulting to 0444. Directories within the path are not affected by this setting.
         */
        defaultMode?: number;
        /**
         * If unspecified, the volume will expose a file whose name is the secret, relative to VolumeMount.mount_path. If specified, the key will be used as the version to fetch from Cloud Secret Manager and the path will be the name of the file exposed in the volume. When items are defined, they must specify a path and a version.
         * Structure is documented below.
         */
        items?: outputs.cloudrunv2.JobTemplateTemplateVolumeSecretItem[];
        /**
         * The name of the secret in Cloud Secret Manager. Format: {secret} if the secret is in the same project. projects/{project}/secrets/{secret} if the secret is in a different project.
         */
        secret: string;
    }

    export interface JobTemplateTemplateVolumeSecretItem {
        /**
         * Integer octal mode bits to use on this file, must be a value between 01 and 0777 (octal). If 0 or not set, the Volume's default mode will be used.
         */
        mode?: number;
        /**
         * The relative path of the secret in the container.
         */
        path: string;
        /**
         * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version
         */
        version: string;
    }

    export interface JobTemplateTemplateVpcAccess {
        /**
         * VPC Access connector name. Format: projects/{project}/locations/{location}/connectors/{connector}, where {project} can be project id or number.
         */
        connector?: string;
        /**
         * Traffic VPC egress settings.
         * Possible values are: `ALL_TRAFFIC`, `PRIVATE_RANGES_ONLY`.
         */
        egress: string;
        /**
         * Direct VPC egress settings. Currently only single network interface is supported.
         * Structure is documented below.
         */
        networkInterfaces?: outputs.cloudrunv2.JobTemplateTemplateVpcAccessNetworkInterface[];
    }

    export interface JobTemplateTemplateVpcAccessNetworkInterface {
        /**
         * The VPC network that the Cloud Run resource will be able to send traffic to. At least one of network or subnetwork must be specified. If both
         * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If network is not specified, it will be
         * looked up from the subnetwork.
         */
        network: string;
        /**
         * The VPC subnetwork that the Cloud Run resource will get IPs from. At least one of network or subnetwork must be specified. If both
         * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If subnetwork is not specified, the
         * subnetwork with the same name with the network will be used.
         */
        subnetwork: string;
        /**
         * Network tags applied to this Cloud Run job.
         *
         * - - -
         */
        tags?: string[];
    }

    export interface JobTerminalCondition {
        /**
         * (Output)
         * A reason for the execution condition.
         */
        executionReason: string;
        /**
         * (Output)
         * Last time the condition transitioned from one status to another.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        lastTransitionTime: string;
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
        /**
         * (Output)
         * A common (service-level) reason for this condition.
         */
        reason: string;
        /**
         * (Output)
         * A reason for the revision condition.
         */
        revisionReason: string;
        /**
         * (Output)
         * How to interpret failures of this condition, one of Error, Warning, Info
         */
        severity: string;
        /**
         * (Output)
         * State of the condition.
         */
        state: string;
        /**
         * (Output)
         * type is used to communicate the status of the reconciliation process. See also: https://github.com/knative/serving/blob/main/docs/spec/errors.md#error-conditions-and-reporting Types common to all resources include: * "Ready": True when the Resource is ready.
         */
        type: string;
    }

    export interface ServiceBinaryAuthorization {
        /**
         * If present, indicates to use Breakglass using this justification. If useDefault is False, then it must be empty. For more information on breakglass, see https://cloud.google.com/binary-authorization/docs/using-breakglass
         */
        breakglassJustification?: string;
        /**
         * If True, indicates to use the default project's binary authorization policy. If False, binary authorization will be disabled.
         */
        useDefault?: boolean;
    }

    export interface ServiceCondition {
        /**
         * (Output)
         * A reason for the execution condition.
         */
        executionReason: string;
        /**
         * (Output)
         * Last time the condition transitioned from one status to another.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        lastTransitionTime: string;
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
        /**
         * (Output)
         * A common (service-level) reason for this condition.
         */
        reason: string;
        /**
         * (Output)
         * A reason for the revision condition.
         */
        revisionReason: string;
        /**
         * (Output)
         * How to interpret failures of this condition, one of Error, Warning, Info
         */
        severity: string;
        /**
         * (Output)
         * State of the condition.
         */
        state: string;
        /**
         * The allocation type for this traffic target.
         * Possible values are: `TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST`, `TRAFFIC_TARGET_ALLOCATION_TYPE_REVISION`.
         */
        type: string;
    }

    export interface ServiceIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServiceIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServiceTemplate {
        /**
         * Unstructured key value map that may be set by external tools to store and arbitrary metadata. They are not queryable and should be preserved when modifying objects.
         * Cloud Run API v2 does not support annotations with `run.googleapis.com`, `cloud.googleapis.com`, `serving.knative.dev`, or `autoscaling.knative.dev` namespaces, and they will be rejected.
         * All system annotations in v1 now have a corresponding field in v2 RevisionTemplate.
         * This field follows Kubernetes annotations' namespacing, limits, and rules.
         */
        annotations?: {[key: string]: string};
        /**
         * Holds the containers that define the unit of execution for this Service.
         * Structure is documented below.
         */
        containers?: outputs.cloudrunv2.ServiceTemplateContainer[];
        /**
         * A reference to a customer managed encryption key (CMEK) to use to encrypt this container image. For more information, go to https://cloud.google.com/run/docs/securing/using-cmek
         */
        encryptionKey?: string;
        /**
         * The sandbox environment to host this Revision.
         * Possible values are: `EXECUTION_ENVIRONMENT_GEN1`, `EXECUTION_ENVIRONMENT_GEN2`.
         */
        executionEnvironment?: string;
        /**
         * Unstructured key value map that can be used to organize and categorize objects. User-provided labels are shared with Google's billing system, so they can be used to filter, or break down billing charges by team, component, environment, state, etc.
         * For more information, visit https://cloud.google.com/resource-manager/docs/creating-managing-labels or https://cloud.google.com/run/docs/configuring/labels.
         * Cloud Run API v2 does not support labels with `run.googleapis.com`, `cloud.googleapis.com`, `serving.knative.dev`, or `autoscaling.knative.dev` namespaces, and they will be rejected.
         * All system labels in v1 now have a corresponding field in v2 RevisionTemplate.
         */
        labels?: {[key: string]: string};
        /**
         * Sets the maximum number of requests that each serving instance can receive.
         */
        maxInstanceRequestConcurrency: number;
        /**
         * The unique name for the revision. If this field is omitted, it will be automatically generated based on the Service name.
         */
        revision?: string;
        /**
         * Scaling settings for this Revision.
         * Structure is documented below.
         */
        scaling: outputs.cloudrunv2.ServiceTemplateScaling;
        /**
         * Email address of the IAM service account associated with the revision of the service. The service account represents the identity of the running revision, and determines what permissions the revision has. If not provided, the revision will use the project's default service account.
         */
        serviceAccount: string;
        /**
         * Enables session affinity. For more information, go to https://cloud.google.com/run/docs/configuring/session-affinity
         */
        sessionAffinity?: boolean;
        /**
         * Max allowed time for an instance to respond to a request.
         * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
         */
        timeout: string;
        /**
         * A list of Volumes to make available to containers.
         * Structure is documented below.
         */
        volumes?: outputs.cloudrunv2.ServiceTemplateVolume[];
        /**
         * VPC Access configuration to use for this Task. For more information, visit https://cloud.google.com/run/docs/configuring/connecting-vpc.
         * Structure is documented below.
         */
        vpcAccess?: outputs.cloudrunv2.ServiceTemplateVpcAccess;
    }

    export interface ServiceTemplateContainer {
        /**
         * Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
         */
        args?: string[];
        /**
         * Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
         */
        commands?: string[];
        dependsOns?: string[];
        /**
         * List of environment variables to set in the container.
         * Structure is documented below.
         */
        envs?: outputs.cloudrunv2.ServiceTemplateContainerEnv[];
        /**
         * URL of the Container image in Google Container Registry or Google Artifact Registry. More info: https://kubernetes.io/docs/concepts/containers/images
         */
        image: string;
        /**
         * Periodic probe of container liveness. Container will be restarted if the probe fails. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         * Structure is documented below.
         */
        livenessProbe: outputs.cloudrunv2.ServiceTemplateContainerLivenessProbe;
        /**
         * Name of the container specified as a DNS_LABEL.
         */
        name?: string;
        /**
         * List of ports to expose from the container. Only a single port can be specified. The specified ports must be listening on all interfaces (0.0.0.0) within the container to be accessible.
         * If omitted, a port number will be chosen and passed to the container through the PORT environment variable for the container to listen on
         * Structure is documented below.
         */
        ports: outputs.cloudrunv2.ServiceTemplateContainerPort[];
        /**
         * Compute Resource requirements by this container. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
         * Structure is documented below.
         */
        resources: outputs.cloudrunv2.ServiceTemplateContainerResources;
        /**
         * Startup probe of application within the container. All other probes are disabled if a startup probe is provided, until it succeeds. Container will not be added to service endpoints if the probe fails. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         * Structure is documented below.
         */
        startupProbe: outputs.cloudrunv2.ServiceTemplateContainerStartupProbe;
        /**
         * Volume to mount into the container's filesystem.
         * Structure is documented below.
         */
        volumeMounts?: outputs.cloudrunv2.ServiceTemplateContainerVolumeMount[];
        /**
         * Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image.
         */
        workingDir?: string;
    }

    export interface ServiceTemplateContainerEnv {
        /**
         * Name of the environment variable. Must be a C_IDENTIFIER, and mnay not exceed 32768 characters.
         */
        name: string;
        /**
         * Variable references $(VAR_NAME) are expanded using the previous defined environment variables in the container and any route environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to "", and the maximum length is 32768 bytes
         */
        value?: string;
        /**
         * Source for the environment variable's value.
         * Structure is documented below.
         */
        valueSource?: outputs.cloudrunv2.ServiceTemplateContainerEnvValueSource;
    }

    export interface ServiceTemplateContainerEnvValueSource {
        /**
         * Selects a secret and a specific version from Cloud Secret Manager.
         * Structure is documented below.
         */
        secretKeyRef?: outputs.cloudrunv2.ServiceTemplateContainerEnvValueSourceSecretKeyRef;
    }

    export interface ServiceTemplateContainerEnvValueSourceSecretKeyRef {
        /**
         * The name of the secret in Cloud Secret Manager. Format: {secretName} if the secret is in the same project. projects/{project}/secrets/{secretName} if the secret is in a different project.
         */
        secret: string;
        /**
         * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version.
         */
        version?: string;
    }

    export interface ServiceTemplateContainerLivenessProbe {
        /**
         * Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1.
         */
        failureThreshold?: number;
        /**
         * GRPC specifies an action involving a GRPC port.
         * Structure is documented below.
         */
        grpc?: outputs.cloudrunv2.ServiceTemplateContainerLivenessProbeGrpc;
        /**
         * HTTPGet specifies the http request to perform.
         * Structure is documented below.
         */
        httpGet?: outputs.cloudrunv2.ServiceTemplateContainerLivenessProbeHttpGet;
        /**
         * Number of seconds after the container has started before the probe is initiated. Defaults to 0 seconds. Minimum value is 0. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        initialDelaySeconds?: number;
        /**
         * How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. Must be greater or equal than timeoutSeconds
         */
        periodSeconds?: number;
        /**
         * (Optional, Deprecated)
         * TCPSocket specifies an action involving a TCP port. This field is not supported in liveness probe currently.
         * Structure is documented below.
         *
         * > **Warning:** `tcpSocket` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         *
         * @deprecated `tcp_socket` is deprecated and will be removed in a future major release. This field is not supported by the Cloud Run API.
         */
        tcpSocket?: outputs.cloudrunv2.ServiceTemplateContainerLivenessProbeTcpSocket;
        /**
         * Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Maximum value is 3600. Must be smaller than periodSeconds. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        timeoutSeconds?: number;
    }

    export interface ServiceTemplateContainerLivenessProbeGrpc {
        /**
         * Port number to access on the container. Number must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
        /**
         * The name of the service to place in the gRPC HealthCheckRequest
         * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
         * If this is not specified, the default behavior is defined by gRPC.
         */
        service?: string;
    }

    export interface ServiceTemplateContainerLivenessProbeHttpGet {
        /**
         * Custom headers to set in the request. HTTP allows repeated headers.
         * Structure is documented below.
         */
        httpHeaders?: outputs.cloudrunv2.ServiceTemplateContainerLivenessProbeHttpGetHttpHeader[];
        /**
         * Path to access on the HTTP server. Defaults to '/'.
         */
        path?: string;
        /**
         * Port number to access on the container. Must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
    }

    export interface ServiceTemplateContainerLivenessProbeHttpGetHttpHeader {
        /**
         * The header field name
         */
        name: string;
        /**
         * The header field value
         */
        value?: string;
    }

    export interface ServiceTemplateContainerLivenessProbeTcpSocket {
        /**
         * Port number to access on the container. Must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port?: number;
    }

    export interface ServiceTemplateContainerPort {
        /**
         * Port number the container listens on. This must be a valid TCP port number, 0 < containerPort < 65536.
         */
        containerPort?: number;
        /**
         * If specified, used to specify which protocol to use. Allowed values are "http1" and "h2c".
         */
        name: string;
    }

    export interface ServiceTemplateContainerResources {
        /**
         * Determines whether CPU should be throttled or not outside of requests.
         */
        cpuIdle?: boolean;
        /**
         * Only memory and CPU are supported. Note: The only supported values for CPU are '1', '2', '4', and '8'. Setting 4 CPU requires at least 2Gi of memory. The values of the map is string form of the 'quantity' k8s type: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
         */
        limits: {[key: string]: string};
        /**
         * Determines whether CPU should be boosted on startup of a new container instance above the requested CPU threshold, this can help reduce cold-start latency.
         */
        startupCpuBoost?: boolean;
    }

    export interface ServiceTemplateContainerStartupProbe {
        /**
         * Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1.
         */
        failureThreshold?: number;
        /**
         * GRPC specifies an action involving a GRPC port.
         * Structure is documented below.
         */
        grpc?: outputs.cloudrunv2.ServiceTemplateContainerStartupProbeGrpc;
        /**
         * HTTPGet specifies the http request to perform. Exactly one of HTTPGet or TCPSocket must be specified.
         * Structure is documented below.
         */
        httpGet?: outputs.cloudrunv2.ServiceTemplateContainerStartupProbeHttpGet;
        /**
         * Number of seconds after the container has started before the probe is initiated. Defaults to 0 seconds. Minimum value is 0. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        initialDelaySeconds?: number;
        /**
         * How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. Must be greater or equal than timeoutSeconds
         */
        periodSeconds?: number;
        /**
         * TCPSocket specifies an action involving a TCP port. Exactly one of HTTPGet or TCPSocket must be specified.
         * Structure is documented below.
         */
        tcpSocket?: outputs.cloudrunv2.ServiceTemplateContainerStartupProbeTcpSocket;
        /**
         * Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Maximum value is 3600. Must be smaller than periodSeconds. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
         */
        timeoutSeconds?: number;
    }

    export interface ServiceTemplateContainerStartupProbeGrpc {
        /**
         * Port number to access on the container. Number must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
        /**
         * The name of the service to place in the gRPC HealthCheckRequest
         * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
         * If this is not specified, the default behavior is defined by gRPC.
         */
        service?: string;
    }

    export interface ServiceTemplateContainerStartupProbeHttpGet {
        /**
         * Custom headers to set in the request. HTTP allows repeated headers.
         * Structure is documented below.
         */
        httpHeaders?: outputs.cloudrunv2.ServiceTemplateContainerStartupProbeHttpGetHttpHeader[];
        /**
         * Path to access on the HTTP server. Defaults to '/'.
         */
        path?: string;
        /**
         * Port number to access on the container. Must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
    }

    export interface ServiceTemplateContainerStartupProbeHttpGetHttpHeader {
        /**
         * The header field name
         */
        name: string;
        /**
         * The header field value
         */
        value?: string;
    }

    export interface ServiceTemplateContainerStartupProbeTcpSocket {
        /**
         * Port number to access on the container. Must be in the range 1 to 65535.
         * If not specified, defaults to the same value as container.ports[0].containerPort.
         */
        port: number;
    }

    export interface ServiceTemplateContainerVolumeMount {
        /**
         * Path within the container at which the volume should be mounted. Must not contain ':'. For Cloud SQL volumes, it can be left empty, or must otherwise be /cloudsql. All instances defined in the Volume will be available as /cloudsql/[instance]. For more information on Cloud SQL volumes, visit https://cloud.google.com/sql/docs/mysql/connect-run
         */
        mountPath: string;
        /**
         * This must match the Name of a Volume.
         */
        name: string;
    }

    export interface ServiceTemplateScaling {
        /**
         * Maximum number of serving instances that this resource should have.
         */
        maxInstanceCount?: number;
        /**
         * Minimum number of serving instances that this resource should have.
         */
        minInstanceCount?: number;
    }

    export interface ServiceTemplateVolume {
        /**
         * For Cloud SQL volumes, contains the specific instances that should be mounted. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run.
         * Structure is documented below.
         */
        cloudSqlInstance?: outputs.cloudrunv2.ServiceTemplateVolumeCloudSqlInstance;
        emptyDir?: outputs.cloudrunv2.ServiceTemplateVolumeEmptyDir;
        /**
         * Volume's name.
         */
        name: string;
        /**
         * Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
         * Structure is documented below.
         */
        secret?: outputs.cloudrunv2.ServiceTemplateVolumeSecret;
    }

    export interface ServiceTemplateVolumeCloudSqlInstance {
        /**
         * The Cloud SQL instance connection names, as can be found in https://console.cloud.google.com/sql/instances. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run. Format: {project}:{location}:{instance}
         */
        instances?: string[];
    }

    export interface ServiceTemplateVolumeEmptyDir {
        /**
         * The different types of medium supported for EmptyDir.
         * Default value is `MEMORY`.
         * Possible values are: `MEMORY`.
         */
        medium?: string;
        /**
         * Limit on the storage usable by this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. This field's values are of the 'Quantity' k8s type: https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/. The default is nil which means that the limit is undefined. More info: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir.
         *
         * - - -
         */
        sizeLimit?: string;
    }

    export interface ServiceTemplateVolumeSecret {
        /**
         * Integer representation of mode bits to use on created files by default. Must be a value between 0000 and 0777 (octal), defaulting to 0444. Directories within the path are not affected by this setting.
         */
        defaultMode?: number;
        /**
         * If unspecified, the volume will expose a file whose name is the secret, relative to VolumeMount.mount_path. If specified, the key will be used as the version to fetch from Cloud Secret Manager and the path will be the name of the file exposed in the volume. When items are defined, they must specify a path and a version.
         * Structure is documented below.
         */
        items?: outputs.cloudrunv2.ServiceTemplateVolumeSecretItem[];
        /**
         * The name of the secret in Cloud Secret Manager. Format: {secret} if the secret is in the same project. projects/{project}/secrets/{secret} if the secret is in a different project.
         */
        secret: string;
    }

    export interface ServiceTemplateVolumeSecretItem {
        /**
         * Integer octal mode bits to use on this file, must be a value between 01 and 0777 (octal). If 0 or not set, the Volume's default mode will be used.
         */
        mode?: number;
        /**
         * The relative path of the secret in the container.
         */
        path: string;
        /**
         * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version
         */
        version?: string;
    }

    export interface ServiceTemplateVpcAccess {
        /**
         * VPC Access connector name. Format: projects/{project}/locations/{location}/connectors/{connector}, where {project} can be project id or number.
         */
        connector?: string;
        /**
         * Traffic VPC egress settings.
         * Possible values are: `ALL_TRAFFIC`, `PRIVATE_RANGES_ONLY`.
         */
        egress: string;
        /**
         * Direct VPC egress settings. Currently only single network interface is supported.
         * Structure is documented below.
         */
        networkInterfaces?: outputs.cloudrunv2.ServiceTemplateVpcAccessNetworkInterface[];
    }

    export interface ServiceTemplateVpcAccessNetworkInterface {
        /**
         * The VPC network that the Cloud Run resource will be able to send traffic to. At least one of network or subnetwork must be specified. If both
         * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If network is not specified, it will be
         * looked up from the subnetwork.
         */
        network: string;
        /**
         * The VPC subnetwork that the Cloud Run resource will get IPs from. At least one of network or subnetwork must be specified. If both
         * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If subnetwork is not specified, the
         * subnetwork with the same name with the network will be used.
         */
        subnetwork: string;
        /**
         * Network tags applied to this Cloud Run service.
         */
        tags?: string[];
    }

    export interface ServiceTerminalCondition {
        /**
         * (Output)
         * A reason for the execution condition.
         */
        executionReason: string;
        /**
         * (Output)
         * Last time the condition transitioned from one status to another.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        lastTransitionTime: string;
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
        /**
         * (Output)
         * A common (service-level) reason for this condition.
         */
        reason: string;
        /**
         * (Output)
         * A reason for the revision condition.
         */
        revisionReason: string;
        /**
         * (Output)
         * How to interpret failures of this condition, one of Error, Warning, Info
         */
        severity: string;
        /**
         * (Output)
         * State of the condition.
         */
        state: string;
        /**
         * The allocation type for this traffic target.
         * Possible values are: `TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST`, `TRAFFIC_TARGET_ALLOCATION_TYPE_REVISION`.
         */
        type: string;
    }

    export interface ServiceTraffic {
        /**
         * Specifies percent of the traffic to this Revision. This defaults to zero if unspecified.
         */
        percent: number;
        /**
         * Revision to which to send this portion of traffic, if traffic allocation is by revision.
         */
        revision?: string;
        /**
         * Indicates a string to be part of the URI to exclusively reference this target.
         */
        tag?: string;
        /**
         * The allocation type for this traffic target.
         * Possible values are: `TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST`, `TRAFFIC_TARGET_ALLOCATION_TYPE_REVISION`.
         */
        type?: string;
    }

    export interface ServiceTrafficStatus {
        /**
         * Specifies percent of the traffic to this Revision. This defaults to zero if unspecified.
         */
        percent: number;
        /**
         * The unique name for the revision. If this field is omitted, it will be automatically generated based on the Service name.
         *
         * (Optional)
         * Revision to which to send this portion of traffic, if traffic allocation is by revision.
         */
        revision: string;
        /**
         * Indicates a string to be part of the URI to exclusively reference this target.
         */
        tag: string;
        /**
         * The allocation type for this traffic target.
         * Possible values are: `TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST`, `TRAFFIC_TARGET_ALLOCATION_TYPE_REVISION`.
         */
        type: string;
        /**
         * (Output)
         * Displays the target URI.
         */
        uri: string;
    }

}

export namespace cloudscheduler {
    export interface JobAppEngineHttpTarget {
        /**
         * App Engine Routing setting for the job.
         * Structure is documented below.
         */
        appEngineRouting?: outputs.cloudscheduler.JobAppEngineHttpTargetAppEngineRouting;
        /**
         * HTTP request body.
         * A request body is allowed only if the HTTP method is POST or PUT.
         * It will result in invalid argument error to set a body on a job with an incompatible HttpMethod.
         * A base64-encoded string.
         */
        body?: string;
        /**
         * HTTP request headers.
         * This map contains the header field names and values.
         * Headers can be set when the job is created.
         */
        headers?: {[key: string]: string};
        /**
         * Which HTTP method to use for the request.
         */
        httpMethod?: string;
        /**
         * The relative URI.
         * The relative URL must begin with "/" and must be a valid HTTP relative URL.
         * It can contain a path, query string arguments, and \# fragments.
         * If the relative URL is empty, then the root path "/" will be used.
         * No spaces are allowed, and the maximum length allowed is 2083 characters
         */
        relativeUri: string;
    }

    export interface JobAppEngineHttpTargetAppEngineRouting {
        /**
         * App instance.
         * By default, the job is sent to an instance which is available when the job is attempted.
         */
        instance?: string;
        /**
         * App service.
         * By default, the job is sent to the service which is the default service when the job is attempted.
         */
        service?: string;
        /**
         * App version.
         * By default, the job is sent to the version which is the default version when the job is attempted.
         */
        version?: string;
    }

    export interface JobHttpTarget {
        /**
         * HTTP request body.
         * A request body is allowed only if the HTTP method is POST, PUT, or PATCH.
         * It is an error to set body on a job with an incompatible HttpMethod.
         * A base64-encoded string.
         */
        body?: string;
        /**
         * This map contains the header field names and values.
         * Repeated headers are not supported, but a header value can contain commas.
         */
        headers?: {[key: string]: string};
        /**
         * Which HTTP method to use for the request.
         */
        httpMethod?: string;
        /**
         * Contains information needed for generating an OAuth token.
         * This type of authorization should be used when sending requests to a GCP endpoint.
         * Structure is documented below.
         */
        oauthToken?: outputs.cloudscheduler.JobHttpTargetOauthToken;
        /**
         * Contains information needed for generating an OpenID Connect token.
         * This type of authorization should be used when sending requests to third party endpoints or Cloud Run.
         * Structure is documented below.
         */
        oidcToken?: outputs.cloudscheduler.JobHttpTargetOidcToken;
        /**
         * The full URI path that the request will be sent to.
         */
        uri: string;
    }

    export interface JobHttpTargetOauthToken {
        /**
         * OAuth scope to be used for generating OAuth access token. If not specified,
         * "https://www.googleapis.com/auth/cloud-platform" will be used.
         */
        scope?: string;
        /**
         * Service account email to be used for generating OAuth token.
         * The service account must be within the same project as the job.
         */
        serviceAccountEmail: string;
    }

    export interface JobHttpTargetOidcToken {
        /**
         * Audience to be used when generating OIDC token. If not specified,
         * the URI specified in target will be used.
         */
        audience?: string;
        /**
         * Service account email to be used for generating OAuth token.
         * The service account must be within the same project as the job.
         */
        serviceAccountEmail: string;
    }

    export interface JobPubsubTarget {
        /**
         * Attributes for PubsubMessage.
         * Pubsub message must contain either non-empty data, or at least one attribute.
         */
        attributes?: {[key: string]: string};
        /**
         * The message payload for PubsubMessage.
         * Pubsub message must contain either non-empty data, or at least one attribute.
         * A base64-encoded string.
         */
        data?: string;
        /**
         * The full resource name for the Cloud Pub/Sub topic to which
         * messages will be published when a job is delivered. ~>**NOTE:**
         * The topic name must be in the same format as required by PubSub's
         * PublishRequest.name, e.g. `projects/my-project/topics/my-topic`.
         */
        topicName: string;
    }

    export interface JobRetryConfig {
        /**
         * The maximum amount of time to wait before retrying a job after it fails.
         * A duration in seconds with up to nine fractional digits, terminated by 's'.
         */
        maxBackoffDuration: string;
        /**
         * The time between retries will double maxDoublings times.
         * A job's retry interval starts at minBackoffDuration,
         * then doubles maxDoublings times, then increases linearly,
         * and finally retries retries at intervals of maxBackoffDuration up to retryCount times.
         */
        maxDoublings: number;
        /**
         * The time limit for retrying a failed job, measured from time when an execution was first attempted.
         * If specified with retryCount, the job will be retried until both limits are reached.
         * A duration in seconds with up to nine fractional digits, terminated by 's'.
         */
        maxRetryDuration: string;
        /**
         * The minimum amount of time to wait before retrying a job after it fails.
         * A duration in seconds with up to nine fractional digits, terminated by 's'.
         */
        minBackoffDuration: string;
        /**
         * The number of attempts that the system will make to run a
         * job using the exponential backoff procedure described by maxDoublings.
         * Values greater than 5 and negative values are not allowed.
         */
        retryCount: number;
    }

}

export namespace cloudtasks {
    export interface QueueAppEngineRoutingOverride {
        /**
         * (Output)
         * The host that the task is sent to.
         */
        host: string;
        /**
         * App instance.
         * By default, the task is sent to an instance which is available when the task is attempted.
         */
        instance?: string;
        /**
         * App service.
         * By default, the task is sent to the service which is the default service when the task is attempted.
         */
        service?: string;
        /**
         * App version.
         * By default, the task is sent to the version which is the default version when the task is attempted.
         */
        version?: string;
    }

    export interface QueueIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface QueueIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface QueueRateLimits {
        /**
         * (Output)
         * The max burst size.
         * Max burst size limits how fast tasks in queue are processed when many tasks are
         * in the queue and the rate is high. This field allows the queue to have a high
         * rate so processing starts shortly after a task is enqueued, but still limits
         * resource usage when many tasks are enqueued in a short period of time.
         */
        maxBurstSize: number;
        /**
         * The maximum number of concurrent tasks that Cloud Tasks allows to
         * be dispatched for this queue. After this threshold has been
         * reached, Cloud Tasks stops dispatching tasks until the number of
         * concurrent requests decreases.
         */
        maxConcurrentDispatches: number;
        /**
         * The maximum rate at which tasks are dispatched from this queue.
         * If unspecified when the queue is created, Cloud Tasks will pick the default.
         */
        maxDispatchesPerSecond: number;
    }

    export interface QueueRetryConfig {
        /**
         * Number of attempts per task.
         * Cloud Tasks will attempt the task maxAttempts times (that is, if
         * the first attempt fails, then there will be maxAttempts - 1
         * retries). Must be >= -1.
         * If unspecified when the queue is created, Cloud Tasks will pick
         * the default.
         * -1 indicates unlimited attempts.
         */
        maxAttempts: number;
        /**
         * A task will be scheduled for retry between minBackoff and
         * maxBackoff duration after it fails, if the queue's RetryConfig
         * specifies that the task should be retried.
         */
        maxBackoff: string;
        /**
         * The time between retries will double maxDoublings times.
         * A task's retry interval starts at minBackoff, then doubles maxDoublings times,
         * then increases linearly, and finally retries retries at intervals of maxBackoff
         * up to maxAttempts times.
         */
        maxDoublings: number;
        /**
         * If positive, maxRetryDuration specifies the time limit for
         * retrying a failed task, measured from when the task was first
         * attempted. Once maxRetryDuration time has passed and the task has
         * been attempted maxAttempts times, no further attempts will be
         * made and the task will be deleted.
         * If zero, then the task age is unlimited.
         */
        maxRetryDuration: string;
        /**
         * A task will be scheduled for retry between minBackoff and
         * maxBackoff duration after it fails, if the queue's RetryConfig
         * specifies that the task should be retried.
         */
        minBackoff: string;
    }

    export interface QueueStackdriverLoggingConfig {
        /**
         * Specifies the fraction of operations to write to Stackdriver Logging.
         * This field may contain any value between 0.0 and 1.0, inclusive. 0.0 is the
         * default and means that no operations are logged.
         */
        samplingRatio: number;
    }

}

export namespace composer {
    export interface EnvironmentConfig {
        airflowUri: string;
        dagGcsPrefix: string;
        databaseConfig: outputs.composer.EnvironmentConfigDatabaseConfig;
        encryptionConfig: outputs.composer.EnvironmentConfigEncryptionConfig;
        environmentSize: string;
        gkeCluster: string;
        maintenanceWindow: outputs.composer.EnvironmentConfigMaintenanceWindow;
        masterAuthorizedNetworksConfig?: outputs.composer.EnvironmentConfigMasterAuthorizedNetworksConfig;
        nodeConfig: outputs.composer.EnvironmentConfigNodeConfig;
        nodeCount: number;
        privateEnvironmentConfig: outputs.composer.EnvironmentConfigPrivateEnvironmentConfig;
        recoveryConfig?: outputs.composer.EnvironmentConfigRecoveryConfig;
        resilienceMode: string;
        softwareConfig: outputs.composer.EnvironmentConfigSoftwareConfig;
        webServerConfig: outputs.composer.EnvironmentConfigWebServerConfig;
        webServerNetworkAccessControl: outputs.composer.EnvironmentConfigWebServerNetworkAccessControl;
        workloadsConfig: outputs.composer.EnvironmentConfigWorkloadsConfig;
    }

    export interface EnvironmentConfigDatabaseConfig {
        machineType: string;
    }

    export interface EnvironmentConfigEncryptionConfig {
        kmsKeyName: string;
    }

    export interface EnvironmentConfigMaintenanceWindow {
        endTime: string;
        recurrence: string;
        startTime: string;
    }

    export interface EnvironmentConfigMasterAuthorizedNetworksConfig {
        cidrBlocks?: outputs.composer.EnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock[];
        enabled: boolean;
    }

    export interface EnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock {
        cidrBlock: string;
        displayName?: string;
    }

    export interface EnvironmentConfigNodeConfig {
        diskSizeGb: number;
        enableIpMasqAgent: boolean;
        ipAllocationPolicy: outputs.composer.EnvironmentConfigNodeConfigIpAllocationPolicy;
        machineType: string;
        maxPodsPerNode: number;
        network: string;
        oauthScopes: string[];
        serviceAccount: string;
        subnetwork?: string;
        tags?: string[];
        zone: string;
    }

    export interface EnvironmentConfigNodeConfigIpAllocationPolicy {
        clusterIpv4CidrBlock?: string;
        clusterSecondaryRangeName?: string;
        servicesIpv4CidrBlock?: string;
        servicesSecondaryRangeName?: string;
        useIpAliases?: boolean;
    }

    export interface EnvironmentConfigPrivateEnvironmentConfig {
        cloudComposerConnectionSubnetwork: string;
        cloudComposerNetworkIpv4CidrBlock: string;
        cloudSqlIpv4CidrBlock: string;
        connectionType: string;
        enablePrivateEndpoint?: boolean;
        enablePrivatelyUsedPublicIps: boolean;
        masterIpv4CidrBlock: string;
        webServerIpv4CidrBlock: string;
    }

    export interface EnvironmentConfigRecoveryConfig {
        scheduledSnapshotsConfig?: outputs.composer.EnvironmentConfigRecoveryConfigScheduledSnapshotsConfig;
    }

    export interface EnvironmentConfigRecoveryConfigScheduledSnapshotsConfig {
        enabled: boolean;
        snapshotCreationSchedule?: string;
        snapshotLocation?: string;
        timeZone?: string;
    }

    export interface EnvironmentConfigSoftwareConfig {
        airflowConfigOverrides?: {[key: string]: string};
        cloudDataLineageIntegration: outputs.composer.EnvironmentConfigSoftwareConfigCloudDataLineageIntegration;
        envVariables?: {[key: string]: string};
        imageVersion: string;
        pypiPackages?: {[key: string]: string};
        pythonVersion: string;
        schedulerCount: number;
    }

    export interface EnvironmentConfigSoftwareConfigCloudDataLineageIntegration {
        enabled: boolean;
    }

    export interface EnvironmentConfigWebServerConfig {
        machineType: string;
    }

    export interface EnvironmentConfigWebServerNetworkAccessControl {
        allowedIpRanges: outputs.composer.EnvironmentConfigWebServerNetworkAccessControlAllowedIpRange[];
    }

    export interface EnvironmentConfigWebServerNetworkAccessControlAllowedIpRange {
        description?: string;
        value: string;
    }

    export interface EnvironmentConfigWorkloadsConfig {
        scheduler?: outputs.composer.EnvironmentConfigWorkloadsConfigScheduler;
        triggerer?: outputs.composer.EnvironmentConfigWorkloadsConfigTriggerer;
        webServer?: outputs.composer.EnvironmentConfigWorkloadsConfigWebServer;
        worker?: outputs.composer.EnvironmentConfigWorkloadsConfigWorker;
    }

    export interface EnvironmentConfigWorkloadsConfigScheduler {
        count?: number;
        cpu?: number;
        memoryGb?: number;
        storageGb?: number;
    }

    export interface EnvironmentConfigWorkloadsConfigTriggerer {
        count: number;
        cpu: number;
        memoryGb: number;
    }

    export interface EnvironmentConfigWorkloadsConfigWebServer {
        cpu?: number;
        memoryGb?: number;
        storageGb?: number;
    }

    export interface EnvironmentConfigWorkloadsConfigWorker {
        cpu?: number;
        maxCount?: number;
        memoryGb?: number;
        minCount?: number;
        storageGb?: number;
    }

    export interface GetEnvironmentConfig {
        airflowUri: string;
        dagGcsPrefix: string;
        databaseConfigs: outputs.composer.GetEnvironmentConfigDatabaseConfig[];
        encryptionConfigs: outputs.composer.GetEnvironmentConfigEncryptionConfig[];
        environmentSize: string;
        gkeCluster: string;
        maintenanceWindows: outputs.composer.GetEnvironmentConfigMaintenanceWindow[];
        masterAuthorizedNetworksConfigs: outputs.composer.GetEnvironmentConfigMasterAuthorizedNetworksConfig[];
        nodeConfigs: outputs.composer.GetEnvironmentConfigNodeConfig[];
        nodeCount: number;
        privateEnvironmentConfigs: outputs.composer.GetEnvironmentConfigPrivateEnvironmentConfig[];
        recoveryConfigs: outputs.composer.GetEnvironmentConfigRecoveryConfig[];
        resilienceMode: string;
        softwareConfigs: outputs.composer.GetEnvironmentConfigSoftwareConfig[];
        webServerConfigs: outputs.composer.GetEnvironmentConfigWebServerConfig[];
        webServerNetworkAccessControls: outputs.composer.GetEnvironmentConfigWebServerNetworkAccessControl[];
        workloadsConfigs: outputs.composer.GetEnvironmentConfigWorkloadsConfig[];
    }

    export interface GetEnvironmentConfigDatabaseConfig {
        machineType: string;
    }

    export interface GetEnvironmentConfigEncryptionConfig {
        kmsKeyName: string;
    }

    export interface GetEnvironmentConfigMaintenanceWindow {
        endTime: string;
        recurrence: string;
        startTime: string;
    }

    export interface GetEnvironmentConfigMasterAuthorizedNetworksConfig {
        cidrBlocks: outputs.composer.GetEnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock[];
        enabled: boolean;
    }

    export interface GetEnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock {
        cidrBlock: string;
        displayName: string;
    }

    export interface GetEnvironmentConfigNodeConfig {
        diskSizeGb: number;
        enableIpMasqAgent: boolean;
        ipAllocationPolicies: outputs.composer.GetEnvironmentConfigNodeConfigIpAllocationPolicy[];
        machineType: string;
        maxPodsPerNode: number;
        network: string;
        oauthScopes: string[];
        serviceAccount: string;
        subnetwork: string;
        tags: string[];
        zone: string;
    }

    export interface GetEnvironmentConfigNodeConfigIpAllocationPolicy {
        clusterIpv4CidrBlock: string;
        clusterSecondaryRangeName: string;
        servicesIpv4CidrBlock: string;
        servicesSecondaryRangeName: string;
        useIpAliases: boolean;
    }

    export interface GetEnvironmentConfigPrivateEnvironmentConfig {
        cloudComposerConnectionSubnetwork: string;
        cloudComposerNetworkIpv4CidrBlock: string;
        cloudSqlIpv4CidrBlock: string;
        connectionType: string;
        enablePrivateEndpoint: boolean;
        enablePrivatelyUsedPublicIps: boolean;
        masterIpv4CidrBlock: string;
        webServerIpv4CidrBlock: string;
    }

    export interface GetEnvironmentConfigRecoveryConfig {
        scheduledSnapshotsConfigs: outputs.composer.GetEnvironmentConfigRecoveryConfigScheduledSnapshotsConfig[];
    }

    export interface GetEnvironmentConfigRecoveryConfigScheduledSnapshotsConfig {
        enabled: boolean;
        snapshotCreationSchedule: string;
        snapshotLocation: string;
        timeZone: string;
    }

    export interface GetEnvironmentConfigSoftwareConfig {
        airflowConfigOverrides: {[key: string]: string};
        cloudDataLineageIntegrations: outputs.composer.GetEnvironmentConfigSoftwareConfigCloudDataLineageIntegration[];
        envVariables: {[key: string]: string};
        imageVersion: string;
        pypiPackages: {[key: string]: string};
        pythonVersion: string;
        schedulerCount: number;
    }

    export interface GetEnvironmentConfigSoftwareConfigCloudDataLineageIntegration {
        enabled: boolean;
    }

    export interface GetEnvironmentConfigWebServerConfig {
        machineType: string;
    }

    export interface GetEnvironmentConfigWebServerNetworkAccessControl {
        allowedIpRanges: outputs.composer.GetEnvironmentConfigWebServerNetworkAccessControlAllowedIpRange[];
    }

    export interface GetEnvironmentConfigWebServerNetworkAccessControlAllowedIpRange {
        description: string;
        value: string;
    }

    export interface GetEnvironmentConfigWorkloadsConfig {
        schedulers: outputs.composer.GetEnvironmentConfigWorkloadsConfigScheduler[];
        triggerers: outputs.composer.GetEnvironmentConfigWorkloadsConfigTriggerer[];
        webServers: outputs.composer.GetEnvironmentConfigWorkloadsConfigWebServer[];
        workers: outputs.composer.GetEnvironmentConfigWorkloadsConfigWorker[];
    }

    export interface GetEnvironmentConfigWorkloadsConfigScheduler {
        count: number;
        cpu: number;
        memoryGb: number;
        storageGb: number;
    }

    export interface GetEnvironmentConfigWorkloadsConfigTriggerer {
        count: number;
        cpu: number;
        memoryGb: number;
    }

    export interface GetEnvironmentConfigWorkloadsConfigWebServer {
        cpu: number;
        memoryGb: number;
        storageGb: number;
    }

    export interface GetEnvironmentConfigWorkloadsConfigWorker {
        cpu: number;
        maxCount: number;
        memoryGb: number;
        minCount: number;
        storageGb: number;
    }

    export interface GetImageVersionsImageVersion {
        /**
         * The string identifier of the image version, in the form: "composer-x.y.z-airflow-a.b.c"
         */
        imageVersionId: string;
        /**
         * Supported python versions for this image version
         */
        supportedPythonVersions: string[];
    }

}

export namespace compute {
    export interface AutoscalarAutoscalingPolicy {
        /**
         * The number of seconds that the autoscaler should wait before it
         * starts collecting information from a new instance. This prevents
         * the autoscaler from collecting information when the instance is
         * initializing, during which the collected usage would not be
         * reliable. The default time autoscaler waits is 60 seconds.
         * Virtual machine initialization times might vary because of
         * numerous factors. We recommend that you test how long an
         * instance may take to initialize. To do this, create an instance
         * and time the startup process.
         */
        cooldownPeriod?: number;
        /**
         * Defines the CPU utilization policy that allows the autoscaler to
         * scale based on the average CPU utilization of a managed instance
         * group.
         * Structure is documented below.
         */
        cpuUtilization: outputs.compute.AutoscalarAutoscalingPolicyCpuUtilization;
        /**
         * Configuration parameters of autoscaling based on a load balancer.
         * Structure is documented below.
         */
        loadBalancingUtilization?: outputs.compute.AutoscalarAutoscalingPolicyLoadBalancingUtilization;
        /**
         * The maximum number of instances that the autoscaler can scale up
         * to. This is required when creating or updating an autoscaler. The
         * maximum number of replicas should not be lower than minimal number
         * of replicas.
         */
        maxReplicas: number;
        /**
         * Configuration parameters of autoscaling based on a custom metric.
         * Structure is documented below.
         */
        metrics?: outputs.compute.AutoscalarAutoscalingPolicyMetric[];
        /**
         * The minimum number of replicas that the autoscaler can scale down
         * to. This cannot be less than 0. If not provided, autoscaler will
         * choose a default value depending on maximum number of instances
         * allowed.
         */
        minReplicas: number;
        /**
         * Defines operating mode for this policy.
         */
        mode?: string;
        /**
         * Defines scale down controls to reduce the risk of response latency
         * and outages due to abrupt scale-in events
         * Structure is documented below.
         */
        scaleDownControl: outputs.compute.AutoscalarAutoscalingPolicyScaleDownControl;
        /**
         * Defines scale in controls to reduce the risk of response latency
         * and outages due to abrupt scale-in events
         * Structure is documented below.
         */
        scaleInControl?: outputs.compute.AutoscalarAutoscalingPolicyScaleInControl;
        /**
         * Scaling schedules defined for an autoscaler. Multiple schedules can be set on an autoscaler and they can overlap.
         * Structure is documented below.
         */
        scalingSchedules?: outputs.compute.AutoscalarAutoscalingPolicyScalingSchedule[];
    }

    export interface AutoscalarAutoscalingPolicyCpuUtilization {
        /**
         * Indicates whether predictive autoscaling based on CPU metric is enabled. Valid values are:
         * - NONE (default). No predictive method is used. The autoscaler scales the group to meet current demand based on real-time metrics.
         * - OPTIMIZE_AVAILABILITY. Predictive autoscaling improves availability by monitoring daily and weekly load patterns and scaling out ahead of anticipated demand.
         */
        predictiveMethod?: string;
        /**
         * The target CPU utilization that the autoscaler should maintain.
         * Must be a float value in the range (0, 1]. If not specified, the
         * default is 0.6.
         * If the CPU level is below the target utilization, the autoscaler
         * scales down the number of instances until it reaches the minimum
         * number of instances you specified or until the average CPU of
         * your instances reaches the target utilization.
         * If the average CPU is above the target utilization, the autoscaler
         * scales up until it reaches the maximum number of instances you
         * specified or until the average utilization reaches the target
         * utilization.
         */
        target: number;
    }

    export interface AutoscalarAutoscalingPolicyLoadBalancingUtilization {
        /**
         * Fraction of backend capacity utilization (set in HTTP(s) load
         * balancing configuration) that autoscaler should maintain. Must
         * be a positive float value. If not defined, the default is 0.8.
         */
        target: number;
    }

    export interface AutoscalarAutoscalingPolicyMetric {
        /**
         * A filter string to be used as the filter string for
         * a Stackdriver Monitoring TimeSeries.list API call.
         * This filter is used to select a specific TimeSeries for
         * the purpose of autoscaling and to determine whether the metric
         * is exporting per-instance or per-group data.
         * You can only use the AND operator for joining selectors.
         * You can only use direct equality comparison operator (=) without
         * any functions for each selector.
         * You can specify the metric in both the filter string and in the
         * metric field. However, if specified in both places, the metric must
         * be identical.
         * The monitored resource type determines what kind of values are
         * expected for the metric. If it is a gce_instance, the autoscaler
         * expects the metric to include a separate TimeSeries for each
         * instance in a group. In such a case, you cannot filter on resource
         * labels.
         * If the resource type is any other value, the autoscaler expects
         * this metric to contain values that apply to the entire autoscaled
         * instance group and resource label filtering can be performed to
         * point autoscaler at the correct TimeSeries to scale upon.
         * This is called a per-group metric for the purpose of autoscaling.
         * If not specified, the type defaults to gce_instance.
         * You should provide a filter that is selective enough to pick just
         * one TimeSeries for the autoscaled group or for each of the instances
         * (if you are using gceInstance resource type). If multiple
         * TimeSeries are returned upon the query execution, the autoscaler
         * will sum their respective values to obtain its scaling value.
         */
        filter?: string;
        /**
         * The identifier (type) of the Stackdriver Monitoring metric.
         * The metric cannot have negative values.
         * The metric must have a value type of INT64 or DOUBLE.
         */
        name: string;
        /**
         * If scaling is based on a per-group metric value that represents the
         * total amount of work to be done or resource usage, set this value to
         * an amount assigned for a single instance of the scaled group.
         * The autoscaler will keep the number of instances proportional to the
         * value of this metric, the metric itself should not change value due
         * to group resizing.
         * For example, a good metric to use with the target is
         * `pubsub.googleapis.com/subscription/num_undelivered_messages`
         * or a custom metric exporting the total number of requests coming to
         * your instances.
         * A bad example would be a metric exporting an average or median
         * latency, since this value can't include a chunk assignable to a
         * single instance, it could be better used with utilizationTarget
         * instead.
         */
        singleInstanceAssignment?: number;
        /**
         * The target value of the metric that autoscaler should
         * maintain. This must be a positive value. A utilization
         * metric scales number of virtual machines handling requests
         * to increase or decrease proportionally to the metric.
         * For example, a good metric to use as a utilizationTarget is
         * www.googleapis.com/compute/instance/network/received_bytes_count.
         * The autoscaler will work to keep this value constant for each
         * of the instances.
         */
        target?: number;
        /**
         * Defines how target utilization value is expressed for a
         * Stackdriver Monitoring metric.
         * Possible values are: `GAUGE`, `DELTA_PER_SECOND`, `DELTA_PER_MINUTE`.
         */
        type?: string;
    }

    export interface AutoscalarAutoscalingPolicyScaleDownControl {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        maxScaledDownReplicas?: outputs.compute.AutoscalarAutoscalingPolicyScaleDownControlMaxScaledDownReplicas;
        /**
         * How long back autoscaling should look when computing recommendations
         * to include directives regarding slower scale down, as described above.
         */
        timeWindowSec?: number;
    }

    export interface AutoscalarAutoscalingPolicyScaleDownControlMaxScaledDownReplicas {
        /**
         * Specifies a fixed number of VM instances. This must be a positive
         * integer.
         */
        fixed?: number;
        /**
         * Specifies a percentage of instances between 0 to 100%, inclusive.
         * For example, specify 80 for 80%.
         */
        percent?: number;
    }

    export interface AutoscalarAutoscalingPolicyScaleInControl {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        maxScaledInReplicas?: outputs.compute.AutoscalarAutoscalingPolicyScaleInControlMaxScaledInReplicas;
        /**
         * How long back autoscaling should look when computing recommendations
         * to include directives regarding slower scale down, as described above.
         */
        timeWindowSec?: number;
    }

    export interface AutoscalarAutoscalingPolicyScaleInControlMaxScaledInReplicas {
        /**
         * Specifies a fixed number of VM instances. This must be a positive
         * integer.
         */
        fixed?: number;
        /**
         * Specifies a percentage of instances between 0 to 100%, inclusive.
         * For example, specify 80 for 80%.
         */
        percent?: number;
    }

    export interface AutoscalarAutoscalingPolicyScalingSchedule {
        /**
         * An optional description of this resource.
         */
        description?: string;
        /**
         * A boolean value that specifies if a scaling schedule can influence autoscaler recommendations. If set to true, then a scaling schedule has no effect.
         */
        disabled?: boolean;
        /**
         * The duration of time intervals (in seconds) for which this scaling schedule will be running. The minimum allowed value is 300.
         */
        durationSec: number;
        /**
         * Minimum number of VM instances that autoscaler will recommend in time intervals starting according to schedule.
         */
        minRequiredReplicas: number;
        /**
         * The identifier for this object. Format specified above.
         */
        name: string;
        /**
         * The start timestamps of time intervals when this scaling schedule should provide a scaling signal. This field uses the extended cron format (with an optional year field).
         */
        schedule: string;
        /**
         * The time zone to be used when interpreting the schedule. The value of this field must be a time zone name from the tz database: http://en.wikipedia.org/wiki/Tz_database.
         */
        timeZone?: string;
    }

    export interface AutoscalerAutoscalingPolicy {
        /**
         * The number of seconds that the autoscaler should wait before it
         * starts collecting information from a new instance. This prevents
         * the autoscaler from collecting information when the instance is
         * initializing, during which the collected usage would not be
         * reliable. The default time autoscaler waits is 60 seconds.
         * Virtual machine initialization times might vary because of
         * numerous factors. We recommend that you test how long an
         * instance may take to initialize. To do this, create an instance
         * and time the startup process.
         */
        cooldownPeriod?: number;
        /**
         * Defines the CPU utilization policy that allows the autoscaler to
         * scale based on the average CPU utilization of a managed instance
         * group.
         * Structure is documented below.
         */
        cpuUtilization: outputs.compute.AutoscalerAutoscalingPolicyCpuUtilization;
        /**
         * Configuration parameters of autoscaling based on a load balancer.
         * Structure is documented below.
         */
        loadBalancingUtilization?: outputs.compute.AutoscalerAutoscalingPolicyLoadBalancingUtilization;
        /**
         * The maximum number of instances that the autoscaler can scale up
         * to. This is required when creating or updating an autoscaler. The
         * maximum number of replicas should not be lower than minimal number
         * of replicas.
         */
        maxReplicas: number;
        /**
         * Configuration parameters of autoscaling based on a custom metric.
         * Structure is documented below.
         */
        metrics?: outputs.compute.AutoscalerAutoscalingPolicyMetric[];
        /**
         * The minimum number of replicas that the autoscaler can scale down
         * to. This cannot be less than 0. If not provided, autoscaler will
         * choose a default value depending on maximum number of instances
         * allowed.
         */
        minReplicas: number;
        /**
         * Defines operating mode for this policy.
         */
        mode?: string;
        /**
         * Defines scale down controls to reduce the risk of response latency
         * and outages due to abrupt scale-in events
         * Structure is documented below.
         */
        scaleDownControl: outputs.compute.AutoscalerAutoscalingPolicyScaleDownControl;
        /**
         * Defines scale in controls to reduce the risk of response latency
         * and outages due to abrupt scale-in events
         * Structure is documented below.
         */
        scaleInControl?: outputs.compute.AutoscalerAutoscalingPolicyScaleInControl;
        /**
         * Scaling schedules defined for an autoscaler. Multiple schedules can be set on an autoscaler and they can overlap.
         * Structure is documented below.
         */
        scalingSchedules?: outputs.compute.AutoscalerAutoscalingPolicyScalingSchedule[];
    }

    export interface AutoscalerAutoscalingPolicyCpuUtilization {
        /**
         * Indicates whether predictive autoscaling based on CPU metric is enabled. Valid values are:
         * - NONE (default). No predictive method is used. The autoscaler scales the group to meet current demand based on real-time metrics.
         * - OPTIMIZE_AVAILABILITY. Predictive autoscaling improves availability by monitoring daily and weekly load patterns and scaling out ahead of anticipated demand.
         */
        predictiveMethod?: string;
        /**
         * The target CPU utilization that the autoscaler should maintain.
         * Must be a float value in the range (0, 1]. If not specified, the
         * default is 0.6.
         * If the CPU level is below the target utilization, the autoscaler
         * scales down the number of instances until it reaches the minimum
         * number of instances you specified or until the average CPU of
         * your instances reaches the target utilization.
         * If the average CPU is above the target utilization, the autoscaler
         * scales up until it reaches the maximum number of instances you
         * specified or until the average utilization reaches the target
         * utilization.
         */
        target: number;
    }

    export interface AutoscalerAutoscalingPolicyLoadBalancingUtilization {
        /**
         * Fraction of backend capacity utilization (set in HTTP(s) load
         * balancing configuration) that autoscaler should maintain. Must
         * be a positive float value. If not defined, the default is 0.8.
         */
        target: number;
    }

    export interface AutoscalerAutoscalingPolicyMetric {
        /**
         * A filter string to be used as the filter string for
         * a Stackdriver Monitoring TimeSeries.list API call.
         * This filter is used to select a specific TimeSeries for
         * the purpose of autoscaling and to determine whether the metric
         * is exporting per-instance or per-group data.
         * You can only use the AND operator for joining selectors.
         * You can only use direct equality comparison operator (=) without
         * any functions for each selector.
         * You can specify the metric in both the filter string and in the
         * metric field. However, if specified in both places, the metric must
         * be identical.
         * The monitored resource type determines what kind of values are
         * expected for the metric. If it is a gce_instance, the autoscaler
         * expects the metric to include a separate TimeSeries for each
         * instance in a group. In such a case, you cannot filter on resource
         * labels.
         * If the resource type is any other value, the autoscaler expects
         * this metric to contain values that apply to the entire autoscaled
         * instance group and resource label filtering can be performed to
         * point autoscaler at the correct TimeSeries to scale upon.
         * This is called a per-group metric for the purpose of autoscaling.
         * If not specified, the type defaults to gce_instance.
         * You should provide a filter that is selective enough to pick just
         * one TimeSeries for the autoscaled group or for each of the instances
         * (if you are using gceInstance resource type). If multiple
         * TimeSeries are returned upon the query execution, the autoscaler
         * will sum their respective values to obtain its scaling value.
         */
        filter?: string;
        /**
         * The identifier (type) of the Stackdriver Monitoring metric.
         * The metric cannot have negative values.
         * The metric must have a value type of INT64 or DOUBLE.
         */
        name: string;
        /**
         * If scaling is based on a per-group metric value that represents the
         * total amount of work to be done or resource usage, set this value to
         * an amount assigned for a single instance of the scaled group.
         * The autoscaler will keep the number of instances proportional to the
         * value of this metric, the metric itself should not change value due
         * to group resizing.
         * For example, a good metric to use with the target is
         * `pubsub.googleapis.com/subscription/num_undelivered_messages`
         * or a custom metric exporting the total number of requests coming to
         * your instances.
         * A bad example would be a metric exporting an average or median
         * latency, since this value can't include a chunk assignable to a
         * single instance, it could be better used with utilizationTarget
         * instead.
         */
        singleInstanceAssignment?: number;
        /**
         * The target value of the metric that autoscaler should
         * maintain. This must be a positive value. A utilization
         * metric scales number of virtual machines handling requests
         * to increase or decrease proportionally to the metric.
         * For example, a good metric to use as a utilizationTarget is
         * www.googleapis.com/compute/instance/network/received_bytes_count.
         * The autoscaler will work to keep this value constant for each
         * of the instances.
         */
        target?: number;
        /**
         * Defines how target utilization value is expressed for a
         * Stackdriver Monitoring metric.
         * Possible values are: `GAUGE`, `DELTA_PER_SECOND`, `DELTA_PER_MINUTE`.
         */
        type?: string;
    }

    export interface AutoscalerAutoscalingPolicyScaleDownControl {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        maxScaledDownReplicas?: outputs.compute.AutoscalerAutoscalingPolicyScaleDownControlMaxScaledDownReplicas;
        /**
         * How long back autoscaling should look when computing recommendations
         * to include directives regarding slower scale down, as described above.
         */
        timeWindowSec?: number;
    }

    export interface AutoscalerAutoscalingPolicyScaleDownControlMaxScaledDownReplicas {
        /**
         * Specifies a fixed number of VM instances. This must be a positive
         * integer.
         */
        fixed?: number;
        /**
         * Specifies a percentage of instances between 0 to 100%, inclusive.
         * For example, specify 80 for 80%.
         */
        percent?: number;
    }

    export interface AutoscalerAutoscalingPolicyScaleInControl {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        maxScaledInReplicas?: outputs.compute.AutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas;
        /**
         * How long back autoscaling should look when computing recommendations
         * to include directives regarding slower scale down, as described above.
         */
        timeWindowSec?: number;
    }

    export interface AutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas {
        /**
         * Specifies a fixed number of VM instances. This must be a positive
         * integer.
         */
        fixed?: number;
        /**
         * Specifies a percentage of instances between 0 to 100%, inclusive.
         * For example, specify 80 for 80%.
         */
        percent?: number;
    }

    export interface AutoscalerAutoscalingPolicyScalingSchedule {
        /**
         * An optional description of this resource.
         */
        description?: string;
        /**
         * A boolean value that specifies if a scaling schedule can influence autoscaler recommendations. If set to true, then a scaling schedule has no effect.
         */
        disabled?: boolean;
        /**
         * The duration of time intervals (in seconds) for which this scaling schedule will be running. The minimum allowed value is 300.
         */
        durationSec: number;
        /**
         * Minimum number of VM instances that autoscaler will recommend in time intervals starting according to schedule.
         */
        minRequiredReplicas: number;
        /**
         * The identifier for this object. Format specified above.
         */
        name: string;
        /**
         * The start timestamps of time intervals when this scaling schedule should provide a scaling signal. This field uses the extended cron format (with an optional year field).
         */
        schedule: string;
        /**
         * The time zone to be used when interpreting the schedule. The value of this field must be a time zone name from the tz database: http://en.wikipedia.org/wiki/Tz_database.
         */
        timeZone?: string;
    }

    export interface BackendBucketCdnPolicy {
        /**
         * Bypass the cache when the specified request headers are matched - e.g. Pragma or Authorization headers. Up to 5 headers can be specified. The cache is bypassed for all cdnPolicy.cacheMode settings.
         * Structure is documented below.
         */
        bypassCacheOnRequestHeaders?: outputs.compute.BackendBucketCdnPolicyBypassCacheOnRequestHeader[];
        /**
         * The CacheKeyPolicy for this CdnPolicy.
         * Structure is documented below.
         */
        cacheKeyPolicy?: outputs.compute.BackendBucketCdnPolicyCacheKeyPolicy;
        /**
         * Specifies the cache setting for all responses from this backend.
         * The possible values are: USE_ORIGIN_HEADERS, FORCE_CACHE_ALL and CACHE_ALL_STATIC
         * Possible values are: `USE_ORIGIN_HEADERS`, `FORCE_CACHE_ALL`, `CACHE_ALL_STATIC`.
         */
        cacheMode: string;
        /**
         * Specifies the maximum allowed TTL for cached content served by this origin.
         */
        clientTtl: number;
        /**
         * Specifies the default TTL for cached content served by this origin for responses
         * that do not have an existing valid TTL (max-age or s-max-age).
         */
        defaultTtl: number;
        /**
         * Specifies the maximum allowed TTL for cached content served by this origin.
         */
        maxTtl: number;
        /**
         * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects.
         */
        negativeCaching: boolean;
        /**
         * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
         * Omitting the policy and leaving negativeCaching enabled will use Cloud CDN's default cache TTLs.
         * Structure is documented below.
         */
        negativeCachingPolicies?: outputs.compute.BackendBucketCdnPolicyNegativeCachingPolicy[];
        /**
         * If true then Cloud CDN will combine multiple concurrent cache fill requests into a small number of requests to the origin.
         */
        requestCoalescing?: boolean;
        /**
         * Serve existing content from the cache (if available) when revalidating content with the origin, or when an error is encountered when refreshing the cache.
         */
        serveWhileStale: number;
        /**
         * Maximum number of seconds the response to a signed URL request will
         * be considered fresh. After this time period,
         * the response will be revalidated before being served.
         * When serving responses to signed URL requests,
         * Cloud CDN will internally behave as though
         * all responses from this backend had a "Cache-Control: public,
         * max-age=[TTL]" header, regardless of any existing Cache-Control
         * header. The actual headers served in responses will not be altered.
         */
        signedUrlCacheMaxAgeSec?: number;
    }

    export interface BackendBucketCdnPolicyBypassCacheOnRequestHeader {
        /**
         * The header field name to match on when bypassing cache. Values are case-insensitive.
         */
        headerName?: string;
    }

    export interface BackendBucketCdnPolicyCacheKeyPolicy {
        /**
         * Allows HTTP request headers (by name) to be used in the
         * cache key.
         */
        includeHttpHeaders?: string[];
        /**
         * Names of query string parameters to include in cache keys.
         * Default parameters are always included. '&' and '=' will
         * be percent encoded and not treated as delimiters.
         */
        queryStringWhitelists?: string[];
    }

    export interface BackendBucketCdnPolicyNegativeCachingPolicy {
        /**
         * The HTTP status code to define a TTL against. Only HTTP status codes 300, 301, 308, 404, 405, 410, 421, 451 and 501
         * can be specified as values, and you cannot specify a status code more than once.
         */
        code?: number;
        /**
         * The TTL (in seconds) for which to cache responses with the corresponding status code. The maximum allowed value is 1800s
         * (30 minutes), noting that infrequently accessed objects may be evicted from the cache before the defined TTL.
         */
        ttl?: number;
    }

    export interface BackendBucketIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface BackendBucketIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface BackendServiceBackend {
        /**
         * Specifies the balancing mode for this backend.
         * For global HTTP(S) or TCP/SSL load balancing, the default is
         * UTILIZATION. Valid values are UTILIZATION, RATE (for HTTP(S))
         * and CONNECTION (for TCP/SSL).
         * See the [Backend Services Overview](https://cloud.google.com/load-balancing/docs/backend-service#balancing-mode)
         * for an explanation of load balancing modes.
         * Default value is `UTILIZATION`.
         * Possible values are: `UTILIZATION`, `RATE`, `CONNECTION`.
         */
        balancingMode?: string;
        /**
         * A multiplier applied to the group's maximum servicing capacity
         * (based on UTILIZATION, RATE or CONNECTION).
         * Default value is 1, which means the group will serve up to 100%
         * of its configured capacity (depending on balancingMode). A
         * setting of 0 means the group is completely drained, offering
         * 0% of its available Capacity. Valid range is [0.0,1.0].
         */
        capacityScaler?: number;
        /**
         * An optional description of this resource.
         * Provide this property when you create the resource.
         */
        description?: string;
        /**
         * The fully-qualified URL of an Instance Group or Network Endpoint
         * Group resource. In case of instance group this defines the list
         * of instances that serve traffic. Member virtual machine
         * instances from each instance group must live in the same zone as
         * the instance group itself. No two backends in a backend service
         * are allowed to use same Instance Group resource.
         * For Network Endpoint Groups this defines list of endpoints. All
         * endpoints of Network Endpoint Group must be hosted on instances
         * located in the same zone as the Network Endpoint Group.
         * Backend services cannot mix Instance Group and
         * Network Endpoint Group backends.
         * Note that you must specify an Instance Group or Network Endpoint
         * Group resource using the fully-qualified URL, rather than a
         * partial URL.
         */
        group: string;
        /**
         * The max number of simultaneous connections for the group. Can
         * be used with either CONNECTION or UTILIZATION balancing modes.
         * For CONNECTION mode, either maxConnections or one
         * of maxConnectionsPerInstance or maxConnectionsPerEndpoint,
         * as appropriate for group type, must be set.
         */
        maxConnections: number;
        /**
         * The max number of simultaneous connections that a single backend
         * network endpoint can handle. This is used to calculate the
         * capacity of the group. Can be used in either CONNECTION or
         * UTILIZATION balancing modes.
         * For CONNECTION mode, either
         * maxConnections or maxConnectionsPerEndpoint must be set.
         */
        maxConnectionsPerEndpoint: number;
        /**
         * The max number of simultaneous connections that a single
         * backend instance can handle. This is used to calculate the
         * capacity of the group. Can be used in either CONNECTION or
         * UTILIZATION balancing modes.
         * For CONNECTION mode, either maxConnections or
         * maxConnectionsPerInstance must be set.
         */
        maxConnectionsPerInstance: number;
        /**
         * The max requests per second (RPS) of the group.
         * Can be used with either RATE or UTILIZATION balancing modes,
         * but required if RATE mode. For RATE mode, either maxRate or one
         * of maxRatePerInstance or maxRatePerEndpoint, as appropriate for
         * group type, must be set.
         */
        maxRate: number;
        /**
         * The max requests per second (RPS) that a single backend network
         * endpoint can handle. This is used to calculate the capacity of
         * the group. Can be used in either balancing mode. For RATE mode,
         * either maxRate or maxRatePerEndpoint must be set.
         */
        maxRatePerEndpoint: number;
        /**
         * The max requests per second (RPS) that a single backend
         * instance can handle. This is used to calculate the capacity of
         * the group. Can be used in either balancing mode. For RATE mode,
         * either maxRate or maxRatePerInstance must be set.
         */
        maxRatePerInstance: number;
        /**
         * Used when balancingMode is UTILIZATION. This ratio defines the
         * CPU utilization target for the group. Valid range is [0.0, 1.0].
         */
        maxUtilization: number;
    }

    export interface BackendServiceCdnPolicy {
        /**
         * Bypass the cache when the specified request headers are matched - e.g. Pragma or Authorization headers. Up to 5 headers can be specified.
         * The cache is bypassed for all cdnPolicy.cacheMode settings.
         * Structure is documented below.
         */
        bypassCacheOnRequestHeaders?: outputs.compute.BackendServiceCdnPolicyBypassCacheOnRequestHeader[];
        /**
         * The CacheKeyPolicy for this CdnPolicy.
         * Structure is documented below.
         */
        cacheKeyPolicy?: outputs.compute.BackendServiceCdnPolicyCacheKeyPolicy;
        /**
         * Specifies the cache setting for all responses from this backend.
         * The possible values are: USE_ORIGIN_HEADERS, FORCE_CACHE_ALL and CACHE_ALL_STATIC
         * Possible values are: `USE_ORIGIN_HEADERS`, `FORCE_CACHE_ALL`, `CACHE_ALL_STATIC`.
         */
        cacheMode: string;
        /**
         * Specifies the maximum allowed TTL for cached content served by this origin.
         */
        clientTtl: number;
        /**
         * Specifies the default TTL for cached content served by this origin for responses
         * that do not have an existing valid TTL (max-age or s-max-age).
         */
        defaultTtl: number;
        /**
         * Specifies the maximum allowed TTL for cached content served by this origin.
         */
        maxTtl: number;
        /**
         * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects.
         */
        negativeCaching: boolean;
        /**
         * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
         * Omitting the policy and leaving negativeCaching enabled will use Cloud CDN's default cache TTLs.
         * Structure is documented below.
         */
        negativeCachingPolicies?: outputs.compute.BackendServiceCdnPolicyNegativeCachingPolicy[];
        /**
         * Serve existing content from the cache (if available) when revalidating content with the origin, or when an error is encountered when refreshing the cache.
         */
        serveWhileStale: number;
        /**
         * Maximum number of seconds the response to a signed URL request
         * will be considered fresh, defaults to 1hr (3600s). After this
         * time period, the response will be revalidated before
         * being served.
         * When serving responses to signed URL requests, Cloud CDN will
         * internally behave as though all responses from this backend had a
         * "Cache-Control: public, max-age=[TTL]" header, regardless of any
         * existing Cache-Control header. The actual headers served in
         * responses will not be altered.
         */
        signedUrlCacheMaxAgeSec?: number;
    }

    export interface BackendServiceCdnPolicyBypassCacheOnRequestHeader {
        /**
         * The header field name to match on when bypassing cache. Values are case-insensitive.
         */
        headerName: string;
    }

    export interface BackendServiceCdnPolicyCacheKeyPolicy {
        /**
         * If true requests to different hosts will be cached separately.
         */
        includeHost?: boolean;
        /**
         * Allows HTTP request headers (by name) to be used in the
         * cache key.
         */
        includeHttpHeaders?: string[];
        /**
         * Names of cookies to include in cache keys.
         */
        includeNamedCookies?: string[];
        /**
         * If true, http and https requests will be cached separately.
         */
        includeProtocol?: boolean;
        /**
         * If true, include query string parameters in the cache key
         * according to queryStringWhitelist and
         * query_string_blacklist. If neither is set, the entire query
         * string will be included.
         * If false, the query string will be excluded from the cache
         * key entirely.
         */
        includeQueryString?: boolean;
        /**
         * Names of query string parameters to exclude in cache keys.
         * All other parameters will be included. Either specify
         * queryStringWhitelist or query_string_blacklist, not both.
         * '&' and '=' will be percent encoded and not treated as
         * delimiters.
         */
        queryStringBlacklists?: string[];
        /**
         * Names of query string parameters to include in cache keys.
         * All other parameters will be excluded. Either specify
         * queryStringWhitelist or query_string_blacklist, not both.
         * '&' and '=' will be percent encoded and not treated as
         * delimiters.
         */
        queryStringWhitelists?: string[];
    }

    export interface BackendServiceCdnPolicyNegativeCachingPolicy {
        /**
         * The HTTP status code to define a TTL against. Only HTTP status codes 300, 301, 308, 404, 405, 410, 421, 451 and 501
         * can be specified as values, and you cannot specify a status code more than once.
         */
        code?: number;
        /**
         * The TTL (in seconds) for which to cache responses with the corresponding status code. The maximum allowed value is 1800s
         * (30 minutes), noting that infrequently accessed objects may be evicted from the cache before the defined TTL.
         */
        ttl?: number;
    }

    export interface BackendServiceCircuitBreakers {
        /**
         * The timeout for new network connections to hosts.
         * Structure is documented below.
         */
        connectTimeout?: outputs.compute.BackendServiceCircuitBreakersConnectTimeout;
        /**
         * The maximum number of connections to the backend cluster.
         * Defaults to 1024.
         */
        maxConnections?: number;
        /**
         * The maximum number of pending requests to the backend cluster.
         * Defaults to 1024.
         */
        maxPendingRequests?: number;
        /**
         * The maximum number of parallel requests to the backend cluster.
         * Defaults to 1024.
         */
        maxRequests?: number;
        /**
         * Maximum requests for a single backend connection. This parameter
         * is respected by both the HTTP/1.1 and HTTP/2 implementations. If
         * not specified, there is no limit. Setting this parameter to 1
         * will effectively disable keep alive.
         */
        maxRequestsPerConnection?: number;
        /**
         * The maximum number of parallel retries to the backend cluster.
         * Defaults to 3.
         */
        maxRetries?: number;
    }

    export interface BackendServiceCircuitBreakersConnectTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must
         * be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second.
         * Must be from 0 to 315,576,000,000 inclusive.
         */
        seconds: number;
    }

    export interface BackendServiceConsistentHash {
        /**
         * Hash is based on HTTP Cookie. This field describes a HTTP cookie
         * that will be used as the hash key for the consistent hash load
         * balancer. If the cookie is not present, it will be generated.
         * This field is applicable if the sessionAffinity is set to HTTP_COOKIE.
         * Structure is documented below.
         */
        httpCookie?: outputs.compute.BackendServiceConsistentHashHttpCookie;
        /**
         * The hash based on the value of the specified header field.
         * This field is applicable if the sessionAffinity is set to HEADER_FIELD.
         */
        httpHeaderName?: string;
        /**
         * The minimum number of virtual nodes to use for the hash ring.
         * Larger ring sizes result in more granular load
         * distributions. If the number of hosts in the load balancing pool
         * is larger than the ring size, each host will be assigned a single
         * virtual node.
         * Defaults to 1024.
         */
        minimumRingSize?: number;
    }

    export interface BackendServiceConsistentHashHttpCookie {
        /**
         * Name of the cookie.
         */
        name?: string;
        /**
         * Path to set for the cookie.
         */
        path?: string;
        /**
         * Lifetime of the cookie.
         * Structure is documented below.
         */
        ttl?: outputs.compute.BackendServiceConsistentHashHttpCookieTtl;
    }

    export interface BackendServiceConsistentHashHttpCookieTtl {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must
         * be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second.
         * Must be from 0 to 315,576,000,000 inclusive.
         */
        seconds: number;
    }

    export interface BackendServiceIamBindingCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface BackendServiceIamMemberCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface BackendServiceIap {
        /**
         * OAuth2 Client ID for IAP
         */
        oauth2ClientId: string;
        /**
         * OAuth2 Client Secret for IAP
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        oauth2ClientSecret: string;
        /**
         * (Output)
         * OAuth2 Client Secret SHA-256 for IAP
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        oauth2ClientSecretSha256: string;
    }

    export interface BackendServiceLocalityLbPolicy {
        /**
         * The configuration for a custom policy implemented by the user and
         * deployed with the client.
         * Structure is documented below.
         */
        customPolicy?: outputs.compute.BackendServiceLocalityLbPolicyCustomPolicy;
        /**
         * The configuration for a built-in load balancing policy.
         * Structure is documented below.
         */
        policy?: outputs.compute.BackendServiceLocalityLbPolicyPolicy;
    }

    export interface BackendServiceLocalityLbPolicyCustomPolicy {
        /**
         * An optional, arbitrary JSON object with configuration data, understood
         * by a locally installed custom policy implementation.
         */
        data?: string;
        /**
         * Identifies the custom policy.
         * The value should match the type the custom implementation is registered
         * with on the gRPC clients. It should follow protocol buffer
         * message naming conventions and include the full path (e.g.
         * myorg.CustomLbPolicy). The maximum length is 256 characters.
         * Note that specifying the same custom policy more than once for a
         * backend is not a valid configuration and will be rejected.
         */
        name: string;
    }

    export interface BackendServiceLocalityLbPolicyPolicy {
        /**
         * The name of a locality load balancer policy to be used. The value
         * should be one of the predefined ones as supported by localityLbPolicy,
         * although at the moment only ROUND_ROBIN is supported.
         * This field should only be populated when the customPolicy field is not
         * used.
         * Note that specifying the same policy more than once for a backend is
         * not a valid configuration and will be rejected.
         * The possible values are:
         */
        name: string;
    }

    export interface BackendServiceLogConfig {
        /**
         * Whether to enable logging for the load balancer traffic served by this backend service.
         */
        enable?: boolean;
        /**
         * This field can only be specified if logging is enabled for this backend service. The value of
         * the field must be in [0, 1]. This configures the sampling rate of requests to the load balancer
         * where 1.0 means all logged requests are reported and 0.0 means no logged requests are reported.
         * The default value is 1.0.
         */
        sampleRate?: number;
    }

    export interface BackendServiceOutlierDetection {
        /**
         * The base time that a host is ejected for. The real time is equal to the base
         * time multiplied by the number of times the host has been ejected. Defaults to
         * 30000ms or 30s.
         * Structure is documented below.
         */
        baseEjectionTime?: outputs.compute.BackendServiceOutlierDetectionBaseEjectionTime;
        /**
         * Number of errors before a host is ejected from the connection pool. When the
         * backend host is accessed over HTTP, a 5xx return code qualifies as an error.
         * Defaults to 5.
         */
        consecutiveErrors?: number;
        /**
         * The number of consecutive gateway failures (502, 503, 504 status or connection
         * errors that are mapped to one of those status codes) before a consecutive
         * gateway failure ejection occurs. Defaults to 5.
         */
        consecutiveGatewayFailure?: number;
        /**
         * The percentage chance that a host will be actually ejected when an outlier
         * status is detected through consecutive 5xx. This setting can be used to disable
         * ejection or to ramp it up slowly. Defaults to 100.
         */
        enforcingConsecutiveErrors?: number;
        /**
         * The percentage chance that a host will be actually ejected when an outlier
         * status is detected through consecutive gateway failures. This setting can be
         * used to disable ejection or to ramp it up slowly. Defaults to 0.
         */
        enforcingConsecutiveGatewayFailure?: number;
        /**
         * The percentage chance that a host will be actually ejected when an outlier
         * status is detected through success rate statistics. This setting can be used to
         * disable ejection or to ramp it up slowly. Defaults to 100.
         */
        enforcingSuccessRate?: number;
        /**
         * Time interval between ejection sweep analysis. This can result in both new
         * ejections as well as hosts being returned to service. Defaults to 10 seconds.
         * Structure is documented below.
         */
        interval?: outputs.compute.BackendServiceOutlierDetectionInterval;
        /**
         * Maximum percentage of hosts in the load balancing pool for the backend service
         * that can be ejected. Defaults to 10%.
         */
        maxEjectionPercent?: number;
        /**
         * The number of hosts in a cluster that must have enough request volume to detect
         * success rate outliers. If the number of hosts is less than this setting, outlier
         * detection via success rate statistics is not performed for any host in the
         * cluster. Defaults to 5.
         */
        successRateMinimumHosts?: number;
        /**
         * The minimum number of total requests that must be collected in one interval (as
         * defined by the interval duration above) to include this host in success rate
         * based outlier detection. If the volume is lower than this setting, outlier
         * detection via success rate statistics is not performed for that host. Defaults
         * to 100.
         */
        successRateRequestVolume?: number;
        /**
         * This factor is used to determine the ejection threshold for success rate outlier
         * ejection. The ejection threshold is the difference between the mean success
         * rate, and the product of this factor and the standard deviation of the mean
         * success rate: mean - (stdev * success_rate_stdev_factor). This factor is divided
         * by a thousand to get a double. That is, if the desired factor is 1.9, the
         * runtime value should be 1900. Defaults to 1900.
         */
        successRateStdevFactor?: number;
    }

    export interface BackendServiceOutlierDetectionBaseEjectionTime {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations
         * less than one second are represented with a 0 `seconds` field and a positive
         * `nanos` field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
         * inclusive.
         */
        seconds: number;
    }

    export interface BackendServiceOutlierDetectionInterval {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations
         * less than one second are represented with a 0 `seconds` field and a positive
         * `nanos` field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
         * inclusive.
         */
        seconds: number;
    }

    export interface BackendServiceSecuritySettings {
        /**
         * ClientTlsPolicy is a resource that specifies how a client should authenticate
         * connections to backends of a service. This resource itself does not affect
         * configuration unless it is attached to a backend service resource.
         */
        clientTlsPolicy: string;
        /**
         * A list of alternate names to verify the subject identity in the certificate.
         * If specified, the client will verify that the server certificate's subject
         * alt name matches one of the specified values.
         */
        subjectAltNames: string[];
    }

    export interface DiskAsyncPrimaryDisk {
        /**
         * Primary disk for asynchronous disk replication.
         */
        disk: string;
    }

    export interface DiskAsyncReplicationSecondaryDisk {
        /**
         * The secondary disk.
         */
        disk: string;
        /**
         * Output-only. Status of replication on the secondary disk.
         *
         * - - -
         */
        state: string;
    }

    export interface DiskDiskEncryptionKey {
        /**
         * The self link of the encryption key used to encrypt the disk. Also called KmsKeyName
         * in the cloud console. Your project's Compute Engine System service account
         * (`service-{{PROJECT_NUMBER}}@compute-system.iam.gserviceaccount.com`) must have
         * `roles/cloudkms.cryptoKeyEncrypterDecrypter` to use this feature.
         * See https://cloud.google.com/compute/docs/disks/customer-managed-encryption#encrypt_a_new_persistent_disk_with_your_own_keys
         */
        kmsKeySelfLink?: string;
        /**
         * The service account used for the encryption request for the given KMS key.
         * If absent, the Compute Engine Service Agent service account is used.
         */
        kmsKeyServiceAccount?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        rawKey?: string;
        /**
         * Specifies an RFC 4648 base64 encoded, RSA-wrapped 2048-bit
         * customer-supplied encryption key to either encrypt or decrypt
         * this resource. You can provide either the rawKey or the rsaEncryptedKey.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        rsaEncryptedKey?: string;
        /**
         * (Output)
         * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
         * encryption key that protects this resource.
         */
        sha256: string;
    }

    export interface DiskGuestOsFeature {
        /**
         * The type of supported feature. Read [Enabling guest operating system features](https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#guest-os-features) to see a list of available options.
         * Possible values are: `MULTI_IP_SUBNET`, `SECURE_BOOT`, `SEV_CAPABLE`, `UEFI_COMPATIBLE`, `VIRTIO_SCSI_MULTIQUEUE`, `WINDOWS`, `GVNIC`, `SEV_LIVE_MIGRATABLE`, `SEV_SNP_CAPABLE`, `SUSPEND_RESUME_COMPATIBLE`, `TDX_CAPABLE`.
         */
        type: string;
    }

    export interface DiskIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DiskIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DiskSourceImageEncryptionKey {
        /**
         * The self link of the encryption key used to encrypt the disk. Also called KmsKeyName
         * in the cloud console. Your project's Compute Engine System service account
         * (`service-{{PROJECT_NUMBER}}@compute-system.iam.gserviceaccount.com`) must have
         * `roles/cloudkms.cryptoKeyEncrypterDecrypter` to use this feature.
         * See https://cloud.google.com/compute/docs/disks/customer-managed-encryption#encrypt_a_new_persistent_disk_with_your_own_keys
         */
        kmsKeySelfLink?: string;
        /**
         * The service account used for the encryption request for the given KMS key.
         * If absent, the Compute Engine Service Agent service account is used.
         */
        kmsKeyServiceAccount?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         */
        rawKey?: string;
        /**
         * (Output)
         * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
         * encryption key that protects this resource.
         */
        sha256: string;
    }

    export interface DiskSourceSnapshotEncryptionKey {
        /**
         * The self link of the encryption key used to encrypt the disk. Also called KmsKeyName
         * in the cloud console. Your project's Compute Engine System service account
         * (`service-{{PROJECT_NUMBER}}@compute-system.iam.gserviceaccount.com`) must have
         * `roles/cloudkms.cryptoKeyEncrypterDecrypter` to use this feature.
         * See https://cloud.google.com/compute/docs/disks/customer-managed-encryption#encrypt_a_new_persistent_disk_with_your_own_keys
         */
        kmsKeySelfLink?: string;
        /**
         * The service account used for the encryption request for the given KMS key.
         * If absent, the Compute Engine Service Agent service account is used.
         */
        kmsKeyServiceAccount?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         */
        rawKey?: string;
        /**
         * (Output)
         * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
         * encryption key that protects this resource.
         */
        sha256: string;
    }

    export interface ExternalVpnGatewayInterface {
        /**
         * The numeric ID for this interface. Allowed values are based on the redundancy type
         * of this external VPN gateway
         * * `0 - SINGLE_IP_INTERNALLY_REDUNDANT`
         * * `0, 1 - TWO_IPS_REDUNDANCY`
         * * `0, 1, 2, 3 - FOUR_IPS_REDUNDANCY`
         */
        id?: number;
        /**
         * IP address of the interface in the external VPN gateway.
         * Only IPv4 is supported. This IP address can be either from
         * your on-premise gateway or another Cloud provider's VPN gateway,
         * it cannot be an IP address from Google Compute Engine.
         */
        ipAddress?: string;
    }

    export interface FirewallAllow {
        /**
         * An optional list of ports to which this rule applies. This field
         * is only applicable for UDP or TCP protocol. Each entry must be
         * either an integer or a range. If not specified, this rule
         * applies to connections through any port.
         * Example inputs include: ["22"], ["80","443"], and
         * ["12345-12349"].
         */
        ports?: string[];
        /**
         * The IP protocol to which this rule applies. The protocol type is
         * required when creating a firewall rule. This value can either be
         * one of the following well known protocol strings (tcp, udp,
         * icmp, esp, ah, sctp, ipip, all), or the IP protocol number.
         */
        protocol: string;
    }

    export interface FirewallDeny {
        /**
         * An optional list of ports to which this rule applies. This field
         * is only applicable for UDP or TCP protocol. Each entry must be
         * either an integer or a range. If not specified, this rule
         * applies to connections through any port.
         * Example inputs include: ["22"], ["80","443"], and
         * ["12345-12349"].
         */
        ports?: string[];
        /**
         * The IP protocol to which this rule applies. The protocol type is
         * required when creating a firewall rule. This value can either be
         * one of the following well known protocol strings (tcp, udp,
         * icmp, esp, ah, sctp, ipip, all), or the IP protocol number.
         */
        protocol: string;
    }

    export interface FirewallLogConfig {
        /**
         * This field denotes whether to include or exclude metadata for firewall logs.
         * Possible values are: `EXCLUDE_ALL_METADATA`, `INCLUDE_ALL_METADATA`.
         */
        metadata: string;
    }

    export interface FirewallPolicyRuleMatch {
        /**
         * Address groups which should be matched against the traffic destination. Maximum number of destination address groups is 10. Destination address groups is only supported in Egress rules.
         */
        destAddressGroups?: string[];
        /**
         * Domain names that will be used to match against the resolved domain name of destination of traffic. Can only be specified if DIRECTION is egress.
         */
        destFqdns?: string[];
        /**
         * CIDR IP address range. Maximum number of destination CIDR IP ranges allowed is 256.
         */
        destIpRanges?: string[];
        /**
         * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is egress.
         */
        destRegionCodes?: string[];
        /**
         * Name of the Google Cloud Threat Intelligence list.
         */
        destThreatIntelligences?: string[];
        /**
         * Pairs of IP protocols and ports that the rule should match.
         */
        layer4Configs: outputs.compute.FirewallPolicyRuleMatchLayer4Config[];
        /**
         * Address groups which should be matched against the traffic source. Maximum number of source address groups is 10. Source address groups is only supported in Ingress rules.
         */
        srcAddressGroups?: string[];
        /**
         * Domain names that will be used to match against the resolved domain name of source of traffic. Can only be specified if DIRECTION is ingress.
         */
        srcFqdns?: string[];
        /**
         * CIDR IP address range. Maximum number of source CIDR IP ranges allowed is 256.
         */
        srcIpRanges?: string[];
        /**
         * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is ingress.
         */
        srcRegionCodes?: string[];
        /**
         * Name of the Google Cloud Threat Intelligence list.
         *
         * The `layer4Configs` block supports:
         */
        srcThreatIntelligences?: string[];
    }

    export interface FirewallPolicyRuleMatchLayer4Config {
        /**
         * The IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (`tcp`, `udp`, `icmp`, `esp`, `ah`, `ipip`, `sctp`), or the IP protocol number.
         */
        ipProtocol: string;
        /**
         * An optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: ``.
         *
         * - - -
         */
        ports?: string[];
    }

    export interface ForwardingRuleServiceDirectoryRegistration {
        /**
         * Service Directory namespace to register the forwarding rule under.
         */
        namespace: string;
        /**
         * Service Directory service to register the forwarding rule under.
         */
        service?: string;
    }

    export interface GetAddressesAddress {
        /**
         * The IP address (for example `1.2.3.4`).
         */
        address: string;
        /**
         * The IP address type, can be `EXTERNAL` or `INTERNAL`.
         */
        addressType: string;
        /**
         * The IP address description.
         */
        description: string;
        /**
         * (Beta only) A map containing IP labels.
         */
        labels: {[key: string]: string};
        /**
         * The IP address name.
         */
        name: string;
        /**
         * Region that should be considered to search addresses.
         * All regions are considered if missing.
         */
        region: string;
        /**
         * The URI of the created resource.
         */
        selfLink: string;
        /**
         * Indicates if the address is used. Possible values are: RESERVED or IN_USE.
         */
        status: string;
    }

    export interface GetBackendBucketCdnPolicy {
        bypassCacheOnRequestHeaders: outputs.compute.GetBackendBucketCdnPolicyBypassCacheOnRequestHeader[];
        cacheKeyPolicies: outputs.compute.GetBackendBucketCdnPolicyCacheKeyPolicy[];
        cacheMode: string;
        clientTtl: number;
        defaultTtl: number;
        maxTtl: number;
        negativeCaching: boolean;
        negativeCachingPolicies: outputs.compute.GetBackendBucketCdnPolicyNegativeCachingPolicy[];
        requestCoalescing: boolean;
        serveWhileStale: number;
        signedUrlCacheMaxAgeSec: number;
    }

    export interface GetBackendBucketCdnPolicyBypassCacheOnRequestHeader {
        headerName: string;
    }

    export interface GetBackendBucketCdnPolicyCacheKeyPolicy {
        includeHttpHeaders: string[];
        queryStringWhitelists: string[];
    }

    export interface GetBackendBucketCdnPolicyNegativeCachingPolicy {
        code: number;
        ttl: number;
    }

    export interface GetBackendServiceBackend {
        balancingMode: string;
        capacityScaler: number;
        /**
         * Textual description for the Backend Service.
         */
        description: string;
        group: string;
        maxConnections: number;
        maxConnectionsPerEndpoint: number;
        maxConnectionsPerInstance: number;
        maxRate: number;
        maxRatePerEndpoint: number;
        maxRatePerInstance: number;
        maxUtilization: number;
    }

    export interface GetBackendServiceCdnPolicy {
        bypassCacheOnRequestHeaders: outputs.compute.GetBackendServiceCdnPolicyBypassCacheOnRequestHeader[];
        cacheKeyPolicies: outputs.compute.GetBackendServiceCdnPolicyCacheKeyPolicy[];
        cacheMode: string;
        clientTtl: number;
        defaultTtl: number;
        maxTtl: number;
        negativeCaching: boolean;
        negativeCachingPolicies: outputs.compute.GetBackendServiceCdnPolicyNegativeCachingPolicy[];
        serveWhileStale: number;
        signedUrlCacheMaxAgeSec: number;
    }

    export interface GetBackendServiceCdnPolicyBypassCacheOnRequestHeader {
        headerName: string;
    }

    export interface GetBackendServiceCdnPolicyCacheKeyPolicy {
        includeHost: boolean;
        includeHttpHeaders: string[];
        includeNamedCookies: string[];
        includeProtocol: boolean;
        includeQueryString: boolean;
        queryStringBlacklists: string[];
        queryStringWhitelists: string[];
    }

    export interface GetBackendServiceCdnPolicyNegativeCachingPolicy {
        code: number;
        ttl: number;
    }

    export interface GetBackendServiceCircuitBreaker {
        connectTimeouts: outputs.compute.GetBackendServiceCircuitBreakerConnectTimeout[];
        maxConnections: number;
        maxPendingRequests: number;
        maxRequests: number;
        maxRequestsPerConnection: number;
        maxRetries: number;
    }

    export interface GetBackendServiceCircuitBreakerConnectTimeout {
        nanos: number;
        seconds: number;
    }

    export interface GetBackendServiceConsistentHash {
        httpCookies: outputs.compute.GetBackendServiceConsistentHashHttpCooky[];
        httpHeaderName: string;
        minimumRingSize: number;
    }

    export interface GetBackendServiceConsistentHashHttpCooky {
        /**
         * The name of the Backend Service.
         *
         * - - -
         */
        name: string;
        path: string;
        ttls: outputs.compute.GetBackendServiceConsistentHashHttpCookyTtl[];
    }

    export interface GetBackendServiceConsistentHashHttpCookyTtl {
        nanos: number;
        seconds: number;
    }

    export interface GetBackendServiceIap {
        oauth2ClientId: string;
        oauth2ClientSecret: string;
        oauth2ClientSecretSha256: string;
    }

    export interface GetBackendServiceLocalityLbPolicy {
        customPolicies: outputs.compute.GetBackendServiceLocalityLbPolicyCustomPolicy[];
        policies: outputs.compute.GetBackendServiceLocalityLbPolicyPolicy[];
    }

    export interface GetBackendServiceLocalityLbPolicyCustomPolicy {
        data: string;
        /**
         * The name of the Backend Service.
         *
         * - - -
         */
        name: string;
    }

    export interface GetBackendServiceLocalityLbPolicyPolicy {
        /**
         * The name of the Backend Service.
         *
         * - - -
         */
        name: string;
    }

    export interface GetBackendServiceLogConfig {
        enable: boolean;
        sampleRate: number;
    }

    export interface GetBackendServiceOutlierDetection {
        baseEjectionTimes: outputs.compute.GetBackendServiceOutlierDetectionBaseEjectionTime[];
        consecutiveErrors: number;
        consecutiveGatewayFailure: number;
        enforcingConsecutiveErrors: number;
        enforcingConsecutiveGatewayFailure: number;
        enforcingSuccessRate: number;
        intervals: outputs.compute.GetBackendServiceOutlierDetectionInterval[];
        maxEjectionPercent: number;
        successRateMinimumHosts: number;
        successRateRequestVolume: number;
        successRateStdevFactor: number;
    }

    export interface GetBackendServiceOutlierDetectionBaseEjectionTime {
        nanos: number;
        seconds: number;
    }

    export interface GetBackendServiceOutlierDetectionInterval {
        nanos: number;
        seconds: number;
    }

    export interface GetBackendServiceSecuritySetting {
        clientTlsPolicy: string;
        subjectAltNames: string[];
    }

    export interface GetDiskAsyncPrimaryDisk {
        disk: string;
    }

    export interface GetDiskDiskEncryptionKey {
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
        rawKey: string;
        rsaEncryptedKey: string;
        sha256: string;
    }

    export interface GetDiskGuestOsFeature {
        /**
         * URL of the disk type resource describing which disk type to use to
         * create the disk.
         */
        type: string;
    }

    export interface GetDiskSourceImageEncryptionKey {
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
        rawKey: string;
        sha256: string;
    }

    export interface GetDiskSourceSnapshotEncryptionKey {
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
        rawKey: string;
        sha256: string;
    }

    export interface GetForwardingRuleServiceDirectoryRegistration {
        namespace: string;
        service: string;
    }

    export interface GetGlobalForwardingRuleMetadataFilter {
        filterLabels: outputs.compute.GetGlobalForwardingRuleMetadataFilterFilterLabel[];
        filterMatchCriteria: string;
    }

    export interface GetGlobalForwardingRuleMetadataFilterFilterLabel {
        /**
         * The name of the global forwarding rule.
         *
         * - - -
         */
        name: string;
        value: string;
    }

    export interface GetHcVpnGatewayVpnInterface {
        id: number;
        interconnectAttachment: string;
        ipAddress: string;
    }

    export interface GetHealthCheckGrpcHealthCheck {
        grpcServiceName: string;
        port: number;
        portName: string;
        portSpecification: string;
    }

    export interface GetHealthCheckHttp2HealthCheck {
        host: string;
        port: number;
        portName: string;
        portSpecification: string;
        proxyHeader: string;
        requestPath: string;
        response: string;
    }

    export interface GetHealthCheckHttpHealthCheck {
        host: string;
        port: number;
        portName: string;
        portSpecification: string;
        proxyHeader: string;
        requestPath: string;
        response: string;
    }

    export interface GetHealthCheckHttpsHealthCheck {
        host: string;
        port: number;
        portName: string;
        portSpecification: string;
        proxyHeader: string;
        requestPath: string;
        response: string;
    }

    export interface GetHealthCheckLogConfig {
        enable: boolean;
    }

    export interface GetHealthCheckSslHealthCheck {
        port: number;
        portName: string;
        portSpecification: string;
        proxyHeader: string;
        request: string;
        response: string;
    }

    export interface GetHealthCheckTcpHealthCheck {
        port: number;
        portName: string;
        portSpecification: string;
        proxyHeader: string;
        request: string;
        response: string;
    }

    export interface GetInstanceAdvancedMachineFeature {
        enableNestedVirtualization: boolean;
        threadsPerCore: number;
        visibleCoreCount: number;
    }

    export interface GetInstanceAttachedDisk {
        /**
         * Name with which the attached disk is accessible
         * under `/dev/disk/by-id/`
         */
        deviceName: string;
        diskEncryptionKeyRaw: string;
        diskEncryptionKeySha256: string;
        kmsKeySelfLink: string;
        /**
         * Read/write mode for the disk. One of `"READ_ONLY"` or `"READ_WRITE"`.
         */
        mode: string;
        /**
         * The name or selfLink of the disk attached to this instance.
         */
        source: string;
    }

    export interface GetInstanceBootDisk {
        /**
         * Whether the disk will be auto-deleted when the instance is deleted.
         */
        autoDelete: boolean;
        /**
         * Name with which the attached disk is accessible
         * under `/dev/disk/by-id/`
         */
        deviceName: string;
        diskEncryptionKeyRaw: string;
        diskEncryptionKeySha256: string;
        /**
         * Parameters with which a disk was created alongside the instance.
         * Structure is documented below.
         */
        initializeParams: outputs.compute.GetInstanceBootDiskInitializeParam[];
        kmsKeySelfLink: string;
        /**
         * Read/write mode for the disk. One of `"READ_ONLY"` or `"READ_WRITE"`.
         */
        mode: string;
        /**
         * The name or selfLink of the disk attached to this instance.
         */
        source: string;
    }

    export interface GetInstanceBootDiskInitializeParam {
        /**
         * The image from which this disk was initialised.
         */
        image: string;
        /**
         * A set of key/value label pairs assigned to the disk.
         */
        labels: {[key: string]: any};
        resourceManagerTags: {[key: string]: any};
        /**
         * The size of the image in gigabytes.
         */
        size: number;
        /**
         * The accelerator type resource exposed to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetInstanceConfidentialInstanceConfig {
        enableConfidentialCompute: boolean;
    }

    export interface GetInstanceGroupManagerAllInstancesConfig {
        labels: {[key: string]: string};
        metadata: {[key: string]: string};
    }

    export interface GetInstanceGroupManagerAutoHealingPolicy {
        healthCheck: string;
        initialDelaySec: number;
    }

    export interface GetInstanceGroupManagerInstanceLifecyclePolicy {
        forceUpdateOnRepair: string;
    }

    export interface GetInstanceGroupManagerNamedPort {
        /**
         * The name of the instance group. Either `name` or `selfLink` must be provided.
         */
        name: string;
        port: number;
    }

    export interface GetInstanceGroupManagerStatefulDisk {
        deleteRule: string;
        deviceName: string;
    }

    export interface GetInstanceGroupManagerStatefulExternalIp {
        deleteRule: string;
        interfaceName: string;
    }

    export interface GetInstanceGroupManagerStatefulInternalIp {
        deleteRule: string;
        interfaceName: string;
    }

    export interface GetInstanceGroupManagerStatus {
        allInstancesConfigs: outputs.compute.GetInstanceGroupManagerStatusAllInstancesConfig[];
        isStable: boolean;
        statefuls: outputs.compute.GetInstanceGroupManagerStatusStateful[];
        versionTargets: outputs.compute.GetInstanceGroupManagerStatusVersionTarget[];
    }

    export interface GetInstanceGroupManagerStatusAllInstancesConfig {
        effective: boolean;
    }

    export interface GetInstanceGroupManagerStatusStateful {
        hasStatefulConfig: boolean;
        perInstanceConfigs: outputs.compute.GetInstanceGroupManagerStatusStatefulPerInstanceConfig[];
    }

    export interface GetInstanceGroupManagerStatusStatefulPerInstanceConfig {
        allEffective: boolean;
    }

    export interface GetInstanceGroupManagerStatusVersionTarget {
        isReached: boolean;
    }

    export interface GetInstanceGroupManagerUpdatePolicy {
        maxSurgeFixed: number;
        maxSurgePercent: number;
        maxUnavailableFixed: number;
        maxUnavailablePercent: number;
        minReadySec: number;
        minimalAction: string;
        mostDisruptiveAllowedAction: string;
        replacementMethod: string;
        type: string;
    }

    export interface GetInstanceGroupManagerVersion {
        instanceTemplate: string;
        /**
         * The name of the instance group. Either `name` or `selfLink` must be provided.
         */
        name: string;
        targetSizes: outputs.compute.GetInstanceGroupManagerVersionTargetSize[];
    }

    export interface GetInstanceGroupManagerVersionTargetSize {
        fixed: number;
        percent: number;
    }

    export interface GetInstanceGroupNamedPort {
        /**
         * The name of the instance group. Either `name` or `selfLink` must be provided.
         */
        name: string;
        port: number;
    }

    export interface GetInstanceGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * The accelerator type resource exposed to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetInstanceNetworkInterface {
        /**
         * Access configurations, i.e. IPs via which this
         * instance can be accessed via the Internet. Structure documented below.
         */
        accessConfigs: outputs.compute.GetInstanceNetworkInterfaceAccessConfig[];
        /**
         * An array of alias IP ranges for this network interface. Structure documented below.
         */
        aliasIpRanges: outputs.compute.GetInstanceNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        ipv6AccessConfigs: outputs.compute.GetInstanceNetworkInterfaceIpv6AccessConfig[];
        ipv6AccessType: string;
        ipv6Address: string;
        /**
         * The name of the instance. One of `name` or `selfLink` must be provided.
         */
        name: string;
        /**
         * The name or selfLink of the network attached to this interface.
         */
        network: string;
        /**
         * The private IP address assigned to the instance.
         */
        networkIp: string;
        nicType: string;
        queueCount: number;
        stackType: string;
        /**
         * The name or selfLink of the subnetwork attached to this interface.
         */
        subnetwork: string;
        /**
         * The project in which the subnetwork belongs.
         */
        subnetworkProject: string;
    }

    export interface GetInstanceNetworkInterfaceAccessConfig {
        /**
         * The IP address that is be 1:1 mapped to the instance's
         * network ip.
         */
        natIp: string;
        /**
         * The [networking tier][network-tier] used for configuring this instance. One of `PREMIUM` or `STANDARD`.
         */
        networkTier: string;
        /**
         * The DNS domain name for the public PTR record.
         */
        publicPtrDomainName: string;
    }

    export interface GetInstanceNetworkInterfaceAliasIpRange {
        /**
         * The IP CIDR range represented by this alias IP range.
         */
        ipCidrRange: string;
        /**
         * The subnetwork secondary range name specifying
         * the secondary range from which to allocate the IP CIDR range for this alias IP
         * range.
         */
        subnetworkRangeName: string;
    }

    export interface GetInstanceNetworkInterfaceIpv6AccessConfig {
        externalIpv6: string;
        externalIpv6PrefixLength: string;
        /**
         * The name of the instance. One of `name` or `selfLink` must be provided.
         */
        name: string;
        /**
         * The [networking tier][network-tier] used for configuring this instance. One of `PREMIUM` or `STANDARD`.
         */
        networkTier: string;
        /**
         * The DNS domain name for the public PTR record.
         */
        publicPtrDomainName: string;
    }

    export interface GetInstanceNetworkPerformanceConfig {
        /**
         * The egress bandwidth tier for the instance.
         */
        totalEgressBandwidthTier: string;
    }

    export interface GetInstanceParam {
        resourceManagerTags: {[key: string]: any};
    }

    export interface GetInstanceReservationAffinity {
        specificReservations: outputs.compute.GetInstanceReservationAffinitySpecificReservation[];
        /**
         * The accelerator type resource exposed to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetInstanceReservationAffinitySpecificReservation {
        key: string;
        values: string[];
    }

    export interface GetInstanceScheduling {
        /**
         * Specifies if the instance should be
         * restarted if it was terminated by Compute Engine (not a user).
         */
        automaticRestart: boolean;
        /**
         * Describe the type of termination action for `SPOT` VM. Can be `STOP` or `DELETE`.  Read more on [here](https://cloud.google.com/compute/docs/instances/create-use-spot)
         */
        instanceTerminationAction: string;
        localSsdRecoveryTimeouts: outputs.compute.GetInstanceSchedulingLocalSsdRecoveryTimeout[];
        maintenanceInterval: string;
        maxRunDurations: outputs.compute.GetInstanceSchedulingMaxRunDuration[];
        minNodeCpus: number;
        nodeAffinities: outputs.compute.GetInstanceSchedulingNodeAffinity[];
        /**
         * Describes maintenance behavior for the
         * instance. One of `MIGRATE` or `TERMINATE`, for more info, read
         * [here](https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options)
         */
        onHostMaintenance: string;
        /**
         * Whether the instance is preemptible.
         */
        preemptible: boolean;
        /**
         * Describe the type of preemptible VM.
         */
        provisioningModel: string;
    }

    export interface GetInstanceSchedulingLocalSsdRecoveryTimeout {
        nanos: number;
        seconds: number;
    }

    export interface GetInstanceSchedulingMaxRunDuration {
        nanos: number;
        seconds: number;
    }

    export interface GetInstanceSchedulingNodeAffinity {
        key: string;
        operator: string;
        values: string[];
    }

    export interface GetInstanceScratchDisk {
        /**
         * The disk interface used for attaching this disk. One of `SCSI` or `NVME`.
         */
        interface: string;
        /**
         * The size of the image in gigabytes.
         */
        size: number;
    }

    export interface GetInstanceServiceAccount {
        /**
         * The service account e-mail address.
         */
        email: string;
        /**
         * A list of service scopes.
         */
        scopes: string[];
    }

    export interface GetInstanceShieldedInstanceConfig {
        /**
         * - Whether integrity monitoring is enabled for the instance.
         */
        enableIntegrityMonitoring: boolean;
        /**
         * - Whether secure boot is enabled for the instance.
         */
        enableSecureBoot: boolean;
        /**
         * - Whether the instance uses vTPM.
         */
        enableVtpm: boolean;
    }

    export interface GetInstanceTemplateAdvancedMachineFeature {
        enableNestedVirtualization: boolean;
        threadsPerCore: number;
        visibleCoreCount: number;
    }

    export interface GetInstanceTemplateConfidentialInstanceConfig {
        /**
         * Defines whether the instance should have confidential compute enabled. `onHostMaintenance` has to be set to TERMINATE or this will fail to create the VM.
         */
        enableConfidentialCompute: boolean;
    }

    export interface GetInstanceTemplateDisk {
        /**
         * Whether or not the disk should be auto-deleted.
         * This defaults to true.
         */
        autoDelete: boolean;
        /**
         * Indicates that this is a boot disk.
         */
        boot: boolean;
        /**
         * A unique device name that is reflected into the
         * /dev/  tree of a Linux operating system running within the instance. If not
         * specified, the server chooses a default device name to apply to this disk.
         */
        deviceName: string;
        /**
         * Encrypts or decrypts a disk using a customer-supplied encryption key.
         */
        diskEncryptionKeys: outputs.compute.GetInstanceTemplateDiskDiskEncryptionKey[];
        /**
         * Name of the disk. When not provided, this defaults
         * to the name of the instance.
         */
        diskName: string;
        /**
         * The size of the image in gigabytes. If not
         * specified, it will inherit the size of its base image. For SCRATCH disks,
         * the size must be exactly 375GB.
         */
        diskSizeGb: number;
        /**
         * The GCE disk type. Such as `"pd-ssd"`, `"local-ssd"`,
         * `"pd-balanced"` or `"pd-standard"`.
         */
        diskType: string;
        /**
         * Specifies the disk interface to use for attaching this disk,
         * which is either SCSI or NVME. The default is SCSI. Persistent disks must always use SCSI
         * and the request will fail if you attempt to attach a persistent disk in any other format
         * than SCSI. Local SSDs can use either NVME or SCSI.
         */
        interface: string;
        /**
         * (Optional) A set of ket/value label pairs to assign to disk created from
         * this template
         */
        labels: {[key: string]: string};
        /**
         * The mode in which to attach this disk, either READ_WRITE
         * or READ_ONLY. If you are attaching or creating a boot disk, this must
         * read-write mode.
         */
        mode: string;
        provisionedIops: number;
        /**
         * (Optional) -- A list of short names of resource policies to attach to this disk for automatic snapshot creations. Currently a max of 1 resource policy is supported.
         */
        resourcePolicies: string[];
        /**
         * The name (**not self_link**)
         * of the disk (such as those managed by `gcp.compute.Disk`) to attach.
         * > **Note:** Either `source` or `sourceImage` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        source: string;
        /**
         * The image from which to
         * initialize this disk. This can be one of: the image's `selfLink`,
         * `projects/{project}/global/images/{image}`,
         * `projects/{project}/global/images/family/{family}`, `global/images/{image}`,
         * `global/images/family/{family}`, `family/{family}`, `{project}/{family}`,
         * `{project}/{image}`, `{family}`, or `{image}`.
         * > **Note:** Either `source` or `sourceImage` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        sourceImage: string;
        sourceImageEncryptionKeys: outputs.compute.GetInstanceTemplateDiskSourceImageEncryptionKey[];
        sourceSnapshot: string;
        sourceSnapshotEncryptionKeys: outputs.compute.GetInstanceTemplateDiskSourceSnapshotEncryptionKey[];
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetInstanceTemplateDiskDiskEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
    }

    export interface GetInstanceTemplateDiskSourceImageEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
    }

    export interface GetInstanceTemplateDiskSourceSnapshotEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
    }

    export interface GetInstanceTemplateGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetInstanceTemplateNetworkInterface {
        /**
         * Access configurations, i.e. IPs via which this
         * instance can be accessed via the Internet. Omit to ensure that the instance
         * is not accessible from the Internet (this means that ssh provisioners will
         * not work unless you are running the provider can send traffic to the instance's
         * network (e.g. via tunnel or because it is running on another cloud instance
         * on that network). This block can be repeated multiple times. Structure documented below.
         */
        accessConfigs: outputs.compute.GetInstanceTemplateNetworkInterfaceAccessConfig[];
        /**
         * An
         * array of alias IP ranges for this network interface. Can only be specified for network
         * interfaces on subnet-mode networks. Structure documented below.
         */
        aliasIpRanges: outputs.compute.GetInstanceTemplateNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        ipv6AccessConfigs: outputs.compute.GetInstanceTemplateNetworkInterfaceIpv6AccessConfig[];
        ipv6AccessType: string;
        ipv6Address: string;
        /**
         * The name of the instance template. One of `name`, `filter` or `selfLinkUnique` must be provided.
         */
        name: string;
        /**
         * The name or selfLink of the network to attach this interface to.
         * Use `network` attribute for Legacy or Auto subnetted networks and
         * `subnetwork` for custom subnetted networks.
         */
        network: string;
        networkAttachment: string;
        /**
         * The private IP address to assign to the instance. If
         * empty, the address will be automatically assigned.
         */
        networkIp: string;
        nicType: string;
        queueCount: number;
        stackType: string;
        /**
         * the name of the subnetwork to attach this interface
         * to. The subnetwork must exist in the same `region` this instance will be
         * created in. Either `network` or `subnetwork` must be provided.
         */
        subnetwork: string;
        /**
         * The ID of the project in which the subnetwork belongs.
         * If it is not provided, the provider project is used.
         */
        subnetworkProject: string;
    }

    export interface GetInstanceTemplateNetworkInterfaceAccessConfig {
        /**
         * The IP address that will be 1:1 mapped to the instance's
         * network ip. If not given, one will be generated.
         */
        natIp: string;
        /**
         * The [networking tier][network-tier] used for configuring
         * this instance template. This field can take the following values: PREMIUM or
         * STANDARD. If this field is not specified, it is assumed to be PREMIUM.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface GetInstanceTemplateNetworkInterfaceAliasIpRange {
        /**
         * The IP CIDR range represented by this alias IP range. This IP CIDR range
         * must belong to the specified subnetwork and cannot contain IP addresses reserved by
         * system or used by other network interfaces. At the time of writing only a
         * netmask (e.g. /24) may be supplied, with a CIDR format resulting in an API
         * error.
         */
        ipCidrRange: string;
        /**
         * The subnetwork secondary range name specifying
         * the secondary range from which to allocate the IP CIDR range for this alias IP
         * range. If left unspecified, the primary range of the subnetwork will be used.
         */
        subnetworkRangeName: string;
    }

    export interface GetInstanceTemplateNetworkInterfaceIpv6AccessConfig {
        externalIpv6: string;
        externalIpv6PrefixLength: string;
        /**
         * The name of the instance template. One of `name`, `filter` or `selfLinkUnique` must be provided.
         */
        name: string;
        /**
         * The [networking tier][network-tier] used for configuring
         * this instance template. This field can take the following values: PREMIUM or
         * STANDARD. If this field is not specified, it is assumed to be PREMIUM.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface GetInstanceTemplateNetworkPerformanceConfig {
        /**
         * The egress bandwidth tier for the instance.
         */
        totalEgressBandwidthTier: string;
    }

    export interface GetInstanceTemplateReservationAffinity {
        specificReservations: outputs.compute.GetInstanceTemplateReservationAffinitySpecificReservation[];
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetInstanceTemplateReservationAffinitySpecificReservation {
        /**
         * The key for the node affinity label.
         */
        key: string;
        values: string[];
    }

    export interface GetInstanceTemplateScheduling {
        /**
         * Specifies whether the instance should be
         * automatically restarted if it is terminated by Compute Engine (not
         * terminated by a user). This defaults to true.
         */
        automaticRestart: boolean;
        /**
         * Describe the type of termination action for `SPOT` VM. Can be `STOP` or `DELETE`.  Read more on [here](https://cloud.google.com/compute/docs/instances/create-use-spot)
         */
        instanceTerminationAction: string;
        localSsdRecoveryTimeouts: outputs.compute.GetInstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
        maintenanceInterval: string;
        maxRunDurations: outputs.compute.GetInstanceTemplateSchedulingMaxRunDuration[];
        minNodeCpus: number;
        /**
         * Specifies node affinities or anti-affinities
         * to determine which sole-tenant nodes your instances and managed instance
         * groups will use as host systems. Read more on sole-tenant node creation
         * [here](https://cloud.google.com/compute/docs/nodes/create-nodes).
         * Structure documented below.
         */
        nodeAffinities: outputs.compute.GetInstanceTemplateSchedulingNodeAffinity[];
        /**
         * Defines the maintenance behavior for this
         * instance.
         */
        onHostMaintenance: string;
        /**
         * Allows instance to be preempted. This defaults to
         * false. Read more on this
         * [here](https://cloud.google.com/compute/docs/instances/preemptible).
         */
        preemptible: boolean;
        /**
         * Describe the type of preemptible VM.
         */
        provisioningModel: string;
    }

    export interface GetInstanceTemplateSchedulingLocalSsdRecoveryTimeout {
        nanos: number;
        seconds: number;
    }

    export interface GetInstanceTemplateSchedulingMaxRunDuration {
        nanos: number;
        seconds: number;
    }

    export interface GetInstanceTemplateSchedulingNodeAffinity {
        /**
         * The key for the node affinity label.
         */
        key: string;
        /**
         * The operator. Can be `IN` for node-affinities
         * or `NOT_IN` for anti-affinities.
         */
        operator: string;
        values: string[];
    }

    export interface GetInstanceTemplateServiceAccount {
        /**
         * The service account e-mail address. If not given, the
         * default Google Compute Engine service account is used.
         */
        email: string;
        /**
         * A list of service scopes. Both OAuth2 URLs and gcloud
         * short names are supported. To allow full access to all Cloud APIs, use the
         * `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
         */
        scopes: string[];
    }

    export interface GetInstanceTemplateShieldedInstanceConfig {
        /**
         * - Compare the most recent boot measurements to the integrity policy baseline and return a pair of pass/fail results depending on whether they match or not. Defaults to true.
         */
        enableIntegrityMonitoring: boolean;
        /**
         * - Verify the digital signature of all boot components, and halt the boot process if signature verification fails. Defaults to false.
         */
        enableSecureBoot: boolean;
        /**
         * - Use a virtualized trusted platform module, which is a specialized computer chip you can use to encrypt objects like keys and certificates. Defaults to true.
         */
        enableVtpm: boolean;
    }

    export interface GetRegionInstanceGroupInstance {
        /**
         * URL to the instance.
         */
        instance: string;
        /**
         * List of named ports in the group, as a list of resources, each containing:
         */
        namedPorts: outputs.compute.GetRegionInstanceGroupInstanceNamedPort[];
        /**
         * String description of current state of the instance.
         */
        status: string;
    }

    export interface GetRegionInstanceGroupInstanceNamedPort {
        /**
         * The name of the instance group.  One of `name` or `selfLink` must be provided.
         */
        name: string;
        /**
         * Integer port number
         */
        port: number;
    }

    export interface GetRegionInstanceTemplateAdvancedMachineFeature {
        enableNestedVirtualization: boolean;
        threadsPerCore: number;
        visibleCoreCount: number;
    }

    export interface GetRegionInstanceTemplateConfidentialInstanceConfig {
        /**
         * Defines whether the instance should have confidential compute enabled. `onHostMaintenance` has to be set to TERMINATE or this will fail to create the VM.
         */
        enableConfidentialCompute: boolean;
    }

    export interface GetRegionInstanceTemplateDisk {
        /**
         * Whether or not the disk should be auto-deleted.
         * This defaults to true.
         */
        autoDelete: boolean;
        /**
         * Indicates that this is a boot disk.
         */
        boot: boolean;
        /**
         * A unique device name that is reflected into the
         * /dev/  tree of a Linux operating system running within the instance. If not
         * specified, the server chooses a default device name to apply to this disk.
         */
        deviceName: string;
        /**
         * Encrypts or decrypts a disk using a customer-supplied encryption key.
         */
        diskEncryptionKeys: outputs.compute.GetRegionInstanceTemplateDiskDiskEncryptionKey[];
        /**
         * Name of the disk. When not provided, this defaults
         * to the name of the instance.
         */
        diskName: string;
        /**
         * The size of the image in gigabytes. If not
         * specified, it will inherit the size of its base image. For SCRATCH disks,
         * the size must be exactly 375GB.
         */
        diskSizeGb: number;
        /**
         * The GCE disk type. Such as `"pd-ssd"`, `"local-ssd"`,
         * `"pd-balanced"` or `"pd-standard"`.
         */
        diskType: string;
        /**
         * Specifies the disk interface to use for attaching this disk,
         * which is either SCSI or NVME. The default is SCSI. Persistent disks must always use SCSI
         * and the request will fail if you attempt to attach a persistent disk in any other format
         * than SCSI. Local SSDs can use either NVME or SCSI.
         */
        interface: string;
        /**
         * (Optional) A set of ket/value label pairs to assign to disk created from
         * this template
         */
        labels: {[key: string]: string};
        /**
         * The mode in which to attach this disk, either READ_WRITE
         * or READ_ONLY. If you are attaching or creating a boot disk, this must
         * read-write mode.
         */
        mode: string;
        provisionedIops: number;
        /**
         * (Optional) -- A list of short names of resource policies to attach to this disk for automatic snapshot creations. Currently a max of 1 resource policy is supported.
         */
        resourcePolicies: string[];
        /**
         * The name (**not self_link**)
         * of the disk (such as those managed by `gcp.compute.Disk`) to attach.
         * > **Note:** Either `source` or `sourceImage` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        source: string;
        /**
         * The image from which to
         * initialize this disk. This can be one of: the image's `selfLink`,
         * `projects/{project}/global/images/{image}`,
         * `projects/{project}/global/images/family/{family}`, `global/images/{image}`,
         * `global/images/family/{family}`, `family/{family}`, `{project}/{family}`,
         * `{project}/{image}`, `{family}`, or `{image}`.
         * > **Note:** Either `source` or `sourceImage` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        sourceImage: string;
        sourceImageEncryptionKeys: outputs.compute.GetRegionInstanceTemplateDiskSourceImageEncryptionKey[];
        sourceSnapshot: string;
        sourceSnapshotEncryptionKeys: outputs.compute.GetRegionInstanceTemplateDiskSourceSnapshotEncryptionKey[];
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetRegionInstanceTemplateDiskDiskEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
    }

    export interface GetRegionInstanceTemplateDiskSourceImageEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
    }

    export interface GetRegionInstanceTemplateDiskSourceSnapshotEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
    }

    export interface GetRegionInstanceTemplateGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetRegionInstanceTemplateNetworkInterface {
        accessConfigs: outputs.compute.GetRegionInstanceTemplateNetworkInterfaceAccessConfig[];
        /**
         * An
         * array of alias IP ranges for this network interface. Can only be specified for network
         * interfaces on subnet-mode networks. Structure documented below.
         */
        aliasIpRanges: outputs.compute.GetRegionInstanceTemplateNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        ipv6AccessConfigs: outputs.compute.GetRegionInstanceTemplateNetworkInterfaceIpv6AccessConfig[];
        ipv6AccessType: string;
        ipv6Address: string;
        /**
         * The name of the instance template. One of `name` or `filter` must be provided.
         */
        name: string;
        /**
         * The name or selfLink of the network to attach this interface to.
         * Use `network` attribute for Legacy or Auto subnetted networks and
         * `subnetwork` for custom subnetted networks.
         */
        network: string;
        /**
         * The private IP address to assign to the instance. If
         * empty, the address will be automatically assigned.
         */
        networkIp: string;
        nicType: string;
        queueCount: number;
        stackType: string;
        /**
         * the name of the subnetwork to attach this interface
         * to. The subnetwork must exist in the same `region` this instance will be
         * created in. Either `network` or `subnetwork` must be provided.
         */
        subnetwork: string;
        /**
         * The ID of the project in which the subnetwork belongs.
         * If it is not provided, the provider project is used.
         */
        subnetworkProject: string;
    }

    export interface GetRegionInstanceTemplateNetworkInterfaceAccessConfig {
        /**
         * The IP address that will be 1:1 mapped to the instance's
         * network ip. If not given, one will be generated.
         */
        natIp: string;
        /**
         * The [networking tier][network-tier] used for configuring
         * this instance template. This field can take the following values: PREMIUM or
         * STANDARD. If this field is not specified, it is assumed to be PREMIUM.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface GetRegionInstanceTemplateNetworkInterfaceAliasIpRange {
        /**
         * The IP CIDR range represented by this alias IP range. This IP CIDR range
         * must belong to the specified subnetwork and cannot contain IP addresses reserved by
         * system or used by other network interfaces. At the time of writing only a
         * netmask (e.g. /24) may be supplied, with a CIDR format resulting in an API
         * error.
         */
        ipCidrRange: string;
        /**
         * The subnetwork secondary range name specifying
         * the secondary range from which to allocate the IP CIDR range for this alias IP
         * range. If left unspecified, the primary range of the subnetwork will be used.
         */
        subnetworkRangeName: string;
    }

    export interface GetRegionInstanceTemplateNetworkInterfaceIpv6AccessConfig {
        externalIpv6: string;
        externalIpv6PrefixLength: string;
        /**
         * The name of the instance template. One of `name` or `filter` must be provided.
         */
        name: string;
        /**
         * The [networking tier][network-tier] used for configuring
         * this instance template. This field can take the following values: PREMIUM or
         * STANDARD. If this field is not specified, it is assumed to be PREMIUM.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface GetRegionInstanceTemplateNetworkPerformanceConfig {
        /**
         * The egress bandwidth tier for the instance.
         */
        totalEgressBandwidthTier: string;
    }

    export interface GetRegionInstanceTemplateReservationAffinity {
        specificReservations: outputs.compute.GetRegionInstanceTemplateReservationAffinitySpecificReservation[];
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface GetRegionInstanceTemplateReservationAffinitySpecificReservation {
        /**
         * The key for the node affinity label.
         */
        key: string;
        values: string[];
    }

    export interface GetRegionInstanceTemplateScheduling {
        /**
         * Specifies whether the instance should be
         * automatically restarted if it is terminated by Compute Engine (not
         * terminated by a user). This defaults to true.
         */
        automaticRestart: boolean;
        /**
         * Describe the type of termination action for `SPOT` VM. Can be `STOP` or `DELETE`.  Read more on [here](https://cloud.google.com/compute/docs/instances/create-use-spot)
         */
        instanceTerminationAction: string;
        localSsdRecoveryTimeouts: outputs.compute.GetRegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
        maintenanceInterval: string;
        maxRunDurations: outputs.compute.GetRegionInstanceTemplateSchedulingMaxRunDuration[];
        minNodeCpus: number;
        /**
         * Specifies node affinities or anti-affinities
         * to determine which sole-tenant nodes your instances and managed instance
         * groups will use as host systems. Read more on sole-tenant node creation
         * [here](https://cloud.google.com/compute/docs/nodes/create-nodes).
         * Structure documented below.
         */
        nodeAffinities: outputs.compute.GetRegionInstanceTemplateSchedulingNodeAffinity[];
        /**
         * Defines the maintenance behavior for this
         * instance.
         */
        onHostMaintenance: string;
        /**
         * Allows instance to be preempted. This defaults to
         * false. Read more on this
         * [here](https://cloud.google.com/compute/docs/instances/preemptible).
         */
        preemptible: boolean;
        /**
         * Describe the type of preemptible VM.
         */
        provisioningModel: string;
    }

    export interface GetRegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout {
        nanos: number;
        seconds: number;
    }

    export interface GetRegionInstanceTemplateSchedulingMaxRunDuration {
        nanos: number;
        seconds: number;
    }

    export interface GetRegionInstanceTemplateSchedulingNodeAffinity {
        /**
         * The key for the node affinity label.
         */
        key: string;
        /**
         * The operator. Can be `IN` for node-affinities
         * or `NOT_IN` for anti-affinities.
         */
        operator: string;
        values: string[];
    }

    export interface GetRegionInstanceTemplateServiceAccount {
        /**
         * The service account e-mail address. If not given, the
         * default Google Compute Engine service account is used.
         */
        email: string;
        /**
         * A list of service scopes. Both OAuth2 URLs and gcloud
         * short names are supported. To allow full access to all Cloud APIs, use the
         * `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
         */
        scopes: string[];
    }

    export interface GetRegionInstanceTemplateShieldedInstanceConfig {
        /**
         * - Compare the most recent boot measurements to the integrity policy baseline and return a pair of pass/fail results depending on whether they match or not. Defaults to true.
         */
        enableIntegrityMonitoring: boolean;
        /**
         * - Verify the digital signature of all boot components, and halt the boot process if signature verification fails. Defaults to false.
         */
        enableSecureBoot: boolean;
        /**
         * - Use a virtualized trusted platform module, which is a specialized computer chip you can use to encrypt objects like keys and certificates. Defaults to true.
         */
        enableVtpm: boolean;
    }

    export interface GetRegionNetworkEndpointGroupAppEngine {
        service: string;
        urlMask: string;
        version: string;
    }

    export interface GetRegionNetworkEndpointGroupCloudFunction {
        function: string;
        urlMask: string;
    }

    export interface GetRegionNetworkEndpointGroupCloudRun {
        service: string;
        tag: string;
        urlMask: string;
    }

    export interface GetRegionNetworkEndpointGroupServerlessDeployment {
        platform: string;
        resource: string;
        urlMask: string;
        version: string;
    }

    export interface GetResourcePolicyDiskConsistencyGroupPolicy {
        enabled: boolean;
    }

    export interface GetResourcePolicyGroupPlacementPolicy {
        availabilityDomainCount: number;
        collocation: string;
        maxDistance: number;
        vmCount: number;
    }

    export interface GetResourcePolicyInstanceSchedulePolicy {
        expirationTime: string;
        startTime: string;
        timeZone: string;
        vmStartSchedules: outputs.compute.GetResourcePolicyInstanceSchedulePolicyVmStartSchedule[];
        vmStopSchedules: outputs.compute.GetResourcePolicyInstanceSchedulePolicyVmStopSchedule[];
    }

    export interface GetResourcePolicyInstanceSchedulePolicyVmStartSchedule {
        schedule: string;
    }

    export interface GetResourcePolicyInstanceSchedulePolicyVmStopSchedule {
        schedule: string;
    }

    export interface GetResourcePolicySnapshotSchedulePolicy {
        retentionPolicies: outputs.compute.GetResourcePolicySnapshotSchedulePolicyRetentionPolicy[];
        schedules: outputs.compute.GetResourcePolicySnapshotSchedulePolicySchedule[];
        snapshotProperties: outputs.compute.GetResourcePolicySnapshotSchedulePolicySnapshotProperty[];
    }

    export interface GetResourcePolicySnapshotSchedulePolicyRetentionPolicy {
        maxRetentionDays: number;
        onSourceDiskDelete: string;
    }

    export interface GetResourcePolicySnapshotSchedulePolicySchedule {
        dailySchedules: outputs.compute.GetResourcePolicySnapshotSchedulePolicyScheduleDailySchedule[];
        hourlySchedules: outputs.compute.GetResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule[];
        weeklySchedules: outputs.compute.GetResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule[];
    }

    export interface GetResourcePolicySnapshotSchedulePolicyScheduleDailySchedule {
        daysInCycle: number;
        startTime: string;
    }

    export interface GetResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule {
        hoursInCycle: number;
        startTime: string;
    }

    export interface GetResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule {
        dayOfWeeks: outputs.compute.GetResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek[];
    }

    export interface GetResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek {
        day: string;
        startTime: string;
    }

    export interface GetResourcePolicySnapshotSchedulePolicySnapshotProperty {
        chainName: string;
        guestFlush: boolean;
        labels: {[key: string]: string};
        storageLocations: string[];
    }

    export interface GetRouterBgp {
        advertiseMode: string;
        advertisedGroups: string[];
        advertisedIpRanges: outputs.compute.GetRouterBgpAdvertisedIpRange[];
        asn: number;
        keepaliveInterval: number;
    }

    export interface GetRouterBgpAdvertisedIpRange {
        description: string;
        range: string;
    }

    export interface GetRouterNatLogConfig {
        enable: boolean;
        filter: string;
    }

    export interface GetRouterNatRule {
        actions: outputs.compute.GetRouterNatRuleAction[];
        description: string;
        match: string;
        ruleNumber: number;
    }

    export interface GetRouterNatRuleAction {
        sourceNatActiveIps: string[];
        sourceNatDrainIps: string[];
    }

    export interface GetRouterNatSubnetwork {
        /**
         * Name of the NAT service. The name must be 1-63 characters long and
         * comply with RFC1035.
         */
        name: string;
        secondaryIpRangeNames: string[];
        sourceIpRangesToNats: string[];
    }

    export interface GetRouterStatusBestRoute {
        description: string;
        destRange: string;
        /**
         * The name of the router.
         */
        name: string;
        /**
         * The network name or resource link to the parent
         * network of this subnetwork.
         */
        network: string;
        nextHopGateway: string;
        nextHopIlb: string;
        nextHopInstance: string;
        nextHopInstanceZone: string;
        nextHopIp: string;
        nextHopNetwork: string;
        nextHopVpnTunnel: string;
        priority: number;
        /**
         * The ID of the project in which the resource
         * belongs. If it is not provided, the provider project is used.
         */
        project: string;
        selfLink: string;
        tags: string[];
    }

    export interface GetRouterStatusBestRoutesForRouter {
        description: string;
        destRange: string;
        /**
         * The name of the router.
         */
        name: string;
        /**
         * The network name or resource link to the parent
         * network of this subnetwork.
         */
        network: string;
        nextHopGateway: string;
        nextHopIlb: string;
        nextHopInstance: string;
        nextHopInstanceZone: string;
        nextHopIp: string;
        nextHopNetwork: string;
        nextHopVpnTunnel: string;
        priority: number;
        /**
         * The ID of the project in which the resource
         * belongs. If it is not provided, the provider project is used.
         */
        project: string;
        selfLink: string;
        tags: string[];
    }

    export interface GetSnapshotSnapshotEncryptionKey {
        kmsKeySelfLink: string;
        kmsKeyServiceAccount: string;
        rawKey: string;
        sha256: string;
    }

    export interface GetSnapshotSourceDiskEncryptionKey {
        kmsKeyServiceAccount: string;
        rawKey: string;
    }

    export interface GetSubnetworkSecondaryIpRange {
        /**
         * The range of IP addresses belonging to this subnetwork
         * secondary range.
         */
        ipCidrRange: string;
        /**
         * The name associated with this subnetwork secondary range, used
         * when adding an alias IP range to a VM instance.
         */
        rangeName: string;
    }

    export interface GlobalForwardingRuleMetadataFilter {
        /**
         * The list of label value pairs that must match labels in the
         * provided metadata based on filterMatchCriteria
         * This list must not be empty and can have at the most 64 entries.
         * Structure is documented below.
         */
        filterLabels: outputs.compute.GlobalForwardingRuleMetadataFilterFilterLabel[];
        /**
         * Specifies how individual filterLabel matches within the list of
         * filterLabels contribute towards the overall metadataFilter match.
         * MATCH_ANY - At least one of the filterLabels must have a matching
         * label in the provided metadata.
         * MATCH_ALL - All filterLabels must have matching labels in the
         * provided metadata.
         * Possible values are: `MATCH_ANY`, `MATCH_ALL`.
         */
        filterMatchCriteria: string;
    }

    export interface GlobalForwardingRuleMetadataFilterFilterLabel {
        /**
         * Name of the metadata label. The length must be between
         * 1 and 1024 characters, inclusive.
         */
        name: string;
        /**
         * The value that the label must match. The value has a maximum
         * length of 1024 characters.
         */
        value: string;
    }

    export interface HaVpnGatewayVpnInterface {
        /**
         * The numeric ID of this VPN gateway interface.
         */
        id?: number;
        /**
         * URL of the interconnect attachment resource. When the value
         * of this field is present, the VPN Gateway will be used for
         * IPsec-encrypted Cloud Interconnect; all Egress or Ingress
         * traffic for this VPN Gateway interface will go through the
         * specified interconnect attachment resource.
         * Not currently available publicly.
         */
        interconnectAttachment?: string;
        /**
         * (Output)
         * The external IP address for this VPN gateway interface.
         */
        ipAddress: string;
    }

    export interface HealthCheckGrpcHealthCheck {
        /**
         * The gRPC service name for the health check.
         * The value of grpcServiceName has the following meanings by convention:
         * - Empty serviceName means the overall status of all services at the backend.
         * - Non-empty serviceName means the health of that gRPC service, as defined by the owner of the service.
         * The grpcServiceName can only be ASCII.
         */
        grpcServiceName?: string;
        /**
         * The port number for the health check request.
         * Must be specified if portName and portSpecification are not set
         * or if portSpecification is USE_FIXED_PORT. Valid values are 1 through 65535.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
    }

    export interface HealthCheckHttp2HealthCheck {
        /**
         * The value of the host header in the HTTP health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         *
         * (Optional)
         * The value of the host header in the HTTPS health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         *
         * (Optional)
         * The value of the host header in the HTTP2 health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         */
        host?: string;
        /**
         * The TCP port number for the HTTP health check request.
         * The default value is 80.
         *
         * (Optional)
         * The TCP port number for the HTTPS health check request.
         * The default value is 443.
         *
         * (Optional)
         * The TCP port number for the TCP health check request.
         * The default value is 443.
         *
         * (Optional)
         * The TCP port number for the SSL health check request.
         * The default value is 443.
         *
         * (Optional)
         * The TCP port number for the HTTP2 health check request.
         * The default value is 443.
         *
         * (Optional)
         * The port number for the health check request.
         * Must be specified if portName and portSpecification are not set
         * or if portSpecification is USE_FIXED_PORT. Valid values are 1 through 65535.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The request path of the HTTP health check request.
         * The default value is /.
         *
         * (Optional)
         * The request path of the HTTPS health check request.
         * The default value is /.
         *
         * (Optional)
         * The request path of the HTTP2 health check request.
         * The default value is /.
         */
        requestPath?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface HealthCheckHttpHealthCheck {
        /**
         * The value of the host header in the HTTP health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         */
        host?: string;
        /**
         * The TCP port number for the HTTP health check request.
         * The default value is 80.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The request path of the HTTP health check request.
         * The default value is /.
         */
        requestPath?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface HealthCheckHttpsHealthCheck {
        /**
         * The value of the host header in the HTTPS health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         */
        host?: string;
        /**
         * The TCP port number for the HTTPS health check request.
         * The default value is 443.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The request path of the HTTPS health check request.
         * The default value is /.
         */
        requestPath?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface HealthCheckLogConfig {
        /**
         * Indicates whether or not to export logs. This is false by default,
         * which means no health check logging will be done.
         */
        enable?: boolean;
    }

    export interface HealthCheckSslHealthCheck {
        /**
         * The TCP port number for the HTTP2 health check request.
         * The default value is 443.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The application data to send once the SSL connection has been
         * established (default value is empty). If both request and response are
         * empty, the connection establishment alone will indicate health. The request
         * data can only be ASCII.
         */
        request?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface HealthCheckTcpHealthCheck {
        /**
         * The TCP port number for the TCP health check request.
         * The default value is 443.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The application data to send once the TCP connection has been
         * established (default value is empty). If both request and response are
         * empty, the connection establishment alone will indicate health. The request
         * data can only be ASCII.
         */
        request?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface ImageGuestOsFeature {
        /**
         * The type of supported feature. Read [Enabling guest operating system features](https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#guest-os-features) to see a list of available options.
         * Possible values are: `MULTI_IP_SUBNET`, `SECURE_BOOT`, `SEV_CAPABLE`, `UEFI_COMPATIBLE`, `VIRTIO_SCSI_MULTIQUEUE`, `WINDOWS`, `GVNIC`, `SEV_LIVE_MIGRATABLE`, `SEV_SNP_CAPABLE`, `SUSPEND_RESUME_COMPATIBLE`, `TDX_CAPABLE`.
         */
        type: string;
    }

    export interface ImageIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface ImageIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface ImageImageEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud
         * KMS.
         */
        kmsKeySelfLink?: string;
        /**
         * The service account being used for the encryption request for the
         * given KMS key. If absent, the Compute Engine default service
         * account is used.
         */
        kmsKeyServiceAccount?: string;
    }

    export interface ImageRawDisk {
        /**
         * The format used to encode and transmit the block device, which
         * should be TAR. This is just a container and transmission format
         * and not a runtime format. Provided by the client when the disk
         * image is created.
         * Default value is `TAR`.
         * Possible values are: `TAR`.
         */
        containerType?: string;
        /**
         * An optional SHA1 checksum of the disk image before unpackaging.
         * This is provided by the client when the disk image is created.
         */
        sha1?: string;
        /**
         * The full Google Cloud Storage URL where disk storage is stored
         * You must provide either this property or the sourceDisk property
         * but not both.
         */
        source: string;
    }

    export interface InstanceAdvancedMachineFeatures {
        /**
         * Defines whether the instance should have nested virtualization  enabled. Defaults to false.
         */
        enableNestedVirtualization?: boolean;
        /**
         * he number of threads per physical core. To disable [simultaneous multithreading (SMT)](https://cloud.google.com/compute/docs/instances/disabling-smt) set this to 1.
         */
        threadsPerCore?: number;
        /**
         * The number of physical cores to expose to an instance. [visible cores info (VC)](https://cloud.google.com/compute/docs/instances/customize-visible-cores).
         */
        visibleCoreCount?: number;
    }

    export interface InstanceAttachedDisk {
        /**
         * Name with which the attached disk will be accessible
         * under `/dev/disk/by-id/google-*`
         */
        deviceName: string;
        /**
         * A 256-bit [customer-supplied encryption key]
         * (https://cloud.google.com/compute/docs/disks/customer-supplied-encryption),
         * encoded in [RFC 4648 base64](https://tools.ietf.org/html/rfc4648#section-4)
         * to encrypt this disk. Only one of `kmsKeySelfLink` and `diskEncryptionKeyRaw` may be set.
         */
        diskEncryptionKeyRaw?: string;
        diskEncryptionKeySha256: string;
        /**
         * The selfLink of the encryption key that is
         * stored in Google Cloud KMS to encrypt this disk. Only one of `kmsKeySelfLink`
         * and `diskEncryptionKeyRaw` may be set.
         */
        kmsKeySelfLink: string;
        /**
         * Either "READ_ONLY" or "READ_WRITE", defaults to "READ_WRITE"
         * If you have a persistent disk with data that you want to share
         * between multiple instances, detach it from any read-write instances and
         * attach it to one or more instances in read-only mode.
         */
        mode?: string;
        /**
         * The name or selfLink of the disk to attach to this instance.
         */
        source: string;
    }

    export interface InstanceBootDisk {
        /**
         * Whether the disk will be auto-deleted when the instance
         * is deleted. Defaults to true.
         */
        autoDelete?: boolean;
        /**
         * Name with which attached disk will be accessible.
         * On the instance, this device will be `/dev/disk/by-id/google-{{device_name}}`.
         */
        deviceName: string;
        /**
         * A 256-bit [customer-supplied encryption key]
         * (https://cloud.google.com/compute/docs/disks/customer-supplied-encryption),
         * encoded in [RFC 4648 base64](https://tools.ietf.org/html/rfc4648#section-4)
         * to encrypt this disk. Only one of `kmsKeySelfLink` and `diskEncryptionKeyRaw`
         * may be set.
         */
        diskEncryptionKeyRaw?: string;
        diskEncryptionKeySha256: string;
        /**
         * Parameters for a new disk that will be created
         * alongside the new instance. Either `initializeParams` or `source` must be set.
         * Structure is documented below.
         */
        initializeParams: outputs.compute.InstanceBootDiskInitializeParams;
        /**
         * The selfLink of the encryption key that is
         * stored in Google Cloud KMS to encrypt this disk. Only one of `kmsKeySelfLink`
         * and `diskEncryptionKeyRaw` may be set.
         */
        kmsKeySelfLink: string;
        /**
         * The mode in which to attach this disk, either `READ_WRITE`
         * or `READ_ONLY`. If not specified, the default is to attach the disk in `READ_WRITE` mode.
         */
        mode?: string;
        /**
         * The name or selfLink of the existing disk (such as those managed by
         * `gcp.compute.Disk`) or disk image. To create an instance from a snapshot, first create a
         * `gcp.compute.Disk` from a snapshot and reference it here.
         */
        source: string;
    }

    export interface InstanceBootDiskInitializeParams {
        /**
         * The image from which to initialize this disk. This can be
         * one of: the image's `selfLink`, `projects/{project}/global/images/{image}`,
         * `projects/{project}/global/images/family/{family}`, `global/images/{image}`,
         * `global/images/family/{family}`, `family/{family}`, `{project}/{family}`,
         * `{project}/{image}`, `{family}`, or `{image}`. If referred by family, the
         * images names must include the family name. If they don't, use the
         * [gcp.compute.Image data source](https://www.terraform.io/docs/providers/google/d/compute_image.html).
         * For instance, the image `centos-6-v20180104` includes its family name `centos-6`.
         * These images can be referred by family name here.
         */
        image: string;
        /**
         * A set of key/value label pairs assigned to the disk. This  
         * field is only applicable for persistent disks.
         */
        labels: {[key: string]: any};
        resourceManagerTags?: {[key: string]: any};
        /**
         * The size of the image in gigabytes. If not specified, it
         * will inherit the size of its base image.
         */
        size: number;
        /**
         * The GCE disk type. Such as pd-standard, pd-balanced or pd-ssd.
         */
        type: string;
    }

    export interface InstanceConfidentialInstanceConfig {
        /**
         * Defines whether the instance should have confidential compute enabled. `onHostMaintenance` has to be set to TERMINATE or this will fail to create the VM.
         */
        enableConfidentialCompute: boolean;
    }

    export interface InstanceFromMachineImageAdvancedMachineFeatures {
        enableNestedVirtualization: boolean;
        threadsPerCore: number;
        visibleCoreCount: number;
    }

    export interface InstanceFromMachineImageAttachedDisk {
        deviceName: string;
        diskEncryptionKeyRaw: string;
        diskEncryptionKeySha256: string;
        kmsKeySelfLink: string;
        mode: string;
        source: string;
    }

    export interface InstanceFromMachineImageBootDisk {
        autoDelete: boolean;
        deviceName: string;
        diskEncryptionKeyRaw: string;
        diskEncryptionKeySha256: string;
        initializeParams: outputs.compute.InstanceFromMachineImageBootDiskInitializeParams;
        kmsKeySelfLink: string;
        mode: string;
        source: string;
    }

    export interface InstanceFromMachineImageBootDiskInitializeParams {
        image: string;
        labels: {[key: string]: any};
        resourceManagerTags: {[key: string]: any};
        size: number;
        type: string;
    }

    export interface InstanceFromMachineImageConfidentialInstanceConfig {
        enableConfidentialCompute: boolean;
    }

    export interface InstanceFromMachineImageGuestAccelerator {
        count: number;
        type: string;
    }

    export interface InstanceFromMachineImageNetworkInterface {
        accessConfigs: outputs.compute.InstanceFromMachineImageNetworkInterfaceAccessConfig[];
        aliasIpRanges: outputs.compute.InstanceFromMachineImageNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        ipv6AccessConfigs: outputs.compute.InstanceFromMachineImageNetworkInterfaceIpv6AccessConfig[];
        ipv6AccessType: string;
        ipv6Address: string;
        /**
         * A unique name for the resource, required by GCE.
         * Changing this forces a new resource to be created.
         */
        name: string;
        network: string;
        networkIp: string;
        nicType: string;
        queueCount: number;
        stackType: string;
        subnetwork: string;
        subnetworkProject: string;
    }

    export interface InstanceFromMachineImageNetworkInterfaceAccessConfig {
        natIp: string;
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface InstanceFromMachineImageNetworkInterfaceAliasIpRange {
        ipCidrRange: string;
        subnetworkRangeName: string;
    }

    export interface InstanceFromMachineImageNetworkInterfaceIpv6AccessConfig {
        externalIpv6: string;
        externalIpv6PrefixLength: string;
        /**
         * A unique name for the resource, required by GCE.
         * Changing this forces a new resource to be created.
         */
        name: string;
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface InstanceFromMachineImageNetworkPerformanceConfig {
        totalEgressBandwidthTier: string;
    }

    export interface InstanceFromMachineImageParams {
        resourceManagerTags: {[key: string]: any};
    }

    export interface InstanceFromMachineImageReservationAffinity {
        specificReservation: outputs.compute.InstanceFromMachineImageReservationAffinitySpecificReservation;
        type: string;
    }

    export interface InstanceFromMachineImageReservationAffinitySpecificReservation {
        key: string;
        values: string[];
    }

    export interface InstanceFromMachineImageScheduling {
        automaticRestart: boolean;
        instanceTerminationAction: string;
        localSsdRecoveryTimeout: outputs.compute.InstanceFromMachineImageSchedulingLocalSsdRecoveryTimeout;
        maintenanceInterval: string;
        maxRunDuration: outputs.compute.InstanceFromMachineImageSchedulingMaxRunDuration;
        minNodeCpus: number;
        nodeAffinities: outputs.compute.InstanceFromMachineImageSchedulingNodeAffinity[];
        onHostMaintenance: string;
        preemptible: boolean;
        provisioningModel: string;
    }

    export interface InstanceFromMachineImageSchedulingLocalSsdRecoveryTimeout {
        nanos: number;
        seconds: number;
    }

    export interface InstanceFromMachineImageSchedulingMaxRunDuration {
        nanos: number;
        seconds: number;
    }

    export interface InstanceFromMachineImageSchedulingNodeAffinity {
        key: string;
        operator: string;
        values: string[];
    }

    export interface InstanceFromMachineImageScratchDisk {
        interface: string;
        size: number;
    }

    export interface InstanceFromMachineImageServiceAccount {
        email: string;
        scopes: string[];
    }

    export interface InstanceFromMachineImageShieldedInstanceConfig {
        enableIntegrityMonitoring: boolean;
        enableSecureBoot: boolean;
        enableVtpm: boolean;
    }

    export interface InstanceFromTemplateAdvancedMachineFeatures {
        enableNestedVirtualization: boolean;
        threadsPerCore: number;
        visibleCoreCount: number;
    }

    export interface InstanceFromTemplateAttachedDisk {
        deviceName: string;
        diskEncryptionKeyRaw: string;
        diskEncryptionKeySha256: string;
        kmsKeySelfLink: string;
        mode: string;
        source: string;
    }

    export interface InstanceFromTemplateBootDisk {
        autoDelete: boolean;
        deviceName: string;
        diskEncryptionKeyRaw: string;
        diskEncryptionKeySha256: string;
        initializeParams: outputs.compute.InstanceFromTemplateBootDiskInitializeParams;
        kmsKeySelfLink: string;
        mode: string;
        source: string;
    }

    export interface InstanceFromTemplateBootDiskInitializeParams {
        image: string;
        labels: {[key: string]: any};
        resourceManagerTags: {[key: string]: any};
        size: number;
        type: string;
    }

    export interface InstanceFromTemplateConfidentialInstanceConfig {
        enableConfidentialCompute: boolean;
    }

    export interface InstanceFromTemplateGuestAccelerator {
        count: number;
        type: string;
    }

    export interface InstanceFromTemplateNetworkInterface {
        accessConfigs: outputs.compute.InstanceFromTemplateNetworkInterfaceAccessConfig[];
        aliasIpRanges: outputs.compute.InstanceFromTemplateNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        ipv6AccessConfigs: outputs.compute.InstanceFromTemplateNetworkInterfaceIpv6AccessConfig[];
        ipv6AccessType: string;
        ipv6Address: string;
        /**
         * A unique name for the resource, required by GCE.
         * Changing this forces a new resource to be created.
         */
        name: string;
        network: string;
        networkIp: string;
        nicType: string;
        queueCount: number;
        stackType: string;
        subnetwork: string;
        subnetworkProject: string;
    }

    export interface InstanceFromTemplateNetworkInterfaceAccessConfig {
        natIp: string;
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface InstanceFromTemplateNetworkInterfaceAliasIpRange {
        ipCidrRange: string;
        subnetworkRangeName: string;
    }

    export interface InstanceFromTemplateNetworkInterfaceIpv6AccessConfig {
        externalIpv6: string;
        externalIpv6PrefixLength: string;
        /**
         * A unique name for the resource, required by GCE.
         * Changing this forces a new resource to be created.
         */
        name: string;
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface InstanceFromTemplateNetworkPerformanceConfig {
        totalEgressBandwidthTier: string;
    }

    export interface InstanceFromTemplateParams {
        resourceManagerTags: {[key: string]: any};
    }

    export interface InstanceFromTemplateReservationAffinity {
        specificReservation: outputs.compute.InstanceFromTemplateReservationAffinitySpecificReservation;
        type: string;
    }

    export interface InstanceFromTemplateReservationAffinitySpecificReservation {
        key: string;
        values: string[];
    }

    export interface InstanceFromTemplateScheduling {
        automaticRestart: boolean;
        instanceTerminationAction: string;
        localSsdRecoveryTimeout: outputs.compute.InstanceFromTemplateSchedulingLocalSsdRecoveryTimeout;
        maintenanceInterval: string;
        maxRunDuration: outputs.compute.InstanceFromTemplateSchedulingMaxRunDuration;
        minNodeCpus: number;
        nodeAffinities: outputs.compute.InstanceFromTemplateSchedulingNodeAffinity[];
        onHostMaintenance: string;
        preemptible: boolean;
        provisioningModel: string;
    }

    export interface InstanceFromTemplateSchedulingLocalSsdRecoveryTimeout {
        nanos: number;
        seconds: number;
    }

    export interface InstanceFromTemplateSchedulingMaxRunDuration {
        nanos: number;
        seconds: number;
    }

    export interface InstanceFromTemplateSchedulingNodeAffinity {
        key: string;
        operator: string;
        values: string[];
    }

    export interface InstanceFromTemplateScratchDisk {
        interface: string;
        size: number;
    }

    export interface InstanceFromTemplateServiceAccount {
        email: string;
        scopes: string[];
    }

    export interface InstanceFromTemplateShieldedInstanceConfig {
        enableIntegrityMonitoring: boolean;
        enableSecureBoot: boolean;
        enableVtpm: boolean;
    }

    export interface InstanceGroupManagerAllInstancesConfig {
        /**
         * ), The label key-value pairs that you want to patch onto the instance.
         *
         * - - -
         */
        labels?: {[key: string]: string};
        /**
         * ), The metadata key-value pairs that you want to patch onto the instance. For more information, see [Project and instance metadata](https://cloud.google.com/compute/docs/metadata#project_and_instance_metadata).
         */
        metadata?: {[key: string]: string};
    }

    export interface InstanceGroupManagerAutoHealingPolicies {
        /**
         * The health check resource that signals autohealing.
         */
        healthCheck: string;
        /**
         * The number of seconds that the managed instance group waits before
         * it applies autohealing policies to new instances or recently recreated instances. Between 0 and 3600.
         */
        initialDelaySec: number;
    }

    export interface InstanceGroupManagerInstanceLifecyclePolicy {
        /**
         * ), Specifies whether to apply the group's latest configuration when repairing a VM. Valid options are: `YES`, `NO`. If `YES` and you updated the group's instance template or per-instance configurations after the VM was created, then these changes are applied when VM is repaired. If `NO` (default), then updates are applied in accordance with the group's update policy type.
         *
         * - - -
         */
        forceUpdateOnRepair?: string;
    }

    export interface InstanceGroupManagerNamedPort {
        /**
         * The name of the port.
         */
        name: string;
        /**
         * The port number.
         * - - -
         */
        port: number;
    }

    export interface InstanceGroupManagerStatefulDisk {
        /**
         * , A value that prescribes what should happen to the stateful disk when the VM instance is deleted. The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`. `NEVER` - detach the disk when the VM is deleted, but do not delete the disk. `ON_PERMANENT_INSTANCE_DELETION` will delete the stateful disk when the VM is permanently deleted from the instance group. The default is `NEVER`.
         */
        deleteRule?: string;
        /**
         * , The device name of the disk to be attached.
         */
        deviceName: string;
    }

    export interface InstanceGroupManagerStatefulExternalIp {
        /**
         * , A value that prescribes what should happen to the external ip when the VM instance is deleted. The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`. `NEVER` - detach the ip when the VM is deleted, but do not delete the ip. `ON_PERMANENT_INSTANCE_DELETION` will delete the external ip when the VM is permanently deleted from the instance group.
         */
        deleteRule?: string;
        /**
         * , The network interface name of the external Ip. Possible value: `nic0`
         */
        interfaceName?: string;
    }

    export interface InstanceGroupManagerStatefulInternalIp {
        /**
         * , A value that prescribes what should happen to the internal ip when the VM instance is deleted. The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`. `NEVER` - detach the ip when the VM is deleted, but do not delete the ip. `ON_PERMANENT_INSTANCE_DELETION` will delete the internal ip when the VM is permanently deleted from the instance group.
         */
        deleteRule?: string;
        /**
         * , The network interface name of the internal Ip. Possible value: `nic0`
         */
        interfaceName?: string;
    }

    export interface InstanceGroupManagerStatus {
        /**
         * )
         * Properties to set on all instances in the group. After setting
         * allInstancesConfig on the group, you must update the group's instances to
         * apply the configuration.
         */
        allInstancesConfigs: outputs.compute.InstanceGroupManagerStatusAllInstancesConfig[];
        /**
         * A bit indicating whether the managed instance group is in a stable state. A stable state means that: none of the instances in the managed instance group is currently undergoing any type of change (for example, creation, restart, or deletion); no future changes are scheduled for instances in the managed instance group; and the managed instance group itself is not being modified.
         */
        isStable: boolean;
        /**
         * Stateful status of the given Instance Group Manager.
         */
        statefuls: outputs.compute.InstanceGroupManagerStatusStateful[];
        /**
         * A bit indicating whether version target has been reached in this managed instance group, i.e. all instances are in their target version. Instances' target version are specified by version field on Instance Group Manager.
         */
        versionTargets: outputs.compute.InstanceGroupManagerStatusVersionTarget[];
    }

    export interface InstanceGroupManagerStatusAllInstancesConfig {
        effective: boolean;
    }

    export interface InstanceGroupManagerStatusStateful {
        /**
         * A bit indicating whether the managed instance group has stateful configuration, that is, if you have configured any items in a stateful policy or in per-instance configs. The group might report that it has no stateful config even when there is still some preserved state on a managed instance, for example, if you have deleted all PICs but not yet applied those deletions.
         */
        hasStatefulConfig: boolean;
        /**
         * Status of per-instance configs on the instance.
         */
        perInstanceConfigs: outputs.compute.InstanceGroupManagerStatusStatefulPerInstanceConfig[];
    }

    export interface InstanceGroupManagerStatusStatefulPerInstanceConfig {
        /**
         * A bit indicating if all of the group's per-instance configs (listed in the output of a listPerInstanceConfigs API call) have status `EFFECTIVE` or there are no per-instance-configs.
         */
        allEffective: boolean;
    }

    export interface InstanceGroupManagerStatusVersionTarget {
        isReached: boolean;
    }

    export interface InstanceGroupManagerUpdatePolicy {
        /**
         * , The maximum number of instances that can be created above the specified targetSize during the update process. Conflicts with `maxSurgePercent`. If neither is set, defaults to 1
         */
        maxSurgeFixed: number;
        /**
         * , The maximum number of instances(calculated as percentage) that can be created above the specified targetSize during the update process. Conflicts with `maxSurgeFixed`.
         */
        maxSurgePercent?: number;
        /**
         * , The maximum number of instances that can be unavailable during the update process. Conflicts with `maxUnavailablePercent`. If neither is set, defaults to 1
         */
        maxUnavailableFixed: number;
        /**
         * , The maximum number of instances(calculated as percentage) that can be unavailable during the update process. Conflicts with `maxUnavailableFixed`.
         */
        maxUnavailablePercent?: number;
        /**
         * ), Minimum number of seconds to wait for after a newly created instance becomes available. This value must be from range [0, 3600]
         */
        minReadySec?: number;
        /**
         * Minimal action to be taken on an instance. You can specify either `REFRESH` to update without stopping instances, `RESTART` to restart existing instances or `REPLACE` to delete and create new instances from the target template. If you specify a `REFRESH`, the Updater will attempt to perform that action only. However, if the Updater determines that the minimal action you specify is not enough to perform the update, it might perform a more disruptive action.
         */
        minimalAction: string;
        /**
         * Most disruptive action that is allowed to be taken on an instance. You can specify either NONE to forbid any actions, REFRESH to allow actions that do not need instance restart, RESTART to allow actions that can be applied without instance replacing or REPLACE to allow all possible actions. If the Updater determines that the minimal update action needed is more disruptive than most disruptive allowed action you specify it will not perform the update at all.
         */
        mostDisruptiveAllowedAction?: string;
        /**
         * , The instance replacement method for managed instance groups. Valid values are: "RECREATE", "SUBSTITUTE". If SUBSTITUTE (default), the group replaces VM instances with new instances that have randomly generated names. If RECREATE, instance names are preserved.  You must also set maxUnavailableFixed or maxUnavailablePercent to be greater than 0.
         * - - -
         */
        replacementMethod?: string;
        /**
         * The type of update process. You can specify either `PROACTIVE` so that the instance group manager proactively executes actions in order to bring instances to their target versions or `OPPORTUNISTIC` so that no action is proactively executed but the update will be performed as part of other actions (for example, resizes or recreateInstances calls).
         */
        type: string;
    }

    export interface InstanceGroupManagerVersion {
        /**
         * The full URL to an instance template from which all new instances of this version will be created. It is recommended to reference instance templates through their unique id (`selfLinkUnique` attribute).
         */
        instanceTemplate: string;
        /**
         * Version name.
         */
        name?: string;
        /**
         * The number of instances calculated as a fixed number or a percentage depending on the settings. Structure is documented below.
         *
         * > Exactly one `version` you specify must not have a `targetSize` specified. During a rolling update, the instance group manager will fulfill the `targetSize`
         * constraints of every other `version`, and any remaining instances will be provisioned with the version where `targetSize` is unset.
         */
        targetSize?: outputs.compute.InstanceGroupManagerVersionTargetSize;
    }

    export interface InstanceGroupManagerVersionTargetSize {
        /**
         * , The number of instances which are managed for this version. Conflicts with `percent`.
         */
        fixed?: number;
        /**
         * , The number of instances (calculated as percentage) which are managed for this version. Conflicts with `fixed`.
         * Note that when using `percent`, rounding will be in favor of explicitly set `targetSize` values; a managed instance group with 2 instances and 2 `version`s,
         * one of which has a `target_size.percent` of `60` will create 2 instances of that `version`.
         */
        percent?: number;
    }

    export interface InstanceGroupNamedPort {
        /**
         * The name which the port will be mapped to.
         */
        name: string;
        /**
         * The port number to map the name to.
         */
        port: number;
    }

    export interface InstanceGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * The GCE disk type. Such as pd-standard, pd-balanced or pd-ssd.
         */
        type: string;
    }

    export interface InstanceIAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface InstanceIAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface InstanceNetworkInterface {
        accessConfigs?: outputs.compute.InstanceNetworkInterfaceAccessConfig[];
        /**
         * An
         * array of alias IP ranges for this network interface. Can only be specified for network
         * interfaces on subnet-mode networks. Structure documented below.
         */
        aliasIpRanges?: outputs.compute.InstanceNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        /**
         * An array of IPv6 access configurations for this interface.
         * Currently, only one IPv6 access config, DIRECT_IPV6, is supported. If there is no ipv6AccessConfig
         * specified, then this instance will have no external IPv6 Internet access. Structure documented below.
         */
        ipv6AccessConfigs?: outputs.compute.InstanceNetworkInterfaceIpv6AccessConfig[];
        /**
         * One of EXTERNAL, INTERNAL to indicate whether the IP can be accessed from the Internet.
         * This field is always inherited from its subnetwork.
         */
        ipv6AccessType: string;
        ipv6Address: string;
        /**
         * A unique name for the resource, required by GCE.
         * Changing this forces a new resource to be created.
         */
        name: string;
        /**
         * The name or selfLink of the network to attach this interface to.
         * Either `network` or `subnetwork` must be provided. If network isn't provided it will
         * be inferred from the subnetwork.
         */
        network: string;
        /**
         * The private IP address to assign to the instance. If
         * empty, the address will be automatically assigned.
         */
        networkIp: string;
        /**
         * The type of vNIC to be used on this interface. Possible values: GVNIC, VIRTIO_NET.
         */
        nicType?: string;
        /**
         * The networking queue count that's specified by users for the network interface. Both Rx and Tx queues will be set to this number. It will be empty if not specified.
         */
        queueCount?: number;
        /**
         * The stack type for this network interface to identify whether the IPv6 feature is enabled or not. Values are IPV4_IPV6 or IPV4_ONLY. If not specified, IPV4_ONLY will be used.
         */
        stackType: string;
        /**
         * The name or selfLink of the subnetwork to attach this
         * interface to. Either `network` or `subnetwork` must be provided. If network isn't provided
         * it will be inferred from the subnetwork. The subnetwork must exist in the same region this
         * instance will be created in. If the network resource is in
         * [legacy](https://cloud.google.com/vpc/docs/legacy) mode, do not specify this field. If the
         * network is in auto subnet mode, specifying the subnetwork is optional. If the network is
         * in custom subnet mode, specifying the subnetwork is required.
         */
        subnetwork: string;
        /**
         * The project in which the subnetwork belongs.
         * If the `subnetwork` is a self_link, this field is ignored in favor of the project
         * defined in the subnetwork self_link. If the `subnetwork` is a name and this
         * field is not provided, the provider project is used.
         */
        subnetworkProject: string;
    }

    export interface InstanceNetworkInterfaceAccessConfig {
        /**
         * The IP address that will be 1:1 mapped to the instance's
         * network ip. If not given, one will be generated.
         */
        natIp: string;
        /**
         * The service-level to be provided for IPv6 traffic when the
         * subnet has an external subnet. Only PREMIUM or STANDARD tier is valid for IPv6.
         */
        networkTier: string;
        /**
         * The domain name to be used when creating DNSv6
         * records for the external IPv6 ranges..
         */
        publicPtrDomainName?: string;
    }

    export interface InstanceNetworkInterfaceAliasIpRange {
        /**
         * The IP CIDR range represented by this alias IP range. This IP CIDR range
         * must belong to the specified subnetwork and cannot contain IP addresses reserved by
         * system or used by other network interfaces. This range may be a single IP address
         * (e.g. 10.2.3.4), a netmask (e.g. /24) or a CIDR format string (e.g. 10.1.2.0/24).
         */
        ipCidrRange: string;
        /**
         * The subnetwork secondary range name specifying
         * the secondary range from which to allocate the IP CIDR range for this alias IP
         * range. If left unspecified, the primary range of the subnetwork will be used.
         */
        subnetworkRangeName?: string;
    }

    export interface InstanceNetworkInterfaceIpv6AccessConfig {
        /**
         * The first IPv6 address of the external IPv6 range associated 
         * with this instance, prefix length is stored in externalIpv6PrefixLength in ipv6AccessConfig.
         * To use a static external IP address, it must be unused and in the same region as the instance's zone.
         * If not specified, Google Cloud will automatically assign an external IPv6 address from the instance's subnetwork.
         */
        externalIpv6: string;
        /**
         * The prefix length of the external IPv6 range.
         */
        externalIpv6PrefixLength: string;
        /**
         * A unique name for the resource, required by GCE.
         * Changing this forces a new resource to be created.
         */
        name: string;
        /**
         * The [networking tier][network-tier] used for configuring this instance.
         * This field can take the following values: PREMIUM, FIXED_STANDARD or STANDARD. If this field is
         * not specified, it is assumed to be PREMIUM.
         *
         * <a name="nestedIpv6AccessConfig"></a>The `ipv6AccessConfig` block supports:
         *
         * subnet has an external subnet. Only PREMIUM or STANDARD tier is valid for IPv6.
         */
        networkTier: string;
        /**
         * The DNS domain name for the public PTR record.
         * To set this field on an instance, you must be verified as the owner of the domain.
         * See [the docs](https://cloud.google.com/compute/docs/instances/create-ptr-record) for how
         * to become verified as a domain owner.
         *
         * records for the external IPv6 ranges..
         */
        publicPtrDomainName?: string;
    }

    export interface InstanceNetworkPerformanceConfig {
        /**
         * The egress bandwidth tier to enable.
         * Possible values: TIER_1, DEFAULT
         */
        totalEgressBandwidthTier: string;
    }

    export interface InstanceParams {
        resourceManagerTags?: {[key: string]: any};
    }

    export interface InstanceReservationAffinity {
        /**
         * Specifies the label selector for the reservation to use..
         * Structure is documented below.
         */
        specificReservation?: outputs.compute.InstanceReservationAffinitySpecificReservation;
        /**
         * The type of reservation from which this instance can consume resources.
         */
        type: string;
    }

    export interface InstanceReservationAffinitySpecificReservation {
        /**
         * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify compute.googleapis.com/reservation-name as the key and specify the name of your reservation as the only value.
         */
        key: string;
        /**
         * Corresponds to the label values of a reservation resource.
         */
        values: string[];
    }

    export interface InstanceScheduling {
        /**
         * Specifies if the instance should be
         * restarted if it was terminated by Compute Engine (not a user).
         * Defaults to true.
         */
        automaticRestart?: boolean;
        /**
         * Describe the type of termination action for VM. Can be `STOP` or `DELETE`.  Read more on [here](https://cloud.google.com/compute/docs/instances/create-use-spot)
         */
        instanceTerminationAction?: string;
        localSsdRecoveryTimeout?: outputs.compute.InstanceSchedulingLocalSsdRecoveryTimeout;
        maintenanceInterval?: string;
        maxRunDuration?: outputs.compute.InstanceSchedulingMaxRunDuration;
        /**
         * The minimum number of virtual CPUs this instance will consume when running on a sole-tenant node.
         */
        minNodeCpus?: number;
        /**
         * Specifies node affinities or anti-affinities
         * to determine which sole-tenant nodes your instances and managed instance
         * groups will use as host systems. Read more on sole-tenant node creation
         * [here](https://cloud.google.com/compute/docs/nodes/create-nodes).
         * Structure documented below.
         */
        nodeAffinities?: outputs.compute.InstanceSchedulingNodeAffinity[];
        /**
         * Describes maintenance behavior for the
         * instance. Can be MIGRATE or TERMINATE, for more info, read
         * [here](https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options).
         */
        onHostMaintenance: string;
        /**
         * Specifies if the instance is preemptible.
         * If this field is set to true, then `automaticRestart` must be
         * set to false.  Defaults to false.
         */
        preemptible?: boolean;
        /**
         * Describe the type of preemptible VM. This field accepts the value `STANDARD` or `SPOT`. If the value is `STANDARD`, there will be no discount. If this   is set to `SPOT`, 
         * `preemptible` should be `true` and `automaticRestart` should be
         * `false`. For more info about
         * `SPOT`, read [here](https://cloud.google.com/compute/docs/instances/spot)
         */
        provisioningModel: string;
    }

    export interface InstanceSchedulingLocalSsdRecoveryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         *
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         *
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         */
        seconds: number;
    }

    export interface InstanceSchedulingMaxRunDuration {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         *
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         *
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         */
        seconds: number;
    }

    export interface InstanceSchedulingNodeAffinity {
        /**
         * The key for the node affinity label.
         */
        key: string;
        /**
         * The operator. Can be `IN` for node-affinities
         * or `NOT_IN` for anti-affinities.
         */
        operator: string;
        /**
         * The values for the node affinity label.
         */
        values: string[];
    }

    export interface InstanceScratchDisk {
        /**
         * The disk interface to use for attaching this disk; either SCSI or NVME.
         */
        interface: string;
        /**
         * The size of the image in gigabytes. If not specified, it
         * will inherit the size of its base image.
         */
        size?: number;
    }

    export interface InstanceServiceAccount {
        /**
         * The service account e-mail address.
         * **Note**: `allowStoppingForUpdate` must be set to true or your instance must have a `desiredStatus` of `TERMINATED` in order to update this field.
         */
        email: string;
        /**
         * A list of service scopes. Both OAuth2 URLs and gcloud
         * short names are supported. To allow full access to all Cloud APIs, use the
         * `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
         * **Note**: `allowStoppingForUpdate` must be set to true or your instance must have a `desiredStatus` of `TERMINATED` in order to update this field.
         */
        scopes: string[];
    }

    export interface InstanceShieldedInstanceConfig {
        /**
         * - Compare the most recent boot measurements to the integrity policy baseline and return a pair of pass/fail results depending on whether they match or not. Defaults to true.
         * **Note**: `allowStoppingForUpdate` must be set to true or your instance must have a `desiredStatus` of `TERMINATED` in order to update this field.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * - Verify the digital signature of all boot components, and halt the boot process if signature verification fails. Defaults to false.
         * **Note**: `allowStoppingForUpdate` must be set to true or your instance must have a `desiredStatus` of `TERMINATED` in order to update this field.
         */
        enableSecureBoot?: boolean;
        /**
         * - Use a virtualized trusted platform module, which is a specialized computer chip you can use to encrypt objects like keys and certificates. Defaults to true.
         * **Note**: `allowStoppingForUpdate` must be set to true or your instance must have a `desiredStatus` of `TERMINATED` in order to update this field.
         */
        enableVtpm?: boolean;
    }

    export interface InstanceTemplateAdvancedMachineFeatures {
        /**
         * Defines whether the instance should have nested virtualization enabled. Defaults to false.
         */
        enableNestedVirtualization?: boolean;
        /**
         * The number of threads per physical core. To disable [simultaneous multithreading (SMT)](https://cloud.google.com/compute/docs/instances/disabling-smt) set this to 1.
         */
        threadsPerCore?: number;
        /**
         * ) The number of physical cores to expose to an instance. [visible cores info (VC)](https://cloud.google.com/compute/docs/instances/customize-visible-cores).
         */
        visibleCoreCount?: number;
    }

    export interface InstanceTemplateConfidentialInstanceConfig {
        /**
         * Defines whether the instance should have confidential compute enabled. `onHostMaintenance` has to be set to TERMINATE or this will fail to create the VM.
         */
        enableConfidentialCompute: boolean;
    }

    export interface InstanceTemplateDisk {
        /**
         * Whether or not the disk should be auto-deleted.
         * This defaults to true.
         */
        autoDelete?: boolean;
        /**
         * Indicates that this is a boot disk.
         */
        boot: boolean;
        /**
         * A unique device name that is reflected into the
         * /dev/  tree of a Linux operating system running within the instance. If not
         * specified, the server chooses a default device name to apply to this disk.
         */
        deviceName: string;
        /**
         * Encrypts or decrypts a disk using a customer-supplied encryption key.
         *
         * If you are creating a new disk, this field encrypts the new disk using an encryption key that you provide. If you are attaching an existing disk that is already encrypted, this field decrypts the disk using the customer-supplied encryption key.
         *
         * If you encrypt a disk using a customer-supplied key, you must provide the same key again when you attempt to use this resource at a later time. For example, you must provide the key when you create a snapshot or an image from the disk or when you attach the disk to a virtual machine instance.
         *
         * If you do not provide an encryption key, then the disk will be encrypted using an automatically generated key and you do not need to provide a key to use the disk later.
         *
         * Instance templates do not store customer-supplied encryption keys, so you cannot use your own keys to encrypt disks in a managed instance group. Structure documented below.
         */
        diskEncryptionKey?: outputs.compute.InstanceTemplateDiskDiskEncryptionKey;
        /**
         * Name of the disk. When not provided, this defaults
         * to the name of the instance.
         */
        diskName?: string;
        /**
         * The size of the image in gigabytes. If not
         * specified, it will inherit the size of its base image. For SCRATCH disks,
         * the size must be exactly 375GB.
         */
        diskSizeGb: number;
        /**
         * The GCE disk type. Such as `"pd-ssd"`, `"local-ssd"`,
         * `"pd-balanced"` or `"pd-standard"`.
         */
        diskType: string;
        /**
         * Specifies the disk interface to use for attaching this disk,
         * which is either SCSI or NVME. The default is SCSI. Persistent disks must always use SCSI
         * and the request will fail if you attempt to attach a persistent disk in any other format
         * than SCSI. Local SSDs can use either NVME or SCSI.
         */
        interface: string;
        /**
         * A set of ket/value label pairs to assign to disk created from
         * this template
         */
        labels?: {[key: string]: string};
        /**
         * The mode in which to attach this disk, either READ_WRITE
         * or READ_ONLY. If you are attaching or creating a boot disk, this must
         * read-write mode.
         */
        mode: string;
        provisionedIops: number;
        /**
         * - A list (short name or id) of resource policies to attach to this disk for automatic snapshot creations. Currently a max of 1 resource policy is supported.
         */
        resourcePolicies?: string;
        /**
         * The name (**not self_link**)
         * of the disk (such as those managed by `gcp.compute.Disk`) to attach.
         * > **Note:** Either `source`, `sourceImage`, or `sourceSnapshot` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        source?: string;
        /**
         * The image from which to
         * initialize this disk. This can be one of: the image's `selfLink`,
         * `projects/{project}/global/images/{image}`,
         * `projects/{project}/global/images/family/{family}`, `global/images/{image}`,
         * `global/images/family/{family}`, `family/{family}`, `{project}/{family}`,
         * `{project}/{image}`, `{family}`, or `{image}`.
         * > **Note:** Either `source`, `sourceImage`, or `sourceSnapshot` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        sourceImage: string;
        /**
         * The customer-supplied encryption
         * key of the source image. Required if the source image is protected by a
         * customer-supplied encryption key.
         *
         * Instance templates do not store customer-supplied encryption keys, so you
         * cannot create disks for instances in a managed instance group if the source
         * images are encrypted with your own keys. Structure
         * documented below.
         */
        sourceImageEncryptionKey?: outputs.compute.InstanceTemplateDiskSourceImageEncryptionKey;
        /**
         * The source snapshot to create this disk.
         * > **Note:** Either `source`, `sourceImage`, or `sourceSnapshot` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        sourceSnapshot?: string;
        /**
         * The customer-supplied encryption
         * key of the source snapshot. Structure
         * documented below.
         */
        sourceSnapshotEncryptionKey?: outputs.compute.InstanceTemplateDiskSourceSnapshotEncryptionKey;
        /**
         * The type of GCE disk, can be either `"SCRATCH"` or
         * `"PERSISTENT"`.
         */
        type: string;
    }

    export interface InstanceTemplateDiskDiskEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
    }

    export interface InstanceTemplateDiskSourceImageEncryptionKey {
        /**
         * The self link of the encryption key that is
         * stored in Google Cloud KMS.
         */
        kmsKeySelfLink: string;
        /**
         * The service account being used for the
         * encryption request for the given KMS key. If absent, the Compute Engine
         * default service account is used.
         */
        kmsKeyServiceAccount?: string;
    }

    export interface InstanceTemplateDiskSourceSnapshotEncryptionKey {
        /**
         * The self link of the encryption key that is
         * stored in Google Cloud KMS.
         */
        kmsKeySelfLink: string;
        /**
         * The service account being used for the
         * encryption request for the given KMS key. If absent, the Compute Engine
         * default service account is used.
         */
        kmsKeyServiceAccount?: string;
    }

    export interface InstanceTemplateGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * The type of GCE disk, can be either `"SCRATCH"` or
         * `"PERSISTENT"`.
         */
        type: string;
    }

    export interface InstanceTemplateNetworkInterface {
        /**
         * Access configurations, i.e. IPs via which this
         * instance can be accessed via the Internet. Omit to ensure that the instance
         * is not accessible from the Internet (this means that ssh provisioners will
         * not work unless you can send traffic to the instance's
         * network (e.g. via tunnel or because it is running on another cloud instance
         * on that network). This block can be repeated multiple times. Structure documented below.
         */
        accessConfigs?: outputs.compute.InstanceTemplateNetworkInterfaceAccessConfig[];
        /**
         * An
         * array of alias IP ranges for this network interface. Can only be specified for network
         * interfaces on subnet-mode networks. Structure documented below.
         */
        aliasIpRanges?: outputs.compute.InstanceTemplateNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        /**
         * An array of IPv6 access configurations for this interface.
         * Currently, only one IPv6 access config, DIRECT_IPV6, is supported. If there is no ipv6AccessConfig
         * specified, then this instance will have no external IPv6 Internet access. Structure documented below.
         */
        ipv6AccessConfigs?: outputs.compute.InstanceTemplateNetworkInterfaceIpv6AccessConfig[];
        ipv6AccessType: string;
        ipv6Address: string;
        /**
         * The name of the instance template. If you leave
         * this blank, the provider will auto-generate a unique name.
         */
        name: string;
        /**
         * The name or selfLink of the network to attach this interface to.
         * Use `network` attribute for Legacy or Auto subnetted networks and
         * `subnetwork` for custom subnetted networks.
         */
        network: string;
        networkAttachment: string;
        /**
         * The private IP address to assign to the instance. If
         * empty, the address will be automatically assigned.
         */
        networkIp?: string;
        /**
         * The type of vNIC to be used on this interface. Possible values: GVNIC, VIRTIO_NET.
         */
        nicType?: string;
        /**
         * The networking queue count that's specified by users for the network interface. Both Rx and Tx queues will be set to this number. It will be empty if not specified.
         */
        queueCount?: number;
        /**
         * The stack type for this network interface to identify whether the IPv6 feature is enabled or not. Values are IPV4_IPV6 or IPV4_ONLY. If not specified, IPV4_ONLY will be used.
         */
        stackType: string;
        /**
         * the name of the subnetwork to attach this interface
         * to. The subnetwork must exist in the same `region` this instance will be
         * created in. Either `network` or `subnetwork` must be provided.
         */
        subnetwork: string;
        /**
         * The ID of the project in which the subnetwork belongs.
         * If it is not provided, the provider project is used.
         */
        subnetworkProject: string;
    }

    export interface InstanceTemplateNetworkInterfaceAccessConfig {
        /**
         * The IP address that will be 1:1 mapped to the instance's
         * network ip. If not given, one will be generated.
         */
        natIp: string;
        /**
         * The service-level to be provided for IPv6 traffic when the
         * subnet has an external subnet. Only PREMIUM and STANDARD tier is valid for IPv6.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface InstanceTemplateNetworkInterfaceAliasIpRange {
        /**
         * The IP CIDR range represented by this alias IP range. This IP CIDR range
         * must belong to the specified subnetwork and cannot contain IP addresses reserved by
         * system or used by other network interfaces. At the time of writing only a
         * netmask (e.g. /24) may be supplied, with a CIDR format resulting in an API
         * error.
         */
        ipCidrRange: string;
        /**
         * The subnetwork secondary range name specifying
         * the secondary range from which to allocate the IP CIDR range for this alias IP
         * range. If left unspecified, the primary range of the subnetwork will be used.
         */
        subnetworkRangeName?: string;
    }

    export interface InstanceTemplateNetworkInterfaceIpv6AccessConfig {
        externalIpv6: string;
        externalIpv6PrefixLength: string;
        /**
         * The name of the instance template. If you leave
         * this blank, the provider will auto-generate a unique name.
         */
        name: string;
        /**
         * The [networking tier][network-tier] used for configuring
         * this instance template. This field can take the following values: PREMIUM,
         * STANDARD or FIXED_STANDARD. If this field is not specified, it is assumed to be PREMIUM.
         *
         * <a name="nestedIpv6AccessConfig"></a>The `ipv6AccessConfig` block supports:
         *
         * subnet has an external subnet. Only PREMIUM and STANDARD tier is valid for IPv6.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface InstanceTemplateNetworkPerformanceConfig {
        /**
         * The egress bandwidth tier to enable. Possible values: TIER_1, DEFAULT
         */
        totalEgressBandwidthTier: string;
    }

    export interface InstanceTemplateReservationAffinity {
        /**
         * Specifies the label selector for the reservation to use..
         * Structure is documented below.
         */
        specificReservation?: outputs.compute.InstanceTemplateReservationAffinitySpecificReservation;
        /**
         * The type of reservation from which this instance can consume resources.
         */
        type: string;
    }

    export interface InstanceTemplateReservationAffinitySpecificReservation {
        /**
         * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify compute.googleapis.com/reservation-name as the key and specify the name of your reservation as the only value.
         */
        key: string;
        /**
         * Corresponds to the label values of a reservation resource.
         */
        values: string[];
    }

    export interface InstanceTemplateScheduling {
        /**
         * Specifies whether the instance should be
         * automatically restarted if it is terminated by Compute Engine (not
         * terminated by a user). This defaults to true.
         */
        automaticRestart?: boolean;
        /**
         * Describe the type of termination action for `SPOT` VM. Can be `STOP` or `DELETE`.  Read more on [here](https://cloud.google.com/compute/docs/instances/create-use-spot)
         */
        instanceTerminationAction?: string;
        localSsdRecoveryTimeouts?: outputs.compute.InstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
        maintenanceInterval?: string;
        /**
         * Beta - The duration of the instance. Instance will run and be terminated after then, the termination action could be defined in `instanceTerminationAction`. Only support `DELETE` `instanceTerminationAction` at this point. Structure is documented below.
         * <a name="nestedMaxRunDuration"></a>The `maxRunDuration` block supports:
         */
        maxRunDuration?: outputs.compute.InstanceTemplateSchedulingMaxRunDuration;
        minNodeCpus?: number;
        /**
         * Specifies node affinities or anti-affinities
         * to determine which sole-tenant nodes your instances and managed instance
         * groups will use as host systems. Read more on sole-tenant node creation
         * [here](https://cloud.google.com/compute/docs/nodes/create-nodes).
         * Structure documented below.
         */
        nodeAffinities?: outputs.compute.InstanceTemplateSchedulingNodeAffinity[];
        /**
         * Defines the maintenance behavior for this
         * instance.
         */
        onHostMaintenance: string;
        /**
         * Allows instance to be preempted. This defaults to
         * false. Read more on this
         * [here](https://cloud.google.com/compute/docs/instances/preemptible).
         */
        preemptible?: boolean;
        /**
         * Describe the type of preemptible VM. This field accepts the value `STANDARD` or `SPOT`. If the value is `STANDARD`, there will be no discount. If this   is set to `SPOT`, 
         * `preemptible` should be `true` and `automaticRestart` should be
         * `false`. For more info about
         * `SPOT`, read [here](https://cloud.google.com/compute/docs/instances/spot)
         */
        provisioningModel: string;
    }

    export interface InstanceTemplateSchedulingLocalSsdRecoveryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         *
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         *
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         */
        seconds: number;
    }

    export interface InstanceTemplateSchedulingMaxRunDuration {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         *
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         *
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         */
        seconds: number;
    }

    export interface InstanceTemplateSchedulingNodeAffinity {
        /**
         * The key for the node affinity label.
         */
        key: string;
        /**
         * The operator. Can be `IN` for node-affinities
         * or `NOT_IN` for anti-affinities.
         */
        operator: string;
        /**
         * Corresponds to the label values of a reservation resource.
         */
        values: string[];
    }

    export interface InstanceTemplateServiceAccount {
        /**
         * The service account e-mail address. If not given, the
         * default Google Compute Engine service account is used.
         */
        email: string;
        /**
         * A list of service scopes. Both OAuth2 URLs and gcloud
         * short names are supported. To allow full access to all Cloud APIs, use the
         * `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
         *
         * The [service accounts documentation](https://cloud.google.com/compute/docs/access/service-accounts#accesscopesiam)
         * explains that access scopes are the legacy method of specifying permissions for your instance.
         * To follow best practices you should create a dedicated service account with the minimum permissions the VM requires.
         * To use a dedicated service account this field should be configured as a list containing the `cloud-platform` scope.
         * See [Authenticate workloads using service accounts best practices](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices)
         * and [Best practices for using service accounts](https://cloud.google.com/iam/docs/best-practices-service-accounts#single-purpose).
         */
        scopes: string[];
    }

    export interface InstanceTemplateShieldedInstanceConfig {
        /**
         * - Compare the most recent boot measurements to the integrity policy baseline and return a pair of pass/fail results depending on whether they match or not. Defaults to true.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * - Verify the digital signature of all boot components, and halt the boot process if signature verification fails. Defaults to false.
         */
        enableSecureBoot?: boolean;
        /**
         * - Use a virtualized trusted platform module, which is a specialized computer chip you can use to encrypt objects like keys and certificates. Defaults to true.
         */
        enableVtpm?: boolean;
    }

    export interface InterconnectAttachmentPrivateInterconnectInfo {
        /**
         * (Output)
         * 802.1q encapsulation tag to be used for traffic between
         * Google and the customer, going to and from this network and region.
         */
        tag8021q: number;
    }

    export interface MachineImageIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface MachineImageIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface MachineImageMachineImageEncryptionKey {
        /**
         * The name of the encryption key that is stored in Google Cloud KMS.
         */
        kmsKeyName?: string;
        /**
         * The service account used for the encryption request for the given KMS key.
         * If absent, the Compute Engine Service Agent service account is used.
         */
        kmsKeyServiceAccount?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         */
        rawKey?: string;
        /**
         * (Output)
         * The RFC 4648 base64 encoded SHA-256 hash of the
         * customer-supplied encryption key that protects this resource.
         */
        sha256: string;
    }

    export interface ManagedSslCertificateManaged {
        /**
         * Domains for which a managed SSL certificate will be valid.  Currently,
         * there can be up to 100 domains in this list.
         */
        domains: string[];
    }

    export interface MangedSslCertificateManaged {
        /**
         * Domains for which a managed SSL certificate will be valid.  Currently,
         * there can be up to 100 domains in this list.
         */
        domains: string[];
    }

    export interface NetworkAttachmentConnectionEndpoint {
        /**
         * (Output)
         * The IPv4 address assigned to the producer instance network interface. This value will be a range in case of Serverless.
         */
        ipAddress: string;
        /**
         * (Output)
         * The project id or number of the interface to which the IP was assigned.
         */
        projectIdOrNum: string;
        /**
         * (Output)
         * Alias IP ranges from the same subnetwork.
         */
        secondaryIpCidrRanges: string;
        /**
         * (Output)
         * The status of a connected endpoint to this network attachment.
         */
        status: string;
        /**
         * (Output)
         * The subnetwork used to assign the IP to the producer instance network interface.
         */
        subnetwork: string;
    }

    export interface NetworkEndpointListNetworkEndpoint {
        /**
         * The name for a specific VM instance that the IP address belongs to.
         * This is required for network endpoints of type GCE_VM_IP_PORT.
         * The instance must be in the same zone as the network endpoint group.
         */
        instance?: string;
        /**
         * IPv4 address of network endpoint. The IP address must belong
         * to a VM in GCE (either the primary IP or as part of an aliased IP
         * range).
         */
        ipAddress: string;
        /**
         * Port number of network endpoint.
         * **Note** `port` is required unless the Network Endpoint Group is created
         * with the type of `GCE_VM_IP`
         */
        port?: number;
    }

    export interface NetworkFirewallPolicyRuleMatch {
        /**
         * Address groups which should be matched against the traffic destination. Maximum number of destination address groups is 10. Destination address groups is only supported in Egress rules.
         */
        destAddressGroups?: string[];
        /**
         * Domain names that will be used to match against the resolved domain name of destination of traffic. Can only be specified if DIRECTION is egress.
         */
        destFqdns?: string[];
        /**
         * CIDR IP address range. Maximum number of destination CIDR IP ranges allowed is 5000.
         */
        destIpRanges?: string[];
        /**
         * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is egress.
         */
        destRegionCodes?: string[];
        /**
         * Name of the Google Cloud Threat Intelligence list.
         */
        destThreatIntelligences?: string[];
        /**
         * Pairs of IP protocols and ports that the rule should match.
         */
        layer4Configs: outputs.compute.NetworkFirewallPolicyRuleMatchLayer4Config[];
        /**
         * Address groups which should be matched against the traffic source. Maximum number of source address groups is 10. Source address groups is only supported in Ingress rules.
         */
        srcAddressGroups?: string[];
        /**
         * Domain names that will be used to match against the resolved domain name of source of traffic. Can only be specified if DIRECTION is ingress.
         */
        srcFqdns?: string[];
        /**
         * CIDR IP address range. Maximum number of source CIDR IP ranges allowed is 5000.
         */
        srcIpRanges?: string[];
        /**
         * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is ingress.
         */
        srcRegionCodes?: string[];
        /**
         * List of secure tag values, which should be matched at the source of the traffic. For INGRESS rule, if all the <code>srcSecureTag</code> are INEFFECTIVE, and there is no <code>srcIpRange</code>, this rule will be ignored. Maximum number of source tag values allowed is 256.
         */
        srcSecureTags?: outputs.compute.NetworkFirewallPolicyRuleMatchSrcSecureTag[];
        /**
         * Name of the Google Cloud Threat Intelligence list.
         *
         * The `layer4Configs` block supports:
         */
        srcThreatIntelligences?: string[];
    }

    export interface NetworkFirewallPolicyRuleMatchLayer4Config {
        /**
         * The IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (`tcp`, `udp`, `icmp`, `esp`, `ah`, `ipip`, `sctp`), or the IP protocol number.
         */
        ipProtocol: string;
        /**
         * An optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: ``.
         *
         * - - -
         */
        ports?: string[];
    }

    export interface NetworkFirewallPolicyRuleMatchSrcSecureTag {
        /**
         * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
         */
        name: string;
        /**
         * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
         */
        state: string;
    }

    export interface NetworkFirewallPolicyRuleTargetSecureTag {
        /**
         * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
         */
        name: string;
        /**
         * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
         */
        state: string;
    }

    export interface NodeGroupAutoscalingPolicy {
        /**
         * Maximum size of the node group. Set to a value less than or equal
         * to 100 and greater than or equal to min-nodes.
         */
        maxNodes: number;
        /**
         * Minimum size of the node group. Must be less
         * than or equal to max-nodes. The default value is 0.
         */
        minNodes: number;
        /**
         * The autoscaling mode. Set to one of the following:
         * - OFF: Disables the autoscaler.
         * - ON: Enables scaling in and scaling out.
         * - ONLY_SCALE_OUT: Enables only scaling out.
         * You must use this mode if your node groups are configured to
         * restart their hosted VMs on minimal servers.
         * Possible values are: `OFF`, `ON`, `ONLY_SCALE_OUT`.
         */
        mode: string;
    }

    export interface NodeGroupMaintenanceWindow {
        /**
         * instances.start time of the window. This must be in UTC format that resolves to one of 00:00, 04:00, 08:00, 12:00, 16:00, or 20:00. For example, both 13:00-5 and 08:00 are valid.
         */
        startTime: string;
    }

    export interface NodeGroupShareSettings {
        /**
         * A map of project id and project config. This is only valid when shareType's value is SPECIFIC_PROJECTS.
         * Structure is documented below.
         */
        projectMaps?: outputs.compute.NodeGroupShareSettingsProjectMap[];
        /**
         * Node group sharing type.
         * Possible values are: `ORGANIZATION`, `SPECIFIC_PROJECTS`, `LOCAL`.
         */
        shareType: string;
    }

    export interface NodeGroupShareSettingsProjectMap {
        /**
         * The identifier for this object. Format specified above.
         */
        id: string;
        /**
         * The project id/number should be the same as the key of this project config in the project map.
         */
        projectId: string;
    }

    export interface NodeTemplateNodeTypeFlexibility {
        /**
         * Number of virtual CPUs to use.
         */
        cpus?: string;
        /**
         * (Output)
         * Use local SSD
         */
        localSsd: string;
        /**
         * Physical memory available to the node, defined in MB.
         */
        memory?: string;
    }

    export interface NodeTemplateServerBinding {
        /**
         * Type of server binding policy. If `RESTART_NODE_ON_ANY_SERVER`,
         * nodes using this template will restart on any physical server
         * following a maintenance event.
         * If `RESTART_NODE_ON_MINIMAL_SERVER`, nodes using this template
         * will restart on the same physical server following a maintenance
         * event, instead of being live migrated to or restarted on a new
         * physical server. This option may be useful if you are using
         * software licenses tied to the underlying server characteristics
         * such as physical sockets or cores, to avoid the need for
         * additional licenses when maintenance occurs. However, VMs on such
         * nodes will experience outages while maintenance is applied.
         * Possible values are: `RESTART_NODE_ON_ANY_SERVER`, `RESTART_NODE_ON_MINIMAL_SERVERS`.
         */
        type: string;
    }

    export interface OrganizationSecurityPolicyRuleMatch {
        /**
         * The configuration options for matching the rule.
         * Structure is documented below.
         */
        config: outputs.compute.OrganizationSecurityPolicyRuleMatchConfig;
        /**
         * A description of the rule.
         */
        description?: string;
        /**
         * Preconfigured versioned expression. For organization security policy rules,
         * the only supported type is "FIREWALL".
         * Default value is `FIREWALL`.
         * Possible values are: `FIREWALL`.
         */
        versionedExpr?: string;
    }

    export interface OrganizationSecurityPolicyRuleMatchConfig {
        /**
         * Destination IP address range in CIDR format. Required for
         * EGRESS rules.
         */
        destIpRanges?: string[];
        /**
         * Pairs of IP protocols and ports that the rule should match.
         * Structure is documented below.
         *
         *
         * <a name="nestedLayer4Config"></a>The `layer4Config` block supports:
         */
        layer4Configs: outputs.compute.OrganizationSecurityPolicyRuleMatchConfigLayer4Config[];
        /**
         * Source IP address range in CIDR format. Required for
         * INGRESS rules.
         */
        srcIpRanges?: string[];
    }

    export interface OrganizationSecurityPolicyRuleMatchConfigLayer4Config {
        /**
         * The IP protocol to which this rule applies. The protocol
         * type is required when creating a firewall rule.
         * This value can either be one of the following well
         * known protocol strings (tcp, udp, icmp, esp, ah, ipip, sctp),
         * or the IP protocol number.
         */
        ipProtocol: string;
        /**
         * An optional list of ports to which this rule applies. This field
         * is only applicable for UDP or TCP protocol. Each entry must be
         * either an integer or a range. If not specified, this rule
         * applies to connections through any port.
         * Example inputs include: ["22"], ["80","443"], and
         * ["12345-12349"].
         *
         * - - -
         */
        ports?: string[];
    }

    export interface PacketMirroringCollectorIlb {
        /**
         * The URL of the forwarding rule.
         */
        url: string;
    }

    export interface PacketMirroringFilter {
        /**
         * IP CIDR ranges that apply as a filter on the source (ingress) or
         * destination (egress) IP in the IP header. Only IPv4 is supported.
         */
        cidrRanges?: string[];
        /**
         * Direction of traffic to mirror.
         * Default value is `BOTH`.
         * Possible values are: `INGRESS`, `EGRESS`, `BOTH`.
         */
        direction?: string;
        /**
         * Possible IP protocols including tcp, udp, icmp and esp
         */
        ipProtocols?: string[];
    }

    export interface PacketMirroringMirroredResources {
        /**
         * All the listed instances will be mirrored.  Specify at most 50.
         * Structure is documented below.
         */
        instances?: outputs.compute.PacketMirroringMirroredResourcesInstance[];
        /**
         * All instances in one of these subnetworks will be mirrored.
         * Structure is documented below.
         */
        subnetworks?: outputs.compute.PacketMirroringMirroredResourcesSubnetwork[];
        /**
         * All instances with these tags will be mirrored.
         */
        tags?: string[];
    }

    export interface PacketMirroringMirroredResourcesInstance {
        /**
         * The URL of the instances where this rule should be active.
         *
         * - - -
         */
        url: string;
    }

    export interface PacketMirroringMirroredResourcesSubnetwork {
        /**
         * The URL of the subnetwork where this rule should be active.
         */
        url: string;
    }

    export interface PacketMirroringNetwork {
        /**
         * The full selfLink URL of the network where this rule is active.
         */
        url: string;
    }

    export interface PerInstanceConfigPreservedState {
        /**
         * Stateful disks for the instance.
         * Structure is documented below.
         */
        disks?: outputs.compute.PerInstanceConfigPreservedStateDisk[];
        externalIps?: outputs.compute.PerInstanceConfigPreservedStateExternalIp[];
        internalIps?: outputs.compute.PerInstanceConfigPreservedStateInternalIp[];
        /**
         * Preserved metadata defined for this instance. This is a list of key->value pairs.
         */
        metadata?: {[key: string]: string};
    }

    export interface PerInstanceConfigPreservedStateDisk {
        /**
         * A value that prescribes what should happen to the stateful disk when the VM instance is deleted.
         * The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`.
         * `NEVER` - detach the disk when the VM is deleted, but do not delete the disk.
         * `ON_PERMANENT_INSTANCE_DELETION` will delete the stateful disk when the VM is permanently
         * deleted from the instance group.
         * Default value is `NEVER`.
         * Possible values are: `NEVER`, `ON_PERMANENT_INSTANCE_DELETION`.
         */
        deleteRule?: string;
        /**
         * A unique device name that is reflected into the /dev/ tree of a Linux operating system running within the instance.
         */
        deviceName: string;
        /**
         * The mode of the disk.
         * Default value is `READ_WRITE`.
         * Possible values are: `READ_ONLY`, `READ_WRITE`.
         */
        mode?: string;
        /**
         * The URI of an existing persistent disk to attach under the specified device-name in the format
         * `projects/project-id/zones/zone/disks/disk-name`.
         */
        source: string;
    }

    export interface PerInstanceConfigPreservedStateExternalIp {
        /**
         * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted.
         * Default value is `NEVER`.
         * Possible values are: `NEVER`, `ON_PERMANENT_INSTANCE_DELETION`.
         */
        autoDelete?: string;
        /**
         * The identifier for this object. Format specified above.
         */
        interfaceName: string;
        /**
         * Ip address representation
         * Structure is documented below.
         */
        ipAddress?: outputs.compute.PerInstanceConfigPreservedStateExternalIpIpAddress;
    }

    export interface PerInstanceConfigPreservedStateExternalIpIpAddress {
        /**
         * The URL of the reservation for this IP address.
         */
        address?: string;
    }

    export interface PerInstanceConfigPreservedStateInternalIp {
        /**
         * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted.
         * Default value is `NEVER`.
         * Possible values are: `NEVER`, `ON_PERMANENT_INSTANCE_DELETION`.
         */
        autoDelete?: string;
        /**
         * The identifier for this object. Format specified above.
         */
        interfaceName: string;
        /**
         * Ip address representation
         * Structure is documented below.
         */
        ipAddress?: outputs.compute.PerInstanceConfigPreservedStateInternalIpIpAddress;
    }

    export interface PerInstanceConfigPreservedStateInternalIpIpAddress {
        /**
         * The URL of the reservation for this IP address.
         */
        address?: string;
    }

    export interface RegionAutoscalerAutoscalingPolicy {
        /**
         * The number of seconds that the autoscaler should wait before it
         * starts collecting information from a new instance. This prevents
         * the autoscaler from collecting information when the instance is
         * initializing, during which the collected usage would not be
         * reliable. The default time autoscaler waits is 60 seconds.
         * Virtual machine initialization times might vary because of
         * numerous factors. We recommend that you test how long an
         * instance may take to initialize. To do this, create an instance
         * and time the startup process.
         */
        cooldownPeriod?: number;
        /**
         * Defines the CPU utilization policy that allows the autoscaler to
         * scale based on the average CPU utilization of a managed instance
         * group.
         * Structure is documented below.
         */
        cpuUtilization: outputs.compute.RegionAutoscalerAutoscalingPolicyCpuUtilization;
        /**
         * Configuration parameters of autoscaling based on a load balancer.
         * Structure is documented below.
         */
        loadBalancingUtilization?: outputs.compute.RegionAutoscalerAutoscalingPolicyLoadBalancingUtilization;
        /**
         * The maximum number of instances that the autoscaler can scale up
         * to. This is required when creating or updating an autoscaler. The
         * maximum number of replicas should not be lower than minimal number
         * of replicas.
         */
        maxReplicas: number;
        /**
         * Configuration parameters of autoscaling based on a custom metric.
         * Structure is documented below.
         */
        metrics?: outputs.compute.RegionAutoscalerAutoscalingPolicyMetric[];
        /**
         * The minimum number of replicas that the autoscaler can scale down
         * to. This cannot be less than 0. If not provided, autoscaler will
         * choose a default value depending on maximum number of instances
         * allowed.
         */
        minReplicas: number;
        /**
         * Defines operating mode for this policy.
         */
        mode?: string;
        /**
         * Defines scale down controls to reduce the risk of response latency
         * and outages due to abrupt scale-in events
         * Structure is documented below.
         */
        scaleDownControl?: outputs.compute.RegionAutoscalerAutoscalingPolicyScaleDownControl;
        /**
         * Defines scale in controls to reduce the risk of response latency
         * and outages due to abrupt scale-in events
         * Structure is documented below.
         */
        scaleInControl?: outputs.compute.RegionAutoscalerAutoscalingPolicyScaleInControl;
        /**
         * Scaling schedules defined for an autoscaler. Multiple schedules can be set on an autoscaler and they can overlap.
         * Structure is documented below.
         */
        scalingSchedules?: outputs.compute.RegionAutoscalerAutoscalingPolicyScalingSchedule[];
    }

    export interface RegionAutoscalerAutoscalingPolicyCpuUtilization {
        /**
         * Indicates whether predictive autoscaling based on CPU metric is enabled. Valid values are:
         * - NONE (default). No predictive method is used. The autoscaler scales the group to meet current demand based on real-time metrics.
         * - OPTIMIZE_AVAILABILITY. Predictive autoscaling improves availability by monitoring daily and weekly load patterns and scaling out ahead of anticipated demand.
         */
        predictiveMethod?: string;
        /**
         * The target CPU utilization that the autoscaler should maintain.
         * Must be a float value in the range (0, 1]. If not specified, the
         * default is 0.6.
         * If the CPU level is below the target utilization, the autoscaler
         * scales down the number of instances until it reaches the minimum
         * number of instances you specified or until the average CPU of
         * your instances reaches the target utilization.
         * If the average CPU is above the target utilization, the autoscaler
         * scales up until it reaches the maximum number of instances you
         * specified or until the average utilization reaches the target
         * utilization.
         */
        target: number;
    }

    export interface RegionAutoscalerAutoscalingPolicyLoadBalancingUtilization {
        /**
         * Fraction of backend capacity utilization (set in HTTP(s) load
         * balancing configuration) that autoscaler should maintain. Must
         * be a positive float value. If not defined, the default is 0.8.
         */
        target: number;
    }

    export interface RegionAutoscalerAutoscalingPolicyMetric {
        /**
         * A filter string to be used as the filter string for
         * a Stackdriver Monitoring TimeSeries.list API call.
         * This filter is used to select a specific TimeSeries for
         * the purpose of autoscaling and to determine whether the metric
         * is exporting per-instance or per-group data.
         * You can only use the AND operator for joining selectors.
         * You can only use direct equality comparison operator (=) without
         * any functions for each selector.
         * You can specify the metric in both the filter string and in the
         * metric field. However, if specified in both places, the metric must
         * be identical.
         * The monitored resource type determines what kind of values are
         * expected for the metric. If it is a gce_instance, the autoscaler
         * expects the metric to include a separate TimeSeries for each
         * instance in a group. In such a case, you cannot filter on resource
         * labels.
         * If the resource type is any other value, the autoscaler expects
         * this metric to contain values that apply to the entire autoscaled
         * instance group and resource label filtering can be performed to
         * point autoscaler at the correct TimeSeries to scale upon.
         * This is called a per-group metric for the purpose of autoscaling.
         * If not specified, the type defaults to gce_instance.
         * You should provide a filter that is selective enough to pick just
         * one TimeSeries for the autoscaled group or for each of the instances
         * (if you are using gceInstance resource type). If multiple
         * TimeSeries are returned upon the query execution, the autoscaler
         * will sum their respective values to obtain its scaling value.
         */
        filter?: string;
        /**
         * The identifier (type) of the Stackdriver Monitoring metric.
         * The metric cannot have negative values.
         * The metric must have a value type of INT64 or DOUBLE.
         */
        name: string;
        /**
         * If scaling is based on a per-group metric value that represents the
         * total amount of work to be done or resource usage, set this value to
         * an amount assigned for a single instance of the scaled group.
         * The autoscaler will keep the number of instances proportional to the
         * value of this metric, the metric itself should not change value due
         * to group resizing.
         * For example, a good metric to use with the target is
         * `pubsub.googleapis.com/subscription/num_undelivered_messages`
         * or a custom metric exporting the total number of requests coming to
         * your instances.
         * A bad example would be a metric exporting an average or median
         * latency, since this value can't include a chunk assignable to a
         * single instance, it could be better used with utilizationTarget
         * instead.
         */
        singleInstanceAssignment?: number;
        /**
         * The target value of the metric that autoscaler should
         * maintain. This must be a positive value. A utilization
         * metric scales number of virtual machines handling requests
         * to increase or decrease proportionally to the metric.
         * For example, a good metric to use as a utilizationTarget is
         * www.googleapis.com/compute/instance/network/received_bytes_count.
         * The autoscaler will work to keep this value constant for each
         * of the instances.
         */
        target?: number;
        /**
         * Defines how target utilization value is expressed for a
         * Stackdriver Monitoring metric.
         * Possible values are: `GAUGE`, `DELTA_PER_SECOND`, `DELTA_PER_MINUTE`.
         */
        type?: string;
    }

    export interface RegionAutoscalerAutoscalingPolicyScaleDownControl {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        maxScaledDownReplicas?: outputs.compute.RegionAutoscalerAutoscalingPolicyScaleDownControlMaxScaledDownReplicas;
        /**
         * How long back autoscaling should look when computing recommendations
         * to include directives regarding slower scale down, as described above.
         */
        timeWindowSec?: number;
    }

    export interface RegionAutoscalerAutoscalingPolicyScaleDownControlMaxScaledDownReplicas {
        /**
         * Specifies a fixed number of VM instances. This must be a positive
         * integer.
         */
        fixed?: number;
        /**
         * Specifies a percentage of instances between 0 to 100%, inclusive.
         * For example, specify 80 for 80%.
         */
        percent?: number;
    }

    export interface RegionAutoscalerAutoscalingPolicyScaleInControl {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        maxScaledInReplicas?: outputs.compute.RegionAutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas;
        /**
         * How long back autoscaling should look when computing recommendations
         * to include directives regarding slower scale down, as described above.
         */
        timeWindowSec?: number;
    }

    export interface RegionAutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas {
        /**
         * Specifies a fixed number of VM instances. This must be a positive
         * integer.
         */
        fixed?: number;
        /**
         * Specifies a percentage of instances between 0 to 100%, inclusive.
         * For example, specify 80 for 80%.
         */
        percent?: number;
    }

    export interface RegionAutoscalerAutoscalingPolicyScalingSchedule {
        /**
         * An optional description of this resource.
         */
        description?: string;
        /**
         * A boolean value that specifies if a scaling schedule can influence autoscaler recommendations. If set to true, then a scaling schedule has no effect.
         */
        disabled?: boolean;
        /**
         * The duration of time intervals (in seconds) for which this scaling schedule will be running. The minimum allowed value is 300.
         */
        durationSec: number;
        /**
         * Minimum number of VM instances that autoscaler will recommend in time intervals starting according to schedule.
         */
        minRequiredReplicas: number;
        /**
         * The identifier for this object. Format specified above.
         */
        name: string;
        /**
         * The start timestamps of time intervals when this scaling schedule should provide a scaling signal. This field uses the extended cron format (with an optional year field).
         */
        schedule: string;
        /**
         * The time zone to be used when interpreting the schedule. The value of this field must be a time zone name from the tz database: http://en.wikipedia.org/wiki/Tz_database.
         */
        timeZone?: string;
    }

    export interface RegionBackendServiceBackend {
        /**
         * Specifies the balancing mode for this backend.
         * See the [Backend Services Overview](https://cloud.google.com/load-balancing/docs/backend-service#balancing-mode)
         * for an explanation of load balancing modes.
         * Default value is `CONNECTION`.
         * Possible values are: `UTILIZATION`, `RATE`, `CONNECTION`.
         */
        balancingMode?: string;
        /**
         * A multiplier applied to the group's maximum servicing capacity
         * (based on UTILIZATION, RATE or CONNECTION).
         * ~>**NOTE**: This field cannot be set for
         * INTERNAL region backend services (default loadBalancingScheme),
         * but is required for non-INTERNAL backend service. The total
         * capacityScaler for all backends must be non-zero.
         * A setting of 0 means the group is completely drained, offering
         * 0% of its available Capacity. Valid range is [0.0,1.0].
         */
        capacityScaler?: number;
        /**
         * An optional description of this resource.
         * Provide this property when you create the resource.
         */
        description?: string;
        /**
         * This field designates whether this is a failover backend. More
         * than one failover backend can be configured for a given RegionBackendService.
         */
        failover: boolean;
        /**
         * The fully-qualified URL of an Instance Group or Network Endpoint
         * Group resource. In case of instance group this defines the list
         * of instances that serve traffic. Member virtual machine
         * instances from each instance group must live in the same zone as
         * the instance group itself. No two backends in a backend service
         * are allowed to use same Instance Group resource.
         * For Network Endpoint Groups this defines list of endpoints. All
         * endpoints of Network Endpoint Group must be hosted on instances
         * located in the same zone as the Network Endpoint Group.
         * Backend services cannot mix Instance Group and
         * Network Endpoint Group backends.
         * When the `loadBalancingScheme` is INTERNAL, only instance groups
         * are supported.
         * Note that you must specify an Instance Group or Network Endpoint
         * Group resource using the fully-qualified URL, rather than a
         * partial URL.
         */
        group: string;
        /**
         * The max number of simultaneous connections for the group. Can
         * be used with either CONNECTION or UTILIZATION balancing modes.
         * Cannot be set for INTERNAL backend services.
         * For CONNECTION mode, either maxConnections or one
         * of maxConnectionsPerInstance or maxConnectionsPerEndpoint,
         * as appropriate for group type, must be set.
         */
        maxConnections?: number;
        /**
         * The max number of simultaneous connections that a single backend
         * network endpoint can handle. Cannot be set
         * for INTERNAL backend services.
         * This is used to calculate the capacity of the group. Can be
         * used in either CONNECTION or UTILIZATION balancing modes. For
         * CONNECTION mode, either maxConnections or
         * maxConnectionsPerEndpoint must be set.
         */
        maxConnectionsPerEndpoint?: number;
        /**
         * The max number of simultaneous connections that a single
         * backend instance can handle. Cannot be set for INTERNAL backend
         * services.
         * This is used to calculate the capacity of the group.
         * Can be used in either CONNECTION or UTILIZATION balancing modes.
         * For CONNECTION mode, either maxConnections or
         * maxConnectionsPerInstance must be set.
         */
        maxConnectionsPerInstance?: number;
        /**
         * The max requests per second (RPS) of the group. Cannot be set
         * for INTERNAL backend services.
         * Can be used with either RATE or UTILIZATION balancing modes,
         * but required if RATE mode. Either maxRate or one
         * of maxRatePerInstance or maxRatePerEndpoint, as appropriate for
         * group type, must be set.
         */
        maxRate?: number;
        /**
         * The max requests per second (RPS) that a single backend network
         * endpoint can handle. This is used to calculate the capacity of
         * the group. Can be used in either balancing mode. For RATE mode,
         * either maxRate or maxRatePerEndpoint must be set. Cannot be set
         * for INTERNAL backend services.
         */
        maxRatePerEndpoint?: number;
        /**
         * The max requests per second (RPS) that a single backend
         * instance can handle. This is used to calculate the capacity of
         * the group. Can be used in either balancing mode. For RATE mode,
         * either maxRate or maxRatePerInstance must be set. Cannot be set
         * for INTERNAL backend services.
         */
        maxRatePerInstance?: number;
        /**
         * Used when balancingMode is UTILIZATION. This ratio defines the
         * CPU utilization target for the group. Valid range is [0.0, 1.0].
         * Cannot be set for INTERNAL backend services.
         */
        maxUtilization?: number;
    }

    export interface RegionBackendServiceCdnPolicy {
        /**
         * The CacheKeyPolicy for this CdnPolicy.
         * Structure is documented below.
         */
        cacheKeyPolicy?: outputs.compute.RegionBackendServiceCdnPolicyCacheKeyPolicy;
        /**
         * Specifies the cache setting for all responses from this backend.
         * The possible values are: USE_ORIGIN_HEADERS, FORCE_CACHE_ALL and CACHE_ALL_STATIC
         * Possible values are: `USE_ORIGIN_HEADERS`, `FORCE_CACHE_ALL`, `CACHE_ALL_STATIC`.
         */
        cacheMode: string;
        /**
         * Specifies the maximum allowed TTL for cached content served by this origin.
         */
        clientTtl: number;
        /**
         * Specifies the default TTL for cached content served by this origin for responses
         * that do not have an existing valid TTL (max-age or s-max-age).
         */
        defaultTtl: number;
        /**
         * Specifies the maximum allowed TTL for cached content served by this origin.
         */
        maxTtl: number;
        /**
         * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects.
         */
        negativeCaching: boolean;
        /**
         * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
         * Omitting the policy and leaving negativeCaching enabled will use Cloud CDN's default cache TTLs.
         * Structure is documented below.
         */
        negativeCachingPolicies?: outputs.compute.RegionBackendServiceCdnPolicyNegativeCachingPolicy[];
        /**
         * Serve existing content from the cache (if available) when revalidating content with the origin, or when an error is encountered when refreshing the cache.
         */
        serveWhileStale: number;
        /**
         * Maximum number of seconds the response to a signed URL request
         * will be considered fresh, defaults to 1hr (3600s). After this
         * time period, the response will be revalidated before
         * being served.
         * When serving responses to signed URL requests, Cloud CDN will
         * internally behave as though all responses from this backend had a
         * "Cache-Control: public, max-age=[TTL]" header, regardless of any
         * existing Cache-Control header. The actual headers served in
         * responses will not be altered.
         */
        signedUrlCacheMaxAgeSec?: number;
    }

    export interface RegionBackendServiceCdnPolicyCacheKeyPolicy {
        /**
         * If true requests to different hosts will be cached separately.
         */
        includeHost?: boolean;
        /**
         * Names of cookies to include in cache keys.
         */
        includeNamedCookies?: string[];
        /**
         * If true, http and https requests will be cached separately.
         */
        includeProtocol?: boolean;
        /**
         * If true, include query string parameters in the cache key
         * according to queryStringWhitelist and
         * query_string_blacklist. If neither is set, the entire query
         * string will be included.
         * If false, the query string will be excluded from the cache
         * key entirely.
         */
        includeQueryString?: boolean;
        /**
         * Names of query string parameters to exclude in cache keys.
         * All other parameters will be included. Either specify
         * queryStringWhitelist or query_string_blacklist, not both.
         * '&' and '=' will be percent encoded and not treated as
         * delimiters.
         */
        queryStringBlacklists?: string[];
        /**
         * Names of query string parameters to include in cache keys.
         * All other parameters will be excluded. Either specify
         * queryStringWhitelist or query_string_blacklist, not both.
         * '&' and '=' will be percent encoded and not treated as
         * delimiters.
         */
        queryStringWhitelists?: string[];
    }

    export interface RegionBackendServiceCdnPolicyNegativeCachingPolicy {
        /**
         * The HTTP status code to define a TTL against. Only HTTP status codes 300, 301, 308, 404, 405, 410, 421, 451 and 501
         * can be specified as values, and you cannot specify a status code more than once.
         */
        code?: number;
        /**
         * The TTL (in seconds) for which to cache responses with the corresponding status code. The maximum allowed value is 1800s
         * (30 minutes), noting that infrequently accessed objects may be evicted from the cache before the defined TTL.
         */
        ttl?: number;
    }

    export interface RegionBackendServiceCircuitBreakers {
        /**
         * The timeout for new network connections to hosts.
         * Structure is documented below.
         */
        connectTimeout?: outputs.compute.RegionBackendServiceCircuitBreakersConnectTimeout;
        /**
         * The maximum number of connections to the backend cluster.
         * Defaults to 1024.
         */
        maxConnections?: number;
        /**
         * The maximum number of pending requests to the backend cluster.
         * Defaults to 1024.
         */
        maxPendingRequests?: number;
        /**
         * The maximum number of parallel requests to the backend cluster.
         * Defaults to 1024.
         */
        maxRequests?: number;
        /**
         * Maximum requests for a single backend connection. This parameter
         * is respected by both the HTTP/1.1 and HTTP/2 implementations. If
         * not specified, there is no limit. Setting this parameter to 1
         * will effectively disable keep alive.
         */
        maxRequestsPerConnection?: number;
        /**
         * The maximum number of parallel retries to the backend cluster.
         * Defaults to 3.
         */
        maxRetries?: number;
    }

    export interface RegionBackendServiceCircuitBreakersConnectTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must
         * be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second.
         * Must be from 0 to 315,576,000,000 inclusive.
         */
        seconds: number;
    }

    export interface RegionBackendServiceConnectionTrackingPolicy {
        /**
         * Specifies connection persistence when backends are unhealthy.
         * If set to `DEFAULT_FOR_PROTOCOL`, the existing connections persist on
         * unhealthy backends only for connection-oriented protocols (TCP and SCTP)
         * and only if the Tracking Mode is PER_CONNECTION (default tracking mode)
         * or the Session Affinity is configured for 5-tuple. They do not persist
         * for UDP.
         * If set to `NEVER_PERSIST`, after a backend becomes unhealthy, the existing
         * connections on the unhealthy backend are never persisted on the unhealthy
         * backend. They are always diverted to newly selected healthy backends
         * (unless all backends are unhealthy).
         * If set to `ALWAYS_PERSIST`, existing connections always persist on
         * unhealthy backends regardless of protocol and session affinity. It is
         * generally not recommended to use this mode overriding the default.
         * Default value is `DEFAULT_FOR_PROTOCOL`.
         * Possible values are: `DEFAULT_FOR_PROTOCOL`, `NEVER_PERSIST`, `ALWAYS_PERSIST`.
         */
        connectionPersistenceOnUnhealthyBackends?: string;
        /**
         * Enable Strong Session Affinity for Network Load Balancing. This option is not available publicly.
         */
        enableStrongAffinity?: boolean;
        /**
         * Specifies how long to keep a Connection Tracking entry while there is
         * no matching traffic (in seconds).
         * For L4 ILB the minimum(default) is 10 minutes and maximum is 16 hours.
         * For NLB the minimum(default) is 60 seconds and the maximum is 16 hours.
         */
        idleTimeoutSec: number;
        /**
         * Specifies the key used for connection tracking. There are two options:
         * `PER_CONNECTION`: The Connection Tracking is performed as per the
         * Connection Key (default Hash Method) for the specific protocol.
         * `PER_SESSION`: The Connection Tracking is performed as per the
         * configured Session Affinity. It matches the configured Session Affinity.
         * Default value is `PER_CONNECTION`.
         * Possible values are: `PER_CONNECTION`, `PER_SESSION`.
         */
        trackingMode?: string;
    }

    export interface RegionBackendServiceConsistentHash {
        /**
         * Hash is based on HTTP Cookie. This field describes a HTTP cookie
         * that will be used as the hash key for the consistent hash load
         * balancer. If the cookie is not present, it will be generated.
         * This field is applicable if the sessionAffinity is set to HTTP_COOKIE.
         * Structure is documented below.
         */
        httpCookie?: outputs.compute.RegionBackendServiceConsistentHashHttpCookie;
        /**
         * The hash based on the value of the specified header field.
         * This field is applicable if the sessionAffinity is set to HEADER_FIELD.
         */
        httpHeaderName?: string;
        /**
         * The minimum number of virtual nodes to use for the hash ring.
         * Larger ring sizes result in more granular load
         * distributions. If the number of hosts in the load balancing pool
         * is larger than the ring size, each host will be assigned a single
         * virtual node.
         * Defaults to 1024.
         */
        minimumRingSize?: number;
    }

    export interface RegionBackendServiceConsistentHashHttpCookie {
        /**
         * Name of the cookie.
         */
        name?: string;
        /**
         * Path to set for the cookie.
         */
        path?: string;
        /**
         * Lifetime of the cookie.
         * Structure is documented below.
         */
        ttl?: outputs.compute.RegionBackendServiceConsistentHashHttpCookieTtl;
    }

    export interface RegionBackendServiceConsistentHashHttpCookieTtl {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must
         * be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second.
         * Must be from 0 to 315,576,000,000 inclusive.
         */
        seconds: number;
    }

    export interface RegionBackendServiceFailoverPolicy {
        /**
         * On failover or failback, this field indicates whether connection drain
         * will be honored. Setting this to true has the following effect: connections
         * to the old active pool are not drained. Connections to the new active pool
         * use the timeout of 10 min (currently fixed). Setting to false has the
         * following effect: both old and new connections will have a drain timeout
         * of 10 min.
         * This can be set to true only if the protocol is TCP.
         * The default is false.
         */
        disableConnectionDrainOnFailover: boolean;
        /**
         * This option is used only when no healthy VMs are detected in the primary
         * and backup instance groups. When set to true, traffic is dropped. When
         * set to false, new connections are sent across all VMs in the primary group.
         * The default is false.
         */
        dropTrafficIfUnhealthy: boolean;
        /**
         * The value of the field must be in [0, 1]. If the ratio of the healthy
         * VMs in the primary backend is at or below this number, traffic arriving
         * at the load-balanced IP will be directed to the failover backend.
         * In case where 'failoverRatio' is not set or all the VMs in the backup
         * backend are unhealthy, the traffic will be directed back to the primary
         * backend in the "force" mode, where traffic will be spread to the healthy
         * VMs with the best effort, or to all VMs when no VM is healthy.
         * This field is only used with l4 load balancing.
         */
        failoverRatio?: number;
    }

    export interface RegionBackendServiceIamBindingCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface RegionBackendServiceIamMemberCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface RegionBackendServiceIap {
        /**
         * OAuth2 Client ID for IAP
         */
        oauth2ClientId: string;
        /**
         * OAuth2 Client Secret for IAP
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        oauth2ClientSecret: string;
        /**
         * (Output)
         * OAuth2 Client Secret SHA-256 for IAP
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        oauth2ClientSecretSha256: string;
    }

    export interface RegionBackendServiceLogConfig {
        /**
         * Whether to enable logging for the load balancer traffic served by this backend service.
         */
        enable?: boolean;
        /**
         * This field can only be specified if logging is enabled for this backend service. The value of
         * the field must be in [0, 1]. This configures the sampling rate of requests to the load balancer
         * where 1.0 means all logged requests are reported and 0.0 means no logged requests are reported.
         * The default value is 1.0.
         */
        sampleRate?: number;
    }

    export interface RegionBackendServiceOutlierDetection {
        /**
         * The base time that a host is ejected for. The real time is equal to the base
         * time multiplied by the number of times the host has been ejected. Defaults to
         * 30000ms or 30s.
         * Structure is documented below.
         */
        baseEjectionTime?: outputs.compute.RegionBackendServiceOutlierDetectionBaseEjectionTime;
        /**
         * Number of errors before a host is ejected from the connection pool. When the
         * backend host is accessed over HTTP, a 5xx return code qualifies as an error.
         * Defaults to 5.
         */
        consecutiveErrors?: number;
        /**
         * The number of consecutive gateway failures (502, 503, 504 status or connection
         * errors that are mapped to one of those status codes) before a consecutive
         * gateway failure ejection occurs. Defaults to 5.
         */
        consecutiveGatewayFailure?: number;
        /**
         * The percentage chance that a host will be actually ejected when an outlier
         * status is detected through consecutive 5xx. This setting can be used to disable
         * ejection or to ramp it up slowly. Defaults to 100.
         */
        enforcingConsecutiveErrors?: number;
        /**
         * The percentage chance that a host will be actually ejected when an outlier
         * status is detected through consecutive gateway failures. This setting can be
         * used to disable ejection or to ramp it up slowly. Defaults to 0.
         */
        enforcingConsecutiveGatewayFailure?: number;
        /**
         * The percentage chance that a host will be actually ejected when an outlier
         * status is detected through success rate statistics. This setting can be used to
         * disable ejection or to ramp it up slowly. Defaults to 100.
         */
        enforcingSuccessRate?: number;
        /**
         * Time interval between ejection sweep analysis. This can result in both new
         * ejections as well as hosts being returned to service. Defaults to 10 seconds.
         * Structure is documented below.
         */
        interval?: outputs.compute.RegionBackendServiceOutlierDetectionInterval;
        /**
         * Maximum percentage of hosts in the load balancing pool for the backend service
         * that can be ejected. Defaults to 10%.
         */
        maxEjectionPercent?: number;
        /**
         * The number of hosts in a cluster that must have enough request volume to detect
         * success rate outliers. If the number of hosts is less than this setting, outlier
         * detection via success rate statistics is not performed for any host in the
         * cluster. Defaults to 5.
         */
        successRateMinimumHosts?: number;
        /**
         * The minimum number of total requests that must be collected in one interval (as
         * defined by the interval duration above) to include this host in success rate
         * based outlier detection. If the volume is lower than this setting, outlier
         * detection via success rate statistics is not performed for that host. Defaults
         * to 100.
         */
        successRateRequestVolume?: number;
        /**
         * This factor is used to determine the ejection threshold for success rate outlier
         * ejection. The ejection threshold is the difference between the mean success
         * rate, and the product of this factor and the standard deviation of the mean
         * success rate: mean - (stdev * success_rate_stdev_factor). This factor is divided
         * by a thousand to get a double. That is, if the desired factor is 1.9, the
         * runtime value should be 1900. Defaults to 1900.
         */
        successRateStdevFactor?: number;
    }

    export interface RegionBackendServiceOutlierDetectionBaseEjectionTime {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations
         * less than one second are represented with a 0 `seconds` field and a positive
         * `nanos` field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
         * inclusive.
         */
        seconds: number;
    }

    export interface RegionBackendServiceOutlierDetectionInterval {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations
         * less than one second are represented with a 0 `seconds` field and a positive
         * `nanos` field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
         * inclusive.
         */
        seconds: number;
    }

    export interface RegionBackendServiceSubsetting {
        /**
         * The algorithm used for subsetting.
         * Possible values are: `CONSISTENT_HASH_SUBSETTING`.
         */
        policy: string;
    }

    export interface RegionCommitmentLicenseResource {
        /**
         * The number of licenses purchased.
         */
        amount?: string;
        /**
         * Specifies the core range of the instance for which this license applies.
         */
        coresPerLicense?: string;
        /**
         * Any applicable license URI.
         */
        license: string;
    }

    export interface RegionCommitmentResource {
        /**
         * Name of the accelerator type resource. Applicable only when the type is ACCELERATOR.
         */
        acceleratorType?: string;
        /**
         * The amount of the resource purchased (in a type-dependent unit,
         * such as bytes). For vCPUs, this can just be an integer. For memory,
         * this must be provided in MB. Memory must be a multiple of 256 MB,
         * with up to 6.5GB of memory per every vCPU.
         */
        amount?: string;
        /**
         * Type of resource for which this commitment applies.
         * Possible values are VCPU, MEMORY, LOCAL_SSD, and ACCELERATOR.
         */
        type?: string;
    }

    export interface RegionDiskAsyncPrimaryDisk {
        /**
         * Primary disk for asynchronous disk replication.
         */
        disk: string;
    }

    export interface RegionDiskDiskEncryptionKey {
        /**
         * The name of the encryption key that is stored in Google Cloud KMS.
         */
        kmsKeyName?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        rawKey?: string;
        /**
         * (Output)
         * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
         * encryption key that protects this resource.
         */
        sha256: string;
    }

    export interface RegionDiskGuestOsFeature {
        /**
         * The type of supported feature. Read [Enabling guest operating system features](https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#guest-os-features) to see a list of available options.
         * Possible values are: `MULTI_IP_SUBNET`, `SECURE_BOOT`, `SEV_CAPABLE`, `UEFI_COMPATIBLE`, `VIRTIO_SCSI_MULTIQUEUE`, `WINDOWS`, `GVNIC`, `SEV_LIVE_MIGRATABLE`, `SEV_SNP_CAPABLE`, `SUSPEND_RESUME_COMPATIBLE`, `TDX_CAPABLE`.
         */
        type: string;
    }

    export interface RegionDiskIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RegionDiskIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RegionDiskSourceSnapshotEncryptionKey {
        /**
         * The name of the encryption key that is stored in Google Cloud KMS.
         */
        kmsKeyName?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         */
        rawKey?: string;
        /**
         * (Output)
         * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
         * encryption key that protects this resource.
         */
        sha256: string;
    }

    export interface RegionHealthCheckGrpcHealthCheck {
        /**
         * The gRPC service name for the health check.
         * The value of grpcServiceName has the following meanings by convention:
         * * Empty serviceName means the overall status of all services at the backend.
         * * Non-empty serviceName means the health of that gRPC service, as defined by the owner of the service.
         * The grpcServiceName can only be ASCII.
         */
        grpcServiceName?: string;
        /**
         * The port number for the health check request.
         * Must be specified if portName and portSpecification are not set
         * or if portSpecification is USE_FIXED_PORT. Valid values are 1 through 65535.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
    }

    export interface RegionHealthCheckHttp2HealthCheck {
        /**
         * The value of the host header in the HTTP health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         *
         * (Optional)
         * The value of the host header in the HTTPS health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         *
         * (Optional)
         * The value of the host header in the HTTP2 health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         */
        host?: string;
        /**
         * The TCP port number for the HTTP health check request.
         * The default value is 80.
         *
         * (Optional)
         * The TCP port number for the HTTPS health check request.
         * The default value is 443.
         *
         * (Optional)
         * The TCP port number for the TCP health check request.
         * The default value is 80.
         *
         * (Optional)
         * The TCP port number for the SSL health check request.
         * The default value is 443.
         *
         * (Optional)
         * The TCP port number for the HTTP2 health check request.
         * The default value is 443.
         *
         * (Optional)
         * The port number for the health check request.
         * Must be specified if portName and portSpecification are not set
         * or if portSpecification is USE_FIXED_PORT. Valid values are 1 through 65535.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         *
         * (Optional)
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         * (Optional)
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         *
         * (Optional)
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The request path of the HTTP health check request.
         * The default value is /.
         *
         * (Optional)
         * The request path of the HTTPS health check request.
         * The default value is /.
         *
         * (Optional)
         * The request path of the HTTP2 health check request.
         * The default value is /.
         */
        requestPath?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         *
         * (Optional)
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface RegionHealthCheckHttpHealthCheck {
        /**
         * The value of the host header in the HTTP health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         */
        host?: string;
        /**
         * The TCP port number for the HTTP health check request.
         * The default value is 80.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The request path of the HTTP health check request.
         * The default value is /.
         */
        requestPath?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface RegionHealthCheckHttpsHealthCheck {
        /**
         * The value of the host header in the HTTPS health check request.
         * If left empty (default value), the public IP on behalf of which this health
         * check is performed will be used.
         */
        host?: string;
        /**
         * The TCP port number for the HTTPS health check request.
         * The default value is 443.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The request path of the HTTPS health check request.
         * The default value is /.
         */
        requestPath?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface RegionHealthCheckLogConfig {
        /**
         * Indicates whether or not to export logs. This is false by default,
         * which means no health check logging will be done.
         */
        enable?: boolean;
    }

    export interface RegionHealthCheckSslHealthCheck {
        /**
         * The TCP port number for the HTTP2 health check request.
         * The default value is 443.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The application data to send once the SSL connection has been
         * established (default value is empty). If both request and response are
         * empty, the connection establishment alone will indicate health. The request
         * data can only be ASCII.
         */
        request?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface RegionHealthCheckTcpHealthCheck {
        /**
         * The TCP port number for the TCP health check request.
         * The default value is 80.
         */
        port?: number;
        /**
         * Port name as defined in InstanceGroup#NamedPort#name. If both port and
         * portName are defined, port takes precedence.
         */
        portName?: string;
        /**
         * Specifies how port is selected for health checking, can be one of the
         * following values:
         */
        portSpecification?: string;
        /**
         * Specifies the type of proxy header to append before sending data to the
         * backend.
         * Default value is `NONE`.
         * Possible values are: `NONE`, `PROXY_V1`.
         */
        proxyHeader?: string;
        /**
         * The application data to send once the TCP connection has been
         * established (default value is empty). If both request and response are
         * empty, the connection establishment alone will indicate health. The request
         * data can only be ASCII.
         */
        request?: string;
        /**
         * The bytes to match against the beginning of the response data. If left empty
         * (the default value), any response will indicate health. The response data
         * can only be ASCII.
         */
        response?: string;
    }

    export interface RegionInstanceGroupManagerAllInstancesConfig {
        /**
         * ), The label key-value pairs that you want to patch onto the instance.
         *
         * - - -
         */
        labels?: {[key: string]: string};
        /**
         * ), The metadata key-value pairs that you want to patch onto the instance. For more information, see [Project and instance metadata](https://cloud.google.com/compute/docs/metadata#project_and_instance_metadata).
         */
        metadata?: {[key: string]: string};
    }

    export interface RegionInstanceGroupManagerAutoHealingPolicies {
        /**
         * The health check resource that signals autohealing.
         */
        healthCheck: string;
        /**
         * The number of seconds that the managed instance group waits before
         * it applies autohealing policies to new instances or recently recreated instances. Between 0 and 3600.
         */
        initialDelaySec: number;
    }

    export interface RegionInstanceGroupManagerInstanceLifecyclePolicy {
        /**
         * ), Specifies whether to apply the group's latest configuration when repairing a VM. Valid options are: YES, NO. If YES and you updated the group's instance template or per-instance configurations after the VM was created, then these changes are applied when VM is repaired. If NO (default), then updates are applied in accordance with the group's update policy type.
         * - - -
         */
        forceUpdateOnRepair?: string;
    }

    export interface RegionInstanceGroupManagerNamedPort {
        /**
         * The name of the port.
         */
        name: string;
        /**
         * The port number.
         * - - -
         */
        port: number;
    }

    export interface RegionInstanceGroupManagerStatefulDisk {
        /**
         * , A value that prescribes what should happen to the stateful disk when the VM instance is deleted. The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`. `NEVER` - detach the disk when the VM is deleted, but do not delete the disk. `ON_PERMANENT_INSTANCE_DELETION` will delete the stateful disk when the VM is permanently deleted from the instance group. The default is `NEVER`.
         */
        deleteRule?: string;
        /**
         * , The device name of the disk to be attached.
         */
        deviceName: string;
    }

    export interface RegionInstanceGroupManagerStatefulExternalIp {
        /**
         * , A value that prescribes what should happen to the external ip when the VM instance is deleted. The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`. `NEVER` - detach the ip when the VM is deleted, but do not delete the ip. `ON_PERMANENT_INSTANCE_DELETION` will delete the external ip when the VM is permanently deleted from the instance group.
         */
        deleteRule?: string;
        /**
         * , The network interface name of the external Ip. Possible value: `nic0`.
         */
        interfaceName?: string;
    }

    export interface RegionInstanceGroupManagerStatefulInternalIp {
        /**
         * , A value that prescribes what should happen to the internal ip when the VM instance is deleted. The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`. `NEVER` - detach the ip when the VM is deleted, but do not delete the ip. `ON_PERMANENT_INSTANCE_DELETION` will delete the internal ip when the VM is permanently deleted from the instance group.
         */
        deleteRule?: string;
        /**
         * , The network interface name of the internal Ip. Possible value: `nic0`.
         */
        interfaceName?: string;
    }

    export interface RegionInstanceGroupManagerStatus {
        /**
         * )
         * Properties to set on all instances in the group. After setting
         * allInstancesConfig on the group, you must update the group's instances to
         * apply the configuration.
         */
        allInstancesConfigs: outputs.compute.RegionInstanceGroupManagerStatusAllInstancesConfig[];
        /**
         * A bit indicating whether the managed instance group is in a stable state. A stable state means that: none of the instances in the managed instance group is currently undergoing any type of change (for example, creation, restart, or deletion); no future changes are scheduled for instances in the managed instance group; and the managed instance group itself is not being modified.
         */
        isStable: boolean;
        /**
         * Stateful status of the given Instance Group Manager.
         */
        statefuls: outputs.compute.RegionInstanceGroupManagerStatusStateful[];
        /**
         * A bit indicating whether version target has been reached in this managed instance group, i.e. all instances are in their target version. Instances' target version are specified by version field on Instance Group Manager.
         */
        versionTargets: outputs.compute.RegionInstanceGroupManagerStatusVersionTarget[];
    }

    export interface RegionInstanceGroupManagerStatusAllInstancesConfig {
        effective: boolean;
    }

    export interface RegionInstanceGroupManagerStatusStateful {
        /**
         * A bit indicating whether the managed instance group has stateful configuration, that is, if you have configured any items in a stateful policy or in per-instance configs. The group might report that it has no stateful config even when there is still some preserved state on a managed instance, for example, if you have deleted all PICs but not yet applied those deletions.
         */
        hasStatefulConfig: boolean;
        /**
         * Status of per-instance configs on the instance.
         */
        perInstanceConfigs: outputs.compute.RegionInstanceGroupManagerStatusStatefulPerInstanceConfig[];
    }

    export interface RegionInstanceGroupManagerStatusStatefulPerInstanceConfig {
        /**
         * A bit indicating if all of the group's per-instance configs (listed in the output of a listPerInstanceConfigs API call) have status `EFFECTIVE` or there are no per-instance-configs.
         */
        allEffective: boolean;
    }

    export interface RegionInstanceGroupManagerStatusVersionTarget {
        isReached: boolean;
    }

    export interface RegionInstanceGroupManagerUpdatePolicy {
        /**
         * The instance redistribution policy for regional managed instance groups. Valid values are: `"PROACTIVE"`, `"NONE"`. If `PROACTIVE` (default), the group attempts to maintain an even distribution of VM instances across zones in the region. If `NONE`, proactive redistribution is disabled.
         */
        instanceRedistributionType?: string;
        /**
         * , The maximum number of instances that can be created above the specified targetSize during the update process. Conflicts with `maxSurgePercent`. It has to be either 0 or at least equal to the number of zones.  If fixed values are used, at least one of `maxUnavailableFixed` or `maxSurgeFixed` must be greater than 0.
         */
        maxSurgeFixed: number;
        /**
         * , The maximum number of instances(calculated as percentage) that can be created above the specified targetSize during the update process. Conflicts with `maxSurgeFixed`. Percent value is only allowed for regional managed instance groups with size at least 10.
         */
        maxSurgePercent?: number;
        /**
         * , The maximum number of instances that can be unavailable during the update process. Conflicts with `maxUnavailablePercent`. It has to be either 0 or at least equal to the number of zones. If fixed values are used, at least one of `maxUnavailableFixed` or `maxSurgeFixed` must be greater than 0.
         */
        maxUnavailableFixed: number;
        /**
         * , The maximum number of instances(calculated as percentage) that can be unavailable during the update process. Conflicts with `maxUnavailableFixed`. Percent value is only allowed for regional managed instance groups with size at least 10.
         */
        maxUnavailablePercent?: number;
        /**
         * ), Minimum number of seconds to wait for after a newly created instance becomes available. This value must be from range [0, 3600]
         */
        minReadySec?: number;
        /**
         * Minimal action to be taken on an instance. You can specify either `REFRESH` to update without stopping instances, `RESTART` to restart existing instances or `REPLACE` to delete and create new instances from the target template. If you specify a `REFRESH`, the Updater will attempt to perform that action only. However, if the Updater determines that the minimal action you specify is not enough to perform the update, it might perform a more disruptive action.
         */
        minimalAction: string;
        /**
         * Most disruptive action that is allowed to be taken on an instance. You can specify either NONE to forbid any actions, REFRESH to allow actions that do not need instance restart, RESTART to allow actions that can be applied without instance replacing or REPLACE to allow all possible actions. If the Updater determines that the minimal update action needed is more disruptive than most disruptive allowed action you specify it will not perform the update at all.
         */
        mostDisruptiveAllowedAction?: string;
        /**
         * , The instance replacement method for managed instance groups. Valid values are: "RECREATE", "SUBSTITUTE". If SUBSTITUTE (default), the group replaces VM instances with new instances that have randomly generated names. If RECREATE, instance names are preserved.  You must also set maxUnavailableFixed or maxUnavailablePercent to be greater than 0.
         * - - -
         */
        replacementMethod?: string;
        /**
         * The type of update process. You can specify either `PROACTIVE` so that the instance group manager proactively executes actions in order to bring instances to their target versions or `OPPORTUNISTIC` so that no action is proactively executed but the update will be performed as part of other actions (for example, resizes or recreateInstances calls).
         */
        type: string;
    }

    export interface RegionInstanceGroupManagerVersion {
        /**
         * The full URL to an instance template from which all new instances of this version will be created.
         */
        instanceTemplate: string;
        /**
         * Version name.
         */
        name?: string;
        /**
         * The number of instances calculated as a fixed number or a percentage depending on the settings. Structure is documented below.
         *
         * > Exactly one `version` you specify must not have a `targetSize` specified. During a rolling update, the instance group manager will fulfill the `targetSize`
         * constraints of every other `version`, and any remaining instances will be provisioned with the version where `targetSize` is unset.
         */
        targetSize?: outputs.compute.RegionInstanceGroupManagerVersionTargetSize;
    }

    export interface RegionInstanceGroupManagerVersionTargetSize {
        /**
         * , The number of instances which are managed for this version. Conflicts with `percent`.
         */
        fixed?: number;
        /**
         * , The number of instances (calculated as percentage) which are managed for this version. Conflicts with `fixed`.
         * Note that when using `percent`, rounding will be in favor of explicitly set `targetSize` values; a managed instance group with 2 instances and 2 `version`s,
         * one of which has a `target_size.percent` of `60` will create 2 instances of that `version`.
         */
        percent?: number;
    }

    export interface RegionInstanceTemplateAdvancedMachineFeatures {
        /**
         * Defines whether the instance should have nested virtualization enabled. Defaults to false.
         */
        enableNestedVirtualization?: boolean;
        /**
         * The number of threads per physical core. To disable [simultaneous multithreading (SMT)](https://cloud.google.com/compute/docs/instances/disabling-smt) set this to 1.
         */
        threadsPerCore?: number;
        /**
         * The number of physical cores to expose to an instance. [visible cores info (VC)](https://cloud.google.com/compute/docs/instances/customize-visible-cores).
         */
        visibleCoreCount?: number;
    }

    export interface RegionInstanceTemplateConfidentialInstanceConfig {
        /**
         * Defines whether the instance should have confidential compute enabled. `onHostMaintenance` has to be set to TERMINATE or this will fail to create the VM.
         */
        enableConfidentialCompute: boolean;
    }

    export interface RegionInstanceTemplateDisk {
        /**
         * Whether or not the disk should be auto-deleted.
         * This defaults to true.
         */
        autoDelete?: boolean;
        /**
         * Indicates that this is a boot disk.
         */
        boot: boolean;
        /**
         * A unique device name that is reflected into the
         * /dev/  tree of a Linux operating system running within the instance. If not
         * specified, the server chooses a default device name to apply to this disk.
         */
        deviceName: string;
        /**
         * Encrypts or decrypts a disk using a customer-supplied encryption key.
         *
         * If you are creating a new disk, this field encrypts the new disk using an encryption key that you provide. If you are attaching an existing disk that is already encrypted, this field decrypts the disk using the customer-supplied encryption key.
         *
         * If you encrypt a disk using a customer-supplied key, you must provide the same key again when you attempt to use this resource at a later time. For example, you must provide the key when you create a snapshot or an image from the disk or when you attach the disk to a virtual machine instance.
         *
         * If you do not provide an encryption key, then the disk will be encrypted using an automatically generated key and you do not need to provide a key to use the disk later.
         *
         * Instance templates do not store customer-supplied encryption keys, so you cannot use your own keys to encrypt disks in a managed instance group. Structure documented below.
         */
        diskEncryptionKey?: outputs.compute.RegionInstanceTemplateDiskDiskEncryptionKey;
        /**
         * Name of the disk. When not provided, this defaults
         * to the name of the instance.
         */
        diskName?: string;
        /**
         * The size of the image in gigabytes. If not
         * specified, it will inherit the size of its base image. For SCRATCH disks,
         * the size must be exactly 375GB.
         */
        diskSizeGb: number;
        /**
         * The GCE disk type. Such as `"pd-ssd"`, `"local-ssd"`,
         * `"pd-balanced"` or `"pd-standard"`.
         */
        diskType: string;
        /**
         * Specifies the disk interface to use for attaching this disk,
         * which is either SCSI or NVME. The default is SCSI. Persistent disks must always use SCSI
         * and the request will fail if you attempt to attach a persistent disk in any other format
         * than SCSI. Local SSDs can use either NVME or SCSI.
         */
        interface: string;
        /**
         * A set of ket/value label pairs to assign to disk created from
         * this template
         */
        labels?: {[key: string]: string};
        /**
         * The mode in which to attach this disk, either READ_WRITE
         * or READ_ONLY. If you are attaching or creating a boot disk, this must
         * read-write mode.
         */
        mode: string;
        provisionedIops: number;
        /**
         * - A list (short name or id) of resource policies to attach to this disk for automatic snapshot creations. Currently a max of 1 resource policy is supported.
         */
        resourcePolicies?: string;
        /**
         * The name (**not self_link**)
         * of the disk (such as those managed by `gcp.compute.Disk`) to attach.
         * > **Note:** Either `source`, `sourceImage`, or `sourceSnapshot` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        source?: string;
        /**
         * The image from which to
         * initialize this disk. This can be one of: the image's `selfLink`,
         * `projects/{project}/global/images/{image}`,
         * `projects/{project}/global/images/family/{family}`, `global/images/{image}`,
         * `global/images/family/{family}`, `family/{family}`, `{project}/{family}`,
         * `{project}/{image}`, `{family}`, or `{image}`.
         * > **Note:** Either `source`, `sourceImage`, or `sourceSnapshot` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        sourceImage: string;
        /**
         * The customer-supplied encryption
         * key of the source image. Required if the source image is protected by a
         * customer-supplied encryption key.
         *
         * Instance templates do not store customer-supplied encryption keys, so you
         * cannot create disks for instances in a managed instance group if the source
         * images are encrypted with your own keys. Structure
         * documented below.
         */
        sourceImageEncryptionKey?: outputs.compute.RegionInstanceTemplateDiskSourceImageEncryptionKey;
        /**
         * The source snapshot to create this disk.
         * > **Note:** Either `source`, `sourceImage`, or `sourceSnapshot` is **required** in a disk block unless the disk type is `local-ssd`. Check the API [docs](https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates/insert) for details.
         */
        sourceSnapshot?: string;
        /**
         * The customer-supplied encryption
         * key of the source snapshot. Structure
         * documented below.
         */
        sourceSnapshotEncryptionKey?: outputs.compute.RegionInstanceTemplateDiskSourceSnapshotEncryptionKey;
        /**
         * The type of GCE disk, can be either `"SCRATCH"` or
         * `"PERSISTENT"`.
         */
        type: string;
    }

    export interface RegionInstanceTemplateDiskDiskEncryptionKey {
        /**
         * The self link of the encryption key that is stored in Google Cloud KMS
         */
        kmsKeySelfLink: string;
    }

    export interface RegionInstanceTemplateDiskSourceImageEncryptionKey {
        /**
         * The self link of the encryption key that is
         * stored in Google Cloud KMS.
         */
        kmsKeySelfLink: string;
        /**
         * The service account being used for the
         * encryption request for the given KMS key. If absent, the Compute Engine
         * default service account is used.
         */
        kmsKeyServiceAccount?: string;
    }

    export interface RegionInstanceTemplateDiskSourceSnapshotEncryptionKey {
        /**
         * The self link of the encryption key that is
         * stored in Google Cloud KMS.
         */
        kmsKeySelfLink: string;
        /**
         * The service account being used for the
         * encryption request for the given KMS key. If absent, the Compute Engine
         * default service account is used.
         */
        kmsKeyServiceAccount?: string;
    }

    export interface RegionInstanceTemplateGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * The type of GCE disk, can be either `"SCRATCH"` or
         * `"PERSISTENT"`.
         */
        type: string;
    }

    export interface RegionInstanceTemplateNetworkInterface {
        accessConfigs?: outputs.compute.RegionInstanceTemplateNetworkInterfaceAccessConfig[];
        /**
         * An
         * array of alias IP ranges for this network interface. Can only be specified for network
         * interfaces on subnet-mode networks. Structure documented below.
         */
        aliasIpRanges?: outputs.compute.RegionInstanceTemplateNetworkInterfaceAliasIpRange[];
        internalIpv6PrefixLength: number;
        /**
         * An array of IPv6 access configurations for this interface.
         * Currently, only one IPv6 access config, DIRECT_IPV6, is supported. If there is no ipv6AccessConfig
         * specified, then this instance will have no external IPv6 Internet access. Structure documented below.
         */
        ipv6AccessConfigs?: outputs.compute.RegionInstanceTemplateNetworkInterfaceIpv6AccessConfig[];
        ipv6AccessType: string;
        ipv6Address: string;
        name: string;
        /**
         * The name or selfLink of the network to attach this interface to.
         * Use `network` attribute for Legacy or Auto subnetted networks and
         * `subnetwork` for custom subnetted networks.
         */
        network: string;
        /**
         * The private IP address to assign to the instance. If
         * empty, the address will be automatically assigned.
         */
        networkIp?: string;
        /**
         * The type of vNIC to be used on this interface. Possible values: GVNIC, VIRTIO_NET.
         */
        nicType?: string;
        /**
         * The networking queue count that's specified by users for the network interface. Both Rx and Tx queues will be set to this number. It will be empty if not specified.
         */
        queueCount?: number;
        /**
         * The stack type for this network interface to identify whether the IPv6 feature is enabled or not. Values are IPV4_IPV6 or IPV4_ONLY. If not specified, IPV4_ONLY will be used.
         */
        stackType: string;
        /**
         * the name of the subnetwork to attach this interface
         * to. The subnetwork must exist in the same `region` this instance will be
         * created in. Either `network` or `subnetwork` must be provided.
         */
        subnetwork: string;
        /**
         * The ID of the project in which the subnetwork belongs.
         * If it is not provided, the provider project is used.
         */
        subnetworkProject: string;
    }

    export interface RegionInstanceTemplateNetworkInterfaceAccessConfig {
        /**
         * The IP address that will be 1:1 mapped to the instance's
         * network ip. If not given, one will be generated.
         */
        natIp: string;
        /**
         * The service-level to be provided for IPv6 traffic when the
         * subnet has an external subnet. Only PREMIUM and STANDARD tier is valid for IPv6.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface RegionInstanceTemplateNetworkInterfaceAliasIpRange {
        /**
         * The IP CIDR range represented by this alias IP range. This IP CIDR range
         * must belong to the specified subnetwork and cannot contain IP addresses reserved by
         * system or used by other network interfaces. At the time of writing only a
         * netmask (e.g. /24) may be supplied, with a CIDR format resulting in an API
         * error.
         */
        ipCidrRange: string;
        /**
         * The subnetwork secondary range name specifying
         * the secondary range from which to allocate the IP CIDR range for this alias IP
         * range. If left unspecified, the primary range of the subnetwork will be used.
         */
        subnetworkRangeName?: string;
    }

    export interface RegionInstanceTemplateNetworkInterfaceIpv6AccessConfig {
        externalIpv6: string;
        externalIpv6PrefixLength: string;
        name: string;
        /**
         * The [networking tier][network-tier] used for configuring
         * this instance template. This field can take the following values: PREMIUM,
         * STANDARD or FIXED_STANDARD. If this field is not specified, it is assumed to be PREMIUM.
         *
         * <a name="nestedIpv6AccessConfig"></a>The `ipv6AccessConfig` block supports:
         *
         * subnet has an external subnet. Only PREMIUM and STANDARD tier is valid for IPv6.
         */
        networkTier: string;
        publicPtrDomainName: string;
    }

    export interface RegionInstanceTemplateNetworkPerformanceConfig {
        /**
         * The egress bandwidth tier to enable. Possible values: TIER_1, DEFAULT
         */
        totalEgressBandwidthTier: string;
    }

    export interface RegionInstanceTemplateReservationAffinity {
        /**
         * Specifies the label selector for the reservation to use..
         * Structure is documented below.
         */
        specificReservation?: outputs.compute.RegionInstanceTemplateReservationAffinitySpecificReservation;
        /**
         * The type of reservation from which this instance can consume resources.
         */
        type: string;
    }

    export interface RegionInstanceTemplateReservationAffinitySpecificReservation {
        /**
         * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify compute.googleapis.com/reservation-name as the key and specify the name of your reservation as the only value.
         */
        key: string;
        /**
         * Corresponds to the label values of a reservation resource.
         */
        values: string[];
    }

    export interface RegionInstanceTemplateScheduling {
        /**
         * Specifies whether the instance should be
         * automatically restarted if it is terminated by Compute Engine (not
         * terminated by a user). This defaults to true.
         */
        automaticRestart?: boolean;
        /**
         * Describe the type of termination action for `SPOT` VM. Can be `STOP` or `DELETE`.  Read more on [here](https://cloud.google.com/compute/docs/instances/create-use-spot)
         */
        instanceTerminationAction?: string;
        localSsdRecoveryTimeouts?: outputs.compute.RegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
        /**
         * Specifies the frequency of planned maintenance events. The accepted values are: `PERIODIC`.   
         * <a name="nestedGuestAccelerator"></a>The `guestAccelerator` block supports:
         */
        maintenanceInterval?: string;
        /**
         * The duration of the instance. Instance will run and be terminated after then, the termination action could be defined in `instanceTerminationAction`. Only support `DELETE` `instanceTerminationAction` at this point. Structure is documented below.
         */
        maxRunDuration?: outputs.compute.RegionInstanceTemplateSchedulingMaxRunDuration;
        minNodeCpus?: number;
        /**
         * Specifies node affinities or anti-affinities
         * to determine which sole-tenant nodes your instances and managed instance
         * groups will use as host systems. Read more on sole-tenant node creation
         * [here](https://cloud.google.com/compute/docs/nodes/create-nodes).
         * Structure documented below.
         */
        nodeAffinities?: outputs.compute.RegionInstanceTemplateSchedulingNodeAffinity[];
        /**
         * Defines the maintenance behavior for this
         * instance.
         */
        onHostMaintenance: string;
        /**
         * Allows instance to be preempted. This defaults to
         * false. Read more on this
         * [here](https://cloud.google.com/compute/docs/instances/preemptible).
         */
        preemptible?: boolean;
        /**
         * Describe the type of preemptible VM. This field accepts the value `STANDARD` or `SPOT`. If the value is `STANDARD`, there will be no discount. If this   is set to `SPOT`, 
         * `preemptible` should be `true` and `automaticRestart` should be
         * `false`. For more info about
         * `SPOT`, read [here](https://cloud.google.com/compute/docs/instances/spot)
         */
        provisioningModel: string;
    }

    export interface RegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         *
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         *
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         */
        seconds: number;
    }

    export interface RegionInstanceTemplateSchedulingMaxRunDuration {
        /**
         * Span of time that's a fraction of a second at nanosecond
         * resolution. Durations less than one second are represented with a 0
         * `seconds` field and a positive `nanos` field. Must be from 0 to
         * 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to
         * 315,576,000,000 inclusive. Note: these bounds are computed from: 60
         * sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years.
         */
        seconds: number;
    }

    export interface RegionInstanceTemplateSchedulingNodeAffinity {
        /**
         * The key for the node affinity label.
         */
        key: string;
        /**
         * The operator. Can be `IN` for node-affinities
         * or `NOT_IN` for anti-affinities.
         */
        operator: string;
        /**
         * Corresponds to the label values of a reservation resource.
         */
        values: string[];
    }

    export interface RegionInstanceTemplateServiceAccount {
        /**
         * The service account e-mail address. If not given, the
         * default Google Compute Engine service account is used.
         */
        email: string;
        /**
         * A list of service scopes. Both OAuth2 URLs and gcloud
         * short names are supported. To allow full access to all Cloud APIs, use the
         * `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
         *
         * The [service accounts documentation](https://cloud.google.com/compute/docs/access/service-accounts#accesscopesiam)
         * explains that access scopes are the legacy method of specifying permissions for your instance.
         * To follow best practices you should create a dedicated service account with the minimum permissions the VM requires.
         * To use a dedicated service account this field should be configured as a list containing the `cloud-platform` scope.
         * See [Authenticate workloads using service accounts best practices](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices)
         * and [Best practices for using service accounts](https://cloud.google.com/iam/docs/best-practices-service-accounts#single-purpose).
         */
        scopes: string[];
    }

    export interface RegionInstanceTemplateShieldedInstanceConfig {
        /**
         * - Compare the most recent boot measurements to the integrity policy baseline and return a pair of pass/fail results depending on whether they match or not. Defaults to true.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * - Verify the digital signature of all boot components, and halt the boot process if signature verification fails. Defaults to false.
         */
        enableSecureBoot?: boolean;
        /**
         * - Use a virtualized trusted platform module, which is a specialized computer chip you can use to encrypt objects like keys and certificates. Defaults to true.
         */
        enableVtpm?: boolean;
    }

    export interface RegionNetworkEndpointGroupAppEngine {
        /**
         * Optional serving service.
         * The service name must be 1-63 characters long, and comply with RFC1035.
         * Example value: "default", "my-service".
         */
        service?: string;
        /**
         * A template to parse service and version fields from a request URL.
         * URL mask allows for routing to multiple App Engine services without
         * having to create multiple Network Endpoint Groups and backend services.
         * For example, the request URLs "foo1-dot-appname.appspot.com/v1" and
         * "foo1-dot-appname.appspot.com/v2" can be backed by the same Serverless NEG with
         * URL mask "-dot-appname.appspot.com/". The URL mask will parse
         * them to { service = "foo1", version = "v1" } and { service = "foo1", version = "v2" } respectively.
         */
        urlMask?: string;
        /**
         * Optional serving version.
         * The version must be 1-63 characters long, and comply with RFC1035.
         * Example value: "v1", "v2".
         */
        version?: string;
    }

    export interface RegionNetworkEndpointGroupCloudFunction {
        /**
         * A user-defined name of the Cloud Function.
         * The function name is case-sensitive and must be 1-63 characters long.
         * Example value: "func1".
         */
        function?: string;
        /**
         * A template to parse function field from a request URL. URL mask allows
         * for routing to multiple Cloud Functions without having to create
         * multiple Network Endpoint Groups and backend services.
         * For example, request URLs "mydomain.com/function1" and "mydomain.com/function2"
         * can be backed by the same Serverless NEG with URL mask "/". The URL mask
         * will parse them to { function = "function1" } and { function = "function2" } respectively.
         */
        urlMask?: string;
    }

    export interface RegionNetworkEndpointGroupCloudRun {
        /**
         * Cloud Run service is the main resource of Cloud Run.
         * The service must be 1-63 characters long, and comply with RFC1035.
         * Example value: "run-service".
         */
        service?: string;
        /**
         * Cloud Run tag represents the "named-revision" to provide
         * additional fine-grained traffic routing information.
         * The tag must be 1-63 characters long, and comply with RFC1035.
         * Example value: "revision-0010".
         */
        tag?: string;
        /**
         * A template to parse service and tag fields from a request URL.
         * URL mask allows for routing to multiple Run services without having
         * to create multiple network endpoint groups and backend services.
         * For example, request URLs "foo1.domain.com/bar1" and "foo1.domain.com/bar2"
         * an be backed by the same Serverless Network Endpoint Group (NEG) with
         * URL mask ".domain.com/". The URL mask will parse them to { service="bar1", tag="foo1" }
         * and { service="bar2", tag="foo2" } respectively.
         */
        urlMask?: string;
    }

    export interface RegionNetworkEndpointGroupServerlessDeployment {
        /**
         * The platform of the NEG backend target(s). Possible values:
         * API Gateway: apigateway.googleapis.com
         */
        platform: string;
        /**
         * The user-defined name of the workload/instance. This value must be provided explicitly or in the urlMask.
         * The resource identified by this value is platform-specific and is as follows: API Gateway: The gateway ID, App Engine: The service name,
         * Cloud Functions: The function name, Cloud Run: The service name
         */
        resource?: string;
        /**
         * A template to parse platform-specific fields from a request URL. URL mask allows for routing to multiple resources
         * on the same serverless platform without having to create multiple Network Endpoint Groups and backend resources.
         * The fields parsed by this template are platform-specific and are as follows: API Gateway: The gateway ID,
         * App Engine: The service and version, Cloud Functions: The function name, Cloud Run: The service and tag
         */
        urlMask?: string;
        /**
         * The optional resource version. The version identified by this value is platform-specific and is follows:
         * API Gateway: Unused, App Engine: The service version, Cloud Functions: Unused, Cloud Run: The service tag
         */
        version?: string;
    }

    export interface RegionNetworkFirewallPolicyRuleMatch {
        /**
         * Address groups which should be matched against the traffic destination. Maximum number of destination address groups is 10. Destination address groups is only supported in Egress rules.
         */
        destAddressGroups?: string[];
        /**
         * Domain names that will be used to match against the resolved domain name of destination of traffic. Can only be specified if DIRECTION is egress.
         */
        destFqdns?: string[];
        /**
         * CIDR IP address range. Maximum number of destination CIDR IP ranges allowed is 5000.
         */
        destIpRanges?: string[];
        /**
         * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is egress.
         */
        destRegionCodes?: string[];
        /**
         * Name of the Google Cloud Threat Intelligence list.
         */
        destThreatIntelligences?: string[];
        /**
         * Pairs of IP protocols and ports that the rule should match.
         */
        layer4Configs: outputs.compute.RegionNetworkFirewallPolicyRuleMatchLayer4Config[];
        /**
         * Address groups which should be matched against the traffic source. Maximum number of source address groups is 10. Source address groups is only supported in Ingress rules.
         */
        srcAddressGroups?: string[];
        /**
         * Domain names that will be used to match against the resolved domain name of source of traffic. Can only be specified if DIRECTION is ingress.
         */
        srcFqdns?: string[];
        /**
         * CIDR IP address range. Maximum number of source CIDR IP ranges allowed is 5000.
         */
        srcIpRanges?: string[];
        /**
         * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is ingress.
         */
        srcRegionCodes?: string[];
        /**
         * List of secure tag values, which should be matched at the source of the traffic. For INGRESS rule, if all the <code>srcSecureTag</code> are INEFFECTIVE, and there is no <code>srcIpRange</code>, this rule will be ignored. Maximum number of source tag values allowed is 256.
         */
        srcSecureTags?: outputs.compute.RegionNetworkFirewallPolicyRuleMatchSrcSecureTag[];
        /**
         * Name of the Google Cloud Threat Intelligence list.
         *
         * The `layer4Configs` block supports:
         */
        srcThreatIntelligences?: string[];
    }

    export interface RegionNetworkFirewallPolicyRuleMatchLayer4Config {
        /**
         * The IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (`tcp`, `udp`, `icmp`, `esp`, `ah`, `ipip`, `sctp`), or the IP protocol number.
         */
        ipProtocol: string;
        /**
         * An optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: ``.
         *
         * - - -
         */
        ports?: string[];
    }

    export interface RegionNetworkFirewallPolicyRuleMatchSrcSecureTag {
        /**
         * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
         */
        name: string;
        /**
         * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
         */
        state: string;
    }

    export interface RegionNetworkFirewallPolicyRuleTargetSecureTag {
        /**
         * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
         */
        name: string;
        /**
         * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
         */
        state: string;
    }

    export interface RegionPerInstanceConfigPreservedState {
        /**
         * Stateful disks for the instance.
         * Structure is documented below.
         */
        disks?: outputs.compute.RegionPerInstanceConfigPreservedStateDisk[];
        externalIps?: outputs.compute.RegionPerInstanceConfigPreservedStateExternalIp[];
        internalIps?: outputs.compute.RegionPerInstanceConfigPreservedStateInternalIp[];
        /**
         * Preserved metadata defined for this instance. This is a list of key->value pairs.
         */
        metadata?: {[key: string]: string};
    }

    export interface RegionPerInstanceConfigPreservedStateDisk {
        /**
         * A value that prescribes what should happen to the stateful disk when the VM instance is deleted.
         * The available options are `NEVER` and `ON_PERMANENT_INSTANCE_DELETION`.
         * `NEVER` - detach the disk when the VM is deleted, but do not delete the disk.
         * `ON_PERMANENT_INSTANCE_DELETION` will delete the stateful disk when the VM is permanently
         * deleted from the instance group.
         * Default value is `NEVER`.
         * Possible values are: `NEVER`, `ON_PERMANENT_INSTANCE_DELETION`.
         */
        deleteRule?: string;
        /**
         * A unique device name that is reflected into the /dev/ tree of a Linux operating system running within the instance.
         */
        deviceName: string;
        /**
         * The mode of the disk.
         * Default value is `READ_WRITE`.
         * Possible values are: `READ_ONLY`, `READ_WRITE`.
         */
        mode?: string;
        /**
         * The URI of an existing persistent disk to attach under the specified device-name in the format
         * `projects/project-id/zones/zone/disks/disk-name`.
         */
        source: string;
    }

    export interface RegionPerInstanceConfigPreservedStateExternalIp {
        /**
         * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted.
         * Default value is `NEVER`.
         * Possible values are: `NEVER`, `ON_PERMANENT_INSTANCE_DELETION`.
         */
        autoDelete?: string;
        /**
         * The identifier for this object. Format specified above.
         */
        interfaceName: string;
        /**
         * Ip address representation
         * Structure is documented below.
         */
        ipAddress?: outputs.compute.RegionPerInstanceConfigPreservedStateExternalIpIpAddress;
    }

    export interface RegionPerInstanceConfigPreservedStateExternalIpIpAddress {
        /**
         * The URL of the reservation for this IP address.
         */
        address?: string;
    }

    export interface RegionPerInstanceConfigPreservedStateInternalIp {
        /**
         * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted.
         * Default value is `NEVER`.
         * Possible values are: `NEVER`, `ON_PERMANENT_INSTANCE_DELETION`.
         */
        autoDelete?: string;
        /**
         * The identifier for this object. Format specified above.
         */
        interfaceName: string;
        /**
         * Ip address representation
         * Structure is documented below.
         */
        ipAddress?: outputs.compute.RegionPerInstanceConfigPreservedStateInternalIpIpAddress;
    }

    export interface RegionPerInstanceConfigPreservedStateInternalIpIpAddress {
        /**
         * The URL of the reservation for this IP address.
         */
        address?: string;
    }

    export interface RegionSecurityPolicyDdosProtectionConfig {
        /**
         * Google Cloud Armor offers the following options to help protect systems against DDoS attacks:
         * - STANDARD: basic always-on protection for network load balancers, protocol forwarding, or VMs with public IP addresses.
         * - ADVANCED: additional protections for Managed Protection Plus subscribers who use network load balancers, protocol forwarding, or VMs with public IP addresses.
         * - ADVANCED_PREVIEW: flag to enable the security policy in preview mode.
         * Possible values are: `ADVANCED`, `ADVANCED_PREVIEW`, `STANDARD`.
         */
        ddosProtection: string;
    }

    export interface RegionSecurityPolicyRuleMatch {
        /**
         * The configuration options available when specifying versionedExpr.
         * This field must be specified if versionedExpr is specified and cannot be specified if versionedExpr is not specified.
         * Structure is documented below.
         */
        config?: outputs.compute.RegionSecurityPolicyRuleMatchConfig;
        /**
         * Preconfigured versioned expression. If this field is specified, config must also be specified.
         * Available preconfigured expressions along with their requirements are: SRC_IPS_V1 - must specify the corresponding srcIpRange field in config.
         * Possible values are: `SRC_IPS_V1`.
         */
        versionedExpr?: string;
    }

    export interface RegionSecurityPolicyRuleMatchConfig {
        /**
         * CIDR IP address range. Maximum number of srcIpRanges allowed is 10.
         */
        srcIpRanges?: string[];
    }

    export interface RegionSecurityPolicyRuleNetworkMatch {
        /**
         * Destination IPv4/IPv6 addresses or CIDR prefixes, in standard text format.
         */
        destIpRanges?: string[];
        /**
         * Destination port numbers for TCP/UDP/SCTP. Each element can be a 16-bit unsigned decimal number (e.g. "80") or range (e.g. "0-1023").
         */
        destPorts?: string[];
        /**
         * IPv4 protocol / IPv6 next header (after extension headers). Each element can be an 8-bit unsigned decimal number (e.g. "6"), range (e.g. "253-254"), or one of the following protocol names: "tcp", "udp", "icmp", "esp", "ah", "ipip", or "sctp".
         */
        ipProtocols?: string[];
        /**
         * BGP Autonomous System Number associated with the source IP address.
         */
        srcAsns?: number[];
        /**
         * Source IPv4/IPv6 addresses or CIDR prefixes, in standard text format.
         */
        srcIpRanges?: string[];
        /**
         * Source port numbers for TCP/UDP/SCTP. Each element can be a 16-bit unsigned decimal number (e.g. "80") or range (e.g. "0-1023").
         */
        srcPorts?: string[];
        /**
         * Two-letter ISO 3166-1 alpha-2 country code associated with the source IP address.
         */
        srcRegionCodes?: string[];
        /**
         * User-defined fields. Each element names a defined field and lists the matching values for that field.
         * Structure is documented below.
         */
        userDefinedFields?: outputs.compute.RegionSecurityPolicyRuleNetworkMatchUserDefinedField[];
    }

    export interface RegionSecurityPolicyRuleNetworkMatchUserDefinedField {
        /**
         * Name of the user-defined field, as given in the definition.
         */
        name?: string;
        /**
         * Matching values of the field. Each element can be a 32-bit unsigned decimal or hexadecimal (starting with "0x") number (e.g. "64") or range (e.g. "0x400-0x7ff").
         */
        values?: string[];
    }

    export interface RegionSecurityPolicyUserDefinedField {
        /**
         * The base relative to which 'offset' is measured. Possible values are:
         * - IPV4: Points to the beginning of the IPv4 header.
         * - IPV6: Points to the beginning of the IPv6 header.
         * - TCP: Points to the beginning of the TCP header, skipping over any IPv4 options or IPv6 extension headers. Not present for non-first fragments.
         * - UDP: Points to the beginning of the UDP header, skipping over any IPv4 options or IPv6 extension headers. Not present for non-first fragments.
         * Possible values are: `IPV4`, `IPV6`, `TCP`, `UDP`.
         */
        base: string;
        /**
         * If specified, apply this mask (bitwise AND) to the field to ignore bits before matching.
         * Encoded as a hexadecimal number (starting with "0x").
         * The last byte of the field (in network byte order) corresponds to the least significant byte of the mask.
         */
        mask?: string;
        /**
         * The name of this field. Must be unique within the policy.
         */
        name?: string;
        /**
         * Offset of the first byte of the field (in network byte order) relative to 'base'.
         */
        offset?: number;
        /**
         * Size of the field in bytes. Valid values: 1-4.
         */
        size?: number;
    }

    export interface RegionUrlMapDefaultRouteAction {
        /**
         * The specification for allowing client side cross-origin requests. Please see
         * [W3C Recommendation for Cross Origin Resource Sharing](https://www.w3.org/TR/cors/)
         * Structure is documented below.
         */
        corsPolicy?: outputs.compute.RegionUrlMapDefaultRouteActionCorsPolicy;
        /**
         * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
         * As part of fault injection, when clients send requests to a backend service, delays can be introduced by a load balancer on a percentage of requests before sending those requests to the backend service.
         * Similarly requests from clients can be aborted by the load balancer for a percentage of requests.
         * timeout and retryPolicy is ignored by clients that are configured with a faultInjectionPolicy if: 1. The traffic is generated by fault injection AND 2. The fault injection is not a delay fault injection.
         * Fault injection is not supported with the global external HTTP(S) load balancer (classic). To see which load balancers support fault injection, see Load balancing: [Routing and traffic management features](https://cloud.google.com/load-balancing/docs/features#routing-traffic-management).
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.compute.RegionUrlMapDefaultRouteActionFaultInjectionPolicy;
        /**
         * Specifies the policy on how requests intended for the route's backends are shadowed to a separate mirrored backend service.
         * The load balancer does not wait for responses from the shadow service. Before sending traffic to the shadow service, the host / authority header is suffixed with -shadow.
         * Not supported when the URL map is bound to a target gRPC proxy that has the validateForProxyless field set to true.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.compute.RegionUrlMapDefaultRouteActionRequestMirrorPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.compute.RegionUrlMapDefaultRouteActionRetryPolicy;
        /**
         * Specifies the timeout for the selected route. Timeout is computed from the time the request has been fully processed (known as end-of-stream) up until the response has been processed. Timeout includes all retries.
         * If not specified, this field uses the largest timeout among all backend services associated with the route.
         * Not supported when the URL map is bound to a target gRPC proxy that has validateForProxyless field set to true.
         * Structure is documented below.
         */
        timeout?: outputs.compute.RegionUrlMapDefaultRouteActionTimeout;
        /**
         * The spec to modify the URL of the request, before forwarding the request to the matched service.
         * urlRewrite is the only action supported in UrlMaps for external HTTP(S) load balancers.
         * Not supported when the URL map is bound to a target gRPC proxy that has the validateForProxyless field set to true.
         * Structure is documented below.
         */
        urlRewrite?: outputs.compute.RegionUrlMapDefaultRouteActionUrlRewrite;
        /**
         * A list of weighted backend services to send traffic to when a route match occurs. The weights determine the fraction of traffic that flows to their corresponding backend service. If all traffic needs to go to a single backend service, there must be one weightedBackendService with weight set to a non-zero number.
         * After a backend service is identified and before forwarding the request to the backend service, advanced routing actions such as URL rewrites and header transformations are applied depending on additional settings specified in this HttpRouteAction.
         * Structure is documented below.
         */
        weightedBackendServices?: outputs.compute.RegionUrlMapDefaultRouteActionWeightedBackendService[];
    }

    export interface RegionUrlMapDefaultRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials. This field translates to the Access-Control-Allow-Credentials header.
         * Default is false.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regualar expression patterns that match allowed origins. For regular expression grammar
         * please see en.cppreference.com/w/cpp/regex/ecmascript
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOrigins?: string[];
        /**
         * If true, the setting specifies the CORS policy is disabled. The default value of false, which indicates that the CORS policy is in effect.
         */
        disabled?: boolean;
        /**
         * Specifies the content for the Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached in seconds.
         * This translates to the Access-Control-Max-Age header.
         */
        maxAge?: number;
    }

    export interface RegionUrlMapDefaultRouteActionFaultInjectionPolicy {
        /**
         * The specification for how client requests are aborted as part of fault injection.
         * Structure is documented below.
         */
        abort?: outputs.compute.RegionUrlMapDefaultRouteActionFaultInjectionPolicyAbort;
        /**
         * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
         * Structure is documented below.
         */
        delay?: outputs.compute.RegionUrlMapDefaultRouteActionFaultInjectionPolicyDelay;
    }

    export interface RegionUrlMapDefaultRouteActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         * The value must be between 200 and 599 inclusive.
         */
        httpStatus?: number;
        /**
         * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface RegionUrlMapDefaultRouteActionFaultInjectionPolicyDelay {
        /**
         * Specifies the value of the fixed delay interval.
         * Structure is documented below.
         */
        fixedDelay?: outputs.compute.RegionUrlMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay;
        /**
         * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface RegionUrlMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface RegionUrlMapDefaultRouteActionRequestMirrorPolicy {
        /**
         * The full or partial URL to the RegionBackendService resource being mirrored to.
         * The backend service configured for a mirroring policy must reference backends that are of the same type as the original backend service matched in the URL map.
         * Serverless NEG backends are not currently supported as a mirrored backend service.
         */
        backendService?: string;
    }

    export interface RegionUrlMapDefaultRouteActionRetryPolicy {
        /**
         * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
         */
        numRetries?: number;
        /**
         * Specifies a non-zero timeout per retry attempt.
         * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
         * will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        perTryTimeout?: outputs.compute.RegionUrlMapDefaultRouteActionRetryPolicyPerTryTimeout;
        /**
         * Specifies one or more conditions when this retry policy applies.
         * Valid values are listed below. Only the following codes are supported when the URL map is bound to target gRPC proxy that has validateForProxyless field set to true: cancelled, deadline-exceeded, internal, resource-exhausted, unavailable.
         * - 5xx : retry is attempted if the instance or endpoint responds with any 5xx response code, or if the instance or endpoint does not respond at all. For example, disconnects, reset, read timeout, connection failure, and refused streams.
         * - gateway-error : Similar to 5xx, but only applies to response codes 502, 503 or 504.
         * - connect-failure : a retry is attempted on failures connecting to the instance or endpoint. For example, connection timeouts.
         * - retriable-4xx : a retry is attempted if the instance or endpoint responds with a 4xx response code. The only error that you can retry is error code 409.
         * - refused-stream : a retry is attempted if the instance or endpoint resets the stream with a REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
         * - cancelled : a retry is attempted if the gRPC status code in the response header is set to cancelled.
         * - deadline-exceeded : a retry is attempted if the gRPC status code in the response header is set to deadline-exceeded.
         * - internal :  a retry is attempted if the gRPC status code in the response header is set to internal.
         * - resource-exhausted : a retry is attempted if the gRPC status code in the response header is set to resource-exhausted.
         * - unavailable : a retry is attempted if the gRPC status code in the response header is set to unavailable.
         */
        retryConditions?: string[];
    }

    export interface RegionUrlMapDefaultRouteActionRetryPolicyPerTryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface RegionUrlMapDefaultRouteActionTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive. Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface RegionUrlMapDefaultRouteActionUrlRewrite {
        /**
         * Before forwarding the request to the selected service, the request's host header is replaced with contents of hostRewrite.
         * The value must be from 1 to 255 characters.
         */
        hostRewrite?: string;
        /**
         * Before forwarding the request to the selected backend service, the matching portion of the request's path is replaced by pathPrefixRewrite.
         * The value must be from 1 to 1024 characters.
         */
        pathPrefixRewrite?: string;
    }

    export interface RegionUrlMapDefaultRouteActionWeightedBackendService {
        /**
         * The full or partial URL to the default BackendService resource. Before forwarding the request to backendService, the load balancer applies any relevant headerActions specified as part of this backendServiceWeight.
         */
        backendService?: string;
        /**
         * Specifies changes to request and response headers that need to take effect for the selected backendService.
         * headerAction specified here take effect before headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
         * headerAction is not supported for load balancers that have their loadBalancingScheme set to EXTERNAL.
         * Not supported when the URL map is bound to a target gRPC proxy that has validateForProxyless field set to true.
         * Structure is documented below.
         */
        headerAction?: outputs.compute.RegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderAction;
        /**
         * Specifies the fraction of traffic sent to a backend service, computed as weight / (sum of all weightedBackendService weights in routeAction) .
         * The selection of a backend service is determined only for new traffic. Once a user's request has been directed to a backend service, subsequent requests are sent to the same backend service as determined by the backend service's session affinity policy.
         * The value must be from 0 to 1000.
         */
        weight?: number;
    }

    export interface RegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderAction {
        /**
         * Headers to add to a matching request before forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.RegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request before forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response before sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.RegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response before sending the response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface RegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName?: string;
        /**
         * The value of the header to add.
         */
        headerValue?: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace?: boolean;
    }

    export interface RegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName?: string;
        /**
         * The value of the header to add.
         */
        headerValue?: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace?: boolean;
    }

    export interface RegionUrlMapDefaultUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was
         * supplied in the request. The value must be between 1 and 255 characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https. If set to
         * false, the URL scheme of the redirected request will remain the same as that of the
         * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
         * true for TargetHttpsProxy is not permitted. The default is set to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was
         * supplied in the request. pathRedirect cannot be supplied together with
         * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
         * original request will be used for the redirect. The value must be between 1 and 1024
         * characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
         * retaining the remaining portion of the URL before redirecting the request.
         * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
         * neither. If neither is supplied, the path of the original request will be used for
         * the redirect. The value must be between 1 and 1024 characters.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
         * will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
         * the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed prior
         * to redirecting the request. If set to false, the query portion of the original URL is
         * retained.
         * This field is required to ensure an empty block is not set. The normal default value is false.
         */
        stripQuery: boolean;
    }

    export interface RegionUrlMapHostRule {
        /**
         * An optional description of this HostRule. Provide this property
         * when you create the resource.
         */
        description?: string;
        /**
         * The list of host patterns to match. They must be valid
         * hostnames, except * will match any string of ([a-z0-9-.]*). In
         * that case, * must be the first character and must be followed in
         * the pattern by either - or ..
         */
        hosts: string[];
        /**
         * The name of the PathMatcher to use to match the path portion of
         * the URL if the hostRule matches the URL's host portion.
         */
        pathMatcher: string;
    }

    export interface RegionUrlMapPathMatcher {
        /**
         * A reference to a RegionBackendService resource. This will be used if
         * none of the pathRules defined by this PathMatcher is matched by
         * the URL's path portion.
         */
        defaultService?: string;
        /**
         * When none of the specified hostRules match, the request is redirected to a URL specified
         * by defaultUrlRedirect. If defaultUrlRedirect is specified, defaultService or
         * defaultRouteAction must not be set.
         * Structure is documented below.
         */
        defaultUrlRedirect?: outputs.compute.RegionUrlMapPathMatcherDefaultUrlRedirect;
        /**
         * An optional description of this resource.
         */
        description?: string;
        /**
         * The name to which this PathMatcher is referred by the HostRule.
         */
        name: string;
        /**
         * The list of path rules. Use this list instead of routeRules when routing based
         * on simple path matching is all that's required. The order by which path rules
         * are specified does not matter. Matches are always done on the longest-path-first
         * basis. For example: a pathRule with a path /a/b/c/* will match before /a/b/*
         * irrespective of the order in which those paths appear in this list. Within a
         * given pathMatcher, only one of pathRules or routeRules must be set.
         * Structure is documented below.
         */
        pathRules?: outputs.compute.RegionUrlMapPathMatcherPathRule[];
        /**
         * The list of ordered HTTP route rules. Use this list instead of pathRules when
         * advanced route matching and routing actions are desired. The order of specifying
         * routeRules matters: the first rule that matches will cause its specified routing
         * action to take effect. Within a given pathMatcher, only one of pathRules or
         * routeRules must be set. routeRules are not supported in UrlMaps intended for
         * External load balancers.
         * Structure is documented below.
         */
        routeRules?: outputs.compute.RegionUrlMapPathMatcherRouteRule[];
    }

    export interface RegionUrlMapPathMatcherDefaultUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was
         * supplied in the request. The value must be between 1 and 255 characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https. If set to
         * false, the URL scheme of the redirected request will remain the same as that of the
         * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
         * true for TargetHttpsProxy is not permitted. The default is set to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was
         * supplied in the request. pathRedirect cannot be supplied together with
         * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
         * original request will be used for the redirect. The value must be between 1 and 1024
         * characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
         * retaining the remaining portion of the URL before redirecting the request.
         * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
         * neither. If neither is supplied, the path of the original request will be used for
         * the redirect. The value must be between 1 and 1024 characters.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
         * will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
         * the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed prior
         * to redirecting the request. If set to false, the query portion of the original URL is
         * retained.
         * This field is required to ensure an empty block is not set. The normal default value is false.
         */
        stripQuery: boolean;
    }

    export interface RegionUrlMapPathMatcherPathRule {
        /**
         * The list of path patterns to match. Each must start with / and the only place a
         * \* is allowed is at the end following a /. The string fed to the path matcher
         * does not include any text after the first ? or #, and those chars are not
         * allowed here.
         */
        paths: string[];
        /**
         * In response to a matching path, the load balancer performs advanced routing
         * actions like URL rewrites, header transformations, etc. prior to forwarding the
         * request to the selected backend. If routeAction specifies any
         * weightedBackendServices, service must not be set. Conversely if service is set,
         * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
         * or urlRedirect must be set.
         * Structure is documented below.
         */
        routeAction?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteAction;
        /**
         * The region backend service resource to which traffic is
         * directed if this rule is matched. If routeAction is additionally specified,
         * advanced routing actions like URL Rewrites, etc. take effect prior to sending
         * the request to the backend. However, if service is specified, routeAction cannot
         * contain any weightedBackendService s. Conversely, if routeAction specifies any
         * weightedBackendServices, service must not be specified. Only one of urlRedirect,
         * service or routeAction.weightedBackendService must be set.
         */
        service?: string;
        /**
         * When a path pattern is matched, the request is redirected to a URL specified
         * by urlRedirect. If urlRedirect is specified, service or routeAction must not
         * be set.
         * Structure is documented below.
         */
        urlRedirect?: outputs.compute.RegionUrlMapPathMatcherPathRuleUrlRedirect;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteAction {
        /**
         * The specification for allowing client side cross-origin requests. Please see W3C
         * Recommendation for Cross Origin Resource Sharing
         * Structure is documented below.
         */
        corsPolicy?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionCorsPolicy;
        /**
         * The specification for fault injection introduced into traffic to test the
         * resiliency of clients to backend service failure. As part of fault injection,
         * when clients send requests to a backend service, delays can be introduced by
         * Loadbalancer on a percentage of requests before sending those request to the
         * backend service. Similarly requests from clients can be aborted by the
         * Loadbalancer for a percentage of requests. timeout and retryPolicy will be
         * ignored by clients that are configured with a fault_injection_policy.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicy;
        /**
         * Specifies the policy on how requests intended for the route's backends are
         * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
         * responses from the shadow service. Prior to sending traffic to the shadow
         * service, the host / authority header is suffixed with -shadow.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionRequestMirrorPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionRetryPolicy;
        /**
         * Specifies the timeout for the selected route. Timeout is computed from the time
         * the request is has been fully processed (i.e. end-of-stream) up until the
         * response has been completely processed. Timeout includes all retries. If not
         * specified, the default value is 15 seconds.
         * Structure is documented below.
         */
        timeout?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionTimeout;
        /**
         * The spec to modify the URL of the request, prior to forwarding the request to
         * the matched service
         * Structure is documented below.
         */
        urlRewrite?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionUrlRewrite;
        /**
         * A list of weighted backend services to send traffic to when a route match
         * occurs. The weights determine the fraction of traffic that flows to their
         * corresponding backend service. If all traffic needs to go to a single backend
         * service, there must be one  weightedBackendService with weight set to a non 0
         * number. Once a backendService is identified and before forwarding the request to
         * the backend service, advanced routing actions like Url rewrites and header
         * transformations are applied depending on additional settings specified in this
         * HttpRouteAction.
         * Structure is documented below.
         */
        weightedBackendServices?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendService[];
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials. This field translates to the Access-Control-Allow-Credentials header.
         * Default is false.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regualar expression patterns that match allowed origins. For regular expression grammar
         * please see en.cppreference.com/w/cpp/regex/ecmascript
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOrigins?: string[];
        /**
         * If true, the setting specifies the CORS policy is disabled. The default value of false, which indicates that the CORS policy is in effect.
         */
        disabled: boolean;
        /**
         * Specifies the content for the Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached in seconds.
         * This translates to the Access-Control-Max-Age header.
         */
        maxAge?: number;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicy {
        /**
         * The specification for how client requests are aborted as part of fault injection.
         * Structure is documented below.
         */
        abort?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort;
        /**
         * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
         * Structure is documented below.
         */
        delay?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         * The value must be between 200 and 599 inclusive.
         */
        httpStatus: number;
        /**
         * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage: number;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay {
        /**
         * Specifies the value of the fixed delay interval.
         * Structure is documented below.
         */
        fixedDelay: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
        /**
         * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage: number;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionRequestMirrorPolicy {
        /**
         * The full or partial URL to the RegionBackendService resource being mirrored to.
         * The backend service configured for a mirroring policy must reference backends that are of the same type as the original backend service matched in the URL map.
         * Serverless NEG backends are not currently supported as a mirrored backend service.
         */
        backendService: string;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionRetryPolicy {
        /**
         * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
         */
        numRetries?: number;
        /**
         * Specifies a non-zero timeout per retry attempt.
         * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
         * will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        perTryTimeout?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout;
        /**
         * Specifies one or more conditions when this retry policy applies.
         * Valid values are listed below. Only the following codes are supported when the URL map is bound to target gRPC proxy that has validateForProxyless field set to true: cancelled, deadline-exceeded, internal, resource-exhausted, unavailable.
         * - 5xx : retry is attempted if the instance or endpoint responds with any 5xx response code, or if the instance or endpoint does not respond at all. For example, disconnects, reset, read timeout, connection failure, and refused streams.
         * - gateway-error : Similar to 5xx, but only applies to response codes 502, 503 or 504.
         * - connect-failure : a retry is attempted on failures connecting to the instance or endpoint. For example, connection timeouts.
         * - retriable-4xx : a retry is attempted if the instance or endpoint responds with a 4xx response code. The only error that you can retry is error code 409.
         * - refused-stream : a retry is attempted if the instance or endpoint resets the stream with a REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
         * - cancelled : a retry is attempted if the gRPC status code in the response header is set to cancelled.
         * - deadline-exceeded : a retry is attempted if the gRPC status code in the response header is set to deadline-exceeded.
         * - internal :  a retry is attempted if the gRPC status code in the response header is set to internal.
         * - resource-exhausted : a retry is attempted if the gRPC status code in the response header is set to resource-exhausted.
         * - unavailable : a retry is attempted if the gRPC status code in the response header is set to unavailable.
         */
        retryConditions?: string[];
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive. Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionUrlRewrite {
        /**
         * Before forwarding the request to the selected service, the request's host header is replaced with contents of hostRewrite.
         * The value must be from 1 to 255 characters.
         */
        hostRewrite?: string;
        /**
         * Before forwarding the request to the selected backend service, the matching portion of the request's path is replaced by pathPrefixRewrite.
         * The value must be from 1 to 1024 characters.
         */
        pathPrefixRewrite?: string;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendService {
        /**
         * The full or partial URL to the default BackendService resource. Before forwarding the request to backendService, the load balancer applies any relevant headerActions specified as part of this backendServiceWeight.
         */
        backendService: string;
        /**
         * Specifies changes to request and response headers that need to take effect for the selected backendService.
         * headerAction specified here take effect before headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
         * headerAction is not supported for load balancers that have their loadBalancingScheme set to EXTERNAL.
         * Not supported when the URL map is bound to a target gRPC proxy that has validateForProxyless field set to true.
         * Structure is documented below.
         */
        headerAction?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction;
        /**
         * Specifies the fraction of traffic sent to a backend service, computed as weight / (sum of all weightedBackendService weights in routeAction) .
         * The selection of a backend service is determined only for new traffic. Once a user's request has been directed to a backend service, subsequent requests are sent to the same backend service as determined by the backend service's session affinity policy.
         * The value must be from 0 to 1000.
         */
        weight: number;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction {
        /**
         * Headers to add to a matching request before forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request before forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response before sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response before sending the response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace: boolean;
    }

    export interface RegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace: boolean;
    }

    export interface RegionUrlMapPathMatcherPathRuleUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one
         * that was supplied in the request. The value must be between 1 and 255
         * characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https.
         * If set to false, the URL scheme of the redirected request will remain the
         * same as that of the request. This must only be set for UrlMaps used in
         * TargetHttpProxys. Setting this true for TargetHttpsProxy is not
         * permitted. The default is set to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one
         * that was supplied in the request. pathRedirect cannot be supplied
         * together with prefixRedirect. Supply one alone or neither. If neither is
         * supplied, the path of the original request will be used for the redirect.
         * The value must be between 1 and 1024 characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the
         * HttpRouteRuleMatch, retaining the remaining portion of the URL before
         * redirecting the request. prefixRedirect cannot be supplied together with
         * pathRedirect. Supply one alone or neither. If neither is supplied, the
         * path of the original request will be used for the redirect. The value
         * must be between 1 and 1024 characters.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
         * will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
         * the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed
         * prior to redirecting the request. If set to false, the query portion of the
         * original URL is retained.
         * This field is required to ensure an empty block is not set. The normal default value is false.
         */
        stripQuery: boolean;
    }

    export interface RegionUrlMapPathMatcherRouteRule {
        /**
         * Specifies changes to request and response headers that need to take effect for
         * the selected backendService. The headerAction specified here are applied before
         * the matching pathMatchers[].headerAction and after pathMatchers[].routeRules[].r
         * outeAction.weightedBackendService.backendServiceWeightAction[].headerAction
         * Structure is documented below.
         */
        headerAction?: outputs.compute.RegionUrlMapPathMatcherRouteRuleHeaderAction;
        /**
         * The rules for determining a match.
         * Structure is documented below.
         */
        matchRules?: outputs.compute.RegionUrlMapPathMatcherRouteRuleMatchRule[];
        /**
         * For routeRules within a given pathMatcher, priority determines the order
         * in which load balancer will interpret routeRules. RouteRules are evaluated
         * in order of priority, from the lowest to highest number. The priority of
         * a rule decreases as its number increases (1, 2, 3, N+1). The first rule
         * that matches the request is applied.
         * You cannot configure two or more routeRules with the same priority.
         * Priority for each rule must be set to a number between 0 and
         * 2147483647 inclusive.
         * Priority numbers can have gaps, which enable you to add or remove rules
         * in the future without affecting the rest of the rules. For example,
         * 1, 2, 3, 4, 5, 9, 12, 16 is a valid series of priority numbers to which
         * you could add rules numbered from 6 to 8, 10 to 11, and 13 to 15 in the
         * future without any impact on existing rules.
         */
        priority: number;
        /**
         * In response to a matching matchRule, the load balancer performs advanced routing
         * actions like URL rewrites, header transformations, etc. prior to forwarding the
         * request to the selected backend. If  routeAction specifies any
         * weightedBackendServices, service must not be set. Conversely if service is set,
         * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
         * or urlRedirect must be set.
         * Structure is documented below.
         */
        routeAction?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteAction;
        /**
         * The region backend service resource to which traffic is
         * directed if this rule is matched. If routeAction is additionally specified,
         * advanced routing actions like URL Rewrites, etc. take effect prior to sending
         * the request to the backend. However, if service is specified, routeAction cannot
         * contain any weightedBackendService s. Conversely, if routeAction specifies any
         * weightedBackendServices, service must not be specified. Only one of urlRedirect,
         * service or routeAction.weightedBackendService must be set.
         */
        service?: string;
        /**
         * When this rule is matched, the request is redirected to a URL specified by
         * urlRedirect. If urlRedirect is specified, service or routeAction must not be
         * set.
         * Structure is documented below.
         */
        urlRedirect?: outputs.compute.RegionUrlMapPathMatcherRouteRuleUrlRedirect;
    }

    export interface RegionUrlMapPathMatcherRouteRuleHeaderAction {
        /**
         * Headers to add to a matching request before forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.RegionUrlMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request before forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response before sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.RegionUrlMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response before sending the response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface RegionUrlMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace: boolean;
    }

    export interface RegionUrlMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace: boolean;
    }

    export interface RegionUrlMapPathMatcherRouteRuleMatchRule {
        /**
         * For satisfying the matchRule condition, the path of the request must exactly
         * match the value specified in fullPathMatch after removing any query parameters
         * and anchor that may be part of the original URL. FullPathMatch must be between 1
         * and 1024 characters. Only one of prefixMatch, fullPathMatch or regexMatch must
         * be specified.
         */
        fullPathMatch?: string;
        /**
         * Specifies a list of header match criteria, all of which must match corresponding
         * headers in the request.
         * Structure is documented below.
         */
        headerMatches?: outputs.compute.RegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatch[];
        /**
         * Specifies that prefixMatch and fullPathMatch matches are case sensitive.
         * Defaults to false.
         */
        ignoreCase?: boolean;
        /**
         * Opaque filter criteria used by Loadbalancer to restrict routing configuration to
         * a limited set xDS compliant clients. In their xDS requests to Loadbalancer, xDS
         * clients present node metadata. If a match takes place, the relevant routing
         * configuration is made available to those proxies. For each metadataFilter in
         * this list, if its filterMatchCriteria is set to MATCH_ANY, at least one of the
         * filterLabels must match the corresponding label provided in the metadata. If its
         * filterMatchCriteria is set to MATCH_ALL, then all of its filterLabels must match
         * with corresponding labels in the provided metadata. metadataFilters specified
         * here can be overrides those specified in ForwardingRule that refers to this
         * UrlMap. metadataFilters only applies to Loadbalancers that have their
         * loadBalancingScheme set to INTERNAL_SELF_MANAGED.
         * Structure is documented below.
         */
        metadataFilters?: outputs.compute.RegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilter[];
        /**
         * For satisfying the matchRule condition, the request's path must begin with the
         * specified prefixMatch. prefixMatch must begin with a /. The value must be
         * between 1 and 1024 characters. Only one of prefixMatch, fullPathMatch or
         * regexMatch must be specified.
         */
        prefixMatch?: string;
        /**
         * Specifies a list of query parameter match criteria, all of which must match
         * corresponding query parameters in the request.
         * Structure is documented below.
         */
        queryParameterMatches?: outputs.compute.RegionUrlMapPathMatcherRouteRuleMatchRuleQueryParameterMatch[];
        /**
         * For satisfying the matchRule condition, the path of the request must satisfy the
         * regular expression specified in regexMatch after removing any query parameters
         * and anchor supplied with the original URL. For regular expression grammar please
         * see en.cppreference.com/w/cpp/regex/ecmascript  Only one of prefixMatch,
         * fullPathMatch or regexMatch must be specified.
         */
        regexMatch?: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatch {
        /**
         * The value should exactly match contents of exactMatch. Only one of exactMatch,
         * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
         */
        exactMatch?: string;
        /**
         * The name of the HTTP header to match. For matching against the HTTP request's
         * authority, use a headerMatch with the header name ":authority". For matching a
         * request's method, use the headerName ":method".
         */
        headerName: string;
        /**
         * If set to false, the headerMatch is considered a match if the match criteria
         * above are met. If set to true, the headerMatch is considered a match if the
         * match criteria above are NOT met. Defaults to false.
         */
        invertMatch?: boolean;
        /**
         * The value of the header must start with the contents of prefixMatch. Only one of
         * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
         * must be set.
         */
        prefixMatch?: string;
        /**
         * A header with the contents of headerName must exist. The match takes place
         * whether or not the request's header has a value or not. Only one of exactMatch,
         * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
         */
        presentMatch?: boolean;
        /**
         * The header value must be an integer and its value must be in the range specified
         * in rangeMatch. If the header does not contain an integer, number or is empty,
         * the match fails. For example for a range [-5, 0]
         * * -3 will match
         * * 0 will not match
         * * 0.25 will not match
         * * -3someString will not match.
         * Only one of exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or
         * rangeMatch must be set.
         * Structure is documented below.
         */
        rangeMatch?: outputs.compute.RegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch;
        /**
         * The value of the header must match the regular expression specified in
         * regexMatch. For regular expression grammar, please see:
         * en.cppreference.com/w/cpp/regex/ecmascript  For matching against a port
         * specified in the HTTP request, use a headerMatch with headerName set to PORT and
         * a regular expression that satisfies the RFC2616 Host header's port specifier.
         * Only one of exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or
         * rangeMatch must be set.
         */
        regexMatch?: string;
        /**
         * The value of the header must end with the contents of suffixMatch. Only one of
         * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
         * must be set.
         */
        suffixMatch?: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch {
        /**
         * The end of the range (exclusive).
         */
        rangeEnd: number;
        /**
         * The start of the range (inclusive).
         */
        rangeStart: number;
    }

    export interface RegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilter {
        /**
         * The list of label value pairs that must match labels in the provided metadata
         * based on filterMatchCriteria  This list must not be empty and can have at the
         * most 64 entries.
         * Structure is documented below.
         */
        filterLabels: outputs.compute.RegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel[];
        /**
         * Specifies how individual filterLabel matches within the list of filterLabels
         * contribute towards the overall metadataFilter match. Supported values are:
         * * MATCH_ANY: At least one of the filterLabels must have a matching label in the
         * provided metadata.
         * * MATCH_ALL: All filterLabels must have matching labels in
         * the provided metadata.
         * Possible values are: `MATCH_ALL`, `MATCH_ANY`.
         */
        filterMatchCriteria: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel {
        /**
         * Name of metadata label. The name can have a maximum length of 1024 characters
         * and must be at least 1 character long.
         */
        name: string;
        /**
         * The value of the label must match the specified value. value can have a maximum
         * length of 1024 characters.
         */
        value: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleMatchRuleQueryParameterMatch {
        /**
         * The queryParameterMatch matches if the value of the parameter exactly matches
         * the contents of exactMatch. Only one of presentMatch, exactMatch and regexMatch
         * must be set.
         */
        exactMatch?: string;
        /**
         * The name of the query parameter to match. The query parameter must exist in the
         * request, in the absence of which the request match fails.
         */
        name: string;
        /**
         * Specifies that the queryParameterMatch matches if the request contains the query
         * parameter, irrespective of whether the parameter has a value or not. Only one of
         * presentMatch, exactMatch and regexMatch must be set.
         */
        presentMatch?: boolean;
        /**
         * The queryParameterMatch matches if the value of the parameter matches the
         * regular expression specified by regexMatch. For the regular expression grammar,
         * please see en.cppreference.com/w/cpp/regex/ecmascript  Only one of presentMatch,
         * exactMatch and regexMatch must be set.
         */
        regexMatch?: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteAction {
        /**
         * The specification for allowing client side cross-origin requests. Please see W3C
         * Recommendation for Cross Origin Resource Sharing
         * Structure is documented below.
         */
        corsPolicy?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionCorsPolicy;
        /**
         * The specification for fault injection introduced into traffic to test the
         * resiliency of clients to backend service failure. As part of fault injection,
         * when clients send requests to a backend service, delays can be introduced by
         * Loadbalancer on a percentage of requests before sending those request to the
         * backend service. Similarly requests from clients can be aborted by the
         * Loadbalancer for a percentage of requests. timeout and retryPolicy will be
         * ignored by clients that are configured with a fault_injection_policy.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy;
        /**
         * Specifies the policy on how requests intended for the route's backends are
         * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
         * responses from the shadow service. Prior to sending traffic to the shadow
         * service, the host / authority header is suffixed with -shadow.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicy;
        /**
         * Specifies the timeout for the selected route. Timeout is computed from the time
         * the request is has been fully processed (i.e. end-of-stream) up until the
         * response has been completely processed. Timeout includes all retries. If not
         * specified, the default value is 15 seconds.
         * Structure is documented below.
         */
        timeout?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionTimeout;
        /**
         * The spec to modify the URL of the request, prior to forwarding the request to
         * the matched service
         * Structure is documented below.
         */
        urlRewrite?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionUrlRewrite;
        /**
         * A list of weighted backend services to send traffic to when a route match
         * occurs. The weights determine the fraction of traffic that flows to their
         * corresponding backend service. If all traffic needs to go to a single backend
         * service, there must be one  weightedBackendService with weight set to a non 0
         * number. Once a backendService is identified and before forwarding the request to
         * the backend service, advanced routing actions like Url rewrites and header
         * transformations are applied depending on additional settings specified in this
         * HttpRouteAction.
         * Structure is documented below.
         */
        weightedBackendServices?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendService[];
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials. This field translates to the Access-Control-Allow-Credentials header.
         * Default is false.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regualar expression patterns that match allowed origins. For regular expression grammar
         * please see en.cppreference.com/w/cpp/regex/ecmascript
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOrigins?: string[];
        /**
         * If true, the setting specifies the CORS policy is disabled. The default value of false, which indicates that the CORS policy is in effect.
         */
        disabled?: boolean;
        /**
         * Specifies the content for the Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached in seconds.
         * This translates to the Access-Control-Max-Age header.
         */
        maxAge?: number;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy {
        /**
         * The specification for how client requests are aborted as part of fault injection.
         * Structure is documented below.
         */
        abort?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort;
        /**
         * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
         * Structure is documented below.
         */
        delay?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         * The value must be between 200 and 599 inclusive.
         */
        httpStatus?: number;
        /**
         * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay {
        /**
         * Specifies the value of the fixed delay interval.
         * Structure is documented below.
         */
        fixedDelay?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
        /**
         * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy {
        /**
         * The full or partial URL to the RegionBackendService resource being mirrored to.
         * The backend service configured for a mirroring policy must reference backends that are of the same type as the original backend service matched in the URL map.
         * Serverless NEG backends are not currently supported as a mirrored backend service.
         */
        backendService: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicy {
        /**
         * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
         */
        numRetries: number;
        /**
         * Specifies a non-zero timeout per retry attempt.
         * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
         * will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        perTryTimeout?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout;
        /**
         * Specifies one or more conditions when this retry policy applies.
         * Valid values are listed below. Only the following codes are supported when the URL map is bound to target gRPC proxy that has validateForProxyless field set to true: cancelled, deadline-exceeded, internal, resource-exhausted, unavailable.
         * - 5xx : retry is attempted if the instance or endpoint responds with any 5xx response code, or if the instance or endpoint does not respond at all. For example, disconnects, reset, read timeout, connection failure, and refused streams.
         * - gateway-error : Similar to 5xx, but only applies to response codes 502, 503 or 504.
         * - connect-failure : a retry is attempted on failures connecting to the instance or endpoint. For example, connection timeouts.
         * - retriable-4xx : a retry is attempted if the instance or endpoint responds with a 4xx response code. The only error that you can retry is error code 409.
         * - refused-stream : a retry is attempted if the instance or endpoint resets the stream with a REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
         * - cancelled : a retry is attempted if the gRPC status code in the response header is set to cancelled.
         * - deadline-exceeded : a retry is attempted if the gRPC status code in the response header is set to deadline-exceeded.
         * - internal :  a retry is attempted if the gRPC status code in the response header is set to internal.
         * - resource-exhausted : a retry is attempted if the gRPC status code in the response header is set to resource-exhausted.
         * - unavailable : a retry is attempted if the gRPC status code in the response header is set to unavailable.
         */
        retryConditions?: string[];
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive. Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionUrlRewrite {
        /**
         * Before forwarding the request to the selected service, the request's host header is replaced with contents of hostRewrite.
         * The value must be from 1 to 255 characters.
         */
        hostRewrite?: string;
        /**
         * Before forwarding the request to the selected backend service, the matching portion of the request's path is replaced by pathPrefixRewrite.
         * The value must be from 1 to 1024 characters.
         */
        pathPrefixRewrite?: string;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendService {
        /**
         * The full or partial URL to the default BackendService resource. Before forwarding the request to backendService, the load balancer applies any relevant headerActions specified as part of this backendServiceWeight.
         */
        backendService: string;
        /**
         * Specifies changes to request and response headers that need to take effect for the selected backendService.
         * headerAction specified here take effect before headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
         * headerAction is not supported for load balancers that have their loadBalancingScheme set to EXTERNAL.
         * Not supported when the URL map is bound to a target gRPC proxy that has validateForProxyless field set to true.
         * Structure is documented below.
         */
        headerAction?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction;
        /**
         * Specifies the fraction of traffic sent to a backend service, computed as weight / (sum of all weightedBackendService weights in routeAction) .
         * The selection of a backend service is determined only for new traffic. Once a user's request has been directed to a backend service, subsequent requests are sent to the same backend service as determined by the backend service's session affinity policy.
         * The value must be from 0 to 1000.
         */
        weight: number;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction {
        /**
         * Headers to add to a matching request before forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request before forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response before sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response before sending the response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace: boolean;
    }

    export interface RegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
         * The default value is false.
         */
        replace: boolean;
    }

    export interface RegionUrlMapPathMatcherRouteRuleUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one
         * that was supplied in the request. The value must be between 1 and 255
         * characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https.
         * If set to false, the URL scheme of the redirected request will remain the
         * same as that of the request. This must only be set for UrlMaps used in
         * TargetHttpProxys. Setting this true for TargetHttpsProxy is not
         * permitted. The default is set to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one
         * that was supplied in the request. pathRedirect cannot be supplied
         * together with prefixRedirect. Supply one alone or neither. If neither is
         * supplied, the path of the original request will be used for the redirect.
         * The value must be between 1 and 1024 characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the
         * HttpRouteRuleMatch, retaining the remaining portion of the URL before
         * redirecting the request. prefixRedirect cannot be supplied together with
         * pathRedirect. Supply one alone or neither. If neither is supplied, the
         * path of the original request will be used for the redirect. The value
         * must be between 1 and 1024 characters.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
         * will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
         * the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed
         * prior to redirecting the request. If set to false, the query portion of the
         * original URL is retained.
         * This field is required to ensure an empty block is not set. The normal default value is false.
         */
        stripQuery?: boolean;
    }

    export interface RegionUrlMapTest {
        /**
         * Description of this test case.
         */
        description?: string;
        /**
         * Host portion of the URL.
         */
        host: string;
        /**
         * Path portion of the URL.
         */
        path: string;
        /**
         * A reference to expected RegionBackendService resource the given URL should be mapped to.
         */
        service: string;
    }

    export interface ReservationShareSettings {
        /**
         * A map of project number and project config. This is only valid when shareType's value is SPECIFIC_PROJECTS.
         * Structure is documented below.
         */
        projectMaps?: outputs.compute.ReservationShareSettingsProjectMap[];
        /**
         * Type of sharing for this shared-reservation
         * Possible values are: `LOCAL`, `SPECIFIC_PROJECTS`.
         */
        shareType: string;
    }

    export interface ReservationShareSettingsProjectMap {
        /**
         * The identifier for this object. Format specified above.
         */
        id: string;
        /**
         * The project id/number, should be same as the key of this project config in the project map.
         */
        projectId?: string;
    }

    export interface ReservationSpecificReservation {
        /**
         * The number of resources that are allocated.
         */
        count: number;
        /**
         * (Output)
         * How many instances are in use.
         */
        inUseCount: number;
        /**
         * The instance properties for the reservation.
         * Structure is documented below.
         */
        instanceProperties: outputs.compute.ReservationSpecificReservationInstanceProperties;
    }

    export interface ReservationSpecificReservationInstanceProperties {
        /**
         * Guest accelerator type and count.
         * Structure is documented below.
         */
        guestAccelerators?: outputs.compute.ReservationSpecificReservationInstancePropertiesGuestAccelerator[];
        /**
         * The amount of local ssd to reserve with each instance. This
         * reserves disks of type `local-ssd`.
         * Structure is documented below.
         */
        localSsds?: outputs.compute.ReservationSpecificReservationInstancePropertiesLocalSsd[];
        /**
         * The name of the machine type to reserve.
         */
        machineType: string;
        /**
         * The minimum CPU platform for the reservation. For example,
         * `"Intel Skylake"`. See
         * the CPU platform availability reference](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform#availablezones)
         * for information on available CPU platforms.
         */
        minCpuPlatform: string;
    }

    export interface ReservationSpecificReservationInstancePropertiesGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to
         * this instance.
         */
        acceleratorCount: number;
        /**
         * The full or partial URL of the accelerator type to
         * attach to this instance. For example:
         * `projects/my-project/zones/us-central1-c/acceleratorTypes/nvidia-tesla-p100`
         * If you are creating an instance template, specify only the accelerator name.
         */
        acceleratorType: string;
    }

    export interface ReservationSpecificReservationInstancePropertiesLocalSsd {
        /**
         * The size of the disk in base-2 GB.
         *
         * - - -
         */
        diskSizeGb: number;
        /**
         * The disk interface to use for attaching this disk.
         * Default value is `SCSI`.
         * Possible values are: `SCSI`, `NVME`.
         */
        interface?: string;
    }

    export interface ResourcePolicyDiskConsistencyGroupPolicy {
        /**
         * Enable disk consistency on the resource policy.
         */
        enabled: boolean;
    }

    export interface ResourcePolicyGroupPlacementPolicy {
        /**
         * The number of availability domains instances will be spread across. If two instances are in different
         * availability domain, they will not be put in the same low latency network
         */
        availabilityDomainCount?: number;
        /**
         * Collocation specifies whether to place VMs inside the same availability domain on the same low-latency network.
         * Specify `COLLOCATED` to enable collocation. Can only be specified with `vmCount`. If compute instances are created
         * with a COLLOCATED policy, then exactly `vmCount` instances must be created at the same time with the resource policy
         * attached.
         * Possible values are: `COLLOCATED`.
         */
        collocation?: string;
        maxDistance?: number;
        /**
         * Number of VMs in this placement group. Google does not recommend that you use this field
         * unless you use a compact policy and you want your policy to work only if it contains this
         * exact number of VMs.
         */
        vmCount?: number;
    }

    export interface ResourcePolicyInstanceSchedulePolicy {
        /**
         * The expiration time of the schedule. The timestamp is an RFC3339 string.
         */
        expirationTime?: string;
        /**
         * The start time of the schedule. The timestamp is an RFC3339 string.
         */
        startTime?: string;
        /**
         * Specifies the time zone to be used in interpreting the schedule. The value of this field must be a time zone name
         * from the tz database: http://en.wikipedia.org/wiki/Tz_database.
         */
        timeZone: string;
        /**
         * Specifies the schedule for starting instances.
         * Structure is documented below.
         */
        vmStartSchedule?: outputs.compute.ResourcePolicyInstanceSchedulePolicyVmStartSchedule;
        /**
         * Specifies the schedule for stopping instances.
         * Structure is documented below.
         */
        vmStopSchedule?: outputs.compute.ResourcePolicyInstanceSchedulePolicyVmStopSchedule;
    }

    export interface ResourcePolicyInstanceSchedulePolicyVmStartSchedule {
        /**
         * Specifies the frequency for the operation, using the unix-cron format.
         */
        schedule: string;
    }

    export interface ResourcePolicyInstanceSchedulePolicyVmStopSchedule {
        /**
         * Specifies the frequency for the operation, using the unix-cron format.
         */
        schedule: string;
    }

    export interface ResourcePolicySnapshotSchedulePolicy {
        /**
         * Retention policy applied to snapshots created by this resource policy.
         * Structure is documented below.
         */
        retentionPolicy?: outputs.compute.ResourcePolicySnapshotSchedulePolicyRetentionPolicy;
        /**
         * Contains one of an `hourlySchedule`, `dailySchedule`, or `weeklySchedule`.
         * Structure is documented below.
         */
        schedule: outputs.compute.ResourcePolicySnapshotSchedulePolicySchedule;
        /**
         * Properties with which the snapshots are created, such as labels.
         * Structure is documented below.
         */
        snapshotProperties?: outputs.compute.ResourcePolicySnapshotSchedulePolicySnapshotProperties;
    }

    export interface ResourcePolicySnapshotSchedulePolicyRetentionPolicy {
        /**
         * Maximum age of the snapshot that is allowed to be kept.
         */
        maxRetentionDays: number;
        /**
         * Specifies the behavior to apply to scheduled snapshots when
         * the source disk is deleted.
         * Default value is `KEEP_AUTO_SNAPSHOTS`.
         * Possible values are: `KEEP_AUTO_SNAPSHOTS`, `APPLY_RETENTION_POLICY`.
         */
        onSourceDiskDelete?: string;
    }

    export interface ResourcePolicySnapshotSchedulePolicySchedule {
        /**
         * The policy will execute every nth day at the specified time.
         * Structure is documented below.
         */
        dailySchedule?: outputs.compute.ResourcePolicySnapshotSchedulePolicyScheduleDailySchedule;
        /**
         * The policy will execute every nth hour starting at the specified time.
         * Structure is documented below.
         */
        hourlySchedule?: outputs.compute.ResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule;
        /**
         * Allows specifying a snapshot time for each day of the week.
         * Structure is documented below.
         */
        weeklySchedule?: outputs.compute.ResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule;
    }

    export interface ResourcePolicySnapshotSchedulePolicyScheduleDailySchedule {
        /**
         * Defines a schedule with units measured in days. The value determines how many days pass between the start of each cycle. Days in cycle for snapshot schedule policy must be 1.
         */
        daysInCycle: number;
        /**
         * This must be in UTC format that resolves to one of
         * 00:00, 04:00, 08:00, 12:00, 16:00, or 20:00. For example,
         * both 13:00-5 and 08:00 are valid.
         */
        startTime: string;
    }

    export interface ResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule {
        /**
         * The number of hours between snapshots.
         */
        hoursInCycle: number;
        /**
         * Time within the window to start the operations.
         * It must be in an hourly format "HH:MM",
         * where HH : [00-23] and MM : [00] GMT.
         * eg: 21:00
         */
        startTime: string;
    }

    export interface ResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule {
        /**
         * May contain up to seven (one for each day of the week) snapshot times.
         * Structure is documented below.
         */
        dayOfWeeks: outputs.compute.ResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek[];
    }

    export interface ResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek {
        /**
         * The day of the week to create the snapshot. e.g. MONDAY
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        day: string;
        /**
         * Time within the window to start the operations.
         * It must be in format "HH:MM", where HH : [00-23] and MM : [00-00] GMT.
         */
        startTime: string;
    }

    export interface ResourcePolicySnapshotSchedulePolicySnapshotProperties {
        /**
         * Creates the new snapshot in the snapshot chain labeled with the
         * specified name. The chain name must be 1-63 characters long and comply
         * with RFC1035.
         */
        chainName?: string;
        /**
         * Whether to perform a 'guest aware' snapshot.
         */
        guestFlush?: boolean;
        /**
         * A set of key-value pairs.
         */
        labels?: {[key: string]: string};
        /**
         * Cloud Storage bucket location to store the auto snapshot
         * (regional or multi-regional)
         */
        storageLocations?: string;
    }

    export interface RouterBgp {
        /**
         * User-specified flag to indicate which mode to use for advertisement.
         * Default value is `DEFAULT`.
         * Possible values are: `DEFAULT`, `CUSTOM`.
         */
        advertiseMode?: string;
        /**
         * User-specified list of prefix groups to advertise in custom mode.
         * This field can only be populated if advertiseMode is CUSTOM and
         * is advertised to all peers of the router. These groups will be
         * advertised in addition to any specified prefixes. Leave this field
         * blank to advertise no custom groups.
         * This enum field has the one valid value: ALL_SUBNETS
         */
        advertisedGroups?: string[];
        /**
         * User-specified list of individual IP ranges to advertise in
         * custom mode. This field can only be populated if advertiseMode
         * is CUSTOM and is advertised to all peers of the router. These IP
         * ranges will be advertised in addition to any specified groups.
         * Leave this field blank to advertise no custom IP ranges.
         * Structure is documented below.
         */
        advertisedIpRanges?: outputs.compute.RouterBgpAdvertisedIpRange[];
        /**
         * Local BGP Autonomous System Number (ASN). Must be an RFC6996
         * private ASN, either 16-bit or 32-bit. The value will be fixed for
         * this router resource. All VPN tunnels that link to this router
         * will have the same local ASN.
         */
        asn: number;
        /**
         * The interval in seconds between BGP keepalive messages that are sent
         * to the peer. Hold time is three times the interval at which keepalive
         * messages are sent, and the hold time is the maximum number of seconds
         * allowed to elapse between successive keepalive messages that BGP
         * receives from a peer.
         * BGP will use the smaller of either the local hold time value or the
         * peer's hold time value as the hold time for the BGP connection
         * between the two peers. If set, this value must be between 20 and 60.
         * The default is 20.
         */
        keepaliveInterval?: number;
    }

    export interface RouterBgpAdvertisedIpRange {
        /**
         * User-specified description for the IP range.
         */
        description?: string;
        /**
         * The IP range to advertise. The value must be a
         * CIDR-formatted string.
         */
        range: string;
    }

    export interface RouterNatLogConfig {
        /**
         * Indicates whether or not to export logs.
         */
        enable: boolean;
        /**
         * Specifies the desired filtering of logs on this NAT.
         * Possible values are: `ERRORS_ONLY`, `TRANSLATIONS_ONLY`, `ALL`.
         */
        filter: string;
    }

    export interface RouterNatRule {
        /**
         * The action to be enforced for traffic that matches this rule.
         * Structure is documented below.
         */
        action: outputs.compute.RouterNatRuleAction;
        /**
         * An optional description of this rule.
         */
        description?: string;
        /**
         * CEL expression that specifies the match condition that egress traffic from a VM is evaluated against.
         * If it evaluates to true, the corresponding action is enforced.
         * The following examples are valid match expressions for public NAT:
         * "inIpRange(destination.ip, '1.1.0.0/16') || inIpRange(destination.ip, '2.2.0.0/16')"
         * "destination.ip == '1.1.0.1' || destination.ip == '8.8.8.8'"
         * The following example is a valid match expression for private NAT:
         * "nexthop.hub == 'https://networkconnectivity.googleapis.com/v1alpha1/projects/my-project/global/hub/hub-1'"
         */
        match: string;
        /**
         * An integer uniquely identifying a rule in the list.
         * The rule number must be a positive value between 0 and 65000, and must be unique among rules within a NAT.
         */
        ruleNumber: number;
    }

    export interface RouterNatRuleAction {
        /**
         * A list of URLs of the IP resources used for this NAT rule.
         * These IP addresses must be valid static external IP addresses assigned to the project.
         * This field is used for public NAT.
         */
        sourceNatActiveIps?: string[];
        /**
         * A list of URLs of the IP resources to be drained.
         * These IPs must be valid static external IPs that have been assigned to the NAT.
         * These IPs should be used for updating/patching a NAT rule only.
         * This field is used for public NAT.
         */
        sourceNatDrainIps?: string[];
    }

    export interface RouterNatSubnetwork {
        /**
         * Self-link of subnetwork to NAT
         */
        name: string;
        /**
         * List of the secondary ranges of the subnetwork that are allowed
         * to use NAT. This can be populated only if
         * `LIST_OF_SECONDARY_IP_RANGES` is one of the values in
         * sourceIpRangesToNat
         */
        secondaryIpRangeNames?: string[];
        /**
         * List of options for which source IPs in the subnetwork
         * should have NAT enabled. Supported values include:
         * `ALL_IP_RANGES`, `LIST_OF_SECONDARY_IP_RANGES`,
         * `PRIMARY_IP_RANGE`.
         */
        sourceIpRangesToNats: string[];
    }

    export interface RouterPeerAdvertisedIpRange {
        /**
         * User-specified description for the IP range.
         */
        description?: string;
        /**
         * The IP range to advertise. The value must be a
         * CIDR-formatted string.
         */
        range: string;
    }

    export interface RouterPeerBfd {
        /**
         * The minimum interval, in milliseconds, between BFD control packets
         * received from the peer router. The actual value is negotiated
         * between the two routers and is equal to the greater of this value
         * and the transmit interval of the other router. If set, this value
         * must be between 1000 and 30000.
         */
        minReceiveInterval?: number;
        /**
         * The minimum interval, in milliseconds, between BFD control packets
         * transmitted to the peer router. The actual value is negotiated
         * between the two routers and is equal to the greater of this value
         * and the corresponding receive interval of the other router. If set,
         * this value must be between 1000 and 30000.
         */
        minTransmitInterval?: number;
        /**
         * The number of consecutive BFD packets that must be missed before
         * BFD declares that a peer is unavailable. If set, the value must
         * be a value between 5 and 16.
         */
        multiplier?: number;
        /**
         * The BFD session initialization mode for this BGP peer.
         * If set to `ACTIVE`, the Cloud Router will initiate the BFD session
         * for this BGP peer. If set to `PASSIVE`, the Cloud Router will wait
         * for the peer router to initiate the BFD session for this BGP peer.
         * If set to `DISABLED`, BFD is disabled for this BGP peer.
         * Possible values are: `ACTIVE`, `DISABLED`, `PASSIVE`.
         */
        sessionInitializationMode: string;
    }

    export interface RouterStatusBestRoute {
        description: string;
        destRange: string;
        /**
         * The name of the router.
         */
        name: string;
        /**
         * The network name or resource link to the parent
         * network of this subnetwork.
         */
        network: string;
        nextHopGateway: string;
        nextHopIlb: string;
        nextHopInstance: string;
        nextHopInstanceZone: string;
        nextHopIp: string;
        nextHopNetwork: string;
        nextHopVpnTunnel: string;
        priority: number;
        /**
         * The ID of the project in which the resource
         * belongs. If it is not provided, the provider project is used.
         */
        project: string;
        selfLink: string;
        tags: string[];
    }

    export interface RouterStatusBestRoutesForRouter {
        description: string;
        destRange: string;
        /**
         * The name of the router.
         */
        name: string;
        /**
         * The network name or resource link to the parent
         * network of this subnetwork.
         */
        network: string;
        nextHopGateway: string;
        nextHopIlb: string;
        nextHopInstance: string;
        nextHopInstanceZone: string;
        nextHopIp: string;
        nextHopNetwork: string;
        nextHopVpnTunnel: string;
        priority: number;
        /**
         * The ID of the project in which the resource
         * belongs. If it is not provided, the provider project is used.
         */
        project: string;
        selfLink: string;
        tags: string[];
    }

    export interface SecurityPolicyAdaptiveProtectionConfig {
        /**
         * ) Configuration for [Automatically deploy Adaptive Protection suggested rules](https://cloud.google.com/armor/docs/adaptive-protection-auto-deploy?hl=en). Structure is documented below.
         *
         * <a name="nestedLayer7DdosDefenseConfig"></a>The `layer7DdosDefenseConfig` block supports:
         */
        autoDeployConfig?: outputs.compute.SecurityPolicyAdaptiveProtectionConfigAutoDeployConfig;
        /**
         * Configuration for [Google Cloud Armor Adaptive Protection Layer 7 DDoS Defense](https://cloud.google.com/armor/docs/adaptive-protection-overview?hl=en). Structure is documented below.
         */
        layer7DdosDefenseConfig?: outputs.compute.SecurityPolicyAdaptiveProtectionConfigLayer7DdosDefenseConfig;
    }

    export interface SecurityPolicyAdaptiveProtectionConfigAutoDeployConfig {
        /**
         * Rules are only automatically deployed for alerts on potential attacks with confidence scores greater than this threshold.
         */
        confidenceThreshold?: number;
        /**
         * Google Cloud Armor stops applying the action in the automatically deployed rule to an identified attacker after this duration. The rule continues to operate against new requests.
         */
        expirationSec?: number;
        /**
         * Rules are only automatically deployed when the estimated impact to baseline traffic from the suggested mitigation is below this threshold.
         */
        impactedBaselineThreshold?: number;
        /**
         * Identifies new attackers only when the load to the backend service that is under attack exceeds this threshold.
         */
        loadThreshold?: number;
    }

    export interface SecurityPolicyAdaptiveProtectionConfigLayer7DdosDefenseConfig {
        /**
         * If set to true, enables CAAP for L7 DDoS detection.
         */
        enable?: boolean;
        /**
         * Rule visibility can be one of the following: STANDARD - opaque rules. (default) PREMIUM - transparent rules.
         */
        ruleVisibility?: string;
    }

    export interface SecurityPolicyAdvancedOptionsConfig {
        /**
         * Custom configuration to apply the JSON parsing. Only applicable when
         * `jsonParsing` is set to `STANDARD`. Structure is documented below.
         */
        jsonCustomConfig: outputs.compute.SecurityPolicyAdvancedOptionsConfigJsonCustomConfig;
        /**
         * Whether or not to JSON parse the payload body. Defaults to `DISABLED`.
         * * DISABLED - Don't parse JSON payloads in POST bodies.
         * * STANDARD - Parse JSON payloads in POST bodies.
         */
        jsonParsing: string;
        /**
         * Log level to use. Defaults to `NORMAL`.
         * * NORMAL - Normal log level.
         * * VERBOSE - Verbose log level.
         */
        logLevel: string;
        /**
         * ) An optional list of case-insensitive request header names to use for resolving the callers client IP address.
         */
        userIpRequestHeaders?: string[];
    }

    export interface SecurityPolicyAdvancedOptionsConfigJsonCustomConfig {
        /**
         * A list of custom Content-Type header values to apply the JSON parsing. The
         * format of the Content-Type header values is defined in
         * [RFC 1341](https://www.ietf.org/rfc/rfc1341.txt). When configuring a custom Content-Type header
         * value, only the type/subtype needs to be specified, and the parameters should be excluded.
         */
        contentTypes: string[];
    }

    export interface SecurityPolicyRecaptchaOptionsConfig {
        /**
         * A field to supply a reCAPTCHA site key to be used for all the rules using the redirect action with the type of GOOGLE_RECAPTCHA under the security policy. The specified site key needs to be created from the reCAPTCHA API. The user is responsible for the validity of the specified site key. If not specified, a Google-managed site key is used.
         */
        redirectSiteKey: string;
    }

    export interface SecurityPolicyRule {
        /**
         * Action to take when `match` matches the request. Valid values:
         * * allow: allow access to target.
         * * deny(): deny access to target, returns the HTTP response code specified (valid values are 403, 404, and 502).
         * * rate_based_ban: limit client traffic to the configured threshold and ban the client if the traffic exceeds the threshold. Configure parameters for this action in RateLimitOptions. Requires rateLimitOptions to be set.
         * * redirect: redirect to a different target. This can either be an internal reCAPTCHA redirect, or an external URL-based redirect via a 302 response. Parameters for this action can be configured via redirectOptions.
         * * throttle: limit client traffic to the configured threshold. Configure parameters for this action in rateLimitOptions. Requires rateLimitOptions to be set for this.
         */
        action: string;
        /**
         * An optional description of this rule. Max size is 64.
         */
        description?: string;
        /**
         * Additional actions that are performed on headers. Structure is documented below.
         */
        headerAction?: outputs.compute.SecurityPolicyRuleHeaderAction;
        /**
         * A match condition that incoming traffic is evaluated against.
         * If it evaluates to true, the corresponding `action` is enforced. Structure is documented below.
         */
        match: outputs.compute.SecurityPolicyRuleMatch;
        /**
         * ) Preconfigured WAF configuration to be applied for the rule. If the rule does not evaluate preconfigured WAF rules, i.e., if evaluatePreconfiguredWaf() is not used, this field will have no effect. Structure is documented below.
         */
        preconfiguredWafConfig?: outputs.compute.SecurityPolicyRulePreconfiguredWafConfig;
        /**
         * When set to true, the `action` specified above is not enforced.
         * Stackdriver logs for requests that trigger a preview action are annotated as such.
         */
        preview: boolean;
        /**
         * An unique positive integer indicating the priority of evaluation for a rule.
         * Rules are evaluated from highest priority (lowest numerically) to lowest priority (highest numerically) in order.
         */
        priority: number;
        /**
         * Must be specified if the `action` is "rateBasedBan" or "throttle". Cannot be specified for other actions. Structure is documented below.
         */
        rateLimitOptions?: outputs.compute.SecurityPolicyRuleRateLimitOptions;
        /**
         * Can be specified if the `action` is "redirect". Cannot be specified for other actions. Structure is documented below.
         */
        redirectOptions?: outputs.compute.SecurityPolicyRuleRedirectOptions;
    }

    export interface SecurityPolicyRuleHeaderAction {
        /**
         * The list of request headers to add or overwrite if they're already present. Structure is documented below.
         */
        requestHeadersToAdds: outputs.compute.SecurityPolicyRuleHeaderActionRequestHeadersToAdd[];
    }

    export interface SecurityPolicyRuleHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to set.
         */
        headerName: string;
        /**
         * The value to set the named header to.
         */
        headerValue?: string;
    }

    export interface SecurityPolicyRuleMatch {
        /**
         * The configuration options available when specifying `versionedExpr`.
         * This field must be specified if `versionedExpr` is specified and cannot be specified if `versionedExpr` is not specified.
         * Structure is documented below.
         */
        config?: outputs.compute.SecurityPolicyRuleMatchConfig;
        /**
         * User defined CEVAL expression. A CEVAL expression is used to specify match criteria
         * such as origin.ip, source.region_code and contents in the request header.
         * Structure is documented below.
         */
        expr?: outputs.compute.SecurityPolicyRuleMatchExpr;
        /**
         * Predefined rule expression. If this field is specified, `config` must also be specified.
         * Available options:
         * * SRC_IPS_V1: Must specify the corresponding `srcIpRanges` field in `config`.
         */
        versionedExpr?: string;
    }

    export interface SecurityPolicyRuleMatchConfig {
        /**
         * Set of IP addresses or ranges (IPV4 or IPV6) in CIDR notation
         * to match against inbound traffic. There is a limit of 10 IP ranges per rule. A value of '\*' matches all IPs
         * (can be used to override the default behavior).
         */
        srcIpRanges: string[];
    }

    export interface SecurityPolicyRuleMatchExpr {
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         * The application context of the containing message determines which well-known feature set of CEL is supported.
         */
        expression: string;
    }

    export interface SecurityPolicyRulePreconfiguredWafConfig {
        /**
         * An exclusion to apply during preconfigured WAF evaluation. Structure is documented below.
         */
        exclusions?: outputs.compute.SecurityPolicyRulePreconfiguredWafConfigExclusion[];
    }

    export interface SecurityPolicyRulePreconfiguredWafConfigExclusion {
        /**
         * Request cookie whose value will be excluded from inspection during preconfigured WAF evaluation. Structure is documented below.
         */
        requestCookies?: outputs.compute.SecurityPolicyRulePreconfiguredWafConfigExclusionRequestCooky[];
        /**
         * Request header whose value will be excluded from inspection during preconfigured WAF evaluation. Structure is documented below.
         */
        requestHeaders?: outputs.compute.SecurityPolicyRulePreconfiguredWafConfigExclusionRequestHeader[];
        /**
         * Request URI from the request line to be excluded from inspection during preconfigured WAF evaluation. When specifying this field, the query or fragment part should be excluded. Structure is documented below.
         */
        requestQueryParams?: outputs.compute.SecurityPolicyRulePreconfiguredWafConfigExclusionRequestQueryParam[];
        /**
         * Request query parameter whose value will be excluded from inspection during preconfigured WAF evaluation. Note that the parameter can be in the query string or in the POST body. Structure is documented below.
         */
        requestUris?: outputs.compute.SecurityPolicyRulePreconfiguredWafConfigExclusionRequestUri[];
        /**
         * A list of target rule IDs under the WAF rule set to apply the preconfigured WAF exclusion. If omitted, it refers to all the rule IDs under the WAF rule set.
         *
         * <a name="nestedFieldParams"></a>The `requestHeader`, `requestCookie`, `requestUri` and `requestQueryParam` blocks support:
         */
        targetRuleIds?: string[];
        /**
         * Target WAF rule set to apply the preconfigured WAF exclusion.
         */
        targetRuleSet: string;
    }

    export interface SecurityPolicyRulePreconfiguredWafConfigExclusionRequestCooky {
        /**
         * You can specify an exact match or a partial match by using a field operator and a field value.
         *
         * * EQUALS: The operator matches if the field value equals the specified value.
         * * STARTS_WITH: The operator matches if the field value starts with the specified value.
         * * ENDS_WITH: The operator matches if the field value ends with the specified value.
         * * CONTAINS: The operator matches if the field value contains the specified value.
         * * EQUALS_ANY: The operator matches if the field value is any value.
         */
        operator: string;
        /**
         * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
         * The field value must be given if the field `operator` is not "EQUALS_ANY", and cannot be given if the field `operator` is "EQUALS_ANY".
         */
        value?: string;
    }

    export interface SecurityPolicyRulePreconfiguredWafConfigExclusionRequestHeader {
        /**
         * You can specify an exact match or a partial match by using a field operator and a field value.
         *
         * * EQUALS: The operator matches if the field value equals the specified value.
         * * STARTS_WITH: The operator matches if the field value starts with the specified value.
         * * ENDS_WITH: The operator matches if the field value ends with the specified value.
         * * CONTAINS: The operator matches if the field value contains the specified value.
         * * EQUALS_ANY: The operator matches if the field value is any value.
         */
        operator: string;
        /**
         * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
         * The field value must be given if the field `operator` is not "EQUALS_ANY", and cannot be given if the field `operator` is "EQUALS_ANY".
         */
        value?: string;
    }

    export interface SecurityPolicyRulePreconfiguredWafConfigExclusionRequestQueryParam {
        /**
         * You can specify an exact match or a partial match by using a field operator and a field value.
         *
         * * EQUALS: The operator matches if the field value equals the specified value.
         * * STARTS_WITH: The operator matches if the field value starts with the specified value.
         * * ENDS_WITH: The operator matches if the field value ends with the specified value.
         * * CONTAINS: The operator matches if the field value contains the specified value.
         * * EQUALS_ANY: The operator matches if the field value is any value.
         */
        operator: string;
        /**
         * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
         * The field value must be given if the field `operator` is not "EQUALS_ANY", and cannot be given if the field `operator` is "EQUALS_ANY".
         */
        value?: string;
    }

    export interface SecurityPolicyRulePreconfiguredWafConfigExclusionRequestUri {
        /**
         * You can specify an exact match or a partial match by using a field operator and a field value.
         *
         * * EQUALS: The operator matches if the field value equals the specified value.
         * * STARTS_WITH: The operator matches if the field value starts with the specified value.
         * * ENDS_WITH: The operator matches if the field value ends with the specified value.
         * * CONTAINS: The operator matches if the field value contains the specified value.
         * * EQUALS_ANY: The operator matches if the field value is any value.
         */
        operator: string;
        /**
         * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
         * The field value must be given if the field `operator` is not "EQUALS_ANY", and cannot be given if the field `operator` is "EQUALS_ANY".
         */
        value?: string;
    }

    export interface SecurityPolicyRuleRateLimitOptions {
        /**
         * Can only be specified if the `action` for the rule is "rateBasedBan".
         * If specified, determines the time (in seconds) the traffic will continue to be banned by the rate limit after the rate falls below the threshold.
         */
        banDurationSec?: number;
        /**
         * Can only be specified if the `action` for the rule is "rateBasedBan".
         * If specified, the key will be banned for the configured 'ban_duration_sec' when the number of requests that exceed the 'rate_limit_threshold' also
         * exceed this 'ban_threshold'. Structure is documented below.
         */
        banThreshold?: outputs.compute.SecurityPolicyRuleRateLimitOptionsBanThreshold;
        /**
         * Action to take for requests that are under the configured rate limit threshold. Valid option is "allow" only.
         */
        conformAction: string;
        /**
         * Determines the key to enforce the rateLimitThreshold on. If not specified, defaults to "ALL".
         *
         * * ALL: A single rate limit threshold is applied to all the requests matching this rule.
         * * IP: The source IP address of the request is the key. Each IP has this limit enforced separately.
         * * HTTP_HEADER: The value of the HTTP header whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the header value. If no such header is present in the request, the key type defaults to ALL.
         * * XFF_IP: The first IP address (i.e. the originating client IP address) specified in the list of IPs under X-Forwarded-For HTTP header. If no such header is present or the value is not a valid IP, the key type defaults to ALL.
         * * HTTP_COOKIE: The value of the HTTP cookie whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the cookie value. If no such cookie is present in the request, the key type defaults to ALL.
         * * HTTP_PATH: The URL path of the HTTP request. The key value is truncated to the first 128 bytes
         * * SNI: Server name indication in the TLS session of the HTTPS request. The key value is truncated to the first 128 bytes. The key type defaults to ALL on a HTTP session.
         * * REGION_CODE: The country/region from which the request originates.
         */
        enforceOnKey?: string;
        /**
         * ) If specified, any combination of values of enforce_on_key_type/enforce_on_key_name is treated as the key on which ratelimit threshold/action is enforced. You can specify up to 3 enforce_on_key_configs. If `enforceOnKeyConfigs` is specified, enforceOnKey must be set to an empty string. Structure is documented below.
         *
         * **Note:** To avoid the conflict between `enforceOnKey` and `enforceOnKeyConfigs`, the field `enforceOnKey` needs to be set to an empty string.
         */
        enforceOnKeyConfigs?: outputs.compute.SecurityPolicyRuleRateLimitOptionsEnforceOnKeyConfig[];
        /**
         * Rate limit key name applicable only for the following key types: HTTP_HEADER -- Name of the HTTP header whose value is taken as the key value. HTTP_COOKIE -- Name of the HTTP cookie whose value is taken as the key value.
         */
        enforceOnKeyName?: string;
        /**
         * When a request is denied, returns the HTTP response code specified.
         * Valid options are "deny()" where valid values for status are 403, 404, 429, and 502.
         */
        exceedAction: string;
        /**
         * Parameters defining the redirect action that is used as the exceed action. Cannot be specified if the exceed action is not redirect. Structure is documented below.
         *
         * <a name="nestedThreshold"></a>The `{ban/rate_limit}_threshold` block supports:
         */
        exceedRedirectOptions?: outputs.compute.SecurityPolicyRuleRateLimitOptionsExceedRedirectOptions;
        /**
         * Threshold at which to begin ratelimiting. Structure is documented below.
         */
        rateLimitThreshold: outputs.compute.SecurityPolicyRuleRateLimitOptionsRateLimitThreshold;
    }

    export interface SecurityPolicyRuleRateLimitOptionsBanThreshold {
        /**
         * Number of HTTP(S) requests for calculating the threshold.
         */
        count: number;
        /**
         * Interval over which the threshold is computed.
         */
        intervalSec: number;
    }

    export interface SecurityPolicyRuleRateLimitOptionsEnforceOnKeyConfig {
        /**
         * Rate limit key name applicable only for the following key types: HTTP_HEADER: Name of the HTTP header whose value is taken as the key value. HTTP_COOKIE: Name of the HTTP cookie whose value is taken as the key value.
         */
        enforceOnKeyName?: string;
        /**
         * Determines the key to enforce the rateLimitThreshold on. If not specified, defaults to "ALL".
         *
         * * ALL: A single rate limit threshold is applied to all the requests matching this rule.
         * * IP: The source IP address of the request is the key. Each IP has this limit enforced separately.
         * * HTTP_HEADER: The value of the HTTP header whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the header value. If no such header is present in the request, the key type defaults to ALL.
         * * XFF_IP: The first IP address (i.e. the originating client IP address) specified in the list of IPs under X-Forwarded-For HTTP header. If no such header is present or the value is not a valid IP, the key type defaults to ALL.
         * * HTTP_COOKIE: The value of the HTTP cookie whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the cookie value. If no such cookie is present in the request, the key type defaults to ALL.
         * * HTTP_PATH: The URL path of the HTTP request. The key value is truncated to the first 128 bytes
         * * SNI: Server name indication in the TLS session of the HTTPS request. The key value is truncated to the first 128 bytes. The key type defaults to ALL on a HTTP session.
         * * REGION_CODE: The country/region from which the request originates.
         */
        enforceOnKeyType?: string;
    }

    export interface SecurityPolicyRuleRateLimitOptionsExceedRedirectOptions {
        /**
         * Target for the redirect action. This is required if the type is EXTERNAL_302 and cannot be specified for GOOGLE_RECAPTCHA.
         */
        target?: string;
        /**
         * Type of the redirect action.
         */
        type: string;
    }

    export interface SecurityPolicyRuleRateLimitOptionsRateLimitThreshold {
        /**
         * Number of HTTP(S) requests for calculating the threshold.
         */
        count: number;
        /**
         * Interval over which the threshold is computed.
         */
        intervalSec: number;
    }

    export interface SecurityPolicyRuleRedirectOptions {
        /**
         * External redirection target when "EXTERNAL_302" is set in 'type'.
         */
        target?: string;
        /**
         * Type of redirect action.
         *
         * * EXTERNAL_302: Redirect to an external address, configured in 'target'.
         * * GOOGLE_RECAPTCHA: Redirect to Google reCAPTCHA.
         */
        type: string;
    }

    export interface SecurityScanConfigAuthentication {
        /**
         * Describes authentication configuration that uses a custom account.
         * Structure is documented below.
         */
        customAccount?: outputs.compute.SecurityScanConfigAuthenticationCustomAccount;
        /**
         * Describes authentication configuration that uses a Google account.
         * Structure is documented below.
         */
        googleAccount?: outputs.compute.SecurityScanConfigAuthenticationGoogleAccount;
    }

    export interface SecurityScanConfigAuthenticationCustomAccount {
        /**
         * The login form URL of the website.
         */
        loginUrl: string;
        /**
         * The password of the custom account. The credential is stored encrypted
         * in GCP.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * The user name of the custom account.
         */
        username: string;
    }

    export interface SecurityScanConfigAuthenticationGoogleAccount {
        /**
         * The password of the Google account. The credential is stored encrypted
         * in GCP.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * The user name of the Google account.
         */
        username: string;
    }

    export interface SecurityScanConfigSchedule {
        /**
         * The duration of time between executions in days
         */
        intervalDurationDays: number;
        /**
         * A timestamp indicates when the next run will be scheduled. The value is refreshed
         * by the server after each run. If unspecified, it will default to current server time,
         * which means the scan will be scheduled to start immediately.
         */
        scheduleTime?: string;
    }

    export interface ServiceAttachmentConnectedEndpoint {
        /**
         * (Output)
         * The URL of the consumer forwarding rule.
         */
        endpoint: string;
        /**
         * (Output)
         * The status of the connection from the consumer forwarding rule to
         * this service attachment.
         */
        status: string;
    }

    export interface ServiceAttachmentConsumerAcceptList {
        /**
         * The number of consumer forwarding rules the consumer project can
         * create.
         */
        connectionLimit: number;
        /**
         * A project that is allowed to connect to this service attachment.
         */
        projectIdOrNum: string;
    }

    export interface SnapshotIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface SnapshotIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface SnapshotSnapshotEncryptionKey {
        /**
         * The name of the encryption key that is stored in Google Cloud KMS.
         */
        kmsKeySelfLink?: string;
        /**
         * The service account used for the encryption request for the given KMS key.
         * If absent, the Compute Engine Service Agent service account is used.
         */
        kmsKeyServiceAccount?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        rawKey?: string;
        /**
         * (Output)
         * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
         * encryption key that protects this resource.
         */
        sha256: string;
    }

    export interface SnapshotSourceDiskEncryptionKey {
        /**
         * The service account used for the encryption request for the given KMS key.
         * If absent, the Compute Engine Service Agent service account is used.
         */
        kmsKeyServiceAccount?: string;
        /**
         * Specifies a 256-bit customer-supplied encryption key, encoded in
         * RFC 4648 base64 to either encrypt or decrypt this resource.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        rawKey?: string;
    }

    export interface SubnetworkIAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface SubnetworkIAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface SubnetworkLogConfig {
        /**
         * Can only be specified if VPC flow logging for this subnetwork is enabled.
         * Toggles the aggregation interval for collecting flow logs. Increasing the
         * interval time will reduce the amount of generated flow logs for long
         * lasting connections. Default is an interval of 5 seconds per connection.
         * Default value is `INTERVAL_5_SEC`.
         * Possible values are: `INTERVAL_5_SEC`, `INTERVAL_30_SEC`, `INTERVAL_1_MIN`, `INTERVAL_5_MIN`, `INTERVAL_10_MIN`, `INTERVAL_15_MIN`.
         */
        aggregationInterval?: string;
        /**
         * Export filter used to define which VPC flow logs should be logged, as as CEL expression. See
         * https://cloud.google.com/vpc/docs/flow-logs#filtering for details on how to format this field.
         * The default value is 'true', which evaluates to include everything.
         */
        filterExpr?: string;
        /**
         * Can only be specified if VPC flow logging for this subnetwork is enabled.
         * The value of the field must be in [0, 1]. Set the sampling rate of VPC
         * flow logs within the subnetwork where 1.0 means all collected logs are
         * reported and 0.0 means no logs are reported. Default is 0.5 which means
         * half of all collected logs are reported.
         */
        flowSampling?: number;
        /**
         * Can only be specified if VPC flow logging for this subnetwork is enabled.
         * Configures whether metadata fields should be added to the reported VPC
         * flow logs.
         * Default value is `INCLUDE_ALL_METADATA`.
         * Possible values are: `EXCLUDE_ALL_METADATA`, `INCLUDE_ALL_METADATA`, `CUSTOM_METADATA`.
         */
        metadata?: string;
        /**
         * List of metadata fields that should be added to reported logs.
         * Can only be specified if VPC flow logs for this subnetwork is enabled and "metadata" is set to CUSTOM_METADATA.
         */
        metadataFields?: string[];
    }

    export interface SubnetworkSecondaryIpRange {
        /**
         * The range of IP addresses belonging to this subnetwork secondary
         * range. Provide this property when you create the subnetwork.
         * Ranges must be unique and non-overlapping with all primary and
         * secondary IP ranges within a network. Only IPv4 is supported.
         */
        ipCidrRange: string;
        /**
         * The name associated with this subnetwork secondary range, used
         * when adding an alias IP range to a VM instance. The name must
         * be 1-63 characters long, and comply with RFC1035. The name
         * must be unique within the subnetwork.
         */
        rangeName: string;
    }

    export interface URLMapDefaultRouteAction {
        /**
         * The specification for allowing client side cross-origin requests. Please see
         * [W3C Recommendation for Cross Origin Resource Sharing](https://www.w3.org/TR/cors/)
         * Structure is documented below.
         */
        corsPolicy?: outputs.compute.URLMapDefaultRouteActionCorsPolicy;
        /**
         * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
         * As part of fault injection, when clients send requests to a backend service, delays can be introduced by Loadbalancer on a
         * percentage of requests before sending those request to the backend service. Similarly requests from clients can be aborted
         * by the Loadbalancer for a percentage of requests.
         * timeout and retryPolicy will be ignored by clients that are configured with a faultInjectionPolicy.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.compute.URLMapDefaultRouteActionFaultInjectionPolicy;
        /**
         * Specifies the policy on how requests intended for the route's backends are shadowed to a separate mirrored backend service.
         * Loadbalancer does not wait for responses from the shadow service. Prior to sending traffic to the shadow service,
         * the host / authority header is suffixed with -shadow.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.compute.URLMapDefaultRouteActionRequestMirrorPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.compute.URLMapDefaultRouteActionRetryPolicy;
        /**
         * Specifies the timeout for the selected route. Timeout is computed from the time the request has been
         * fully processed (i.e. end-of-stream) up until the response has been completely processed. Timeout includes all retries.
         * If not specified, will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        timeout: outputs.compute.URLMapDefaultRouteActionTimeout;
        /**
         * The spec to modify the URL of the request, prior to forwarding the request to the matched service.
         * Structure is documented below.
         */
        urlRewrite?: outputs.compute.URLMapDefaultRouteActionUrlRewrite;
        /**
         * A list of weighted backend services to send traffic to when a route match occurs.
         * The weights determine the fraction of traffic that flows to their corresponding backend service.
         * If all traffic needs to go to a single backend service, there must be one weightedBackendService
         * with weight set to a non 0 number.
         * Once a backendService is identified and before forwarding the request to the backend service,
         * advanced routing actions like Url rewrites and header transformations are applied depending on
         * additional settings specified in this HttpRouteAction.
         * Structure is documented below.
         */
        weightedBackendServices?: outputs.compute.URLMapDefaultRouteActionWeightedBackendService[];
    }

    export interface URLMapDefaultRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
         * This translates to the Access-Control-Allow-Credentials header.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regular expression patterns that match allowed origins. For regular expression grammar
         * please see en.cppreference.com/w/cpp/regex/ecmascript
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOrigins?: string[];
        /**
         * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
         */
        disabled?: boolean;
        /**
         * Specifies the content for the Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached in seconds.
         * This translates to the Access-Control-Max-Age header.
         */
        maxAge?: number;
    }

    export interface URLMapDefaultRouteActionFaultInjectionPolicy {
        /**
         * The specification for how client requests are aborted as part of fault injection.
         * Structure is documented below.
         */
        abort?: outputs.compute.URLMapDefaultRouteActionFaultInjectionPolicyAbort;
        /**
         * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
         * Structure is documented below.
         */
        delay?: outputs.compute.URLMapDefaultRouteActionFaultInjectionPolicyDelay;
    }

    export interface URLMapDefaultRouteActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         * The value must be between 200 and 599 inclusive.
         */
        httpStatus?: number;
        /**
         * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface URLMapDefaultRouteActionFaultInjectionPolicyDelay {
        /**
         * Specifies the value of the fixed delay interval.
         * Structure is documented below.
         */
        fixedDelay?: outputs.compute.URLMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay;
        /**
         * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface URLMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface URLMapDefaultRouteActionRequestMirrorPolicy {
        /**
         * The full or partial URL to the BackendService resource being mirrored to.
         */
        backendService: string;
    }

    export interface URLMapDefaultRouteActionRetryPolicy {
        /**
         * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
         */
        numRetries?: number;
        /**
         * Specifies a non-zero timeout per retry attempt.
         * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
         * will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        perTryTimeout?: outputs.compute.URLMapDefaultRouteActionRetryPolicyPerTryTimeout;
        /**
         * Specfies one or more conditions when this retry rule applies. Valid values are:
         * * 5xx: Loadbalancer will attempt a retry if the backend service responds with any 5xx response code,
         * or if the backend service does not respond at all, example: disconnects, reset, read timeout,
         * * connection failure, and refused streams.
         * * gateway-error: Similar to 5xx, but only applies to response codes 502, 503 or 504.
         * * connect-failure: Loadbalancer will retry on failures connecting to backend services,
         * for example due to connection timeouts.
         * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
         * Currently the only retriable error supported is 409.
         * * refused-stream:Loadbalancer will retry if the backend service resets the stream with a REFUSED_STREAM error code.
         * This reset type indicates that it is safe to retry.
         * * cancelled: Loadbalancer will retry if the gRPC status code in the response header is set to cancelled
         * * deadline-exceeded: Loadbalancer will retry if the gRPC status code in the response header is set to deadline-exceeded
         * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response header is set to resource-exhausted
         * * unavailable: Loadbalancer will retry if the gRPC status code in the response header is set to unavailable
         */
        retryConditions?: string[];
    }

    export interface URLMapDefaultRouteActionRetryPolicyPerTryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface URLMapDefaultRouteActionTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface URLMapDefaultRouteActionUrlRewrite {
        /**
         * Prior to forwarding the request to the selected service, the request's host header is replaced
         * with contents of hostRewrite.
         * The value must be between 1 and 255 characters.
         */
        hostRewrite?: string;
        /**
         * Prior to forwarding the request to the selected backend service, the matching portion of the
         * request's path is replaced by pathPrefixRewrite.
         * The value must be between 1 and 1024 characters.
         */
        pathPrefixRewrite?: string;
    }

    export interface URLMapDefaultRouteActionWeightedBackendService {
        /**
         * The full or partial URL to the default BackendService resource. Before forwarding the
         * request to backendService, the loadbalancer applies any relevant headerActions
         * specified as part of this backendServiceWeight.
         */
        backendService?: string;
        /**
         * Specifies changes to request and response headers that need to take effect for
         * the selected backendService.
         * headerAction specified here take effect before headerAction in the enclosing
         * HttpRouteRule, PathMatcher and UrlMap.
         * Structure is documented below.
         */
        headerAction?: outputs.compute.URLMapDefaultRouteActionWeightedBackendServiceHeaderAction;
        /**
         * Specifies the fraction of traffic sent to backendService, computed as
         * weight / (sum of all weightedBackendService weights in routeAction) .
         * The selection of a backend service is determined only for new traffic. Once a user's request
         * has been directed to a backendService, subsequent requests will be sent to the same backendService
         * as determined by the BackendService's session affinity policy.
         * The value must be between 0 and 1000
         */
        weight?: number;
    }

    export interface URLMapDefaultRouteActionWeightedBackendServiceHeaderAction {
        /**
         * Headers to add to a matching request prior to forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.URLMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to
         * forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.URLMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response prior to sending the
         * response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface URLMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName?: string;
        /**
         * The value of the header to add.
         */
        headerValue?: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace?: boolean;
    }

    export interface URLMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName?: string;
        /**
         * The value of the header to add.
         */
        headerValue?: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace?: boolean;
    }

    export interface URLMapDefaultUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was
         * supplied in the request. The value must be between 1 and 255 characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https. If set to
         * false, the URL scheme of the redirected request will remain the same as that of the
         * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
         * true for TargetHttpsProxy is not permitted. The default is set to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was
         * supplied in the request. pathRedirect cannot be supplied together with
         * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
         * original request will be used for the redirect. The value must be between 1 and 1024
         * characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
         * retaining the remaining portion of the URL before redirecting the request.
         * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
         * neither. If neither is supplied, the path of the original request will be used for
         * the redirect. The value must be between 1 and 1024 characters.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
         * will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
         * the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed prior
         * to redirecting the request. If set to false, the query portion of the original URL is
         * retained. The default is set to false.
         * This field is required to ensure an empty block is not set. The normal default value is false.
         */
        stripQuery: boolean;
    }

    export interface URLMapHeaderAction {
        /**
         * Headers to add to a matching request prior to forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.URLMapHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to
         * forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.URLMapHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response prior to sending the
         * response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface URLMapHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapHostRule {
        /**
         * An optional description of this resource. Provide this property when you create
         * the resource.
         */
        description?: string;
        /**
         * The list of host patterns to match. They must be valid hostnames, except * will
         * match any string of ([a-z0-9-.]*). In that case, * must be the first character
         * and must be followed in the pattern by either - or ..
         */
        hosts: string[];
        /**
         * The name of the PathMatcher to use to match the path portion of the URL if the
         * hostRule matches the URL's host portion.
         */
        pathMatcher: string;
    }

    export interface URLMapPathMatcher {
        /**
         * defaultRouteAction takes effect when none of the pathRules or routeRules match. The load balancer performs
         * advanced routing actions like URL rewrites, header transformations, etc. prior to forwarding the request
         * to the selected backend. If defaultRouteAction specifies any weightedBackendServices, defaultService must not be set.
         * Conversely if defaultService is set, defaultRouteAction cannot contain any weightedBackendServices.
         * Only one of defaultRouteAction or defaultUrlRedirect must be set.
         * Structure is documented below.
         */
        defaultRouteAction?: outputs.compute.URLMapPathMatcherDefaultRouteAction;
        /**
         * The backend service or backend bucket to use when none of the given paths match.
         */
        defaultService?: string;
        /**
         * When none of the specified hostRules match, the request is redirected to a URL specified
         * by defaultUrlRedirect. If defaultUrlRedirect is specified, defaultService or
         * defaultRouteAction must not be set.
         * Structure is documented below.
         */
        defaultUrlRedirect?: outputs.compute.URLMapPathMatcherDefaultUrlRedirect;
        /**
         * An optional description of this resource. Provide this property when you create
         * the resource.
         */
        description?: string;
        /**
         * Specifies changes to request and response headers that need to take effect for
         * the selected backendService. HeaderAction specified here are applied after the
         * matching HttpRouteRule HeaderAction and before the HeaderAction in the UrlMap
         * Structure is documented below.
         */
        headerAction?: outputs.compute.URLMapPathMatcherHeaderAction;
        /**
         * The name to which this PathMatcher is referred by the HostRule.
         */
        name: string;
        /**
         * The list of path rules. Use this list instead of routeRules when routing based
         * on simple path matching is all that's required. The order by which path rules
         * are specified does not matter. Matches are always done on the longest-path-first
         * basis. For example: a pathRule with a path /a/b/c/* will match before /a/b/*
         * irrespective of the order in which those paths appear in this list. Within a
         * given pathMatcher, only one of pathRules or routeRules must be set.
         * Structure is documented below.
         */
        pathRules?: outputs.compute.URLMapPathMatcherPathRule[];
        /**
         * The list of ordered HTTP route rules. Use this list instead of pathRules when
         * advanced route matching and routing actions are desired. The order of specifying
         * routeRules matters: the first rule that matches will cause its specified routing
         * action to take effect. Within a given pathMatcher, only one of pathRules or
         * routeRules must be set. routeRules are not supported in UrlMaps intended for
         * External load balancers.
         * Structure is documented below.
         */
        routeRules?: outputs.compute.URLMapPathMatcherRouteRule[];
    }

    export interface URLMapPathMatcherDefaultRouteAction {
        /**
         * The specification for allowing client side cross-origin requests. Please see
         * [W3C Recommendation for Cross Origin Resource Sharing](https://www.w3.org/TR/cors/)
         * Structure is documented below.
         */
        corsPolicy?: outputs.compute.URLMapPathMatcherDefaultRouteActionCorsPolicy;
        /**
         * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
         * As part of fault injection, when clients send requests to a backend service, delays can be introduced by Loadbalancer on a
         * percentage of requests before sending those request to the backend service. Similarly requests from clients can be aborted
         * by the Loadbalancer for a percentage of requests.
         * timeout and retryPolicy will be ignored by clients that are configured with a faultInjectionPolicy.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.compute.URLMapPathMatcherDefaultRouteActionFaultInjectionPolicy;
        /**
         * Specifies the policy on how requests intended for the route's backends are shadowed to a separate mirrored backend service.
         * Loadbalancer does not wait for responses from the shadow service. Prior to sending traffic to the shadow service,
         * the host / authority header is suffixed with -shadow.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.compute.URLMapPathMatcherDefaultRouteActionRequestMirrorPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.compute.URLMapPathMatcherDefaultRouteActionRetryPolicy;
        /**
         * Specifies the timeout for the selected route. Timeout is computed from the time the request has been
         * fully processed (i.e. end-of-stream) up until the response has been completely processed. Timeout includes all retries.
         * If not specified, will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        timeout: outputs.compute.URLMapPathMatcherDefaultRouteActionTimeout;
        /**
         * The spec to modify the URL of the request, prior to forwarding the request to the matched service.
         * Structure is documented below.
         */
        urlRewrite?: outputs.compute.URLMapPathMatcherDefaultRouteActionUrlRewrite;
        /**
         * A list of weighted backend services to send traffic to when a route match occurs.
         * The weights determine the fraction of traffic that flows to their corresponding backend service.
         * If all traffic needs to go to a single backend service, there must be one weightedBackendService
         * with weight set to a non 0 number.
         * Once a backendService is identified and before forwarding the request to the backend service,
         * advanced routing actions like Url rewrites and header transformations are applied depending on
         * additional settings specified in this HttpRouteAction.
         * Structure is documented below.
         */
        weightedBackendServices?: outputs.compute.URLMapPathMatcherDefaultRouteActionWeightedBackendService[];
    }

    export interface URLMapPathMatcherDefaultRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
         * This translates to the Access-Control-Allow-Credentials header.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regular expression patterns that match allowed origins. For regular expression grammar
         * please see en.cppreference.com/w/cpp/regex/ecmascript
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOrigins?: string[];
        /**
         * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
         */
        disabled?: boolean;
        /**
         * Specifies the content for the Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached in seconds.
         * This translates to the Access-Control-Max-Age header.
         */
        maxAge?: number;
    }

    export interface URLMapPathMatcherDefaultRouteActionFaultInjectionPolicy {
        /**
         * The specification for how client requests are aborted as part of fault injection.
         * Structure is documented below.
         */
        abort?: outputs.compute.URLMapPathMatcherDefaultRouteActionFaultInjectionPolicyAbort;
        /**
         * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
         * Structure is documented below.
         */
        delay?: outputs.compute.URLMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelay;
    }

    export interface URLMapPathMatcherDefaultRouteActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         * The value must be between 200 and 599 inclusive.
         */
        httpStatus?: number;
        /**
         * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface URLMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelay {
        /**
         * Specifies the value of the fixed delay interval.
         * Structure is documented below.
         */
        fixedDelay?: outputs.compute.URLMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelayFixedDelay;
        /**
         * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface URLMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelayFixedDelay {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface URLMapPathMatcherDefaultRouteActionRequestMirrorPolicy {
        /**
         * The full or partial URL to the BackendService resource being mirrored to.
         */
        backendService: string;
    }

    export interface URLMapPathMatcherDefaultRouteActionRetryPolicy {
        /**
         * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
         */
        numRetries?: number;
        /**
         * Specifies a non-zero timeout per retry attempt.
         * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
         * will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        perTryTimeout?: outputs.compute.URLMapPathMatcherDefaultRouteActionRetryPolicyPerTryTimeout;
        /**
         * Specfies one or more conditions when this retry rule applies. Valid values are:
         * * 5xx: Loadbalancer will attempt a retry if the backend service responds with any 5xx response code,
         * or if the backend service does not respond at all, example: disconnects, reset, read timeout,
         * * connection failure, and refused streams.
         * * gateway-error: Similar to 5xx, but only applies to response codes 502, 503 or 504.
         * * connect-failure: Loadbalancer will retry on failures connecting to backend services,
         * for example due to connection timeouts.
         * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
         * Currently the only retriable error supported is 409.
         * * refused-stream:Loadbalancer will retry if the backend service resets the stream with a REFUSED_STREAM error code.
         * This reset type indicates that it is safe to retry.
         * * cancelled: Loadbalancer will retry if the gRPC status code in the response header is set to cancelled
         * * deadline-exceeded: Loadbalancer will retry if the gRPC status code in the response header is set to deadline-exceeded
         * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response header is set to resource-exhausted
         * * unavailable: Loadbalancer will retry if the gRPC status code in the response header is set to unavailable
         */
        retryConditions?: string[];
    }

    export interface URLMapPathMatcherDefaultRouteActionRetryPolicyPerTryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface URLMapPathMatcherDefaultRouteActionTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds?: string;
    }

    export interface URLMapPathMatcherDefaultRouteActionUrlRewrite {
        /**
         * Prior to forwarding the request to the selected service, the request's host header is replaced
         * with contents of hostRewrite.
         * The value must be between 1 and 255 characters.
         */
        hostRewrite?: string;
        /**
         * Prior to forwarding the request to the selected backend service, the matching portion of the
         * request's path is replaced by pathPrefixRewrite.
         * The value must be between 1 and 1024 characters.
         */
        pathPrefixRewrite?: string;
    }

    export interface URLMapPathMatcherDefaultRouteActionWeightedBackendService {
        /**
         * The full or partial URL to the default BackendService resource. Before forwarding the
         * request to backendService, the loadbalancer applies any relevant headerActions
         * specified as part of this backendServiceWeight.
         */
        backendService?: string;
        /**
         * Specifies changes to request and response headers that need to take effect for
         * the selected backendService.
         * headerAction specified here take effect before headerAction in the enclosing
         * HttpRouteRule, PathMatcher and UrlMap.
         * Structure is documented below.
         */
        headerAction?: outputs.compute.URLMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderAction;
        /**
         * Specifies the fraction of traffic sent to backendService, computed as
         * weight / (sum of all weightedBackendService weights in routeAction) .
         * The selection of a backend service is determined only for new traffic. Once a user's request
         * has been directed to a backendService, subsequent requests will be sent to the same backendService
         * as determined by the BackendService's session affinity policy.
         * The value must be between 0 and 1000
         */
        weight?: number;
    }

    export interface URLMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderAction {
        /**
         * Headers to add to a matching request prior to forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.URLMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to
         * forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.URLMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response prior to sending the
         * response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface URLMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName?: string;
        /**
         * The value of the header to add.
         */
        headerValue?: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace?: boolean;
    }

    export interface URLMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName?: string;
        /**
         * The value of the header to add.
         */
        headerValue?: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace?: boolean;
    }

    export interface URLMapPathMatcherDefaultUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was
         * supplied in the request. The value must be between 1 and 255 characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https. If set to
         * false, the URL scheme of the redirected request will remain the same as that of the
         * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
         * true for TargetHttpsProxy is not permitted. The default is set to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was
         * supplied in the request. pathRedirect cannot be supplied together with
         * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
         * original request will be used for the redirect. The value must be between 1 and 1024
         * characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
         * retaining the remaining portion of the URL before redirecting the request.
         * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
         * neither. If neither is supplied, the path of the original request will be used for
         * the redirect. The value must be between 1 and 1024 characters.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
         * will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
         * the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed prior
         * to redirecting the request. If set to false, the query portion of the original URL is
         * retained. The default is set to false.
         * This field is required to ensure an empty block is not set. The normal default value is false.
         */
        stripQuery: boolean;
    }

    export interface URLMapPathMatcherHeaderAction {
        /**
         * Headers to add to a matching request prior to forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.URLMapPathMatcherHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to
         * forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.URLMapPathMatcherHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response prior to sending the
         * response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface URLMapPathMatcherHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherPathRule {
        /**
         * The list of path patterns to match. Each must start with / and the only place a
         * \* is allowed is at the end following a /. The string fed to the path matcher
         * does not include any text after the first ? or #, and those chars are not
         * allowed here.
         */
        paths: string[];
        /**
         * In response to a matching path, the load balancer performs advanced routing
         * actions like URL rewrites, header transformations, etc. prior to forwarding the
         * request to the selected backend. If routeAction specifies any
         * weightedBackendServices, service must not be set. Conversely if service is set,
         * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
         * or urlRedirect must be set.
         * Structure is documented below.
         */
        routeAction?: outputs.compute.URLMapPathMatcherPathRuleRouteAction;
        /**
         * The backend service or backend bucket to use if any of the given paths match.
         */
        service?: string;
        /**
         * When a path pattern is matched, the request is redirected to a URL specified
         * by urlRedirect. If urlRedirect is specified, service or routeAction must not
         * be set.
         * Structure is documented below.
         */
        urlRedirect?: outputs.compute.URLMapPathMatcherPathRuleUrlRedirect;
    }

    export interface URLMapPathMatcherPathRuleRouteAction {
        /**
         * The specification for allowing client side cross-origin requests. Please see W3C
         * Recommendation for Cross Origin Resource Sharing
         * Structure is documented below.
         */
        corsPolicy?: outputs.compute.URLMapPathMatcherPathRuleRouteActionCorsPolicy;
        /**
         * The specification for fault injection introduced into traffic to test the
         * resiliency of clients to backend service failure. As part of fault injection,
         * when clients send requests to a backend service, delays can be introduced by
         * Loadbalancer on a percentage of requests before sending those request to the
         * backend service. Similarly requests from clients can be aborted by the
         * Loadbalancer for a percentage of requests. timeout and retryPolicy will be
         * ignored by clients that are configured with a fault_injection_policy.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.compute.URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicy;
        /**
         * Specifies the policy on how requests intended for the route's backends are
         * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
         * responses from the shadow service. Prior to sending traffic to the shadow
         * service, the host / authority header is suffixed with -shadow.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.compute.URLMapPathMatcherPathRuleRouteActionRequestMirrorPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.compute.URLMapPathMatcherPathRuleRouteActionRetryPolicy;
        /**
         * Specifies the timeout for the selected route. Timeout is computed from the time
         * the request is has been fully processed (i.e. end-of-stream) up until the
         * response has been completely processed. Timeout includes all retries. If not
         * specified, the default value is 15 seconds.
         * Structure is documented below.
         */
        timeout?: outputs.compute.URLMapPathMatcherPathRuleRouteActionTimeout;
        /**
         * The spec to modify the URL of the request, prior to forwarding the request to
         * the matched service
         * Structure is documented below.
         */
        urlRewrite?: outputs.compute.URLMapPathMatcherPathRuleRouteActionUrlRewrite;
        /**
         * A list of weighted backend services to send traffic to when a route match
         * occurs. The weights determine the fraction of traffic that flows to their
         * corresponding backend service. If all traffic needs to go to a single backend
         * service, there must be one  weightedBackendService with weight set to a non 0
         * number. Once a backendService is identified and before forwarding the request to
         * the backend service, advanced routing actions like Url rewrites and header
         * transformations are applied depending on additional settings specified in this
         * HttpRouteAction.
         * Structure is documented below.
         */
        weightedBackendServices?: outputs.compute.URLMapPathMatcherPathRuleRouteActionWeightedBackendService[];
    }

    export interface URLMapPathMatcherPathRuleRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
         * This translates to the Access-Control-Allow-Credentials header.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regular expression patterns that match allowed origins. For regular expression grammar
         * please see en.cppreference.com/w/cpp/regex/ecmascript
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOrigins?: string[];
        /**
         * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
         */
        disabled: boolean;
        /**
         * Specifies the content for the Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached in seconds.
         * This translates to the Access-Control-Max-Age header.
         */
        maxAge?: number;
    }

    export interface URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicy {
        /**
         * The specification for how client requests are aborted as part of fault injection.
         * Structure is documented below.
         */
        abort?: outputs.compute.URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort;
        /**
         * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
         * Structure is documented below.
         */
        delay?: outputs.compute.URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay;
    }

    export interface URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         * The value must be between 200 and 599 inclusive.
         */
        httpStatus: number;
        /**
         * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage: number;
    }

    export interface URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay {
        /**
         * Specifies the value of the fixed delay interval.
         * Structure is documented below.
         */
        fixedDelay: outputs.compute.URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
        /**
         * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage: number;
    }

    export interface URLMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface URLMapPathMatcherPathRuleRouteActionRequestMirrorPolicy {
        /**
         * The full or partial URL to the BackendService resource being mirrored to.
         */
        backendService: string;
    }

    export interface URLMapPathMatcherPathRuleRouteActionRetryPolicy {
        /**
         * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
         */
        numRetries?: number;
        /**
         * Specifies a non-zero timeout per retry attempt.
         * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
         * will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        perTryTimeout?: outputs.compute.URLMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout;
        /**
         * Specfies one or more conditions when this retry rule applies. Valid values are:
         * * 5xx: Loadbalancer will attempt a retry if the backend service responds with any 5xx response code,
         * or if the backend service does not respond at all, example: disconnects, reset, read timeout,
         * * connection failure, and refused streams.
         * * gateway-error: Similar to 5xx, but only applies to response codes 502, 503 or 504.
         * * connect-failure: Loadbalancer will retry on failures connecting to backend services,
         * for example due to connection timeouts.
         * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
         * Currently the only retriable error supported is 409.
         * * refused-stream:Loadbalancer will retry if the backend service resets the stream with a REFUSED_STREAM error code.
         * This reset type indicates that it is safe to retry.
         * * cancelled: Loadbalancer will retry if the gRPC status code in the response header is set to cancelled
         * * deadline-exceeded: Loadbalancer will retry if the gRPC status code in the response header is set to deadline-exceeded
         * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response header is set to resource-exhausted
         * * unavailable: Loadbalancer will retry if the gRPC status code in the response header is set to unavailable
         */
        retryConditions?: string[];
    }

    export interface URLMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface URLMapPathMatcherPathRuleRouteActionTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface URLMapPathMatcherPathRuleRouteActionUrlRewrite {
        /**
         * Prior to forwarding the request to the selected service, the request's host header is replaced
         * with contents of hostRewrite.
         * The value must be between 1 and 255 characters.
         */
        hostRewrite?: string;
        /**
         * Prior to forwarding the request to the selected backend service, the matching portion of the
         * request's path is replaced by pathPrefixRewrite.
         * The value must be between 1 and 1024 characters.
         */
        pathPrefixRewrite?: string;
    }

    export interface URLMapPathMatcherPathRuleRouteActionWeightedBackendService {
        /**
         * The full or partial URL to the default BackendService resource. Before forwarding the
         * request to backendService, the loadbalancer applies any relevant headerActions
         * specified as part of this backendServiceWeight.
         */
        backendService: string;
        /**
         * Specifies changes to request and response headers that need to take effect for
         * the selected backendService.
         * headerAction specified here take effect before headerAction in the enclosing
         * HttpRouteRule, PathMatcher and UrlMap.
         * Structure is documented below.
         */
        headerAction?: outputs.compute.URLMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction;
        /**
         * Specifies the fraction of traffic sent to backendService, computed as
         * weight / (sum of all weightedBackendService weights in routeAction) .
         * The selection of a backend service is determined only for new traffic. Once a user's request
         * has been directed to a backendService, subsequent requests will be sent to the same backendService
         * as determined by the BackendService's session affinity policy.
         * The value must be between 0 and 1000
         */
        weight: number;
    }

    export interface URLMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction {
        /**
         * Headers to add to a matching request prior to forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.URLMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to
         * forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.URLMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response prior to sending the
         * response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface URLMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherPathRuleUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was
         * supplied in the request. The value must be between 1 and 255 characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https. If set
         * to false, the URL scheme of the redirected request will remain the same as that
         * of the request. This must only be set for UrlMaps used in TargetHttpProxys.
         * Setting this true for TargetHttpsProxy is not permitted. Defaults to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was
         * supplied in the request. Only one of pathRedirect or prefixRedirect must be
         * specified. The value must be between 1 and 1024 characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
         * retaining the remaining portion of the URL before redirecting the request.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case, the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed
         * prior to redirecting the request. If set to false, the query portion of the
         * original URL is retained. Defaults to false.
         */
        stripQuery: boolean;
    }

    export interface URLMapPathMatcherRouteRule {
        /**
         * Specifies changes to request and response headers that need to take effect for
         * the selected backendService. The headerAction specified here are applied before
         * the matching pathMatchers[].headerAction and after pathMatchers[].routeRules[].r
         * outeAction.weightedBackendService.backendServiceWeightAction[].headerAction
         * Structure is documented below.
         */
        headerAction?: outputs.compute.URLMapPathMatcherRouteRuleHeaderAction;
        /**
         * The rules for determining a match.
         * Structure is documented below.
         */
        matchRules?: outputs.compute.URLMapPathMatcherRouteRuleMatchRule[];
        /**
         * For routeRules within a given pathMatcher, priority determines the order
         * in which load balancer will interpret routeRules. RouteRules are evaluated
         * in order of priority, from the lowest to highest number. The priority of
         * a rule decreases as its number increases (1, 2, 3, N+1). The first rule
         * that matches the request is applied.
         * You cannot configure two or more routeRules with the same priority.
         * Priority for each rule must be set to a number between 0 and
         * 2147483647 inclusive.
         * Priority numbers can have gaps, which enable you to add or remove rules
         * in the future without affecting the rest of the rules. For example,
         * 1, 2, 3, 4, 5, 9, 12, 16 is a valid series of priority numbers to which
         * you could add rules numbered from 6 to 8, 10 to 11, and 13 to 15 in the
         * future without any impact on existing rules.
         */
        priority: number;
        /**
         * In response to a matching matchRule, the load balancer performs advanced routing
         * actions like URL rewrites, header transformations, etc. prior to forwarding the
         * request to the selected backend. If  routeAction specifies any
         * weightedBackendServices, service must not be set. Conversely if service is set,
         * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
         * or urlRedirect must be set.
         * Structure is documented below.
         */
        routeAction?: outputs.compute.URLMapPathMatcherRouteRuleRouteAction;
        /**
         * The backend service resource to which traffic is
         * directed if this rule is matched. If routeAction is additionally specified,
         * advanced routing actions like URL Rewrites, etc. take effect prior to sending
         * the request to the backend. However, if service is specified, routeAction cannot
         * contain any weightedBackendService s. Conversely, if routeAction specifies any
         * weightedBackendServices, service must not be specified. Only one of urlRedirect,
         * service or routeAction.weightedBackendService must be set.
         */
        service?: string;
        /**
         * When this rule is matched, the request is redirected to a URL specified by
         * urlRedirect. If urlRedirect is specified, service or routeAction must not be
         * set.
         * Structure is documented below.
         */
        urlRedirect?: outputs.compute.URLMapPathMatcherRouteRuleUrlRedirect;
    }

    export interface URLMapPathMatcherRouteRuleHeaderAction {
        /**
         * Headers to add to a matching request prior to forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.URLMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to
         * forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.URLMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response prior to sending the
         * response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface URLMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherRouteRuleMatchRule {
        /**
         * For satisfying the matchRule condition, the path of the request must exactly
         * match the value specified in fullPathMatch after removing any query parameters
         * and anchor that may be part of the original URL. FullPathMatch must be between 1
         * and 1024 characters. Only one of prefixMatch, fullPathMatch or regexMatch must
         * be specified.
         */
        fullPathMatch?: string;
        /**
         * Specifies a list of header match criteria, all of which must match corresponding
         * headers in the request.
         * Structure is documented below.
         */
        headerMatches?: outputs.compute.URLMapPathMatcherRouteRuleMatchRuleHeaderMatch[];
        /**
         * Specifies that prefixMatch and fullPathMatch matches are case sensitive.
         * Defaults to false.
         */
        ignoreCase?: boolean;
        /**
         * Opaque filter criteria used by Loadbalancer to restrict routing configuration to
         * a limited set xDS compliant clients. In their xDS requests to Loadbalancer, xDS
         * clients present node metadata. If a match takes place, the relevant routing
         * configuration is made available to those proxies. For each metadataFilter in
         * this list, if its filterMatchCriteria is set to MATCH_ANY, at least one of the
         * filterLabels must match the corresponding label provided in the metadata. If its
         * filterMatchCriteria is set to MATCH_ALL, then all of its filterLabels must match
         * with corresponding labels in the provided metadata. metadataFilters specified
         * here can be overrides those specified in ForwardingRule that refers to this
         * UrlMap. metadataFilters only applies to Loadbalancers that have their
         * loadBalancingScheme set to INTERNAL_SELF_MANAGED.
         * Structure is documented below.
         */
        metadataFilters?: outputs.compute.URLMapPathMatcherRouteRuleMatchRuleMetadataFilter[];
        /**
         * For satisfying the matchRule condition, the path of the request
         * must match the wildcard pattern specified in pathTemplateMatch
         * after removing any query parameters and anchor that may be part
         * of the original URL.
         * pathTemplateMatch must be between 1 and 255 characters
         * (inclusive).  The pattern specified by pathTemplateMatch may
         * have at most 5 wildcard operators and at most 5 variable
         * captures in total.
         */
        pathTemplateMatch?: string;
        /**
         * For satisfying the matchRule condition, the request's path must begin with the
         * specified prefixMatch. prefixMatch must begin with a /. The value must be
         * between 1 and 1024 characters. Only one of prefixMatch, fullPathMatch or
         * regexMatch must be specified.
         */
        prefixMatch?: string;
        /**
         * Specifies a list of query parameter match criteria, all of which must match
         * corresponding query parameters in the request.
         * Structure is documented below.
         */
        queryParameterMatches?: outputs.compute.URLMapPathMatcherRouteRuleMatchRuleQueryParameterMatch[];
        /**
         * For satisfying the matchRule condition, the path of the request must satisfy the
         * regular expression specified in regexMatch after removing any query parameters
         * and anchor supplied with the original URL. For regular expression grammar please
         * see en.cppreference.com/w/cpp/regex/ecmascript  Only one of prefixMatch,
         * fullPathMatch or regexMatch must be specified.
         */
        regexMatch?: string;
    }

    export interface URLMapPathMatcherRouteRuleMatchRuleHeaderMatch {
        /**
         * The value should exactly match contents of exactMatch. Only one of exactMatch,
         * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
         */
        exactMatch?: string;
        /**
         * The name of the HTTP header to match. For matching against the HTTP request's
         * authority, use a headerMatch with the header name ":authority". For matching a
         * request's method, use the headerName ":method".
         */
        headerName: string;
        /**
         * If set to false, the headerMatch is considered a match if the match criteria
         * above are met. If set to true, the headerMatch is considered a match if the
         * match criteria above are NOT met. Defaults to false.
         */
        invertMatch?: boolean;
        /**
         * The value of the header must start with the contents of prefixMatch. Only one of
         * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
         * must be set.
         */
        prefixMatch?: string;
        /**
         * A header with the contents of headerName must exist. The match takes place
         * whether or not the request's header has a value or not. Only one of exactMatch,
         * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
         */
        presentMatch?: boolean;
        /**
         * The header value must be an integer and its value must be in the range specified
         * in rangeMatch. If the header does not contain an integer, number or is empty,
         * the match fails. For example for a range [-5, 0]   - -3 will match.  - 0 will
         * not match.  - 0.25 will not match.  - -3someString will not match.   Only one of
         * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
         * must be set.
         * Structure is documented below.
         */
        rangeMatch?: outputs.compute.URLMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch;
        /**
         * The value of the header must match the regular expression specified in
         * regexMatch. For regular expression grammar, please see:
         * en.cppreference.com/w/cpp/regex/ecmascript  For matching against a port
         * specified in the HTTP request, use a headerMatch with headerName set to PORT and
         * a regular expression that satisfies the RFC2616 Host header's port specifier.
         * Only one of exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or
         * rangeMatch must be set.
         */
        regexMatch?: string;
        /**
         * The value of the header must end with the contents of suffixMatch. Only one of
         * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
         * must be set.
         */
        suffixMatch?: string;
    }

    export interface URLMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch {
        /**
         * The end of the range (exclusive).
         */
        rangeEnd: number;
        /**
         * The start of the range (inclusive).
         */
        rangeStart: number;
    }

    export interface URLMapPathMatcherRouteRuleMatchRuleMetadataFilter {
        /**
         * The list of label value pairs that must match labels in the provided metadata
         * based on filterMatchCriteria  This list must not be empty and can have at the
         * most 64 entries.
         * Structure is documented below.
         */
        filterLabels: outputs.compute.URLMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel[];
        /**
         * Specifies how individual filterLabel matches within the list of filterLabels
         * contribute towards the overall metadataFilter match. Supported values are:
         * - MATCH_ANY: At least one of the filterLabels must have a matching label in the
         * provided metadata.
         * - MATCH_ALL: All filterLabels must have matching labels in
         * the provided metadata.
         * Possible values are: `MATCH_ALL`, `MATCH_ANY`.
         */
        filterMatchCriteria: string;
    }

    export interface URLMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel {
        /**
         * Name of metadata label. The name can have a maximum length of 1024 characters
         * and must be at least 1 character long.
         */
        name: string;
        /**
         * The value of the label must match the specified value. value can have a maximum
         * length of 1024 characters.
         */
        value: string;
    }

    export interface URLMapPathMatcherRouteRuleMatchRuleQueryParameterMatch {
        /**
         * The queryParameterMatch matches if the value of the parameter exactly matches
         * the contents of exactMatch. Only one of presentMatch, exactMatch and regexMatch
         * must be set.
         */
        exactMatch?: string;
        /**
         * The name of the query parameter to match. The query parameter must exist in the
         * request, in the absence of which the request match fails.
         */
        name: string;
        /**
         * Specifies that the queryParameterMatch matches if the request contains the query
         * parameter, irrespective of whether the parameter has a value or not. Only one of
         * presentMatch, exactMatch and regexMatch must be set.
         */
        presentMatch?: boolean;
        /**
         * The queryParameterMatch matches if the value of the parameter matches the
         * regular expression specified by regexMatch. For the regular expression grammar,
         * please see en.cppreference.com/w/cpp/regex/ecmascript  Only one of presentMatch,
         * exactMatch and regexMatch must be set.
         */
        regexMatch?: string;
    }

    export interface URLMapPathMatcherRouteRuleRouteAction {
        /**
         * The specification for allowing client side cross-origin requests. Please see W3C
         * Recommendation for Cross Origin Resource Sharing
         * Structure is documented below.
         */
        corsPolicy?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionCorsPolicy;
        /**
         * The specification for fault injection introduced into traffic to test the
         * resiliency of clients to backend service failure. As part of fault injection,
         * when clients send requests to a backend service, delays can be introduced by
         * Loadbalancer on a percentage of requests before sending those request to the
         * backend service. Similarly requests from clients can be aborted by the
         * Loadbalancer for a percentage of requests. timeout and retryPolicy will be
         * ignored by clients that are configured with a fault_injection_policy.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy;
        /**
         * Specifies the policy on how requests intended for the route's backends are
         * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
         * responses from the shadow service. Prior to sending traffic to the shadow
         * service, the host / authority header is suffixed with -shadow.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionRetryPolicy;
        /**
         * Specifies the timeout for the selected route. Timeout is computed from the time
         * the request is has been fully processed (i.e. end-of-stream) up until the
         * response has been completely processed. Timeout includes all retries. If not
         * specified, the default value is 15 seconds.
         * Structure is documented below.
         */
        timeout?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionTimeout;
        /**
         * The spec to modify the URL of the request, prior to forwarding the request to
         * the matched service
         * Structure is documented below.
         */
        urlRewrite?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionUrlRewrite;
        /**
         * A list of weighted backend services to send traffic to when a route match
         * occurs. The weights determine the fraction of traffic that flows to their
         * corresponding backend service. If all traffic needs to go to a single backend
         * service, there must be one  weightedBackendService with weight set to a non 0
         * number. Once a backendService is identified and before forwarding the request to
         * the backend service, advanced routing actions like Url rewrites and header
         * transformations are applied depending on additional settings specified in this
         * HttpRouteAction.
         * Structure is documented below.
         */
        weightedBackendServices?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionWeightedBackendService[];
    }

    export interface URLMapPathMatcherRouteRuleRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
         * This translates to the Access-Control-Allow-Credentials header.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regular expression patterns that match allowed origins. For regular expression grammar
         * please see en.cppreference.com/w/cpp/regex/ecmascript
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
         */
        allowOrigins?: string[];
        /**
         * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
         */
        disabled?: boolean;
        /**
         * Specifies the content for the Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached in seconds.
         * This translates to the Access-Control-Max-Age header.
         */
        maxAge?: number;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy {
        /**
         * The specification for how client requests are aborted as part of fault injection.
         * Structure is documented below.
         */
        abort?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort;
        /**
         * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
         * Structure is documented below.
         */
        delay?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         * The value must be between 200 and 599 inclusive.
         */
        httpStatus?: number;
        /**
         * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay {
        /**
         * Specifies the value of the fixed delay interval.
         * Structure is documented below.
         */
        fixedDelay?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
        /**
         * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
         * The value must be between 0.0 and 100.0 inclusive.
         */
        percentage?: number;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy {
        /**
         * The full or partial URL to the BackendService resource being mirrored to.
         */
        backendService: string;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionRetryPolicy {
        /**
         * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
         */
        numRetries: number;
        /**
         * Specifies a non-zero timeout per retry attempt.
         * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
         * will use the largest timeout among all backend services associated with the route.
         * Structure is documented below.
         */
        perTryTimeout?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout;
        /**
         * Specfies one or more conditions when this retry rule applies. Valid values are:
         * * 5xx: Loadbalancer will attempt a retry if the backend service responds with any 5xx response code,
         * or if the backend service does not respond at all, example: disconnects, reset, read timeout,
         * * connection failure, and refused streams.
         * * gateway-error: Similar to 5xx, but only applies to response codes 502, 503 or 504.
         * * connect-failure: Loadbalancer will retry on failures connecting to backend services,
         * for example due to connection timeouts.
         * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
         * Currently the only retriable error supported is 409.
         * * refused-stream:Loadbalancer will retry if the backend service resets the stream with a REFUSED_STREAM error code.
         * This reset type indicates that it is safe to retry.
         * * cancelled: Loadbalancer will retry if the gRPC status code in the response header is set to cancelled
         * * deadline-exceeded: Loadbalancer will retry if the gRPC status code in the response header is set to deadline-exceeded
         * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response header is set to resource-exhausted
         * * unavailable: Loadbalancer will retry if the gRPC status code in the response header is set to unavailable
         */
        retryConditions?: string[];
    }

    export interface URLMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
         * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionTimeout {
        /**
         * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented
         * with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
         */
        nanos?: number;
        /**
         * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
         * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
         */
        seconds: string;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionUrlRewrite {
        /**
         * Prior to forwarding the request to the selected service, the request's host header is replaced
         * with contents of hostRewrite.
         * The value must be between 1 and 255 characters.
         */
        hostRewrite?: string;
        /**
         * Prior to forwarding the request to the selected backend service, the matching portion of the
         * request's path is replaced by pathPrefixRewrite.
         * The value must be between 1 and 1024 characters.
         */
        pathPrefixRewrite?: string;
        /**
         * Prior to forwarding the request to the selected origin, if the
         * request matched a pathTemplateMatch, the matching portion of the
         * request's path is replaced re-written using the pattern specified
         * by pathTemplateRewrite.
         * pathTemplateRewrite must be between 1 and 255 characters
         * (inclusive), must start with a '/', and must only use variables
         * captured by the route's pathTemplate matchers.
         * pathTemplateRewrite may only be used when all of a route's
         * MatchRules specify pathTemplate.
         * Only one of pathPrefixRewrite and pathTemplateRewrite may be
         * specified.
         */
        pathTemplateRewrite?: string;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionWeightedBackendService {
        /**
         * The full or partial URL to the default BackendService resource. Before forwarding the
         * request to backendService, the loadbalancer applies any relevant headerActions
         * specified as part of this backendServiceWeight.
         */
        backendService: string;
        /**
         * Specifies changes to request and response headers that need to take effect for
         * the selected backendService.
         * headerAction specified here take effect before headerAction in the enclosing
         * HttpRouteRule, PathMatcher and UrlMap.
         * Structure is documented below.
         */
        headerAction?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction;
        /**
         * Specifies the fraction of traffic sent to backendService, computed as
         * weight / (sum of all weightedBackendService weights in routeAction) .
         * The selection of a backend service is determined only for new traffic. Once a user's request
         * has been directed to a backendService, subsequent requests will be sent to the same backendService
         * as determined by the BackendService's session affinity policy.
         * The value must be between 0 and 1000
         */
        weight: number;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction {
        /**
         * Headers to add to a matching request prior to forwarding the request to the backendService.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to
         * forwarding the request to the backendService.
         */
        requestHeadersToRemoves?: string[];
        /**
         * Headers to add the response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeadersToAdds?: outputs.compute.URLMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
        /**
         * A list of header names for headers that need to be removed from the response prior to sending the
         * response back to the client.
         */
        responseHeadersToRemoves?: string[];
    }

    export interface URLMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * If false, headerValue is appended to any values that already exist for the header.
         * If true, headerValue is set for the header, discarding any values that were set for that header.
         */
        replace: boolean;
    }

    export interface URLMapPathMatcherRouteRuleUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was
         * supplied in the request. The value must be between 1 and 255 characters.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https. If set
         * to false, the URL scheme of the redirected request will remain the same as that
         * of the request. This must only be set for UrlMaps used in TargetHttpProxys.
         * Setting this true for TargetHttpsProxy is not permitted. Defaults to false.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was
         * supplied in the request. Only one of pathRedirect or prefixRedirect must be
         * specified. The value must be between 1 and 1024 characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
         * retaining the remaining portion of the URL before redirecting the request.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction. Supported values are:
         * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
         * * FOUND, which corresponds to 302.
         * * SEE_OTHER which corresponds to 303.
         * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method will be retained.
         * * PERMANENT_REDIRECT, which corresponds to 308. In this case, the request method will be retained.
         */
        redirectResponseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed
         * prior to redirecting the request. If set to false, the query portion of the
         * original URL is retained. Defaults to false.
         */
        stripQuery?: boolean;
    }

    export interface URLMapTest {
        /**
         * Description of this test case.
         */
        description?: string;
        /**
         * Host portion of the URL.
         */
        host: string;
        /**
         * Path portion of the URL.
         */
        path: string;
        /**
         * The backend service or backend bucket link that should be matched by this test.
         */
        service: string;
    }

}

export namespace config {
    export interface Batching {
        enableBatching?: boolean;
        sendAfter?: string;
    }

}

export namespace container {
    export interface AttachedClusterAuthorization {
        /**
         * Users that can perform operations as a cluster admin. A managed
         * ClusterRoleBinding will be created to grant the `cluster-admin` ClusterRole
         * to the users. Up to ten admin users can be provided.
         * For more info on RBAC, see
         * https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
         */
        adminUsers?: string[];
    }

    export interface AttachedClusterBinaryAuthorization {
        /**
         * Configure Binary Authorization evaluation mode.
         * Possible values are: `DISABLED`, `PROJECT_SINGLETON_POLICY_ENFORCE`.
         */
        evaluationMode?: string;
    }

    export interface AttachedClusterError {
        /**
         * Human-friendly description of the error.
         */
        message?: string;
    }

    export interface AttachedClusterFleet {
        /**
         * (Output)
         * The name of the managed Hub Membership resource associated to this
         * cluster. Membership names are formatted as
         * projects/<project-number>/locations/global/membership/<cluster-id>.
         */
        membership: string;
        /**
         * The ID of the project in which the resource belongs.
         * If it is not provided, the provider project is used.
         */
        project: string;
    }

    export interface AttachedClusterLoggingConfig {
        /**
         * The configuration of the logging components
         * Structure is documented below.
         */
        componentConfig?: outputs.container.AttachedClusterLoggingConfigComponentConfig;
    }

    export interface AttachedClusterLoggingConfigComponentConfig {
        /**
         * The components to be enabled.
         * Each value may be one of: `SYSTEM_COMPONENTS`, `WORKLOADS`.
         */
        enableComponents?: string[];
    }

    export interface AttachedClusterMonitoringConfig {
        /**
         * Enable Google Cloud Managed Service for Prometheus in the cluster.
         * Structure is documented below.
         */
        managedPrometheusConfig?: outputs.container.AttachedClusterMonitoringConfigManagedPrometheusConfig;
    }

    export interface AttachedClusterMonitoringConfigManagedPrometheusConfig {
        /**
         * Enable Managed Collection.
         */
        enabled?: boolean;
    }

    export interface AttachedClusterOidcConfig {
        /**
         * A JSON Web Token (JWT) issuer URI. `issuer` must start with `https://`
         */
        issuerUrl: string;
        /**
         * OIDC verification keys in JWKS format (RFC 7517).
         */
        jwks?: string;
    }

    export interface AttachedClusterWorkloadIdentityConfig {
        /**
         * The ID of the OIDC Identity Provider (IdP) associated to
         * the Workload Identity Pool.
         */
        identityProvider?: string;
        /**
         * The OIDC issuer URL for this cluster.
         */
        issuerUri?: string;
        /**
         * The Workload Identity Pool associated to the cluster.
         */
        workloadPool?: string;
    }

    export interface AwsClusterAuthorization {
        /**
         * Users to perform operations as a cluster admin. A managed ClusterRoleBinding will be created to grant the `cluster-admin` ClusterRole to the users. Up to ten admin users can be provided. For more info on RBAC, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
         */
        adminUsers: outputs.container.AwsClusterAuthorizationAdminUser[];
    }

    export interface AwsClusterAuthorizationAdminUser {
        /**
         * The name of the user, e.g. `my-gcp-id@gmail.com`.
         */
        username: string;
    }

    export interface AwsClusterControlPlane {
        /**
         * Authentication configuration for management of AWS resources.
         */
        awsServicesAuthentication: outputs.container.AwsClusterControlPlaneAwsServicesAuthentication;
        /**
         * The ARN of the AWS KMS key used to encrypt cluster configuration.
         */
        configEncryption: outputs.container.AwsClusterControlPlaneConfigEncryption;
        /**
         * The ARN of the AWS KMS key used to encrypt cluster secrets.
         */
        databaseEncryption: outputs.container.AwsClusterControlPlaneDatabaseEncryption;
        /**
         * The name of the AWS IAM instance pofile to assign to each control plane replica.
         */
        iamInstanceProfile: string;
        /**
         * (Beta only) Details of placement information for an instance.
         */
        instancePlacement: outputs.container.AwsClusterControlPlaneInstancePlacement;
        /**
         * Optional. The AWS instance type. When unspecified, it defaults to `m5.large`.
         */
        instanceType: string;
        /**
         * Optional. Configuration related to the main volume provisioned for each control plane replica. The main volume is in charge of storing all of the cluster's etcd state. Volumes will be provisioned in the availability zone associated with the corresponding subnet. When unspecified, it defaults to 8 GiB with the GP2 volume type.
         */
        mainVolume: outputs.container.AwsClusterControlPlaneMainVolume;
        /**
         * Proxy configuration for outbound HTTP(S) traffic.
         */
        proxyConfig?: outputs.container.AwsClusterControlPlaneProxyConfig;
        /**
         * Optional. Configuration related to the root volume provisioned for each control plane replica. Volumes will be provisioned in the availability zone associated with the corresponding subnet. When unspecified, it defaults to 32 GiB with the GP2 volume type.
         */
        rootVolume: outputs.container.AwsClusterControlPlaneRootVolume;
        /**
         * Optional. The IDs of additional security groups to add to control plane replicas. The Anthos Multi-Cloud API will automatically create and manage security groups with the minimum rules needed for a functioning cluster.
         */
        securityGroupIds?: string[];
        /**
         * Optional. SSH configuration for how to access the underlying control plane machines.
         */
        sshConfig?: outputs.container.AwsClusterControlPlaneSshConfig;
        /**
         * The list of subnets where control plane replicas will run. A replica will be provisioned on each subnet and up to three values can be provided. Each subnet must be in a different AWS Availability Zone (AZ).
         */
        subnetIds: string[];
        /**
         * Optional. A set of AWS resource tags to propagate to all underlying managed AWS resources. Specify at most 50 pairs containing alphanumerics, spaces, and symbols (.+-=_:@/). Keys can be up to 127 Unicode characters. Values can be up to 255 Unicode characters.
         */
        tags?: {[key: string]: string};
        /**
         * The Kubernetes version to run on control plane replicas (e.g. `1.19.10-gke.1000`). You can list all supported versions on a given Google Cloud region by calling .
         */
        version: string;
    }

    export interface AwsClusterControlPlaneAwsServicesAuthentication {
        /**
         * The Amazon Resource Name (ARN) of the role that the Anthos Multi-Cloud API will assume when managing AWS resources on your account.
         */
        roleArn: string;
        /**
         * Optional. An identifier for the assumed role session. When unspecified, it defaults to `multicloud-service-agent`.
         */
        roleSessionName: string;
    }

    export interface AwsClusterControlPlaneConfigEncryption {
        /**
         * The ARN of the AWS KMS key used to encrypt cluster configuration.
         */
        kmsKeyArn: string;
    }

    export interface AwsClusterControlPlaneDatabaseEncryption {
        /**
         * The ARN of the AWS KMS key used to encrypt cluster secrets.
         */
        kmsKeyArn: string;
    }

    export interface AwsClusterControlPlaneInstancePlacement {
        /**
         * The tenancy for the instance. Possible values: TENANCY_UNSPECIFIED, DEFAULT, DEDICATED, HOST
         */
        tenancy: string;
    }

    export interface AwsClusterControlPlaneMainVolume {
        /**
         * Optional. The number of I/O operations per second (IOPS) to provision for GP3 volume.
         */
        iops: number;
        /**
         * Optional. The Amazon Resource Name (ARN) of the Customer Managed Key (CMK) used to encrypt AWS EBS volumes. If not specified, the default Amazon managed key associated to the AWS region where this cluster runs will be used.
         */
        kmsKeyArn?: string;
        /**
         * Optional. The size of the volume, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
         */
        sizeGib: number;
        /**
         * Optional. The throughput to provision for the volume, in MiB/s. Only valid if the volume type is GP3.
         */
        throughput: number;
        /**
         * Optional. Type of the EBS volume. When unspecified, it defaults to GP2 volume. Possible values: VOLUME_TYPE_UNSPECIFIED, GP2, GP3
         */
        volumeType: string;
    }

    export interface AwsClusterControlPlaneProxyConfig {
        /**
         * The ARN of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
         */
        secretArn: string;
        /**
         * The version string of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
         */
        secretVersion: string;
    }

    export interface AwsClusterControlPlaneRootVolume {
        /**
         * Optional. The number of I/O operations per second (IOPS) to provision for GP3 volume.
         */
        iops: number;
        /**
         * Optional. The Amazon Resource Name (ARN) of the Customer Managed Key (CMK) used to encrypt AWS EBS volumes. If not specified, the default Amazon managed key associated to the AWS region where this cluster runs will be used.
         */
        kmsKeyArn?: string;
        /**
         * Optional. The size of the volume, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
         */
        sizeGib: number;
        /**
         * Optional. The throughput to provision for the volume, in MiB/s. Only valid if the volume type is GP3.
         */
        throughput: number;
        /**
         * Optional. Type of the EBS volume. When unspecified, it defaults to GP2 volume. Possible values: VOLUME_TYPE_UNSPECIFIED, GP2, GP3
         */
        volumeType: string;
    }

    export interface AwsClusterControlPlaneSshConfig {
        /**
         * The name of the EC2 key pair used to login into cluster machines.
         */
        ec2KeyPair: string;
    }

    export interface AwsClusterFleet {
        /**
         * The name of the managed Hub Membership resource associated to this cluster. Membership names are formatted as projects/<project-number>/locations/global/membership/<cluster-id>.
         */
        membership: string;
        /**
         * The number of the Fleet host project where this cluster will be registered.
         */
        project: string;
    }

    export interface AwsClusterLoggingConfig {
        /**
         * Configuration of the logging components.
         */
        componentConfig: outputs.container.AwsClusterLoggingConfigComponentConfig;
    }

    export interface AwsClusterLoggingConfigComponentConfig {
        /**
         * Components of the logging configuration to be enabled.
         */
        enableComponents: string[];
    }

    export interface AwsClusterNetworking {
        /**
         * Disable the per node pool subnet security group rules on the control plane security group. When set to true, you must also provide one or more security groups that ensure node pools are able to send requests to the control plane on TCP/443 and TCP/8132. Failure to do so may result in unavailable node pools.
         */
        perNodePoolSgRulesDisabled?: boolean;
        /**
         * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creation.
         */
        podAddressCidrBlocks: string[];
        /**
         * All services in the cluster are assigned an RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creation.
         */
        serviceAddressCidrBlocks: string[];
        /**
         * The VPC associated with the cluster. All component clusters (i.e. control plane and node pools) run on a single VPC. This field cannot be changed after creation.
         *
         * - - -
         */
        vpcId: string;
    }

    export interface AwsClusterWorkloadIdentityConfig {
        identityProvider: string;
        issuerUri: string;
        workloadPool: string;
    }

    export interface AwsNodePoolAutoscaling {
        /**
         * Maximum number of nodes in the NodePool. Must be >= min_node_count.
         */
        maxNodeCount: number;
        /**
         * Minimum number of nodes in the NodePool. Must be >= 1 and <= max_node_count.
         */
        minNodeCount: number;
    }

    export interface AwsNodePoolConfig {
        /**
         * Optional. Configuration related to CloudWatch metrics collection on the Auto Scaling group of the node pool. When unspecified, metrics collection is disabled.
         */
        autoscalingMetricsCollection?: outputs.container.AwsNodePoolConfigAutoscalingMetricsCollection;
        /**
         * The ARN of the AWS KMS key used to encrypt node pool configuration.
         */
        configEncryption: outputs.container.AwsNodePoolConfigConfigEncryption;
        /**
         * The name of the AWS IAM role assigned to nodes in the pool.
         */
        iamInstanceProfile: string;
        /**
         * (Beta only) The OS image type to use on node pool instances.
         */
        imageType: string;
        /**
         * (Beta only) Details of placement information for an instance.
         */
        instancePlacement: outputs.container.AwsNodePoolConfigInstancePlacement;
        /**
         * Optional. The AWS instance type. When unspecified, it defaults to `m5.large`.
         */
        instanceType: string;
        /**
         * Optional. The initial labels assigned to nodes of this node pool. An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * Proxy configuration for outbound HTTP(S) traffic.
         */
        proxyConfig?: outputs.container.AwsNodePoolConfigProxyConfig;
        /**
         * Optional. Template for the root volume provisioned for node pool nodes. Volumes will be provisioned in the availability zone assigned to the node pool subnet. When unspecified, it defaults to 32 GiB with the GP2 volume type.
         */
        rootVolume: outputs.container.AwsNodePoolConfigRootVolume;
        /**
         * Optional. The IDs of additional security groups to add to nodes in this pool. The manager will automatically create security groups with minimum rules needed for a functioning cluster.
         */
        securityGroupIds?: string[];
        /**
         * (Beta only) Optional. When specified, the node pool will provision Spot instances from the set of spot_config.instance_types. This field is mutually exclusive with `instanceType`
         */
        spotConfig?: outputs.container.AwsNodePoolConfigSpotConfig;
        /**
         * Optional. The SSH configuration.
         */
        sshConfig?: outputs.container.AwsNodePoolConfigSshConfig;
        /**
         * Optional. Key/value metadata to assign to each underlying AWS resource. Specify at most 50 pairs containing alphanumerics, spaces, and symbols (.+-=_:@/). Keys can be up to 127 Unicode characters. Values can be up to 255 Unicode characters.
         */
        tags?: {[key: string]: string};
        /**
         * Optional. The initial taints assigned to nodes of this node pool.
         */
        taints?: outputs.container.AwsNodePoolConfigTaint[];
    }

    export interface AwsNodePoolConfigAutoscalingMetricsCollection {
        /**
         * The frequency at which EC2 Auto Scaling sends aggregated data to AWS CloudWatch. The only valid value is "1Minute".
         */
        granularity: string;
        /**
         * The metrics to enable. For a list of valid metrics, see https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_EnableMetricsCollection.html. If you specify granularity and don't specify any metrics, all metrics are enabled.
         */
        metrics?: string[];
    }

    export interface AwsNodePoolConfigConfigEncryption {
        /**
         * The ARN of the AWS KMS key used to encrypt node pool configuration.
         */
        kmsKeyArn: string;
    }

    export interface AwsNodePoolConfigInstancePlacement {
        /**
         * The tenancy for the instance. Possible values: TENANCY_UNSPECIFIED, DEFAULT, DEDICATED, HOST
         */
        tenancy: string;
    }

    export interface AwsNodePoolConfigProxyConfig {
        /**
         * The ARN of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
         */
        secretArn: string;
        /**
         * The version string of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
         */
        secretVersion: string;
    }

    export interface AwsNodePoolConfigRootVolume {
        /**
         * Optional. The number of I/O operations per second (IOPS) to provision for GP3 volume.
         */
        iops: number;
        /**
         * Optional. The Amazon Resource Name (ARN) of the Customer Managed Key (CMK) used to encrypt AWS EBS volumes. If not specified, the default Amazon managed key associated to the AWS region where this cluster runs will be used.
         */
        kmsKeyArn?: string;
        /**
         * Optional. The size of the volume, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
         */
        sizeGib: number;
        /**
         * Optional. The throughput to provision for the volume, in MiB/s. Only valid if the volume type is GP3.
         */
        throughput: number;
        /**
         * Optional. Type of the EBS volume. When unspecified, it defaults to GP2 volume. Possible values: VOLUME_TYPE_UNSPECIFIED, GP2, GP3
         */
        volumeType: string;
    }

    export interface AwsNodePoolConfigSpotConfig {
        /**
         * List of AWS EC2 instance types for creating a spot node pool's nodes. The specified instance types must have the same number of CPUs and memory. You can use the Amazon EC2 Instance Selector tool (https://github.com/aws/amazon-ec2-instance-selector) to choose instance types with matching CPU and memory
         */
        instanceTypes: string[];
    }

    export interface AwsNodePoolConfigSshConfig {
        /**
         * The name of the EC2 key pair used to login into cluster machines.
         */
        ec2KeyPair: string;
    }

    export interface AwsNodePoolConfigTaint {
        /**
         * The taint effect. Possible values: EFFECT_UNSPECIFIED, NO_SCHEDULE, PREFER_NO_SCHEDULE, NO_EXECUTE
         */
        effect: string;
        /**
         * Key for the taint.
         */
        key: string;
        /**
         * Value for the taint.
         */
        value: string;
    }

    export interface AwsNodePoolManagement {
        /**
         * Optional. Whether or not the nodes will be automatically repaired.
         */
        autoRepair: boolean;
    }

    export interface AwsNodePoolMaxPodsConstraint {
        /**
         * The maximum number of pods to schedule on a single node.
         *
         * - - -
         */
        maxPodsPerNode: number;
    }

    export interface AzureClusterAuthorization {
        /**
         * Users that can perform operations as a cluster admin. A new ClusterRoleBinding will be created to grant the cluster-admin ClusterRole to the users. Up to ten admin users can be provided. For more info on RBAC, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
         */
        adminUsers: outputs.container.AzureClusterAuthorizationAdminUser[];
    }

    export interface AzureClusterAuthorizationAdminUser {
        /**
         * The name of the user, e.g. `my-gcp-id@gmail.com`.
         */
        username: string;
    }

    export interface AzureClusterAzureServicesAuthentication {
        /**
         * The Azure Active Directory Application ID for Authentication configuration.
         */
        applicationId: string;
        /**
         * The Azure Active Directory Tenant ID for Authentication configuration.
         */
        tenantId: string;
    }

    export interface AzureClusterControlPlane {
        /**
         * Optional. Configuration related to application-layer secrets encryption.
         */
        databaseEncryption?: outputs.container.AzureClusterControlPlaneDatabaseEncryption;
        /**
         * Optional. Configuration related to the main volume provisioned for each control plane replica. The main volume is in charge of storing all of the cluster's etcd state. When unspecified, it defaults to a 8-GiB Azure Disk.
         */
        mainVolume: outputs.container.AzureClusterControlPlaneMainVolume;
        /**
         * Proxy configuration for outbound HTTP(S) traffic.
         */
        proxyConfig?: outputs.container.AzureClusterControlPlaneProxyConfig;
        /**
         * Configuration for where to place the control plane replicas. Up to three replica placement instances can be specified. If replicaPlacements is set, the replica placement instances will be applied to the three control plane replicas as evenly as possible.
         */
        replicaPlacements?: outputs.container.AzureClusterControlPlaneReplicaPlacement[];
        /**
         * Optional. Configuration related to the root volume provisioned for each control plane replica. When unspecified, it defaults to 32-GiB Azure Disk.
         */
        rootVolume: outputs.container.AzureClusterControlPlaneRootVolume;
        /**
         * SSH configuration for how to access the underlying control plane machines.
         */
        sshConfig: outputs.container.AzureClusterControlPlaneSshConfig;
        /**
         * The ARM ID of the subnet where the control plane VMs are deployed. Example: `/subscriptions//resourceGroups//providers/Microsoft.Network/virtualNetworks//subnets/default`.
         */
        subnetId: string;
        /**
         * Optional. A set of tags to apply to all underlying control plane Azure resources.
         */
        tags?: {[key: string]: string};
        /**
         * The Kubernetes version to run on control plane replicas (e.g. `1.19.10-gke.1000`). You can list all supported versions on a given Google Cloud region by calling GetAzureServerConfig.
         */
        version: string;
        /**
         * Optional. The Azure VM size name. Example: `Standard_DS2_v2`. For available VM sizes, see https://docs.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions. When unspecified, it defaults to `Standard_DS2_v2`.
         */
        vmSize: string;
    }

    export interface AzureClusterControlPlaneDatabaseEncryption {
        /**
         * The ARM ID of the Azure Key Vault key to encrypt / decrypt data. For example: `/subscriptions/<subscription-id>/resourceGroups/<resource-group-id>/providers/Microsoft.KeyVault/vaults/<key-vault-id>/keys/<key-name>` Encryption will always take the latest version of the key and hence specific version is not supported.
         */
        keyId: string;
    }

    export interface AzureClusterControlPlaneMainVolume {
        /**
         * Optional. The size of the disk, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
         */
        sizeGib: number;
    }

    export interface AzureClusterControlPlaneProxyConfig {
        /**
         * The ARM ID the of the resource group containing proxy keyvault. Resource group ids are formatted as `/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>`
         */
        resourceGroupId: string;
        /**
         * The URL the of the proxy setting secret with its version. Secret ids are formatted as `https:<key-vault-name>.vault.azure.net/secrets/<secret-name>/<secret-version>`.
         */
        secretId: string;
    }

    export interface AzureClusterControlPlaneReplicaPlacement {
        /**
         * For a given replica, the Azure availability zone where to provision the control plane VM and the ETCD disk.
         */
        azureAvailabilityZone: string;
        /**
         * For a given replica, the ARM ID of the subnet where the control plane VM is deployed. Make sure it's a subnet under the virtual network in the cluster configuration.
         */
        subnetId: string;
    }

    export interface AzureClusterControlPlaneRootVolume {
        /**
         * Optional. The size of the disk, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
         */
        sizeGib: number;
    }

    export interface AzureClusterControlPlaneSshConfig {
        /**
         * The SSH public key data for VMs managed by Anthos. This accepts the authorizedKeys file format used in OpenSSH according to the sshd(8) manual page.
         */
        authorizedKey: string;
    }

    export interface AzureClusterFleet {
        /**
         * The name of the managed Hub Membership resource associated to this cluster. Membership names are formatted as projects/<project-number>/locations/global/membership/<cluster-id>.
         */
        membership: string;
        /**
         * The number of the Fleet host project where this cluster will be registered.
         */
        project: string;
    }

    export interface AzureClusterLoggingConfig {
        /**
         * Configuration of the logging components.
         */
        componentConfig: outputs.container.AzureClusterLoggingConfigComponentConfig;
    }

    export interface AzureClusterLoggingConfigComponentConfig {
        /**
         * Components of the logging configuration to be enabled.
         */
        enableComponents: string[];
    }

    export interface AzureClusterNetworking {
        /**
         * The IP address range of the pods in this cluster, in CIDR notation (e.g. `10.96.0.0/14`). All pods in the cluster get assigned a unique RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creation.
         */
        podAddressCidrBlocks: string[];
        /**
         * The IP address range for services in this cluster, in CIDR notation (e.g. `10.96.0.0/14`). All services in the cluster get assigned a unique RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creating a cluster.
         */
        serviceAddressCidrBlocks: string[];
        /**
         * The Azure Resource Manager (ARM) ID of the VNet associated with your cluster. All components in the cluster (i.e. control plane and node pools) run on a single VNet. Example: `/subscriptions/*&#47;resourceGroups/*&#47;providers/Microsoft.Network/virtualNetworks/*` This field cannot be changed after creation.
         *
         * - - -
         */
        virtualNetworkId: string;
    }

    export interface AzureClusterWorkloadIdentityConfig {
        identityProvider: string;
        issuerUri: string;
        workloadPool: string;
    }

    export interface AzureNodePoolAutoscaling {
        /**
         * Maximum number of nodes in the node pool. Must be >= min_node_count.
         */
        maxNodeCount: number;
        /**
         * Minimum number of nodes in the node pool. Must be >= 1 and <= max_node_count.
         */
        minNodeCount: number;
    }

    export interface AzureNodePoolConfig {
        /**
         * (Beta only) The OS image type to use on node pool instances.
         */
        imageType: string;
        /**
         * Proxy configuration for outbound HTTP(S) traffic.
         */
        proxyConfig?: outputs.container.AzureNodePoolConfigProxyConfig;
        /**
         * Optional. Configuration related to the root volume provisioned for each node pool machine. When unspecified, it defaults to a 32-GiB Azure Disk.
         */
        rootVolume: outputs.container.AzureNodePoolConfigRootVolume;
        /**
         * SSH configuration for how to access the node pool machines.
         */
        sshConfig: outputs.container.AzureNodePoolConfigSshConfig;
        /**
         * Optional. A set of tags to apply to all underlying Azure resources for this node pool. This currently only includes Virtual Machine Scale Sets. Specify at most 50 pairs containing alphanumerics, spaces, and symbols (.+-=_:@/). Keys can be up to 127 Unicode characters. Values can be up to 255 Unicode characters.
         */
        tags?: {[key: string]: string};
        /**
         * Optional. The Azure VM size name. Example: `Standard_DS2_v2`. See (/anthos/clusters/docs/azure/reference/supported-vms) for options. When unspecified, it defaults to `Standard_DS2_v2`.
         */
        vmSize: string;
    }

    export interface AzureNodePoolConfigProxyConfig {
        /**
         * The ARM ID the of the resource group containing proxy keyvault. Resource group ids are formatted as `/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>`
         */
        resourceGroupId: string;
        /**
         * The URL the of the proxy setting secret with its version. Secret ids are formatted as `https:<key-vault-name>.vault.azure.net/secrets/<secret-name>/<secret-version>`.
         */
        secretId: string;
    }

    export interface AzureNodePoolConfigRootVolume {
        /**
         * Optional. The size of the disk, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
         */
        sizeGib: number;
    }

    export interface AzureNodePoolConfigSshConfig {
        /**
         * The SSH public key data for VMs managed by Anthos. This accepts the authorizedKeys file format used in OpenSSH according to the sshd(8) manual page.
         */
        authorizedKey: string;
    }

    export interface AzureNodePoolManagement {
        /**
         * Optional. Whether or not the nodes will be automatically repaired.
         */
        autoRepair: boolean;
    }

    export interface AzureNodePoolMaxPodsConstraint {
        /**
         * The maximum number of pods to schedule on a single node.
         *
         * - - -
         */
        maxPodsPerNode: number;
    }

    export interface ClusterAddonsConfig {
        /**
         * . Structure is documented below.
         */
        cloudrunConfig: outputs.container.ClusterAddonsConfigCloudrunConfig;
        /**
         * .
         * The status of the ConfigConnector addon. It is disabled by default; Set `enabled = true` to enable.
         *
         *
         * This example `addonsConfig` disables two addons:
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         * <a name="nestedBinaryAuthorization"></a>The `binaryAuthorization` block supports:
         */
        configConnectorConfig: outputs.container.ClusterAddonsConfigConfigConnectorConfig;
        /**
         * .
         * The status of the NodeLocal DNSCache addon. It is disabled by default.
         * Set `enabled = true` to enable.
         *
         * **Enabling/Disabling NodeLocal DNSCache in an existing cluster is a disruptive operation.
         * All cluster nodes running GKE 1.15 and higher are recreated.**
         */
        dnsCacheConfig: outputs.container.ClusterAddonsConfigDnsCacheConfig;
        /**
         * .
         * Whether this cluster should enable the Google Compute Engine Persistent Disk Container Storage Interface (CSI) Driver. Set `enabled = true` to enable.
         *
         * **Note:** The Compute Engine persistent disk CSI Driver is enabled by default on newly created clusters for the following versions: Linux clusters: GKE version 1.18.10-gke.2100 or later, or 1.19.3-gke.2100 or later.
         */
        gcePersistentDiskCsiDriverConfig: outputs.container.ClusterAddonsConfigGcePersistentDiskCsiDriverConfig;
        /**
         * The status of the Filestore CSI driver addon,
         * which allows the usage of filestore instance as volumes.
         * It is disabled by default; set `enabled = true` to enable.
         */
        gcpFilestoreCsiDriverConfig: outputs.container.ClusterAddonsConfigGcpFilestoreCsiDriverConfig;
        /**
         * The status of the GCSFuse CSI driver addon,
         * which allows the usage of a gcs bucket as volumes.
         * It is disabled by default; set `enabled = true` to enable.
         */
        gcsFuseCsiDriverConfig: outputs.container.ClusterAddonsConfigGcsFuseCsiDriverConfig;
        /**
         * .
         * The status of the Backup for GKE agent addon. It is disabled by default; Set `enabled = true` to enable.
         */
        gkeBackupAgentConfig: outputs.container.ClusterAddonsConfigGkeBackupAgentConfig;
        /**
         * The status of the Horizontal Pod Autoscaling
         * addon, which increases or decreases the number of replica pods a replication controller
         * has based on the resource usage of the existing pods.
         * It is enabled by default;
         * set `disabled = true` to disable.
         */
        horizontalPodAutoscaling: outputs.container.ClusterAddonsConfigHorizontalPodAutoscaling;
        /**
         * The status of the HTTP (L7) load balancing
         * controller addon, which makes it easy to set up HTTP load balancers for services in a
         * cluster. It is enabled by default; set `disabled = true` to disable.
         */
        httpLoadBalancing: outputs.container.ClusterAddonsConfigHttpLoadBalancing;
        /**
         * .
         * Structure is documented below.
         */
        istioConfig: outputs.container.ClusterAddonsConfigIstioConfig;
        /**
         * .
         * Configuration for the KALM addon, which manages the lifecycle of k8s. It is disabled by default; Set `enabled = true` to enable.
         */
        kalmConfig: outputs.container.ClusterAddonsConfigKalmConfig;
        /**
         * Whether we should enable the network policy addon
         * for the master.  This must be enabled in order to enable network policy for the nodes.
         * To enable this, you must also define a `networkPolicy` block,
         * otherwise nothing will happen.
         * It can only be disabled if the nodes already do not have network policies enabled.
         * Defaults to disabled; set `disabled = false` to enable.
         */
        networkPolicyConfig: outputs.container.ClusterAddonsConfigNetworkPolicyConfig;
    }

    export interface ClusterAddonsConfigCloudrunConfig {
        /**
         * The status of the CloudRun addon. It is disabled by default. Set `disabled=false` to enable.
         */
        disabled: boolean;
        /**
         * The load balancer type of CloudRun ingress service. It is external load balancer by default.
         * Set `load_balancer_type=LOAD_BALANCER_TYPE_INTERNAL` to configure it as internal load balancer.
         */
        loadBalancerType?: string;
    }

    export interface ClusterAddonsConfigConfigConnectorConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterAddonsConfigDnsCacheConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterAddonsConfigGcePersistentDiskCsiDriverConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterAddonsConfigGcpFilestoreCsiDriverConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterAddonsConfigGcsFuseCsiDriverConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterAddonsConfigGkeBackupAgentConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterAddonsConfigHorizontalPodAutoscaling {
        /**
         * Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when defaultSnatStatus is disabled.When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic
         *
         * <a name="nestedClusterTelemetry"></a>The `clusterTelemetry` block supports
         */
        disabled: boolean;
    }

    export interface ClusterAddonsConfigHttpLoadBalancing {
        /**
         * Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when defaultSnatStatus is disabled.When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic
         *
         * <a name="nestedClusterTelemetry"></a>The `clusterTelemetry` block supports
         */
        disabled: boolean;
    }

    export interface ClusterAddonsConfigIstioConfig {
        /**
         * The authentication type between services in Istio. Available options include `AUTH_MUTUAL_TLS`.
         */
        auth?: string;
        /**
         * The status of the Istio addon, which makes it easy to set up Istio for services in a
         * cluster. It is disabled by default. Set `disabled = false` to enable.
         */
        disabled: boolean;
    }

    export interface ClusterAddonsConfigKalmConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterAddonsConfigNetworkPolicyConfig {
        /**
         * Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when defaultSnatStatus is disabled.When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic
         *
         * <a name="nestedClusterTelemetry"></a>The `clusterTelemetry` block supports
         */
        disabled: boolean;
    }

    export interface ClusterAuthenticatorGroupsConfig {
        /**
         * The name of the RBAC security group for use with Google security groups in Kubernetes RBAC. Group name must be in format `gke-security-groups@yourdomain.com`.
         */
        securityGroup: string;
    }

    export interface ClusterBinaryAuthorization {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         *
         * @deprecated Deprecated in favor of evaluation_mode.
         */
        enabled?: boolean;
        /**
         * Mode of operation for Binary Authorization policy evaluation. Valid values are `DISABLED`
         * and `PROJECT_SINGLETON_POLICY_ENFORCE`. `PROJECT_SINGLETON_POLICY_ENFORCE` is functionally equivalent to the
         * deprecated `enableBinaryAuthorization` parameter being set to `true`.
         */
        evaluationMode?: string;
    }

    export interface ClusterClusterAutoscaling {
        /**
         * Contains defaults for a node pool created by NAP. A subset of fields also apply to
         * GKE Autopilot clusters.
         * Structure is documented below.
         */
        autoProvisioningDefaults: outputs.container.ClusterClusterAutoscalingAutoProvisioningDefaults;
        /**
         * ) Configuration
         * options for the [Autoscaling profile](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles)
         * feature, which lets you choose whether the cluster autoscaler should optimize for resource utilization or resource availability
         * when deciding to remove nodes from a cluster. Can be `BALANCED` or `OPTIMIZE_UTILIZATION`. Defaults to `BALANCED`.
         */
        autoscalingProfile?: string;
        /**
         * Whether node auto-provisioning is enabled. Must be supplied for GKE Standard clusters, `true` is implied
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         */
        enabled: boolean;
        /**
         * Global constraints for machine resources in the
         * cluster. Configuring the `cpu` and `memory` types is required if node
         * auto-provisioning is enabled. These limits will apply to node pool autoscaling
         * in addition to node auto-provisioning. Structure is documented below.
         */
        resourceLimits?: outputs.container.ClusterClusterAutoscalingResourceLimit[];
    }

    export interface ClusterClusterAutoscalingAutoProvisioningDefaults {
        /**
         * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
         */
        bootDiskKmsKey?: string;
        /**
         * Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB. Defaults to `100`
         */
        diskSize?: number;
        /**
         * Type of the disk attached to each node (e.g. 'pd-standard', 'pd-ssd' or 'pd-balanced'). Defaults to `pd-standard`
         */
        diskType?: string;
        /**
         * The default image type used by NAP once a new node pool is being created. Please note that according to the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning#default-image-type) the value must be one of the [COS_CONTAINERD, COS, UBUNTU_CONTAINERD, UBUNTU]. __NOTE__ : COS AND UBUNTU are deprecated as of `GKE 1.24`
         */
        imageType?: string;
        /**
         * NodeManagement configuration for this NodePool. Structure is documented below.
         */
        management: outputs.container.ClusterClusterAutoscalingAutoProvisioningDefaultsManagement;
        /**
         * Minimum CPU platform to be used for NAP created node pools. The instance may be scheduled on the
         * specified or newer CPU platform. Applicable values are the friendly names of CPU platforms, such
         * as "Intel Haswell" or "Intel Sandy Bridge".
         */
        minCpuPlatform?: string;
        /**
         * Scopes that are used by NAP and GKE Autopilot when creating node pools. Use the "https://www.googleapis.com/auth/cloud-platform" scope to grant access to all APIs. It is recommended that you set `serviceAccount` to a non-default service account and grant IAM roles to that service account for only the resources that it needs.
         *
         * > `monitoring.write` is always enabled regardless of user input.  `monitoring` and `logging.write` may also be enabled depending on the values for `monitoringService` and `loggingService`.
         */
        oauthScopes: string[];
        /**
         * The Google Cloud Platform Service Account to be used by the node VMs created by GKE Autopilot or NAP.
         */
        serviceAccount?: string;
        /**
         * Shielded Instance options. Structure is documented below.
         */
        shieldedInstanceConfig?: outputs.container.ClusterClusterAutoscalingAutoProvisioningDefaultsShieldedInstanceConfig;
        /**
         * Specifies the upgrade settings for NAP created node pools. Structure is documented below.
         */
        upgradeSettings: outputs.container.ClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettings;
    }

    export interface ClusterClusterAutoscalingAutoProvisioningDefaultsManagement {
        /**
         * Specifies whether the node auto-repair is enabled for the node pool. If enabled, the nodes in this node pool will be monitored and, if they fail health checks too many times, an automatic repair action will be triggered.
         *
         * This block also contains several computed attributes, documented below.
         */
        autoRepair: boolean;
        /**
         * Specifies whether node auto-upgrade is enabled for the node pool. If enabled, node auto-upgrade helps keep the nodes in your node pool up to date with the latest release version of Kubernetes.
         */
        autoUpgrade: boolean;
        upgradeOptions: outputs.container.ClusterClusterAutoscalingAutoProvisioningDefaultsManagementUpgradeOption[];
    }

    export interface ClusterClusterAutoscalingAutoProvisioningDefaultsManagementUpgradeOption {
        autoUpgradeStartTime: string;
        /**
         * Description of the cluster.
         */
        description: string;
    }

    export interface ClusterClusterAutoscalingAutoProvisioningDefaultsShieldedInstanceConfig {
        /**
         * Defines if the instance has integrity monitoring enabled.
         *
         * Enables monitoring and attestation of the boot integrity of the instance. The attestation is performed against the integrity policy baseline. This baseline is initially derived from the implicitly trusted boot image when the instance is created.  Defaults to `true`.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Defines if the instance has Secure Boot enabled.
         *
         * Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.  Defaults to `false`.
         */
        enableSecureBoot?: boolean;
    }

    export interface ClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettings {
        /**
         * Settings for blue-green upgrade strategy. To be specified when strategy is set to BLUE_GREEN. Structure is documented below.
         */
        blueGreenSettings: outputs.container.ClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettings;
        /**
         * The maximum number of nodes that can be created beyond the current size of the node pool during the upgrade process. To be used when strategy is set to SURGE. Default is 0.
         */
        maxSurge?: number;
        /**
         * The maximum number of nodes that can be simultaneously unavailable during the upgrade process. To be used when strategy is set to SURGE. Default is 0.
         */
        maxUnavailable?: number;
        /**
         * Strategy used for node pool update. Strategy can only be one of BLUE_GREEN or SURGE. The default is value is SURGE.
         */
        strategy: string;
    }

    export interface ClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettings {
        /**
         * Time needed after draining entire blue pool. After this period, blue pool will be cleaned up. A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
         */
        nodePoolSoakDuration: string;
        /**
         * Standard policy for the blue-green upgrade. To be specified when strategy is set to BLUE_GREEN. Structure is documented below.
         */
        standardRolloutPolicy: outputs.container.ClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy;
    }

    export interface ClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy {
        /**
         * Number of blue nodes to drain in a batch. Only one of the batchPercentage or batchNodeCount can be specified.
         */
        batchNodeCount: number;
        /**
         * Percentage of the bool pool nodes to drain in a batch. The range of this field should be (0.0, 1.0). Only one of the batchPercentage or batchNodeCount can be specified.
         */
        batchPercentage: number;
        /**
         * Soak time after each batch gets drained. A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".`.
         */
        batchSoakDuration?: string;
    }

    export interface ClusterClusterAutoscalingResourceLimit {
        /**
         * Maximum amount of the resource in the cluster.
         */
        maximum?: number;
        /**
         * Minimum amount of the resource in the cluster.
         */
        minimum?: number;
        /**
         * The type of the resource. For example, `cpu` and
         * `memory`.  See the [guide to using Node Auto-Provisioning](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning)
         * for a list of types.
         */
        resourceType: string;
    }

    export interface ClusterClusterTelemetry {
        /**
         * Telemetry integration for the cluster. Supported values (`ENABLED, DISABLED, SYSTEM_ONLY`);
         * `SYSTEM_ONLY` (Only system components are monitored and logged) is only available in GKE versions 1.15 and later.
         */
        type: string;
    }

    export interface ClusterConfidentialNodes {
        /**
         * Enable Confidential GKE Nodes for this cluster, to
         * enforce encryption of data in-use.
         */
        enabled: boolean;
    }

    export interface ClusterCostManagementConfig {
        /**
         * Whether to enable the [cost allocation](https://cloud.google.com/kubernetes-engine/docs/how-to/cost-allocations) feature.
         */
        enabled: boolean;
    }

    export interface ClusterDatabaseEncryption {
        /**
         * the key to use to encrypt/decrypt secrets.  See the [DatabaseEncryption definition](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters#Cluster.DatabaseEncryption) for more information.
         *
         * <a name="nestedEnableK8sBetaApis"></a>The `enableK8sBetaApis` block supports:
         */
        keyName?: string;
        /**
         * `ENCRYPTED` or `DECRYPTED`
         */
        state: string;
    }

    export interface ClusterDefaultSnatStatus {
        /**
         * Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when defaultSnatStatus is disabled.When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic
         *
         * <a name="nestedClusterTelemetry"></a>The `clusterTelemetry` block supports
         */
        disabled: boolean;
    }

    export interface ClusterDnsConfig {
        /**
         * Which in-cluster DNS provider should be used. `PROVIDER_UNSPECIFIED` (default) or `PLATFORM_DEFAULT` or `CLOUD_DNS`.
         */
        clusterDns?: string;
        /**
         * The suffix used for all cluster service records.
         */
        clusterDnsDomain?: string;
        /**
         * The scope of access to cluster DNS records. `DNS_SCOPE_UNSPECIFIED` (default) or `CLUSTER_SCOPE` or `VPC_SCOPE`.
         */
        clusterDnsScope?: string;
    }

    export interface ClusterEnableK8sBetaApis {
        /**
         * Enabled Kubernetes Beta APIs. To list a Beta API resource, use the representation {group}/{version}/{resource}. The version must be a Beta version. Note that you cannot disable beta APIs that are already enabled on a cluster without recreating it. See the [Configure beta APIs](https://cloud.google.com/kubernetes-engine/docs/how-to/use-beta-apis#configure-beta-apis) for more information.
         */
        enabledApis: string[];
    }

    export interface ClusterGatewayApiConfig {
        /**
         * Which Gateway Api channel should be used. `CHANNEL_DISABLED`, `CHANNEL_EXPERIMENTAL` or `CHANNEL_STANDARD`.
         */
        channel: string;
    }

    export interface ClusterIdentityServiceConfig {
        /**
         * Whether to enable the Identity Service component. It is disabled by default. Set `enabled=true` to enable.
         */
        enabled?: boolean;
    }

    export interface ClusterIpAllocationPolicy {
        /**
         * The configuration for additional pod secondary ranges at
         * the cluster level. Used for Autopilot clusters and Standard clusters with which control of the
         * secondary Pod IP address assignment to node pools isn't needed. Structure is documented below.
         */
        additionalPodRangesConfig?: outputs.container.ClusterIpAllocationPolicyAdditionalPodRangesConfig;
        /**
         * The IP address range for the cluster pod IPs.
         * Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
         * to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
         * from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
         * pick a specific range to use.
         */
        clusterIpv4CidrBlock: string;
        /**
         * The name of the existing secondary
         * range in the cluster's subnetwork to use for pod IP addresses. Alternatively,
         * `clusterIpv4CidrBlock` can be used to automatically create a GKE-managed one.
         */
        clusterSecondaryRangeName: string;
        podCidrOverprovisionConfig: outputs.container.ClusterIpAllocationPolicyPodCidrOverprovisionConfig;
        /**
         * The IP address range of the services IPs in this cluster.
         * Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14)
         * to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14)
         * from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to
         * pick a specific range to use.
         */
        servicesIpv4CidrBlock: string;
        /**
         * The name of the existing
         * secondary range in the cluster's subnetwork to use for service `ClusterIP`s.
         * Alternatively, `servicesIpv4CidrBlock` can be used to automatically create a
         * GKE-managed one.
         */
        servicesSecondaryRangeName: string;
        /**
         * The IP Stack Type of the cluster.
         * Default value is `IPV4`.
         * Possible values are `IPV4` and `IPV4_IPV6`.
         */
        stackType?: string;
    }

    export interface ClusterIpAllocationPolicyAdditionalPodRangesConfig {
        /**
         * The names of the Pod ranges to add to the cluster.
         */
        podRangeNames: string[];
    }

    export interface ClusterIpAllocationPolicyPodCidrOverprovisionConfig {
        /**
         * Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when defaultSnatStatus is disabled.When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic
         *
         * <a name="nestedClusterTelemetry"></a>The `clusterTelemetry` block supports
         */
        disabled: boolean;
    }

    export interface ClusterLoggingConfig {
        /**
         * The GKE components exposing logs. Supported values include:
         * `SYSTEM_COMPONENTS`, `APISERVER`, `CONTROLLER_MANAGER`, `SCHEDULER`, and `WORKLOADS`.
         */
        enableComponents: string[];
    }

    export interface ClusterMaintenancePolicy {
        /**
         * Time window specified for daily maintenance operations.
         * Specify `startTime` in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) format "HH:MM,
         * where HH : \[00-23\] and MM : \[00-59\] GMT. For example:
         *
         * Examples:
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        dailyMaintenanceWindow?: outputs.container.ClusterMaintenancePolicyDailyMaintenanceWindow;
        /**
         * Exceptions to maintenance window. Non-emergency maintenance should not occur in these windows. A cluster can have up to 20 maintenance exclusions at a time [Maintenance Window and Exclusions](https://cloud.google.com/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions)
         */
        maintenanceExclusions?: outputs.container.ClusterMaintenancePolicyMaintenanceExclusion[];
        /**
         * Time window for recurring maintenance operations.
         *
         * Specify `startTime` and `endTime` in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) "Zulu" date format.  The start time's date is
         * the initial date that the window starts, and the end time is used for calculating duration.  Specify `recurrence` in
         * [RFC5545](https://tools.ietf.org/html/rfc5545#section-3.8.5.3) RRULE format, to specify when this recurs.
         * Note that GKE may accept other formats, but will return values in UTC, causing a permanent diff.
         *
         * Examples:
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        recurringWindow?: outputs.container.ClusterMaintenancePolicyRecurringWindow;
    }

    export interface ClusterMaintenancePolicyDailyMaintenanceWindow {
        duration: string;
        startTime: string;
    }

    export interface ClusterMaintenancePolicyMaintenanceExclusion {
        endTime: string;
        exclusionName: string;
        /**
         * MaintenanceExclusionOptions provides maintenance exclusion related options.
         */
        exclusionOptions?: outputs.container.ClusterMaintenancePolicyMaintenanceExclusionExclusionOptions;
        startTime: string;
    }

    export interface ClusterMaintenancePolicyMaintenanceExclusionExclusionOptions {
        /**
         * The scope of automatic upgrades to restrict in the exclusion window. One of: **NO_UPGRADES | NO_MINOR_UPGRADES | NO_MINOR_OR_NODE_UPGRADES**
         *
         * Specify `startTime` and `endTime` in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) "Zulu" date format.  The start time's date is
         * the initial date that the window starts, and the end time is used for calculating duration.Specify `recurrence` in
         * [RFC5545](https://tools.ietf.org/html/rfc5545#section-3.8.5.3) RRULE format, to specify when this recurs.
         * Note that GKE may accept other formats, but will return values in UTC, causing a permanent diff.
         *
         * Examples:
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        scope: string;
    }

    export interface ClusterMaintenancePolicyRecurringWindow {
        endTime: string;
        recurrence: string;
        startTime: string;
    }

    export interface ClusterMasterAuth {
        clientCertificate: string;
        /**
         * Whether client certificate authorization is enabled for this cluster.  For example:
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         *
         * This block also contains several computed attributes, documented below.
         */
        clientCertificateConfig: outputs.container.ClusterMasterAuthClientCertificateConfig;
        clientKey: string;
        clusterCaCertificate: string;
    }

    export interface ClusterMasterAuthClientCertificateConfig {
        issueClientCertificate: boolean;
    }

    export interface ClusterMasterAuthorizedNetworksConfig {
        /**
         * External networks that can access the
         * Kubernetes cluster master through HTTPS.
         */
        cidrBlocks?: outputs.container.ClusterMasterAuthorizedNetworksConfigCidrBlock[];
        /**
         * Whether Kubernetes master is
         * accessible via Google Compute Engine Public IPs.
         */
        gcpPublicCidrsAccessEnabled: boolean;
    }

    export interface ClusterMasterAuthorizedNetworksConfigCidrBlock {
        /**
         * External network that can access Kubernetes master through HTTPS.
         * Must be specified in CIDR notation.
         */
        cidrBlock: string;
        /**
         * Field for users to identify CIDR blocks.
         */
        displayName?: string;
    }

    export interface ClusterMeshCertificates {
        /**
         * Controls the issuance of workload mTLS certificates. It is enabled by default. Workload Identity is required, see workload_config.
         */
        enableCertificates: boolean;
    }

    export interface ClusterMonitoringConfig {
        /**
         * Configuration for Advanced Datapath Monitoring. Structure is documented below.
         */
        advancedDatapathObservabilityConfigs: outputs.container.ClusterMonitoringConfigAdvancedDatapathObservabilityConfig[];
        /**
         * The GKE components exposing metrics. Supported values include: `SYSTEM_COMPONENTS`, `APISERVER`, `SCHEDULER`, `CONTROLLER_MANAGER`, `STORAGE`, `HPA`, `POD`, `DAEMONSET`, `DEPLOYMENT` and `STATEFULSET`. In beta provider, `WORKLOADS` is supported on top of those 10 values. (`WORKLOADS` is deprecated and removed in GKE 1.24.)
         */
        enableComponents: string[];
        /**
         * Configuration for Managed Service for Prometheus. Structure is documented below.
         */
        managedPrometheus: outputs.container.ClusterMonitoringConfigManagedPrometheus;
    }

    export interface ClusterMonitoringConfigAdvancedDatapathObservabilityConfig {
        /**
         * Whether or not to enable advanced datapath metrics.
         */
        enableMetrics: boolean;
        /**
         * Mode used to make Relay available.
         */
        relayMode: string;
    }

    export interface ClusterMonitoringConfigManagedPrometheus {
        /**
         * Whether or not the managed collection is enabled.
         */
        enabled: boolean;
    }

    export interface ClusterNetworkPolicy {
        /**
         * Whether network policy is enabled on the cluster.
         */
        enabled: boolean;
        /**
         * The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.
         */
        provider?: string;
    }

    export interface ClusterNodeConfig {
        /**
         * Specifies options for controlling
         * advanced machine features. Structure is documented below.
         */
        advancedMachineFeatures?: outputs.container.ClusterNodeConfigAdvancedMachineFeatures;
        /**
         * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: <https://cloud.google.com/compute/docs/disks/customer-managed-encryption>
         */
        bootDiskKmsKey?: string;
        /**
         * Configuration for [Confidential Nodes](https://cloud.google.com/kubernetes-engine/docs/how-to/confidential-gke-nodes) feature. Structure is documented below documented below.
         */
        confidentialNodes: outputs.container.ClusterNodeConfigConfidentialNodes;
        /**
         * Size of the disk attached to each node, specified
         * in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
         */
        diskSizeGb: number;
        /**
         * Type of the disk attached to each node
         * (e.g. 'pd-standard', 'pd-balanced' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
         */
        diskType: string;
        /**
         * ) Parameters for the ephemeral storage filesystem. If unspecified, ephemeral storage is backed by the boot disk. Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        ephemeralStorageConfig?: outputs.container.ClusterNodeConfigEphemeralStorageConfig;
        /**
         * Parameters for the ephemeral storage filesystem. If unspecified, ephemeral storage is backed by the boot disk. Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        ephemeralStorageLocalSsdConfig?: outputs.container.ClusterNodeConfigEphemeralStorageLocalSsdConfig;
        /**
         * Parameters for the NCCL Fast Socket feature. If unspecified, NCCL Fast Socket will not be enabled on the node pool.
         * Node Pool must enable gvnic.
         * GKE version 1.25.2-gke.1700 or later.
         * Structure is documented below.
         */
        fastSocket?: outputs.container.ClusterNodeConfigFastSocket;
        /**
         * Parameters for the Google Container Filesystem (GCFS).
         * If unspecified, GCFS will not be enabled on the node pool. When enabling this feature you must specify `imageType = "COS_CONTAINERD"` and `nodeVersion` from GKE versions 1.19 or later to use it.
         * For GKE versions 1.19, 1.20, and 1.21, the recommended minimum `nodeVersion` would be 1.19.15-gke.1300, 1.20.11-gke.1300, and 1.21.5-gke.1300 respectively.
         * A `machineType` that has more than 16 GiB of memory is also recommended.
         * GCFS must be enabled in order to use [image streaming](https://cloud.google.com/kubernetes-engine/docs/how-to/image-streaming).
         * Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        gcfsConfig?: outputs.container.ClusterNodeConfigGcfsConfig;
        /**
         * List of the type and count of accelerator cards attached to the instance.
         * Structure documented below.
         */
        guestAccelerators: outputs.container.ClusterNodeConfigGuestAccelerator[];
        /**
         * Google Virtual NIC (gVNIC) is a virtual network interface.
         * Installing the gVNIC driver allows for more efficient traffic transmission across the Google network infrastructure.
         * gVNIC is an alternative to the virtIO-based ethernet driver. GKE nodes must use a Container-Optimized OS node image.
         * GKE node version 1.15.11-gke.15 or later
         * Structure is documented below.
         *
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        gvnic?: outputs.container.ClusterNodeConfigGvnic;
        hostMaintenancePolicy?: outputs.container.ClusterNodeConfigHostMaintenancePolicy;
        /**
         * The image type to use for this node. Note that changing the image type
         * will delete and recreate all nodes in the node pool.
         */
        imageType: string;
        /**
         * Kubelet configuration, currently supported attributes can be found [here](https://cloud.google.com/sdk/gcloud/reference/beta/container/node-pools/create#--system-config-from-file).
         * Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        kubeletConfig?: outputs.container.ClusterNodeConfigKubeletConfig;
        /**
         * The Kubernetes labels (key/value pairs) to be applied to each node. The kubernetes.io/ and k8s.io/ prefixes are
         * reserved by Kubernetes Core components and cannot be specified.
         */
        labels: {[key: string]: string};
        /**
         * Linux node configuration, currently supported attributes can be found [here](https://cloud.google.com/sdk/gcloud/reference/beta/container/node-pools/create#--system-config-from-file).
         * Note that validations happen all server side. All attributes are optional.
         * Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        linuxNodeConfig?: outputs.container.ClusterNodeConfigLinuxNodeConfig;
        /**
         * Parameters for the local NVMe SSDs. Structure is documented below.
         */
        localNvmeSsdBlockConfig?: outputs.container.ClusterNodeConfigLocalNvmeSsdBlockConfig;
        /**
         * The amount of local SSD disks that will be
         * attached to each cluster node. Defaults to 0.
         */
        localSsdCount: number;
        /**
         * Parameter for specifying the type of logging agent used in a node pool. This will override any cluster-wide default value. Valid values include DEFAULT and MAX_THROUGHPUT. See [Increasing logging agent throughput](https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#throughput) for more information.
         */
        loggingVariant?: string;
        /**
         * The name of a Google Compute Engine machine type.
         * Defaults to `e2-medium`. To create a custom machine type, value should be set as specified
         * [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
         */
        machineType: string;
        /**
         * The metadata key/value pairs assigned to instances in
         * the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
         * `true` by the API; if `metadata` is set but that default value is not
         * included, the provider will attempt to unset the value. To avoid this, set the
         * value in your config.
         */
        metadata: {[key: string]: string};
        /**
         * Minimum CPU platform to be used by this instance.
         * The instance may be scheduled on the specified or newer CPU platform. Applicable
         * values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
         * [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
         * for more information.
         */
        minCpuPlatform: string;
        /**
         * Setting this field will assign instances of this pool to run on the specified node group. This is useful for running workloads on [sole tenant nodes](https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes).
         */
        nodeGroup?: string;
        /**
         * The set of Google API scopes to be made available
         * on all of the node VMs under the "default" service account.
         * Use the "https://www.googleapis.com/auth/cloud-platform" scope to grant access to all APIs. It is recommended that you set `serviceAccount` to a non-default service account and grant IAM roles to that service account for only the resources that it needs.
         *
         * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/access-scopes) for information on migrating off of legacy access scopes.
         */
        oauthScopes: string[];
        /**
         * A boolean that represents whether or not the underlying node VMs
         * are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
         * for more information. Defaults to false.
         */
        preemptible?: boolean;
        /**
         * The configuration of the desired reservation which instances could take capacity from. Structure is documented below.
         */
        reservationAffinity?: outputs.container.ClusterNodeConfigReservationAffinity;
        /**
         * The GCP labels (key/value pairs) to be applied to each node. Refer [here](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-managing-labels)
         * for how these labels are applied to clusters, node pools and nodes.
         */
        resourceLabels?: {[key: string]: string};
        sandboxConfig?: outputs.container.ClusterNodeConfigSandboxConfig;
        /**
         * The service account to be used by the Node VMs.
         * If not specified, the "default" service account is used.
         */
        serviceAccount: string;
        /**
         * Shielded Instance options. Structure is documented below.
         */
        shieldedInstanceConfig: outputs.container.ClusterNodeConfigShieldedInstanceConfig;
        /**
         * Allows specifying multiple [node affinities](https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#node_affinity_and_anti-affinity) useful for running workloads on [sole tenant nodes](https://cloud.google.com/kubernetes-engine/docs/how-to/sole-tenancy). `nodeAffinity` structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        soleTenantConfig?: outputs.container.ClusterNodeConfigSoleTenantConfig;
        /**
         * A boolean that represents whether the underlying node VMs are spot.
         * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms)
         * for more information. Defaults to false.
         */
        spot?: boolean;
        /**
         * The list of instance tags applied to all nodes. Tags are used to identify
         * valid sources or targets for network firewalls.
         */
        tags?: string[];
        /**
         * A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
         * to apply to nodes. GKE's API can only set this field on cluster creation.
         * However, GKE will add taints to your nodes if you enable certain features such
         * as GPUs. If this field is set, any diffs on this field will cause the provider to
         * recreate the underlying resource. Taint values can be updated safely in
         * Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
         * this field to manage taints. If you do, `lifecycle.ignore_changes` is
         * recommended. Structure is documented below.
         */
        taints: outputs.container.ClusterNodeConfigTaint[];
        /**
         * Metadata configuration to expose to workloads on the node pool.
         * Structure is documented below.
         */
        workloadMetadataConfig: outputs.container.ClusterNodeConfigWorkloadMetadataConfig;
    }

    export interface ClusterNodeConfigAdvancedMachineFeatures {
        /**
         * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
         */
        threadsPerCore: number;
    }

    export interface ClusterNodeConfigConfidentialNodes {
        /**
         * Enable Confidential GKE Nodes for this cluster, to
         * enforce encryption of data in-use.
         */
        enabled: boolean;
    }

    export interface ClusterNodeConfigEphemeralStorageConfig {
        /**
         * Number of local SSDs to use to back ephemeral storage. Uses NVMe interfaces. Each local SSD is 375 GB in size. If zero, it means to disable using local SSDs as ephemeral storage.
         */
        localSsdCount: number;
    }

    export interface ClusterNodeConfigEphemeralStorageLocalSsdConfig {
        /**
         * Number of local SSDs to use to back ephemeral storage. Uses NVMe interfaces. Each local SSD is 375 GB in size. If zero, it means to disable using local SSDs as ephemeral storage.
         */
        localSsdCount: number;
    }

    export interface ClusterNodeConfigFastSocket {
        /**
         * Whether or not the NCCL Fast Socket is enabled
         */
        enabled: boolean;
    }

    export interface ClusterNodeConfigGcfsConfig {
        /**
         * Whether or not the Google Container Filesystem (GCFS) is enabled
         */
        enabled: boolean;
    }

    export interface ClusterNodeConfigGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * Configuration for auto installation of GPU driver. Structure is documented below.
         */
        gpuDriverInstallationConfig?: outputs.container.ClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig;
        /**
         * Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig [user guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
         */
        gpuPartitionSize?: string;
        /**
         * Configuration for GPU sharing. Structure is documented below.
         */
        gpuSharingConfig?: outputs.container.ClusterNodeConfigGuestAcceleratorGpuSharingConfig;
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface ClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
        /**
         * Mode for how the GPU driver is installed.
         * Accepted values are:
         * * `"GPU_DRIVER_VERSION_UNSPECIFIED"`: Default value is to not install any GPU driver.
         * * `"INSTALLATION_DISABLED"`: Disable GPU driver auto installation and needs manual installation.
         * * `"DEFAULT"`: "Default" GPU driver in COS and Ubuntu.
         * * `"LATEST"`: "Latest" GPU driver in COS.
         */
        gpuDriverVersion: string;
    }

    export interface ClusterNodeConfigGuestAcceleratorGpuSharingConfig {
        /**
         * The type of GPU sharing strategy to enable on the GPU node.
         * Accepted values are:
         * * `"TIME_SHARING"`: Allow multiple containers to have [time-shared](https://cloud.google.com/kubernetes-engine/docs/concepts/timesharing-gpus) access to a single GPU device.
         */
        gpuSharingStrategy: string;
        /**
         * The maximum number of containers that can share a GPU.
         */
        maxSharedClientsPerGpu: number;
    }

    export interface ClusterNodeConfigGvnic {
        /**
         * Whether or not the Google Virtual NIC (gVNIC) is enabled
         */
        enabled: boolean;
    }

    export interface ClusterNodeConfigHostMaintenancePolicy {
        maintenanceInterval: string;
    }

    export interface ClusterNodeConfigKubeletConfig {
        /**
         * If true, enables CPU CFS quota enforcement for
         * containers that specify CPU limits.
         */
        cpuCfsQuota?: boolean;
        /**
         * The CPU CFS quota period value. Specified
         * as a sequence of decimal numbers, each with optional fraction and a unit suffix,
         * such as `"300ms"`. Valid time units are "ns", "us" (or "s"), "ms", "s", "m",
         * "h". The value must be a positive duration.
         *
         * > Note: At the time of writing (2020/08/18) the GKE API rejects the `none`
         * value and accepts an invalid `default` value instead. While this remains true,
         * not specifying the `kubeletConfig` block should be the equivalent of specifying
         * `none`.
         */
        cpuCfsQuotaPeriod?: string;
        /**
         * The CPU management policy on the node. See
         * [K8S CPU Management Policies](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/).
         * One of `"none"` or `"static"`. Defaults to `none` when `kubeletConfig` is unset.
         */
        cpuManagerPolicy: string;
        /**
         * Controls the maximum number of processes allowed to run in a pod. The value must be greater than or equal to 1024 and less than 4194304.
         */
        podPidsLimit?: number;
    }

    export interface ClusterNodeConfigLinuxNodeConfig {
        /**
         * The Linux kernel parameters to be applied to the nodes
         * and all pods running on the nodes. Specified as a map from the key, such as
         * `net.core.wmem_max`, to a string value.
         */
        sysctls: {[key: string]: string};
    }

    export interface ClusterNodeConfigLocalNvmeSsdBlockConfig {
        /**
         * Number of raw-block local NVMe SSD disks to be attached to the node. Each local SSD is 375 GB in size. If zero, it means no raw-block local NVMe SSD disks to be attached to the node.
         * > Note: Local NVMe SSD storage available in GKE versions v1.25.3-gke.1800 and later.
         */
        localSsdCount: number;
    }

    export interface ClusterNodeConfigReservationAffinity {
        /**
         * The type of reservation consumption
         * Accepted values are:
         *
         * * `"UNSPECIFIED"`: Default value. This should not be used.
         * * `"NO_RESERVATION"`: Do not consume from any reserved capacity.
         * * `"ANY_RESERVATION"`: Consume any reservation available.
         * * `"SPECIFIC_RESERVATION"`: Must consume from a specific reservation. Must specify key value fields for specifying the reservations.
         */
        consumeReservationType: string;
        /**
         * The label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify "compute.googleapis.com/reservation-name" as the key and specify the name of your reservation as its value.
         */
        key?: string;
        /**
         * The list of label values of reservation resources. For example: the name of the specific reservation when using a key of "compute.googleapis.com/reservation-name"
         */
        values?: string[];
    }

    export interface ClusterNodeConfigSandboxConfig {
        /**
         * Which sandbox to use for pods in the node pool.
         * Accepted values are:
         *
         * * `"gvisor"`: Pods run within a gVisor sandbox.
         */
        sandboxType: string;
    }

    export interface ClusterNodeConfigShieldedInstanceConfig {
        /**
         * Defines if the instance has integrity monitoring enabled.
         *
         * Enables monitoring and attestation of the boot integrity of the instance. The attestation is performed against the integrity policy baseline. This baseline is initially derived from the implicitly trusted boot image when the instance is created.  Defaults to `true`.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Defines if the instance has Secure Boot enabled.
         *
         * Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.  Defaults to `false`.
         */
        enableSecureBoot?: boolean;
    }

    export interface ClusterNodeConfigSoleTenantConfig {
        nodeAffinities: outputs.container.ClusterNodeConfigSoleTenantConfigNodeAffinity[];
    }

    export interface ClusterNodeConfigSoleTenantConfigNodeAffinity {
        /**
         * The default or custom node affinity label key name.
         */
        key: string;
        /**
         * Specifies affinity or anti-affinity. Accepted values are `"IN"` or `"NOT_IN"`
         */
        operator: string;
        /**
         * List of node affinity label values as strings.
         */
        values: string[];
    }

    export interface ClusterNodeConfigTaint {
        /**
         * Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
         */
        effect: string;
        /**
         * Key for taint.
         */
        key: string;
        /**
         * Value for taint.
         */
        value: string;
    }

    export interface ClusterNodeConfigWorkloadMetadataConfig {
        /**
         * How to expose the node metadata to the workload running on the node.
         * Accepted values are:
         * * UNSPECIFIED: Not Set
         * * GCE_METADATA: Expose all Compute Engine metadata to pods.
         * * GKE_METADATA: Run the GKE Metadata Server on this node. The GKE Metadata Server exposes a metadata API to workloads that is compatible with the V1 Compute Metadata APIs exposed by the Compute Engine and App Engine Metadata Servers. This feature can only be enabled if [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) is enabled at the cluster level.
         */
        mode: string;
    }

    export interface ClusterNodePool {
        autoscaling?: outputs.container.ClusterNodePoolAutoscaling;
        /**
         * The number of nodes to create in this
         * cluster's default node pool. In regional or multi-zonal clusters, this is the
         * number of nodes per zone. Must be set if `nodePool` is not set. If you're using
         * `gcp.container.NodePool` objects with no default node pool, you'll need to
         * set this to a value of at least `1`, alongside setting
         * `removeDefaultNodePool` to `true`.
         */
        initialNodeCount: number;
        instanceGroupUrls: string[];
        managedInstanceGroupUrls: string[];
        /**
         * NodeManagement configuration for this NodePool. Structure is documented below.
         */
        management: outputs.container.ClusterNodePoolManagement;
        maxPodsPerNode: number;
        /**
         * The name of the cluster, unique within the project and
         * location.
         *
         * - - -
         */
        name: string;
        namePrefix: string;
        /**
         * Configuration for
         * [Adding Pod IP address ranges](https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr)) to the node pool. Structure is documented below
         */
        networkConfig: outputs.container.ClusterNodePoolNetworkConfig;
        /**
         * Parameters used in creating the default node pool.
         * Generally, this field should not be used at the same time as a
         * `gcp.container.NodePool` or a `nodePool` block; this configuration
         * manages the default node pool, which isn't recommended to be used.
         * Structure is documented below.
         */
        nodeConfig: outputs.container.ClusterNodePoolNodeConfig;
        nodeCount: number;
        /**
         * The list of zones in which the cluster's nodes
         * are located. Nodes must be in the region of their regional cluster or in the
         * same region as their cluster's zone for zonal clusters. If this is specified for
         * a zonal cluster, omit the cluster's zone.
         *
         * > A "multi-zonal" cluster is a zonal cluster with at least one additional zone
         * defined; in a multi-zonal cluster, the cluster master is only present in a
         * single zone while nodes are present in each of the primary zone and the node
         * locations. In contrast, in a regional cluster, cluster master nodes are present
         * in multiple zones in the region. For that reason, regional clusters should be
         * preferred.
         */
        nodeLocations: string[];
        placementPolicy?: outputs.container.ClusterNodePoolPlacementPolicy;
        /**
         * Specifies the upgrade settings for NAP created node pools. Structure is documented below.
         */
        upgradeSettings: outputs.container.ClusterNodePoolUpgradeSettings;
        version: string;
    }

    export interface ClusterNodePoolAutoConfig {
        /**
         * The network tag config for the cluster's automatically provisioned node pools.
         */
        networkTags?: outputs.container.ClusterNodePoolAutoConfigNetworkTags;
    }

    export interface ClusterNodePoolAutoConfigNetworkTags {
        /**
         * List of network tags applied to auto-provisioned node pools.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        tags?: string[];
    }

    export interface ClusterNodePoolAutoscaling {
        locationPolicy: string;
        maxNodeCount?: number;
        minNodeCount?: number;
        totalMaxNodeCount?: number;
        totalMinNodeCount?: number;
    }

    export interface ClusterNodePoolDefaults {
        /**
         * Subset of NodeConfig message that has defaults.
         */
        nodeConfigDefaults?: outputs.container.ClusterNodePoolDefaultsNodeConfigDefaults;
    }

    export interface ClusterNodePoolDefaultsNodeConfigDefaults {
        /**
         * ) The default Google Container Filesystem (GCFS) configuration at the cluster level. e.g. enable [image streaming](https://cloud.google.com/kubernetes-engine/docs/how-to/image-streaming) across all the node pools within the cluster. Structure is documented below.
         */
        gcfsConfig?: outputs.container.ClusterNodePoolDefaultsNodeConfigDefaultsGcfsConfig;
        /**
         * The type of logging agent that is deployed by default for newly created node pools in the cluster. Valid values include DEFAULT and MAX_THROUGHPUT. See [Increasing logging agent throughput](https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#throughput) for more information.
         */
        loggingVariant?: string;
    }

    export interface ClusterNodePoolDefaultsNodeConfigDefaultsGcfsConfig {
        /**
         * Whether or not the Google Container Filesystem (GCFS) is enabled
         */
        enabled: boolean;
    }

    export interface ClusterNodePoolManagement {
        /**
         * Specifies whether the node auto-repair is enabled for the node pool. If enabled, the nodes in this node pool will be monitored and, if they fail health checks too many times, an automatic repair action will be triggered.
         *
         * This block also contains several computed attributes, documented below.
         */
        autoRepair?: boolean;
        /**
         * Specifies whether node auto-upgrade is enabled for the node pool. If enabled, node auto-upgrade helps keep the nodes in your node pool up to date with the latest release version of Kubernetes.
         */
        autoUpgrade?: boolean;
    }

    export interface ClusterNodePoolNetworkConfig {
        additionalNodeNetworkConfigs?: outputs.container.ClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig[];
        additionalPodNetworkConfigs?: outputs.container.ClusterNodePoolNetworkConfigAdditionalPodNetworkConfig[];
        /**
         * Whether to create a new range for pod IPs in this node pool. Defaults are provided for `podRange` and `podIpv4CidrBlock` if they are not specified.
         */
        createPodRange?: boolean;
        /**
         * Enables the private cluster feature,
         * creating a private endpoint on the cluster. In a private cluster, nodes only
         * have RFC 1918 private addresses and communicate with the master's private
         * endpoint via private networking.
         */
        enablePrivateNodes: boolean;
        podCidrOverprovisionConfig: outputs.container.ClusterNodePoolNetworkConfigPodCidrOverprovisionConfig;
        /**
         * The IP address range for pod IPs in this node pool. Only applicable if createPodRange is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) to pick a specific range to use.
         */
        podIpv4CidrBlock: string;
        /**
         * The ID of the secondary range for pod IPs. If `createPodRange` is true, this ID is used for the new range. If `createPodRange` is false, uses an existing secondary range with this ID.
         */
        podRange: string;
    }

    export interface ClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig {
        /**
         * The name or selfLink of the Google Compute Engine
         * network to which the cluster is connected. For Shared VPC, set this to the self link of the
         * shared network.
         */
        network?: string;
        /**
         * The name or selfLink of the Google Compute Engine
         * subnetwork in which the cluster's instances are launched.
         */
        subnetwork?: string;
    }

    export interface ClusterNodePoolNetworkConfigAdditionalPodNetworkConfig {
        maxPodsPerNode: number;
        secondaryPodRange?: string;
        /**
         * The name or selfLink of the Google Compute Engine
         * subnetwork in which the cluster's instances are launched.
         */
        subnetwork?: string;
    }

    export interface ClusterNodePoolNetworkConfigPodCidrOverprovisionConfig {
        /**
         * Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when defaultSnatStatus is disabled.When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic
         *
         * <a name="nestedClusterTelemetry"></a>The `clusterTelemetry` block supports
         */
        disabled: boolean;
    }

    export interface ClusterNodePoolNodeConfig {
        /**
         * Specifies options for controlling
         * advanced machine features. Structure is documented below.
         */
        advancedMachineFeatures?: outputs.container.ClusterNodePoolNodeConfigAdvancedMachineFeatures;
        /**
         * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: <https://cloud.google.com/compute/docs/disks/customer-managed-encryption>
         */
        bootDiskKmsKey?: string;
        /**
         * Configuration for [Confidential Nodes](https://cloud.google.com/kubernetes-engine/docs/how-to/confidential-gke-nodes) feature. Structure is documented below documented below.
         */
        confidentialNodes: outputs.container.ClusterNodePoolNodeConfigConfidentialNodes;
        /**
         * Size of the disk attached to each node, specified
         * in GB. The smallest allowed disk size is 10GB. Defaults to 100GB.
         */
        diskSizeGb: number;
        /**
         * Type of the disk attached to each node
         * (e.g. 'pd-standard', 'pd-balanced' or 'pd-ssd'). If unspecified, the default disk type is 'pd-standard'
         */
        diskType: string;
        /**
         * ) Parameters for the ephemeral storage filesystem. If unspecified, ephemeral storage is backed by the boot disk. Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        ephemeralStorageConfig?: outputs.container.ClusterNodePoolNodeConfigEphemeralStorageConfig;
        /**
         * Parameters for the ephemeral storage filesystem. If unspecified, ephemeral storage is backed by the boot disk. Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        ephemeralStorageLocalSsdConfig?: outputs.container.ClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig;
        /**
         * Parameters for the NCCL Fast Socket feature. If unspecified, NCCL Fast Socket will not be enabled on the node pool.
         * Node Pool must enable gvnic.
         * GKE version 1.25.2-gke.1700 or later.
         * Structure is documented below.
         */
        fastSocket?: outputs.container.ClusterNodePoolNodeConfigFastSocket;
        /**
         * Parameters for the Google Container Filesystem (GCFS).
         * If unspecified, GCFS will not be enabled on the node pool. When enabling this feature you must specify `imageType = "COS_CONTAINERD"` and `nodeVersion` from GKE versions 1.19 or later to use it.
         * For GKE versions 1.19, 1.20, and 1.21, the recommended minimum `nodeVersion` would be 1.19.15-gke.1300, 1.20.11-gke.1300, and 1.21.5-gke.1300 respectively.
         * A `machineType` that has more than 16 GiB of memory is also recommended.
         * GCFS must be enabled in order to use [image streaming](https://cloud.google.com/kubernetes-engine/docs/how-to/image-streaming).
         * Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        gcfsConfig?: outputs.container.ClusterNodePoolNodeConfigGcfsConfig;
        /**
         * List of the type and count of accelerator cards attached to the instance.
         * Structure documented below.
         */
        guestAccelerators: outputs.container.ClusterNodePoolNodeConfigGuestAccelerator[];
        /**
         * Google Virtual NIC (gVNIC) is a virtual network interface.
         * Installing the gVNIC driver allows for more efficient traffic transmission across the Google network infrastructure.
         * gVNIC is an alternative to the virtIO-based ethernet driver. GKE nodes must use a Container-Optimized OS node image.
         * GKE node version 1.15.11-gke.15 or later
         * Structure is documented below.
         *
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        gvnic?: outputs.container.ClusterNodePoolNodeConfigGvnic;
        hostMaintenancePolicy?: outputs.container.ClusterNodePoolNodeConfigHostMaintenancePolicy;
        /**
         * The image type to use for this node. Note that changing the image type
         * will delete and recreate all nodes in the node pool.
         */
        imageType: string;
        /**
         * Kubelet configuration, currently supported attributes can be found [here](https://cloud.google.com/sdk/gcloud/reference/beta/container/node-pools/create#--system-config-from-file).
         * Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        kubeletConfig?: outputs.container.ClusterNodePoolNodeConfigKubeletConfig;
        /**
         * The Kubernetes labels (key/value pairs) to be applied to each node. The kubernetes.io/ and k8s.io/ prefixes are
         * reserved by Kubernetes Core components and cannot be specified.
         */
        labels: {[key: string]: string};
        /**
         * Linux node configuration, currently supported attributes can be found [here](https://cloud.google.com/sdk/gcloud/reference/beta/container/node-pools/create#--system-config-from-file).
         * Note that validations happen all server side. All attributes are optional.
         * Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        linuxNodeConfig?: outputs.container.ClusterNodePoolNodeConfigLinuxNodeConfig;
        /**
         * Parameters for the local NVMe SSDs. Structure is documented below.
         */
        localNvmeSsdBlockConfig?: outputs.container.ClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig;
        /**
         * The amount of local SSD disks that will be
         * attached to each cluster node. Defaults to 0.
         */
        localSsdCount: number;
        /**
         * Parameter for specifying the type of logging agent used in a node pool. This will override any cluster-wide default value. Valid values include DEFAULT and MAX_THROUGHPUT. See [Increasing logging agent throughput](https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#throughput) for more information.
         */
        loggingVariant?: string;
        /**
         * The name of a Google Compute Engine machine type.
         * Defaults to `e2-medium`. To create a custom machine type, value should be set as specified
         * [here](https://cloud.google.com/compute/docs/reference/latest/instances#machineType).
         */
        machineType: string;
        /**
         * The metadata key/value pairs assigned to instances in
         * the cluster. From GKE `1.12` onwards, `disable-legacy-endpoints` is set to
         * `true` by the API; if `metadata` is set but that default value is not
         * included, the provider will attempt to unset the value. To avoid this, set the
         * value in your config.
         */
        metadata: {[key: string]: string};
        /**
         * Minimum CPU platform to be used by this instance.
         * The instance may be scheduled on the specified or newer CPU platform. Applicable
         * values are the friendly names of CPU platforms, such as `Intel Haswell`. See the
         * [official documentation](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
         * for more information.
         */
        minCpuPlatform: string;
        /**
         * Setting this field will assign instances of this pool to run on the specified node group. This is useful for running workloads on [sole tenant nodes](https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes).
         */
        nodeGroup?: string;
        /**
         * The set of Google API scopes to be made available
         * on all of the node VMs under the "default" service account.
         * Use the "https://www.googleapis.com/auth/cloud-platform" scope to grant access to all APIs. It is recommended that you set `serviceAccount` to a non-default service account and grant IAM roles to that service account for only the resources that it needs.
         *
         * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/access-scopes) for information on migrating off of legacy access scopes.
         */
        oauthScopes: string[];
        /**
         * A boolean that represents whether or not the underlying node VMs
         * are preemptible. See the [official documentation](https://cloud.google.com/container-engine/docs/preemptible-vm)
         * for more information. Defaults to false.
         */
        preemptible?: boolean;
        /**
         * The configuration of the desired reservation which instances could take capacity from. Structure is documented below.
         */
        reservationAffinity?: outputs.container.ClusterNodePoolNodeConfigReservationAffinity;
        /**
         * The GCP labels (key/value pairs) to be applied to each node. Refer [here](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-managing-labels)
         * for how these labels are applied to clusters, node pools and nodes.
         */
        resourceLabels?: {[key: string]: string};
        sandboxConfig?: outputs.container.ClusterNodePoolNodeConfigSandboxConfig;
        /**
         * The service account to be used by the Node VMs.
         * If not specified, the "default" service account is used.
         */
        serviceAccount: string;
        /**
         * Shielded Instance options. Structure is documented below.
         */
        shieldedInstanceConfig: outputs.container.ClusterNodePoolNodeConfigShieldedInstanceConfig;
        /**
         * Allows specifying multiple [node affinities](https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#node_affinity_and_anti-affinity) useful for running workloads on [sole tenant nodes](https://cloud.google.com/kubernetes-engine/docs/how-to/sole-tenancy). `nodeAffinity` structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        soleTenantConfig?: outputs.container.ClusterNodePoolNodeConfigSoleTenantConfig;
        /**
         * A boolean that represents whether the underlying node VMs are spot.
         * See the [official documentation](https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms)
         * for more information. Defaults to false.
         */
        spot?: boolean;
        /**
         * The list of instance tags applied to all nodes. Tags are used to identify
         * valid sources or targets for network firewalls.
         */
        tags?: string[];
        /**
         * A list of [Kubernetes taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)
         * to apply to nodes. GKE's API can only set this field on cluster creation.
         * However, GKE will add taints to your nodes if you enable certain features such
         * as GPUs. If this field is set, any diffs on this field will cause the provider to
         * recreate the underlying resource. Taint values can be updated safely in
         * Kubernetes (eg. through `kubectl`), and it's recommended that you do not use
         * this field to manage taints. If you do, `lifecycle.ignore_changes` is
         * recommended. Structure is documented below.
         */
        taints: outputs.container.ClusterNodePoolNodeConfigTaint[];
        /**
         * Metadata configuration to expose to workloads on the node pool.
         * Structure is documented below.
         */
        workloadMetadataConfig: outputs.container.ClusterNodePoolNodeConfigWorkloadMetadataConfig;
    }

    export interface ClusterNodePoolNodeConfigAdvancedMachineFeatures {
        /**
         * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
         */
        threadsPerCore: number;
    }

    export interface ClusterNodePoolNodeConfigConfidentialNodes {
        /**
         * Enable Confidential GKE Nodes for this cluster, to
         * enforce encryption of data in-use.
         */
        enabled: boolean;
    }

    export interface ClusterNodePoolNodeConfigEphemeralStorageConfig {
        /**
         * Number of local SSDs to use to back ephemeral storage. Uses NVMe interfaces. Each local SSD is 375 GB in size. If zero, it means to disable using local SSDs as ephemeral storage.
         */
        localSsdCount: number;
    }

    export interface ClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig {
        /**
         * Number of local SSDs to use to back ephemeral storage. Uses NVMe interfaces. Each local SSD is 375 GB in size. If zero, it means to disable using local SSDs as ephemeral storage.
         */
        localSsdCount: number;
    }

    export interface ClusterNodePoolNodeConfigFastSocket {
        /**
         * Whether or not the NCCL Fast Socket is enabled
         */
        enabled: boolean;
    }

    export interface ClusterNodePoolNodeConfigGcfsConfig {
        /**
         * Whether or not the Google Container Filesystem (GCFS) is enabled
         */
        enabled: boolean;
    }

    export interface ClusterNodePoolNodeConfigGuestAccelerator {
        /**
         * The number of the guest accelerator cards exposed to this instance.
         */
        count: number;
        /**
         * Configuration for auto installation of GPU driver. Structure is documented below.
         */
        gpuDriverInstallationConfig?: outputs.container.ClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig;
        /**
         * Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig [user guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
         */
        gpuPartitionSize?: string;
        /**
         * Configuration for GPU sharing. Structure is documented below.
         */
        gpuSharingConfig?: outputs.container.ClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig;
        /**
         * The accelerator type resource to expose to this instance. E.g. `nvidia-tesla-k80`.
         */
        type: string;
    }

    export interface ClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
        /**
         * Mode for how the GPU driver is installed.
         * Accepted values are:
         * * `"GPU_DRIVER_VERSION_UNSPECIFIED"`: Default value is to not install any GPU driver.
         * * `"INSTALLATION_DISABLED"`: Disable GPU driver auto installation and needs manual installation.
         * * `"DEFAULT"`: "Default" GPU driver in COS and Ubuntu.
         * * `"LATEST"`: "Latest" GPU driver in COS.
         */
        gpuDriverVersion: string;
    }

    export interface ClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig {
        /**
         * The type of GPU sharing strategy to enable on the GPU node.
         * Accepted values are:
         * * `"TIME_SHARING"`: Allow multiple containers to have [time-shared](https://cloud.google.com/kubernetes-engine/docs/concepts/timesharing-gpus) access to a single GPU device.
         */
        gpuSharingStrategy: string;
        /**
         * The maximum number of containers that can share a GPU.
         */
        maxSharedClientsPerGpu: number;
    }

    export interface ClusterNodePoolNodeConfigGvnic {
        /**
         * Whether or not the Google Virtual NIC (gVNIC) is enabled
         */
        enabled: boolean;
    }

    export interface ClusterNodePoolNodeConfigHostMaintenancePolicy {
        maintenanceInterval: string;
    }

    export interface ClusterNodePoolNodeConfigKubeletConfig {
        /**
         * If true, enables CPU CFS quota enforcement for
         * containers that specify CPU limits.
         */
        cpuCfsQuota?: boolean;
        /**
         * The CPU CFS quota period value. Specified
         * as a sequence of decimal numbers, each with optional fraction and a unit suffix,
         * such as `"300ms"`. Valid time units are "ns", "us" (or "s"), "ms", "s", "m",
         * "h". The value must be a positive duration.
         *
         * > Note: At the time of writing (2020/08/18) the GKE API rejects the `none`
         * value and accepts an invalid `default` value instead. While this remains true,
         * not specifying the `kubeletConfig` block should be the equivalent of specifying
         * `none`.
         */
        cpuCfsQuotaPeriod?: string;
        /**
         * The CPU management policy on the node. See
         * [K8S CPU Management Policies](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/).
         * One of `"none"` or `"static"`. Defaults to `none` when `kubeletConfig` is unset.
         */
        cpuManagerPolicy: string;
        /**
         * Controls the maximum number of processes allowed to run in a pod. The value must be greater than or equal to 1024 and less than 4194304.
         */
        podPidsLimit?: number;
    }

    export interface ClusterNodePoolNodeConfigLinuxNodeConfig {
        /**
         * The Linux kernel parameters to be applied to the nodes
         * and all pods running on the nodes. Specified as a map from the key, such as
         * `net.core.wmem_max`, to a string value.
         */
        sysctls: {[key: string]: string};
    }

    export interface ClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig {
        /**
         * Number of raw-block local NVMe SSD disks to be attached to the node. Each local SSD is 375 GB in size. If zero, it means no raw-block local NVMe SSD disks to be attached to the node.
         * > Note: Local NVMe SSD storage available in GKE versions v1.25.3-gke.1800 and later.
         */
        localSsdCount: number;
    }

    export interface ClusterNodePoolNodeConfigReservationAffinity {
        /**
         * The type of reservation consumption
         * Accepted values are:
         *
         * * `"UNSPECIFIED"`: Default value. This should not be used.
         * * `"NO_RESERVATION"`: Do not consume from any reserved capacity.
         * * `"ANY_RESERVATION"`: Consume any reservation available.
         * * `"SPECIFIC_RESERVATION"`: Must consume from a specific reservation. Must specify key value fields for specifying the reservations.
         */
        consumeReservationType: string;
        /**
         * The label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify "compute.googleapis.com/reservation-name" as the key and specify the name of your reservation as its value.
         */
        key?: string;
        /**
         * The list of label values of reservation resources. For example: the name of the specific reservation when using a key of "compute.googleapis.com/reservation-name"
         */
        values?: string[];
    }

    export interface ClusterNodePoolNodeConfigSandboxConfig {
        /**
         * Which sandbox to use for pods in the node pool.
         * Accepted values are:
         *
         * * `"gvisor"`: Pods run within a gVisor sandbox.
         */
        sandboxType: string;
    }

    export interface ClusterNodePoolNodeConfigShieldedInstanceConfig {
        /**
         * Defines if the instance has integrity monitoring enabled.
         *
         * Enables monitoring and attestation of the boot integrity of the instance. The attestation is performed against the integrity policy baseline. This baseline is initially derived from the implicitly trusted boot image when the instance is created.  Defaults to `true`.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Defines if the instance has Secure Boot enabled.
         *
         * Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.  Defaults to `false`.
         */
        enableSecureBoot?: boolean;
    }

    export interface ClusterNodePoolNodeConfigSoleTenantConfig {
        nodeAffinities: outputs.container.ClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity[];
    }

    export interface ClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity {
        /**
         * The default or custom node affinity label key name.
         */
        key: string;
        /**
         * Specifies affinity or anti-affinity. Accepted values are `"IN"` or `"NOT_IN"`
         */
        operator: string;
        /**
         * List of node affinity label values as strings.
         */
        values: string[];
    }

    export interface ClusterNodePoolNodeConfigTaint {
        /**
         * Effect for taint. Accepted values are `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, and `NO_EXECUTE`.
         */
        effect: string;
        /**
         * Key for taint.
         */
        key: string;
        /**
         * Value for taint.
         */
        value: string;
    }

    export interface ClusterNodePoolNodeConfigWorkloadMetadataConfig {
        /**
         * How to expose the node metadata to the workload running on the node.
         * Accepted values are:
         * * UNSPECIFIED: Not Set
         * * GCE_METADATA: Expose all Compute Engine metadata to pods.
         * * GKE_METADATA: Run the GKE Metadata Server on this node. The GKE Metadata Server exposes a metadata API to workloads that is compatible with the V1 Compute Metadata APIs exposed by the Compute Engine and App Engine Metadata Servers. This feature can only be enabled if [workload identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) is enabled at the cluster level.
         */
        mode: string;
    }

    export interface ClusterNodePoolPlacementPolicy {
        policyName?: string;
        tpuTopology?: string;
        /**
         * Telemetry integration for the cluster. Supported values (`ENABLED, DISABLED, SYSTEM_ONLY`);
         * `SYSTEM_ONLY` (Only system components are monitored and logged) is only available in GKE versions 1.15 and later.
         */
        type: string;
    }

    export interface ClusterNodePoolUpgradeSettings {
        /**
         * Settings for blue-green upgrade strategy. To be specified when strategy is set to BLUE_GREEN. Structure is documented below.
         */
        blueGreenSettings: outputs.container.ClusterNodePoolUpgradeSettingsBlueGreenSettings;
        /**
         * The maximum number of nodes that can be created beyond the current size of the node pool during the upgrade process. To be used when strategy is set to SURGE. Default is 0.
         */
        maxSurge: number;
        /**
         * The maximum number of nodes that can be simultaneously unavailable during the upgrade process. To be used when strategy is set to SURGE. Default is 0.
         */
        maxUnavailable: number;
        /**
         * Strategy used for node pool update. Strategy can only be one of BLUE_GREEN or SURGE. The default is value is SURGE.
         */
        strategy?: string;
    }

    export interface ClusterNodePoolUpgradeSettingsBlueGreenSettings {
        /**
         * Time needed after draining entire blue pool. After this period, blue pool will be cleaned up. A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
         */
        nodePoolSoakDuration: string;
        /**
         * Standard policy for the blue-green upgrade. To be specified when strategy is set to BLUE_GREEN. Structure is documented below.
         */
        standardRolloutPolicy: outputs.container.ClusterNodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy;
    }

    export interface ClusterNodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy {
        /**
         * Number of blue nodes to drain in a batch. Only one of the batchPercentage or batchNodeCount can be specified.
         */
        batchNodeCount: number;
        /**
         * Percentage of the bool pool nodes to drain in a batch. The range of this field should be (0.0, 1.0). Only one of the batchPercentage or batchNodeCount can be specified.
         */
        batchPercentage: number;
        /**
         * Soak time after each batch gets drained. A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".`.
         */
        batchSoakDuration: string;
    }

    export interface ClusterNotificationConfig {
        /**
         * The pubsub config for the cluster's upgrade notifications.
         */
        pubsub: outputs.container.ClusterNotificationConfigPubsub;
    }

    export interface ClusterNotificationConfigPubsub {
        /**
         * Whether or not the notification config is enabled
         */
        enabled: boolean;
        /**
         * Choose what type of notifications you want to receive. If no filters are applied, you'll receive all notification types. Structure is documented below.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        filter?: outputs.container.ClusterNotificationConfigPubsubFilter;
        /**
         * The pubsub topic to push upgrade notifications to. Must be in the same project as the cluster. Must be in the format: `projects/{project}/topics/{topic}`.
         */
        topic?: string;
    }

    export interface ClusterNotificationConfigPubsubFilter {
        /**
         * Can be used to filter what notifications are sent. Accepted values are `UPGRADE_AVAILABLE_EVENT`, `UPGRADE_EVENT` and `SECURITY_BULLETIN_EVENT`. See [Filtering notifications](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-notifications#filtering) for more details.
         */
        eventTypes: string[];
    }

    export interface ClusterPodSecurityPolicyConfig {
        /**
         * Enable the PodSecurityPolicy controller for this cluster.
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         */
        enabled: boolean;
    }

    export interface ClusterPrivateClusterConfig {
        /**
         * When `true`, the cluster's private
         * endpoint is used as the cluster endpoint and access through the public endpoint
         * is disabled. When `false`, either endpoint can be used. This field only applies
         * to private clusters, when `enablePrivateNodes` is `true`.
         */
        enablePrivateEndpoint?: boolean;
        /**
         * Enables the private cluster feature,
         * creating a private endpoint on the cluster. In a private cluster, nodes only
         * have RFC 1918 private addresses and communicate with the master's private
         * endpoint via private networking.
         */
        enablePrivateNodes?: boolean;
        /**
         * Controls cluster master global
         * access settings. If unset, the provider will no longer manage this field and will
         * not modify the previously-set value. Structure is documented below.
         */
        masterGlobalAccessConfig: outputs.container.ClusterPrivateClusterConfigMasterGlobalAccessConfig;
        /**
         * The IP range in CIDR notation to use for
         * the hosted master network. This range will be used for assigning private IP
         * addresses to the cluster master(s) and the ILB VIP. This range must not overlap
         * with any other ranges in use within the cluster's network, and it must be a /28
         * subnet. See [Private Cluster Limitations](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#req_res_lim)
         * for more details. This field only applies to private clusters, when
         * `enablePrivateNodes` is `true`.
         */
        masterIpv4CidrBlock: string;
        /**
         * The name of the peering between this cluster and the Google owned VPC.
         */
        peeringName: string;
        /**
         * The internal IP address of this cluster's master endpoint.
         */
        privateEndpoint: string;
        /**
         * Subnetwork in cluster's network where master's endpoint will be provisioned.
         */
        privateEndpointSubnetwork?: string;
        /**
         * The external IP address of this cluster's master endpoint.
         *
         * !> The Google provider is unable to validate certain configurations of
         * `privateClusterConfig` when `enablePrivateNodes` is `false`. It's
         * recommended that you omit the block entirely if the field is not set to `true`.
         */
        publicEndpoint: string;
    }

    export interface ClusterPrivateClusterConfigMasterGlobalAccessConfig {
        /**
         * Whether the cluster master is accessible globally or
         * not.
         */
        enabled: boolean;
    }

    export interface ClusterProtectConfig {
        /**
         * ) WorkloadConfig defines which actions are enabled for a cluster's workload configurations. Structure is documented below
         */
        workloadConfig: outputs.container.ClusterProtectConfigWorkloadConfig;
        /**
         * ) Sets which mode to use for Protect workload vulnerability scanning feature. Accepted values are DISABLED, BASIC.
         */
        workloadVulnerabilityMode: string;
    }

    export interface ClusterProtectConfigWorkloadConfig {
        /**
         * ) Sets which mode of auditing should be used for the cluster's workloads. Accepted values are DISABLED, BASIC.
         */
        auditMode: string;
    }

    export interface ClusterReleaseChannel {
        /**
         * The selected release channel.
         * Accepted values are:
         * * UNSPECIFIED: Not set.
         * * RAPID: Weekly upgrade cadence; Early testers and developers who requires new features.
         * * REGULAR: Multiple per month upgrade cadence; Production users who need features not yet offered in the Stable channel.
         * * STABLE: Every few months upgrade cadence; Production users who need stability above all else, and for whom frequent upgrades are too risky.
         */
        channel: string;
    }

    export interface ClusterResourceUsageExportConfig {
        /**
         * Parameters for using BigQuery as the destination of resource usage export.
         *
         * * `bigquery_destination.dataset_id` (Required) - The ID of a BigQuery Dataset. For Example:
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        bigqueryDestination: outputs.container.ClusterResourceUsageExportConfigBigqueryDestination;
        /**
         * Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created
         * in the cluster to meter network egress traffic.
         */
        enableNetworkEgressMetering?: boolean;
        /**
         * Whether to enable resource
         * consumption metering on this cluster. When enabled, a table will be created in
         * the resource export BigQuery dataset to store resource consumption data. The
         * resulting table can be joined with the resource usage table or with BigQuery
         * billing export. Defaults to `true`.
         */
        enableResourceConsumptionMetering?: boolean;
    }

    export interface ClusterResourceUsageExportConfigBigqueryDestination {
        datasetId: string;
    }

    export interface ClusterSecurityPostureConfig {
        /**
         * Sets the mode of the Kubernetes security posture API's off-cluster features. Available options include `DISABLED` and `BASIC`.
         */
        mode: string;
        /**
         * Sets the mode of the Kubernetes security posture API's workload vulnerability scanning. Available options include `VULNERABILITY_DISABLED` and `VULNERABILITY_BASIC`.
         */
        vulnerabilityMode: string;
    }

    export interface ClusterServiceExternalIpsConfig {
        /**
         * Controls whether external ips specified by a service will be allowed. It is enabled by default.
         */
        enabled: boolean;
    }

    export interface ClusterTpuConfig {
        /**
         * Enable Binary Authorization for this cluster. Deprecated in favor of `evaluationMode`.
         *
         *
         *
         * for autopilot clusters. Resource limits for `cpu` and `memory` must be defined to enable node auto-provisioning for GKE Standard.
         *
         *
         *
         *
         *
         *
         *
         * enforce encryption of data in-use.
         *
         * If enabled, pods must be valid under a PodSecurityPolicy to be created.
         *
         * not.
         */
        enabled: boolean;
        ipv4CidrBlock: string;
        useServiceNetworking?: boolean;
    }

    export interface ClusterVerticalPodAutoscaling {
        /**
         * Enables vertical pod autoscaling
         */
        enabled: boolean;
    }

    export interface ClusterWorkloadIdentityConfig {
        /**
         * The workload pool to attach all Kubernetes service accounts to.
         *
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        workloadPool?: string;
    }

    export interface GetClusterAddonsConfig {
        cloudrunConfigs: outputs.container.GetClusterAddonsConfigCloudrunConfig[];
        configConnectorConfigs: outputs.container.GetClusterAddonsConfigConfigConnectorConfig[];
        dnsCacheConfigs: outputs.container.GetClusterAddonsConfigDnsCacheConfig[];
        gcePersistentDiskCsiDriverConfigs: outputs.container.GetClusterAddonsConfigGcePersistentDiskCsiDriverConfig[];
        gcpFilestoreCsiDriverConfigs: outputs.container.GetClusterAddonsConfigGcpFilestoreCsiDriverConfig[];
        gcsFuseCsiDriverConfigs: outputs.container.GetClusterAddonsConfigGcsFuseCsiDriverConfig[];
        gkeBackupAgentConfigs: outputs.container.GetClusterAddonsConfigGkeBackupAgentConfig[];
        horizontalPodAutoscalings: outputs.container.GetClusterAddonsConfigHorizontalPodAutoscaling[];
        httpLoadBalancings: outputs.container.GetClusterAddonsConfigHttpLoadBalancing[];
        istioConfigs: outputs.container.GetClusterAddonsConfigIstioConfig[];
        kalmConfigs: outputs.container.GetClusterAddonsConfigKalmConfig[];
        networkPolicyConfigs: outputs.container.GetClusterAddonsConfigNetworkPolicyConfig[];
    }

    export interface GetClusterAddonsConfigCloudrunConfig {
        disabled: boolean;
        loadBalancerType: string;
    }

    export interface GetClusterAddonsConfigConfigConnectorConfig {
        enabled: boolean;
    }

    export interface GetClusterAddonsConfigDnsCacheConfig {
        enabled: boolean;
    }

    export interface GetClusterAddonsConfigGcePersistentDiskCsiDriverConfig {
        enabled: boolean;
    }

    export interface GetClusterAddonsConfigGcpFilestoreCsiDriverConfig {
        enabled: boolean;
    }

    export interface GetClusterAddonsConfigGcsFuseCsiDriverConfig {
        enabled: boolean;
    }

    export interface GetClusterAddonsConfigGkeBackupAgentConfig {
        enabled: boolean;
    }

    export interface GetClusterAddonsConfigHorizontalPodAutoscaling {
        disabled: boolean;
    }

    export interface GetClusterAddonsConfigHttpLoadBalancing {
        disabled: boolean;
    }

    export interface GetClusterAddonsConfigIstioConfig {
        auth: string;
        disabled: boolean;
    }

    export interface GetClusterAddonsConfigKalmConfig {
        enabled: boolean;
    }

    export interface GetClusterAddonsConfigNetworkPolicyConfig {
        disabled: boolean;
    }

    export interface GetClusterAuthenticatorGroupsConfig {
        securityGroup: string;
    }

    export interface GetClusterBinaryAuthorization {
        enabled: boolean;
        evaluationMode: string;
    }

    export interface GetClusterClusterAutoscaling {
        autoProvisioningDefaults: outputs.container.GetClusterClusterAutoscalingAutoProvisioningDefault[];
        autoscalingProfile: string;
        enabled: boolean;
        resourceLimits: outputs.container.GetClusterClusterAutoscalingResourceLimit[];
    }

    export interface GetClusterClusterAutoscalingAutoProvisioningDefault {
        bootDiskKmsKey: string;
        diskSize: number;
        diskType: string;
        imageType: string;
        managements: outputs.container.GetClusterClusterAutoscalingAutoProvisioningDefaultManagement[];
        minCpuPlatform: string;
        oauthScopes: string[];
        serviceAccount: string;
        shieldedInstanceConfigs: outputs.container.GetClusterClusterAutoscalingAutoProvisioningDefaultShieldedInstanceConfig[];
        upgradeSettings: outputs.container.GetClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSetting[];
    }

    export interface GetClusterClusterAutoscalingAutoProvisioningDefaultManagement {
        autoRepair: boolean;
        autoUpgrade: boolean;
        upgradeOptions: outputs.container.GetClusterClusterAutoscalingAutoProvisioningDefaultManagementUpgradeOption[];
    }

    export interface GetClusterClusterAutoscalingAutoProvisioningDefaultManagementUpgradeOption {
        autoUpgradeStartTime: string;
        description: string;
    }

    export interface GetClusterClusterAutoscalingAutoProvisioningDefaultShieldedInstanceConfig {
        enableIntegrityMonitoring: boolean;
        enableSecureBoot: boolean;
    }

    export interface GetClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSetting {
        blueGreenSettings: outputs.container.GetClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSetting[];
        maxSurge: number;
        maxUnavailable: number;
        strategy: string;
    }

    export interface GetClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSetting {
        nodePoolSoakDuration: string;
        standardRolloutPolicies: outputs.container.GetClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSettingStandardRolloutPolicy[];
    }

    export interface GetClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSettingStandardRolloutPolicy {
        batchNodeCount: number;
        batchPercentage: number;
        batchSoakDuration: string;
    }

    export interface GetClusterClusterAutoscalingResourceLimit {
        maximum: number;
        minimum: number;
        resourceType: string;
    }

    export interface GetClusterClusterTelemetry {
        type: string;
    }

    export interface GetClusterConfidentialNode {
        enabled: boolean;
    }

    export interface GetClusterCostManagementConfig {
        enabled: boolean;
    }

    export interface GetClusterDatabaseEncryption {
        keyName: string;
        state: string;
    }

    export interface GetClusterDefaultSnatStatus {
        disabled: boolean;
    }

    export interface GetClusterDnsConfig {
        clusterDns: string;
        clusterDnsDomain: string;
        clusterDnsScope: string;
    }

    export interface GetClusterEnableK8sBetaApi {
        enabledApis: string[];
    }

    export interface GetClusterGatewayApiConfig {
        channel: string;
    }

    export interface GetClusterIdentityServiceConfig {
        enabled: boolean;
    }

    export interface GetClusterIpAllocationPolicy {
        additionalPodRangesConfigs: outputs.container.GetClusterIpAllocationPolicyAdditionalPodRangesConfig[];
        clusterIpv4CidrBlock: string;
        clusterSecondaryRangeName: string;
        podCidrOverprovisionConfigs: outputs.container.GetClusterIpAllocationPolicyPodCidrOverprovisionConfig[];
        servicesIpv4CidrBlock: string;
        servicesSecondaryRangeName: string;
        stackType: string;
    }

    export interface GetClusterIpAllocationPolicyAdditionalPodRangesConfig {
        podRangeNames: string[];
    }

    export interface GetClusterIpAllocationPolicyPodCidrOverprovisionConfig {
        disabled: boolean;
    }

    export interface GetClusterLoggingConfig {
        enableComponents: string[];
    }

    export interface GetClusterMaintenancePolicy {
        dailyMaintenanceWindows: outputs.container.GetClusterMaintenancePolicyDailyMaintenanceWindow[];
        maintenanceExclusions: outputs.container.GetClusterMaintenancePolicyMaintenanceExclusion[];
        recurringWindows: outputs.container.GetClusterMaintenancePolicyRecurringWindow[];
    }

    export interface GetClusterMaintenancePolicyDailyMaintenanceWindow {
        duration: string;
        startTime: string;
    }

    export interface GetClusterMaintenancePolicyMaintenanceExclusion {
        endTime: string;
        exclusionName: string;
        exclusionOptions: outputs.container.GetClusterMaintenancePolicyMaintenanceExclusionExclusionOption[];
        startTime: string;
    }

    export interface GetClusterMaintenancePolicyMaintenanceExclusionExclusionOption {
        scope: string;
    }

    export interface GetClusterMaintenancePolicyRecurringWindow {
        endTime: string;
        recurrence: string;
        startTime: string;
    }

    export interface GetClusterMasterAuth {
        clientCertificate: string;
        clientCertificateConfigs: outputs.container.GetClusterMasterAuthClientCertificateConfig[];
        clientKey: string;
        clusterCaCertificate: string;
    }

    export interface GetClusterMasterAuthClientCertificateConfig {
        issueClientCertificate: boolean;
    }

    export interface GetClusterMasterAuthorizedNetworksConfig {
        cidrBlocks: outputs.container.GetClusterMasterAuthorizedNetworksConfigCidrBlock[];
        gcpPublicCidrsAccessEnabled: boolean;
    }

    export interface GetClusterMasterAuthorizedNetworksConfigCidrBlock {
        cidrBlock: string;
        displayName: string;
    }

    export interface GetClusterMeshCertificate {
        enableCertificates: boolean;
    }

    export interface GetClusterMonitoringConfig {
        advancedDatapathObservabilityConfigs: outputs.container.GetClusterMonitoringConfigAdvancedDatapathObservabilityConfig[];
        enableComponents: string[];
        managedPrometheuses: outputs.container.GetClusterMonitoringConfigManagedPrometheus[];
    }

    export interface GetClusterMonitoringConfigAdvancedDatapathObservabilityConfig {
        enableMetrics: boolean;
        relayMode: string;
    }

    export interface GetClusterMonitoringConfigManagedPrometheus {
        enabled: boolean;
    }

    export interface GetClusterNetworkPolicy {
        enabled: boolean;
        provider: string;
    }

    export interface GetClusterNodeConfig {
        advancedMachineFeatures: outputs.container.GetClusterNodeConfigAdvancedMachineFeature[];
        bootDiskKmsKey: string;
        confidentialNodes: outputs.container.GetClusterNodeConfigConfidentialNode[];
        diskSizeGb: number;
        diskType: string;
        ephemeralStorageConfigs: outputs.container.GetClusterNodeConfigEphemeralStorageConfig[];
        ephemeralStorageLocalSsdConfigs: outputs.container.GetClusterNodeConfigEphemeralStorageLocalSsdConfig[];
        fastSockets: outputs.container.GetClusterNodeConfigFastSocket[];
        gcfsConfigs: outputs.container.GetClusterNodeConfigGcfsConfig[];
        guestAccelerators: outputs.container.GetClusterNodeConfigGuestAccelerator[];
        gvnics: outputs.container.GetClusterNodeConfigGvnic[];
        hostMaintenancePolicies: outputs.container.GetClusterNodeConfigHostMaintenancePolicy[];
        imageType: string;
        kubeletConfigs: outputs.container.GetClusterNodeConfigKubeletConfig[];
        labels: {[key: string]: string};
        linuxNodeConfigs: outputs.container.GetClusterNodeConfigLinuxNodeConfig[];
        localNvmeSsdBlockConfigs: outputs.container.GetClusterNodeConfigLocalNvmeSsdBlockConfig[];
        localSsdCount: number;
        loggingVariant: string;
        machineType: string;
        metadata: {[key: string]: string};
        minCpuPlatform: string;
        nodeGroup: string;
        oauthScopes: string[];
        preemptible: boolean;
        reservationAffinities: outputs.container.GetClusterNodeConfigReservationAffinity[];
        resourceLabels: {[key: string]: string};
        sandboxConfigs: outputs.container.GetClusterNodeConfigSandboxConfig[];
        serviceAccount: string;
        shieldedInstanceConfigs: outputs.container.GetClusterNodeConfigShieldedInstanceConfig[];
        soleTenantConfigs: outputs.container.GetClusterNodeConfigSoleTenantConfig[];
        spot: boolean;
        tags: string[];
        taints: outputs.container.GetClusterNodeConfigTaint[];
        workloadMetadataConfigs: outputs.container.GetClusterNodeConfigWorkloadMetadataConfig[];
    }

    export interface GetClusterNodeConfigAdvancedMachineFeature {
        threadsPerCore: number;
    }

    export interface GetClusterNodeConfigConfidentialNode {
        enabled: boolean;
    }

    export interface GetClusterNodeConfigEphemeralStorageConfig {
        localSsdCount: number;
    }

    export interface GetClusterNodeConfigEphemeralStorageLocalSsdConfig {
        localSsdCount: number;
    }

    export interface GetClusterNodeConfigFastSocket {
        enabled: boolean;
    }

    export interface GetClusterNodeConfigGcfsConfig {
        enabled: boolean;
    }

    export interface GetClusterNodeConfigGuestAccelerator {
        count: number;
        gpuDriverInstallationConfigs: outputs.container.GetClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig[];
        gpuPartitionSize: string;
        gpuSharingConfigs: outputs.container.GetClusterNodeConfigGuestAcceleratorGpuSharingConfig[];
        type: string;
    }

    export interface GetClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
        gpuDriverVersion: string;
    }

    export interface GetClusterNodeConfigGuestAcceleratorGpuSharingConfig {
        gpuSharingStrategy: string;
        maxSharedClientsPerGpu: number;
    }

    export interface GetClusterNodeConfigGvnic {
        enabled: boolean;
    }

    export interface GetClusterNodeConfigHostMaintenancePolicy {
        maintenanceInterval: string;
    }

    export interface GetClusterNodeConfigKubeletConfig {
        cpuCfsQuota: boolean;
        cpuCfsQuotaPeriod: string;
        cpuManagerPolicy: string;
        podPidsLimit: number;
    }

    export interface GetClusterNodeConfigLinuxNodeConfig {
        sysctls: {[key: string]: string};
    }

    export interface GetClusterNodeConfigLocalNvmeSsdBlockConfig {
        localSsdCount: number;
    }

    export interface GetClusterNodeConfigReservationAffinity {
        consumeReservationType: string;
        key: string;
        values: string[];
    }

    export interface GetClusterNodeConfigSandboxConfig {
        sandboxType: string;
    }

    export interface GetClusterNodeConfigShieldedInstanceConfig {
        enableIntegrityMonitoring: boolean;
        enableSecureBoot: boolean;
    }

    export interface GetClusterNodeConfigSoleTenantConfig {
        nodeAffinities: outputs.container.GetClusterNodeConfigSoleTenantConfigNodeAffinity[];
    }

    export interface GetClusterNodeConfigSoleTenantConfigNodeAffinity {
        key: string;
        operator: string;
        values: string[];
    }

    export interface GetClusterNodeConfigTaint {
        effect: string;
        key: string;
        value: string;
    }

    export interface GetClusterNodeConfigWorkloadMetadataConfig {
        mode: string;
    }

    export interface GetClusterNodePool {
        autoscalings: outputs.container.GetClusterNodePoolAutoscaling[];
        initialNodeCount: number;
        instanceGroupUrls: string[];
        managedInstanceGroupUrls: string[];
        managements: outputs.container.GetClusterNodePoolManagement[];
        maxPodsPerNode: number;
        /**
         * The name of the cluster.
         */
        name: string;
        namePrefix: string;
        networkConfigs: outputs.container.GetClusterNodePoolNetworkConfig[];
        nodeConfigs: outputs.container.GetClusterNodePoolNodeConfig[];
        nodeCount: number;
        nodeLocations: string[];
        placementPolicies: outputs.container.GetClusterNodePoolPlacementPolicy[];
        upgradeSettings: outputs.container.GetClusterNodePoolUpgradeSetting[];
        version: string;
    }

    export interface GetClusterNodePoolAutoConfig {
        networkTags: outputs.container.GetClusterNodePoolAutoConfigNetworkTag[];
    }

    export interface GetClusterNodePoolAutoConfigNetworkTag {
        tags: string[];
    }

    export interface GetClusterNodePoolAutoscaling {
        locationPolicy: string;
        maxNodeCount: number;
        minNodeCount: number;
        totalMaxNodeCount: number;
        totalMinNodeCount: number;
    }

    export interface GetClusterNodePoolDefault {
        nodeConfigDefaults: outputs.container.GetClusterNodePoolDefaultNodeConfigDefault[];
    }

    export interface GetClusterNodePoolDefaultNodeConfigDefault {
        gcfsConfigs: outputs.container.GetClusterNodePoolDefaultNodeConfigDefaultGcfsConfig[];
        loggingVariant: string;
    }

    export interface GetClusterNodePoolDefaultNodeConfigDefaultGcfsConfig {
        enabled: boolean;
    }

    export interface GetClusterNodePoolManagement {
        autoRepair: boolean;
        autoUpgrade: boolean;
    }

    export interface GetClusterNodePoolNetworkConfig {
        additionalNodeNetworkConfigs: outputs.container.GetClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig[];
        additionalPodNetworkConfigs: outputs.container.GetClusterNodePoolNetworkConfigAdditionalPodNetworkConfig[];
        createPodRange: boolean;
        enablePrivateNodes: boolean;
        podCidrOverprovisionConfigs: outputs.container.GetClusterNodePoolNetworkConfigPodCidrOverprovisionConfig[];
        podIpv4CidrBlock: string;
        podRange: string;
    }

    export interface GetClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig {
        network: string;
        subnetwork: string;
    }

    export interface GetClusterNodePoolNetworkConfigAdditionalPodNetworkConfig {
        maxPodsPerNode: number;
        secondaryPodRange: string;
        subnetwork: string;
    }

    export interface GetClusterNodePoolNetworkConfigPodCidrOverprovisionConfig {
        disabled: boolean;
    }

    export interface GetClusterNodePoolNodeConfig {
        advancedMachineFeatures: outputs.container.GetClusterNodePoolNodeConfigAdvancedMachineFeature[];
        bootDiskKmsKey: string;
        confidentialNodes: outputs.container.GetClusterNodePoolNodeConfigConfidentialNode[];
        diskSizeGb: number;
        diskType: string;
        ephemeralStorageConfigs: outputs.container.GetClusterNodePoolNodeConfigEphemeralStorageConfig[];
        ephemeralStorageLocalSsdConfigs: outputs.container.GetClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig[];
        fastSockets: outputs.container.GetClusterNodePoolNodeConfigFastSocket[];
        gcfsConfigs: outputs.container.GetClusterNodePoolNodeConfigGcfsConfig[];
        guestAccelerators: outputs.container.GetClusterNodePoolNodeConfigGuestAccelerator[];
        gvnics: outputs.container.GetClusterNodePoolNodeConfigGvnic[];
        hostMaintenancePolicies: outputs.container.GetClusterNodePoolNodeConfigHostMaintenancePolicy[];
        imageType: string;
        kubeletConfigs: outputs.container.GetClusterNodePoolNodeConfigKubeletConfig[];
        labels: {[key: string]: string};
        linuxNodeConfigs: outputs.container.GetClusterNodePoolNodeConfigLinuxNodeConfig[];
        localNvmeSsdBlockConfigs: outputs.container.GetClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig[];
        localSsdCount: number;
        loggingVariant: string;
        machineType: string;
        metadata: {[key: string]: string};
        minCpuPlatform: string;
        nodeGroup: string;
        oauthScopes: string[];
        preemptible: boolean;
        reservationAffinities: outputs.container.GetClusterNodePoolNodeConfigReservationAffinity[];
        resourceLabels: {[key: string]: string};
        sandboxConfigs: outputs.container.GetClusterNodePoolNodeConfigSandboxConfig[];
        serviceAccount: string;
        shieldedInstanceConfigs: outputs.container.GetClusterNodePoolNodeConfigShieldedInstanceConfig[];
        soleTenantConfigs: outputs.container.GetClusterNodePoolNodeConfigSoleTenantConfig[];
        spot: boolean;
        tags: string[];
        taints: outputs.container.GetClusterNodePoolNodeConfigTaint[];
        workloadMetadataConfigs: outputs.container.GetClusterNodePoolNodeConfigWorkloadMetadataConfig[];
    }

    export interface GetClusterNodePoolNodeConfigAdvancedMachineFeature {
        threadsPerCore: number;
    }

    export interface GetClusterNodePoolNodeConfigConfidentialNode {
        enabled: boolean;
    }

    export interface GetClusterNodePoolNodeConfigEphemeralStorageConfig {
        localSsdCount: number;
    }

    export interface GetClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig {
        localSsdCount: number;
    }

    export interface GetClusterNodePoolNodeConfigFastSocket {
        enabled: boolean;
    }

    export interface GetClusterNodePoolNodeConfigGcfsConfig {
        enabled: boolean;
    }

    export interface GetClusterNodePoolNodeConfigGuestAccelerator {
        count: number;
        gpuDriverInstallationConfigs: outputs.container.GetClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig[];
        gpuPartitionSize: string;
        gpuSharingConfigs: outputs.container.GetClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig[];
        type: string;
    }

    export interface GetClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
        gpuDriverVersion: string;
    }

    export interface GetClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig {
        gpuSharingStrategy: string;
        maxSharedClientsPerGpu: number;
    }

    export interface GetClusterNodePoolNodeConfigGvnic {
        enabled: boolean;
    }

    export interface GetClusterNodePoolNodeConfigHostMaintenancePolicy {
        maintenanceInterval: string;
    }

    export interface GetClusterNodePoolNodeConfigKubeletConfig {
        cpuCfsQuota: boolean;
        cpuCfsQuotaPeriod: string;
        cpuManagerPolicy: string;
        podPidsLimit: number;
    }

    export interface GetClusterNodePoolNodeConfigLinuxNodeConfig {
        sysctls: {[key: string]: string};
    }

    export interface GetClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig {
        localSsdCount: number;
    }

    export interface GetClusterNodePoolNodeConfigReservationAffinity {
        consumeReservationType: string;
        key: string;
        values: string[];
    }

    export interface GetClusterNodePoolNodeConfigSandboxConfig {
        sandboxType: string;
    }

    export interface GetClusterNodePoolNodeConfigShieldedInstanceConfig {
        enableIntegrityMonitoring: boolean;
        enableSecureBoot: boolean;
    }

    export interface GetClusterNodePoolNodeConfigSoleTenantConfig {
        nodeAffinities: outputs.container.GetClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity[];
    }

    export interface GetClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity {
        key: string;
        operator: string;
        values: string[];
    }

    export interface GetClusterNodePoolNodeConfigTaint {
        effect: string;
        key: string;
        value: string;
    }

    export interface GetClusterNodePoolNodeConfigWorkloadMetadataConfig {
        mode: string;
    }

    export interface GetClusterNodePoolPlacementPolicy {
        policyName: string;
        tpuTopology: string;
        type: string;
    }

    export interface GetClusterNodePoolUpgradeSetting {
        blueGreenSettings: outputs.container.GetClusterNodePoolUpgradeSettingBlueGreenSetting[];
        maxSurge: number;
        maxUnavailable: number;
        strategy: string;
    }

    export interface GetClusterNodePoolUpgradeSettingBlueGreenSetting {
        nodePoolSoakDuration: string;
        standardRolloutPolicies: outputs.container.GetClusterNodePoolUpgradeSettingBlueGreenSettingStandardRolloutPolicy[];
    }

    export interface GetClusterNodePoolUpgradeSettingBlueGreenSettingStandardRolloutPolicy {
        batchNodeCount: number;
        batchPercentage: number;
        batchSoakDuration: string;
    }

    export interface GetClusterNotificationConfig {
        pubsubs: outputs.container.GetClusterNotificationConfigPubsub[];
    }

    export interface GetClusterNotificationConfigPubsub {
        enabled: boolean;
        filters: outputs.container.GetClusterNotificationConfigPubsubFilter[];
        topic: string;
    }

    export interface GetClusterNotificationConfigPubsubFilter {
        eventTypes: string[];
    }

    export interface GetClusterPodSecurityPolicyConfig {
        enabled: boolean;
    }

    export interface GetClusterPrivateClusterConfig {
        enablePrivateEndpoint: boolean;
        enablePrivateNodes: boolean;
        masterGlobalAccessConfigs: outputs.container.GetClusterPrivateClusterConfigMasterGlobalAccessConfig[];
        masterIpv4CidrBlock: string;
        peeringName: string;
        privateEndpoint: string;
        privateEndpointSubnetwork: string;
        publicEndpoint: string;
    }

    export interface GetClusterPrivateClusterConfigMasterGlobalAccessConfig {
        enabled: boolean;
    }

    export interface GetClusterProtectConfig {
        workloadConfigs: outputs.container.GetClusterProtectConfigWorkloadConfig[];
        workloadVulnerabilityMode: string;
    }

    export interface GetClusterProtectConfigWorkloadConfig {
        auditMode: string;
    }

    export interface GetClusterReleaseChannel {
        channel: string;
    }

    export interface GetClusterResourceUsageExportConfig {
        bigqueryDestinations: outputs.container.GetClusterResourceUsageExportConfigBigqueryDestination[];
        enableNetworkEgressMetering: boolean;
        enableResourceConsumptionMetering: boolean;
    }

    export interface GetClusterResourceUsageExportConfigBigqueryDestination {
        datasetId: string;
    }

    export interface GetClusterSecurityPostureConfig {
        mode: string;
        vulnerabilityMode: string;
    }

    export interface GetClusterServiceExternalIpsConfig {
        enabled: boolean;
    }

    export interface GetClusterTpuConfig {
        enabled: boolean;
        ipv4CidrBlock: string;
        useServiceNetworking: boolean;
    }

    export interface GetClusterVerticalPodAutoscaling {
        enabled: boolean;
    }

    export interface GetClusterWorkloadIdentityConfig {
        workloadPool: string;
    }

    export interface NodePoolAutoscaling {
        /**
         * Location policy specifies the algorithm used when
         * scaling-up the node pool. Location policy is supported only in 1.24.1+ clusters.
         * * "BALANCED" - Is a best effort policy that aims to balance the sizes of available zones.
         * * "ANY" - Instructs the cluster autoscaler to prioritize utilization of unused reservations,
         * and reduce preemption risk for Spot VMs.
         */
        locationPolicy: string;
        /**
         * Maximum number of nodes per zone in the NodePool.
         * Must be >= min_node_count. Cannot be used with total limits.
         */
        maxNodeCount?: number;
        /**
         * Minimum number of nodes per zone in the NodePool.
         * Must be >=0 and <= `maxNodeCount`. Cannot be used with total limits.
         */
        minNodeCount?: number;
        /**
         * Total maximum number of nodes in the NodePool.
         * Must be >= total_min_node_count. Cannot be used with per zone limits.
         * Total size limits are supported only in 1.24.1+ clusters.
         */
        totalMaxNodeCount?: number;
        /**
         * Total minimum number of nodes in the NodePool.
         * Must be >=0 and <= `totalMaxNodeCount`. Cannot be used with per zone limits.
         * Total size limits are supported only in 1.24.1+ clusters.
         */
        totalMinNodeCount?: number;
    }

    export interface NodePoolManagement {
        /**
         * Whether the nodes will be automatically repaired.
         */
        autoRepair?: boolean;
        /**
         * Whether the nodes will be automatically upgraded.
         */
        autoUpgrade?: boolean;
    }

    export interface NodePoolNetworkConfig {
        /**
         * We specify the additional node networks for this node pool using this list. Each node network corresponds to an additional interface.
         * Structure is documented below
         */
        additionalNodeNetworkConfigs?: outputs.container.NodePoolNetworkConfigAdditionalNodeNetworkConfig[];
        /**
         * We specify the additional pod networks for this node pool using this list. Each pod network corresponds to an additional alias IP range for the node.
         * Structure is documented below
         */
        additionalPodNetworkConfigs?: outputs.container.NodePoolNetworkConfigAdditionalPodNetworkConfig[];
        /**
         * Whether to create a new range for pod IPs in this node pool. Defaults are provided for `podRange` and `podIpv4CidrBlock` if they are not specified.
         */
        createPodRange?: boolean;
        /**
         * Whether nodes have internal IP addresses only.
         */
        enablePrivateNodes: boolean;
        podCidrOverprovisionConfig: outputs.container.NodePoolNetworkConfigPodCidrOverprovisionConfig;
        /**
         * The IP address range for pod IPs in this node pool. Only applicable if createPodRange is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) to pick a specific range to use.
         */
        podIpv4CidrBlock: string;
        /**
         * The ID of the secondary range for pod IPs. If `createPodRange` is true, this ID is used for the new range. If `createPodRange` is false, uses an existing secondary range with this ID.
         */
        podRange: string;
    }

    export interface NodePoolNetworkConfigAdditionalNodeNetworkConfig {
        /**
         * Name of the VPC where the additional interface belongs.
         */
        network?: string;
        /**
         * Name of the subnetwork where the additional interface belongs.
         */
        subnetwork?: string;
    }

    export interface NodePoolNetworkConfigAdditionalPodNetworkConfig {
        /**
         * The maximum number of pods per node which use this pod network.
         */
        maxPodsPerNode: number;
        /**
         * The name of the secondary range on the subnet which provides IP address for this pod range.
         */
        secondaryPodRange?: string;
        /**
         * Name of the subnetwork where the additional pod network belongs.
         */
        subnetwork?: string;
    }

    export interface NodePoolNetworkConfigPodCidrOverprovisionConfig {
        disabled: boolean;
    }

    export interface NodePoolNodeConfig {
        advancedMachineFeatures?: outputs.container.NodePoolNodeConfigAdvancedMachineFeatures;
        bootDiskKmsKey?: string;
        /**
         * Configuration for Confidential Nodes feature. Structure is documented below.
         */
        confidentialNodes: outputs.container.NodePoolNodeConfigConfidentialNodes;
        diskSizeGb: number;
        diskType: string;
        ephemeralStorageConfig?: outputs.container.NodePoolNodeConfigEphemeralStorageConfig;
        ephemeralStorageLocalSsdConfig?: outputs.container.NodePoolNodeConfigEphemeralStorageLocalSsdConfig;
        fastSocket?: outputs.container.NodePoolNodeConfigFastSocket;
        gcfsConfig?: outputs.container.NodePoolNodeConfigGcfsConfig;
        guestAccelerators: outputs.container.NodePoolNodeConfigGuestAccelerator[];
        gvnic?: outputs.container.NodePoolNodeConfigGvnic;
        hostMaintenancePolicy?: outputs.container.NodePoolNodeConfigHostMaintenancePolicy;
        imageType: string;
        kubeletConfig?: outputs.container.NodePoolNodeConfigKubeletConfig;
        labels: {[key: string]: string};
        linuxNodeConfig?: outputs.container.NodePoolNodeConfigLinuxNodeConfig;
        localNvmeSsdBlockConfig?: outputs.container.NodePoolNodeConfigLocalNvmeSsdBlockConfig;
        localSsdCount: number;
        loggingVariant?: string;
        machineType: string;
        metadata: {[key: string]: string};
        minCpuPlatform: string;
        nodeGroup?: string;
        oauthScopes: string[];
        preemptible?: boolean;
        reservationAffinity?: outputs.container.NodePoolNodeConfigReservationAffinity;
        resourceLabels?: {[key: string]: string};
        sandboxConfig?: outputs.container.NodePoolNodeConfigSandboxConfig;
        serviceAccount: string;
        shieldedInstanceConfig: outputs.container.NodePoolNodeConfigShieldedInstanceConfig;
        soleTenantConfig?: outputs.container.NodePoolNodeConfigSoleTenantConfig;
        spot?: boolean;
        tags?: string[];
        taints: outputs.container.NodePoolNodeConfigTaint[];
        workloadMetadataConfig: outputs.container.NodePoolNodeConfigWorkloadMetadataConfig;
    }

    export interface NodePoolNodeConfigAdvancedMachineFeatures {
        threadsPerCore: number;
    }

    export interface NodePoolNodeConfigConfidentialNodes {
        /**
         * Enable Confidential GKE Nodes for this cluster, to
         * enforce encryption of data in-use.
         */
        enabled: boolean;
    }

    export interface NodePoolNodeConfigEphemeralStorageConfig {
        localSsdCount: number;
    }

    export interface NodePoolNodeConfigEphemeralStorageLocalSsdConfig {
        localSsdCount: number;
    }

    export interface NodePoolNodeConfigFastSocket {
        /**
         * Enable Confidential GKE Nodes for this cluster, to
         * enforce encryption of data in-use.
         */
        enabled: boolean;
    }

    export interface NodePoolNodeConfigGcfsConfig {
        /**
         * Enable Confidential GKE Nodes for this cluster, to
         * enforce encryption of data in-use.
         */
        enabled: boolean;
    }

    export interface NodePoolNodeConfigGuestAccelerator {
        count: number;
        gpuDriverInstallationConfig?: outputs.container.NodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig;
        gpuPartitionSize?: string;
        gpuSharingConfig?: outputs.container.NodePoolNodeConfigGuestAcceleratorGpuSharingConfig;
        /**
         * The type of the policy. Supports a single value: COMPACT.
         * Specifying COMPACT placement policy type places node pool's nodes in a closer
         * physical proximity in order to reduce network latency between nodes.
         */
        type: string;
    }

    export interface NodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
        gpuDriverVersion: string;
    }

    export interface NodePoolNodeConfigGuestAcceleratorGpuSharingConfig {
        gpuSharingStrategy: string;
        maxSharedClientsPerGpu: number;
    }

    export interface NodePoolNodeConfigGvnic {
        /**
         * Enable Confidential GKE Nodes for this cluster, to
         * enforce encryption of data in-use.
         */
        enabled: boolean;
    }

    export interface NodePoolNodeConfigHostMaintenancePolicy {
        maintenanceInterval: string;
    }

    export interface NodePoolNodeConfigKubeletConfig {
        cpuCfsQuota?: boolean;
        cpuCfsQuotaPeriod?: string;
        cpuManagerPolicy: string;
        podPidsLimit?: number;
    }

    export interface NodePoolNodeConfigLinuxNodeConfig {
        sysctls: {[key: string]: string};
    }

    export interface NodePoolNodeConfigLocalNvmeSsdBlockConfig {
        localSsdCount: number;
    }

    export interface NodePoolNodeConfigReservationAffinity {
        consumeReservationType: string;
        key?: string;
        values?: string[];
    }

    export interface NodePoolNodeConfigSandboxConfig {
        sandboxType: string;
    }

    export interface NodePoolNodeConfigShieldedInstanceConfig {
        enableIntegrityMonitoring?: boolean;
        enableSecureBoot?: boolean;
    }

    export interface NodePoolNodeConfigSoleTenantConfig {
        nodeAffinities: outputs.container.NodePoolNodeConfigSoleTenantConfigNodeAffinity[];
    }

    export interface NodePoolNodeConfigSoleTenantConfigNodeAffinity {
        key: string;
        operator: string;
        values: string[];
    }

    export interface NodePoolNodeConfigTaint {
        effect: string;
        key: string;
        value: string;
    }

    export interface NodePoolNodeConfigWorkloadMetadataConfig {
        mode: string;
    }

    export interface NodePoolPlacementPolicy {
        /**
         * If set, refers to the name of a custom resource policy supplied by the user.
         * The resource policy must be in the same project and region as the node pool.
         * If not found, InvalidArgument error is returned.
         */
        policyName?: string;
        /**
         * The [TPU placement topology](https://cloud.google.com/tpu/docs/types-topologies#tpu_topologies) for pod slice node pool.
         */
        tpuTopology?: string;
        /**
         * The type of the policy. Supports a single value: COMPACT.
         * Specifying COMPACT placement policy type places node pool's nodes in a closer
         * physical proximity in order to reduce network latency between nodes.
         */
        type: string;
    }

    export interface NodePoolUpgradeSettings {
        /**
         * The settings to adjust [blue green upgrades](https://cloud.google.com/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy).
         * Structure is documented below
         */
        blueGreenSettings: outputs.container.NodePoolUpgradeSettingsBlueGreenSettings;
        /**
         * The number of additional nodes that can be added to the node pool during
         * an upgrade. Increasing `maxSurge` raises the number of nodes that can be upgraded simultaneously.
         * Can be set to 0 or greater.
         */
        maxSurge: number;
        /**
         * The number of nodes that can be simultaneously unavailable during
         * an upgrade. Increasing `maxUnavailable` raises the number of nodes that can be upgraded in
         * parallel. Can be set to 0 or greater.
         *
         * `maxSurge` and `maxUnavailable` must not be negative and at least one of them must be greater than zero.
         */
        maxUnavailable: number;
        /**
         * The upgrade stragey to be used for upgrading the nodes.
         */
        strategy?: string;
    }

    export interface NodePoolUpgradeSettingsBlueGreenSettings {
        /**
         * Time needed after draining the entire blue pool.
         * After this period, the blue pool will be cleaned up.
         */
        nodePoolSoakDuration: string;
        /**
         * Specifies the standard policy settings for blue-green upgrades.
         */
        standardRolloutPolicy: outputs.container.NodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy;
    }

    export interface NodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy {
        /**
         * Number of blue nodes to drain in a batch.
         */
        batchNodeCount: number;
        /**
         * Percentage of the blue pool nodes to drain in a batch.
         */
        batchPercentage: number;
        /**
         * Soak time after each batch gets drained.
         */
        batchSoakDuration: string;
    }

}

export namespace containeranalysis {
    export interface NoteAttestationAuthority {
        /**
         * This submessage provides human-readable hints about the purpose of
         * the AttestationAuthority. Because the name of a Note acts as its
         * resource reference, it is important to disambiguate the canonical
         * name of the Note (which might be a UUID for security purposes)
         * from "readable" names more suitable for debug output. Note that
         * these hints should NOT be used to look up AttestationAuthorities
         * in security sensitive contexts, such as when looking up
         * Attestations to verify.
         * Structure is documented below.
         */
        hint: outputs.containeranalysis.NoteAttestationAuthorityHint;
    }

    export interface NoteAttestationAuthorityHint {
        /**
         * The human readable name of this Attestation Authority, for
         * example "qa".
         *
         * - - -
         */
        humanReadableName: string;
    }

    export interface NoteIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface NoteIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface NoteRelatedUrl {
        /**
         * Label to describe usage of the URL
         */
        label?: string;
        /**
         * Specific URL associated with the resource.
         */
        url: string;
    }

    export interface OccurenceAttestation {
        /**
         * The serialized payload that is verified by one or
         * more signatures. A base64-encoded string.
         */
        serializedPayload: string;
        /**
         * One or more signatures over serializedPayload.
         * Verifier implementations should consider this attestation
         * message verified if at least one signature verifies
         * serializedPayload. See Signature in common.proto for more
         * details on signature structure and verification.
         * Structure is documented below.
         */
        signatures: outputs.containeranalysis.OccurenceAttestationSignature[];
    }

    export interface OccurenceAttestationSignature {
        /**
         * The identifier for the public key that verifies this
         * signature. MUST be an RFC3986 conformant
         * URI. * When possible, the key id should be an
         * immutable reference, such as a cryptographic digest.
         * Examples of valid values:
         * * OpenPGP V4 public key fingerprint. See https://www.iana.org/assignments/uri-schemes/prov/openpgp4fpr
         * for more details on this scheme.
         * * `openpgp4fpr:74FAF3B861BDA0870C7B6DEF607E48D2A663AEEA`
         * * RFC6920 digest-named SubjectPublicKeyInfo (digest of the DER serialization):
         * * "ni:///sha-256;cD9o9Cq6LG3jD0iKXqEi_vdjJGecm_iXkbqVoScViaU"
         *
         * - - -
         */
        publicKeyId: string;
        /**
         * The content of the signature, an opaque bytestring.
         * The payload that this signature verifies MUST be
         * unambiguously provided with the Signature during
         * verification. A wrapper message might provide the
         * payload explicitly. Alternatively, a message might
         * have a canonical serialization that can always be
         * unambiguously computed to derive the payload.
         */
        signature?: string;
    }

}

export namespace databasemigrationservice {
    export interface ConnectionProfileAlloydb {
        /**
         * Required. The AlloyDB cluster ID that this connection profile is associated with.
         */
        clusterId: string;
        /**
         * Immutable. Metadata used to create the destination AlloyDB cluster.
         * Structure is documented below.
         */
        settings?: outputs.databasemigrationservice.ConnectionProfileAlloydbSettings;
    }

    export interface ConnectionProfileAlloydbSettings {
        /**
         * Required. Input only. Initial user to setup during cluster creation.
         * Structure is documented below.
         */
        initialUser: outputs.databasemigrationservice.ConnectionProfileAlloydbSettingsInitialUser;
        /**
         * Labels for the AlloyDB cluster created by DMS.
         */
        labels?: {[key: string]: string};
        /**
         * Settings for the cluster's primary instance
         * Structure is documented below.
         */
        primaryInstanceSettings?: outputs.databasemigrationservice.ConnectionProfileAlloydbSettingsPrimaryInstanceSettings;
        /**
         * Required. The resource link for the VPC network in which cluster resources are created and from which they are accessible via Private IP. The network must belong to the same project as the cluster.
         * It is specified in the form: 'projects/{project_number}/global/networks/{network_id}'. This is required to create a cluster.
         */
        vpcNetwork: string;
    }

    export interface ConnectionProfileAlloydbSettingsInitialUser {
        /**
         * The initial password for the user.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * (Output)
         * Output only. Indicates if the initialUser.password field has been set.
         */
        passwordSet: boolean;
        /**
         * The database username.
         */
        user: string;
    }

    export interface ConnectionProfileAlloydbSettingsPrimaryInstanceSettings {
        /**
         * Database flags to pass to AlloyDB when DMS is creating the AlloyDB cluster and instances. See the AlloyDB documentation for how these can be used.
         */
        databaseFlags?: {[key: string]: string};
        /**
         * The database username.
         */
        id: string;
        /**
         * Labels for the AlloyDB primary instance created by DMS.
         */
        labels?: {[key: string]: string};
        /**
         * Configuration for the machines that host the underlying database engine.
         * Structure is documented below.
         */
        machineConfig: outputs.databasemigrationservice.ConnectionProfileAlloydbSettingsPrimaryInstanceSettingsMachineConfig;
        /**
         * (Output)
         * Output only. The private IP address for the Instance. This is the connection endpoint for an end-user application.
         */
        privateIp: string;
    }

    export interface ConnectionProfileAlloydbSettingsPrimaryInstanceSettingsMachineConfig {
        /**
         * The number of CPU's in the VM instance.
         */
        cpuCount: number;
    }

    export interface ConnectionProfileCloudsql {
        /**
         * (Output)
         * Output only. The Cloud SQL instance ID that this connection profile is associated with.
         */
        cloudSqlId: string;
        /**
         * (Output)
         * Output only. The Cloud SQL database instance's private IP.
         */
        privateIp: string;
        /**
         * (Output)
         * Output only. The Cloud SQL database instance's public IP.
         */
        publicIp: string;
        /**
         * Immutable. Metadata used to create the destination Cloud SQL database.
         * Structure is documented below.
         */
        settings?: outputs.databasemigrationservice.ConnectionProfileCloudsqlSettings;
    }

    export interface ConnectionProfileCloudsqlSettings {
        /**
         * The activation policy specifies when the instance is activated; it is applicable only when the instance state is 'RUNNABLE'.
         * Possible values are: `ALWAYS`, `NEVER`.
         */
        activationPolicy?: string;
        /**
         * If you enable this setting, Cloud SQL checks your available storage every 30 seconds. If the available storage falls below a threshold size, Cloud SQL automatically adds additional storage capacity.
         * If the available storage repeatedly falls below the threshold size, Cloud SQL continues to add storage until it reaches the maximum of 30 TB.
         */
        autoStorageIncrease?: boolean;
        /**
         * The KMS key name used for the csql instance.
         */
        cmekKeyName?: string;
        /**
         * The Cloud SQL default instance level collation.
         */
        collation?: string;
        /**
         * The storage capacity available to the database, in GB. The minimum (and default) size is 10GB.
         */
        dataDiskSizeGb?: string;
        /**
         * The type of storage.
         * Possible values are: `PD_SSD`, `PD_HDD`.
         */
        dataDiskType?: string;
        /**
         * The database flags passed to the Cloud SQL instance at startup.
         */
        databaseFlags?: {[key: string]: string};
        /**
         * The database engine type and version.
         * Currently supported values located at https://cloud.google.com/database-migration/docs/reference/rest/v1/projects.locations.connectionProfiles#sqldatabaseversion
         */
        databaseVersion?: string;
        /**
         * The edition of the given Cloud SQL instance.
         * Possible values are: `ENTERPRISE`, `ENTERPRISE_PLUS`.
         */
        edition?: string;
        /**
         * The settings for IP Management. This allows to enable or disable the instance IP and manage which external networks can connect to the instance. The IPv4 address cannot be disabled.
         * Structure is documented below.
         */
        ipConfig?: outputs.databasemigrationservice.ConnectionProfileCloudsqlSettingsIpConfig;
        /**
         * Input only. Initial root password.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        rootPassword?: string;
        /**
         * (Output)
         * Output only. Indicates If this connection profile root password is stored.
         */
        rootPasswordSet: boolean;
        /**
         * The Database Migration Service source connection profile ID, in the format: projects/my_project_name/locations/us-central1/connectionProfiles/connection_profile_ID
         */
        sourceId: string;
        /**
         * The maximum size to which storage capacity can be automatically increased. The default value is 0, which specifies that there is no limit.
         */
        storageAutoResizeLimit?: string;
        /**
         * The tier (or machine type) for this instance, for example: db-n1-standard-1 (MySQL instances) or db-custom-1-3840 (PostgreSQL instances).
         * For more information, see https://cloud.google.com/sql/docs/mysql/instance-settings
         */
        tier?: string;
        /**
         * The resource labels for a Cloud SQL instance to use to annotate any related underlying resources such as Compute Engine VMs.
         */
        userLabels?: {[key: string]: string};
        /**
         * The Google Cloud Platform zone where your Cloud SQL datdabse instance is located.
         */
        zone?: string;
    }

    export interface ConnectionProfileCloudsqlSettingsIpConfig {
        /**
         * The list of external networks that are allowed to connect to the instance using the IP.
         * Structure is documented below.
         */
        authorizedNetworks?: outputs.databasemigrationservice.ConnectionProfileCloudsqlSettingsIpConfigAuthorizedNetwork[];
        /**
         * Whether the instance should be assigned an IPv4 address or not.
         */
        enableIpv4?: boolean;
        /**
         * The resource link for the VPC network from which the Cloud SQL instance is accessible for private IP. For example, projects/myProject/global/networks/default.
         * This setting can be updated, but it cannot be removed after it is set.
         */
        privateNetwork?: string;
        /**
         * Whether SSL connections over IP should be enforced or not.
         */
        requireSsl?: boolean;
    }

    export interface ConnectionProfileCloudsqlSettingsIpConfigAuthorizedNetwork {
        /**
         * The time when this access control entry expires in RFC 3339 format.
         */
        expireTime?: string;
        /**
         * A label to identify this entry.
         */
        label?: string;
        /**
         * Input only. The time-to-leave of this access control entry.
         */
        ttl?: string;
        /**
         * The allowlisted value for the access control list.
         */
        value: string;
    }

    export interface ConnectionProfileError {
        /**
         * (Output)
         * The status code, which should be an enum value of google.rpc.Code.
         */
        code: number;
        /**
         * (Output)
         * A list of messages that carry the error details.
         */
        details: {[key: string]: any}[];
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
    }

    export interface ConnectionProfileMysql {
        /**
         * If the source is a Cloud SQL database, use this field to provide the Cloud SQL instance ID of the source.
         */
        cloudSqlId?: string;
        /**
         * Required. The IP or hostname of the source MySQL database.
         */
        host: string;
        /**
         * Required. Input only. The password for the user that Database Migration Service will be using to connect to the database.
         * This field is not returned on request, and the value is encrypted when stored in Database Migration Service.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * (Output)
         * Output only. Indicates If this connection profile password is stored.
         */
        passwordSet: boolean;
        /**
         * Required. The network port of the source MySQL database.
         */
        port: number;
        /**
         * SSL configuration for the destination to connect to the source database.
         * Structure is documented below.
         */
        ssl?: outputs.databasemigrationservice.ConnectionProfileMysqlSsl;
        /**
         * Required. The username that Database Migration Service will use to connect to the database. The value is encrypted when stored in Database Migration Service.
         */
        username: string;
    }

    export interface ConnectionProfileMysqlSsl {
        /**
         * Required. Input only. The x509 PEM-encoded certificate of the CA that signed the source database server's certificate.
         * The replica will use this certificate to verify it's connecting to the right host.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        caCertificate: string;
        /**
         * Input only. The x509 PEM-encoded certificate that will be used by the replica to authenticate against the source database server.
         * If this field is used then the 'clientKey' field is mandatory
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        clientCertificate?: string;
        /**
         * Input only. The unencrypted PKCS#1 or PKCS#8 PEM-encoded private key associated with the Client Certificate.
         * If this field is used then the 'clientCertificate' field is mandatory.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        clientKey?: string;
        /**
         * (Output)
         * The current connection profile state.
         */
        type: string;
    }

    export interface ConnectionProfilePostgresql {
        /**
         * If the source is a Cloud SQL database, use this field to provide the Cloud SQL instance ID of the source.
         */
        cloudSqlId?: string;
        /**
         * Required. The IP or hostname of the source MySQL database.
         */
        host: string;
        /**
         * (Output)
         * Output only. If the source is a Cloud SQL database, this field indicates the network architecture it's associated with.
         */
        networkArchitecture: string;
        /**
         * Required. Input only. The password for the user that Database Migration Service will be using to connect to the database.
         * This field is not returned on request, and the value is encrypted when stored in Database Migration Service.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * (Output)
         * Output only. Indicates If this connection profile password is stored.
         */
        passwordSet: boolean;
        /**
         * Required. The network port of the source MySQL database.
         */
        port: number;
        /**
         * SSL configuration for the destination to connect to the source database.
         * Structure is documented below.
         */
        ssl?: outputs.databasemigrationservice.ConnectionProfilePostgresqlSsl;
        /**
         * Required. The username that Database Migration Service will use to connect to the database. The value is encrypted when stored in Database Migration Service.
         */
        username: string;
    }

    export interface ConnectionProfilePostgresqlSsl {
        /**
         * Required. Input only. The x509 PEM-encoded certificate of the CA that signed the source database server's certificate.
         * The replica will use this certificate to verify it's connecting to the right host.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        caCertificate: string;
        /**
         * Input only. The x509 PEM-encoded certificate that will be used by the replica to authenticate against the source database server.
         * If this field is used then the 'clientKey' field is mandatory
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        clientCertificate?: string;
        /**
         * Input only. The unencrypted PKCS#1 or PKCS#8 PEM-encoded private key associated with the Client Certificate.
         * If this field is used then the 'clientCertificate' field is mandatory.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        clientKey?: string;
        /**
         * (Output)
         * The current connection profile state.
         */
        type: string;
    }

}

export namespace datacatalog {
    export interface EntryBigqueryDateShardedSpec {
        /**
         * (Output)
         * The Data Catalog resource name of the dataset entry the current table belongs to, for example,
         * projects/{project_id}/locations/{location}/entrygroups/{entryGroupId}/entries/{entryId}
         */
        dataset: string;
        /**
         * (Output)
         * Total number of shards.
         */
        shardCount: number;
        /**
         * (Output)
         * The table name prefix of the shards. The name of any given shard is [tablePrefix]YYYYMMDD,
         * for example, for shard MyTable20180101, the tablePrefix is MyTable.
         */
        tablePrefix: string;
    }

    export interface EntryBigqueryTableSpec {
        /**
         * (Output)
         * The table source type.
         */
        tableSourceType: string;
        /**
         * (Output)
         * Spec of a BigQuery table. This field should only be populated if tableSourceType is BIGQUERY_TABLE.
         * Structure is documented below.
         */
        tableSpecs: outputs.datacatalog.EntryBigqueryTableSpecTableSpec[];
        /**
         * (Output)
         * Table view specification. This field should only be populated if tableSourceType is BIGQUERY_VIEW.
         * Structure is documented below.
         */
        viewSpecs: outputs.datacatalog.EntryBigqueryTableSpecViewSpec[];
    }

    export interface EntryBigqueryTableSpecTableSpec {
        /**
         * (Output)
         * If the table is a dated shard, i.e., with name pattern [prefix]YYYYMMDD, groupedEntry is the
         * Data Catalog resource name of the date sharded grouped entry, for example,
         * projects/{project_id}/locations/{location}/entrygroups/{entryGroupId}/entries/{entryId}.
         * Otherwise, groupedEntry is empty.
         */
        groupedEntry: string;
    }

    export interface EntryBigqueryTableSpecViewSpec {
        /**
         * (Output)
         * The query that defines the table view.
         */
        viewQuery: string;
    }

    export interface EntryGcsFilesetSpec {
        /**
         * Patterns to identify a set of files in Google Cloud Storage.
         * See [Cloud Storage documentation](https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames)
         * for more information. Note that bucket wildcards are currently not supported. Examples of valid filePatterns:
         * * gs://bucket_name/dir/*: matches all files within bucket_name/dir directory.
         * * gs://bucket_name/dir/**: matches all files in bucket_name/dir spanning all subdirectories.
         * * gs://bucket_name/file*: matches files prefixed by file in bucketName
         * * gs://bucket_name/??.txt: matches files with two characters followed by .txt in bucketName
         * * gs://bucket_name/[aeiou].txt: matches files that contain a single vowel character followed by .txt in bucketName
         * * gs://bucket_name/[a-m].txt: matches files that contain a, b, ... or m followed by .txt in bucketName
         * * gs://bucket_name/a/*&#47;b: matches all files in bucketName that match a/*&#47;b pattern, such as a/c/b, a/d/b
         * * gs://another_bucket/a.txt: matches gs://another_bucket/a.txt
         */
        filePatterns: string[];
        /**
         * (Output)
         * Sample files contained in this fileset, not all files contained in this fileset are represented here.
         * Structure is documented below.
         *
         *
         * <a name="nestedSampleGcsFileSpecs"></a>The `sampleGcsFileSpecs` block contains:
         */
        sampleGcsFileSpecs: outputs.datacatalog.EntryGcsFilesetSpecSampleGcsFileSpec[];
    }

    export interface EntryGcsFilesetSpecSampleGcsFileSpec {
        /**
         * (Output)
         * The full file path
         */
        filePath: string;
        /**
         * (Output)
         * The size of the file, in bytes.
         */
        sizeBytes: number;
    }

    export interface EntryGroupIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface EntryGroupIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface PolicyTagIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface PolicyTagIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TagField {
        /**
         * Holds the value for a tag field with boolean type.
         */
        boolValue?: boolean;
        /**
         * (Output)
         * The display name of this field
         */
        displayName: string;
        /**
         * Holds the value for a tag field with double type.
         */
        doubleValue?: number;
        /**
         * Holds the value for a tag field with enum type. This value must be one of the allowed values in the definition of this enum.
         *
         * - - -
         */
        enumValue?: string;
        /**
         * The identifier for this object. Format specified above.
         */
        fieldName: string;
        /**
         * (Output)
         * The order of this field with respect to other fields in this tag. For example, a higher value can indicate
         * a more important field. The value can be negative. Multiple fields can have the same order, and field orders
         * within a tag do not have to be sequential.
         */
        order: number;
        /**
         * Holds the value for a tag field with string type.
         */
        stringValue?: string;
        /**
         * Holds the value for a tag field with timestamp type.
         */
        timestampValue?: string;
    }

    export interface TagTemplateField {
        /**
         * A description for this field.
         */
        description: string;
        /**
         * The display name for this field.
         */
        displayName: string;
        /**
         * The identifier for this object. Format specified above.
         */
        fieldId: string;
        /**
         * Whether this is a required field. Defaults to false.
         */
        isRequired: boolean;
        /**
         * (Output)
         * The resource name of the tag template field in URL format. Example: projects/{project_id}/locations/{location}/tagTemplates/{tagTemplateId}/fields/{field}
         */
        name: string;
        /**
         * The order of this field with respect to other fields in this tag template.
         * A higher value indicates a more important field. The value can be negative.
         * Multiple fields can have the same order, and field orders within a tag do not have to be sequential.
         */
        order: number;
        /**
         * The type of value this tag field can contain.
         * Structure is documented below.
         */
        type: outputs.datacatalog.TagTemplateFieldType;
    }

    export interface TagTemplateFieldType {
        /**
         * Represents an enum type.
         * Exactly one of `primitiveType` or `enumType` must be set
         * Structure is documented below.
         */
        enumType?: outputs.datacatalog.TagTemplateFieldTypeEnumType;
        /**
         * Represents primitive types - string, bool etc.
         * Exactly one of `primitiveType` or `enumType` must be set
         * Possible values are: `DOUBLE`, `STRING`, `BOOL`, `TIMESTAMP`.
         */
        primitiveType: string;
    }

    export interface TagTemplateFieldTypeEnumType {
        /**
         * The set of allowed values for this enum. The display names of the
         * values must be case-insensitively unique within this set. Currently,
         * enum values can only be added to the list of allowed values. Deletion
         * and renaming of enum values are not supported.
         * Can have up to 500 allowed values.
         * Structure is documented below.
         */
        allowedValues: outputs.datacatalog.TagTemplateFieldTypeEnumTypeAllowedValue[];
    }

    export interface TagTemplateFieldTypeEnumTypeAllowedValue {
        /**
         * The display name for this template.
         */
        displayName: string;
    }

    export interface TagTemplateIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TagTemplateIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TaxonomyIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TaxonomyIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace dataflow {
    export interface PipelineScheduleInfo {
        /**
         * (Output)
         * When the next Scheduler job is going to run.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        nextJobTime: string;
        /**
         * Unix-cron format of the schedule. This information is retrieved from the linked Cloud Scheduler.
         */
        schedule?: string;
        /**
         * Timezone ID. This matches the timezone IDs used by the Cloud Scheduler API. If empty, UTC time is assumed.
         */
        timeZone?: string;
    }

    export interface PipelineWorkload {
        /**
         * Template information and additional parameters needed to launch a Dataflow job using the flex launch API.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchflextemplaterequest
         * Structure is documented below.
         */
        dataflowFlexTemplateRequest?: outputs.dataflow.PipelineWorkloadDataflowFlexTemplateRequest;
        /**
         * Template information and additional parameters needed to launch a Dataflow job using the standard launch API.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchtemplaterequest
         * Structure is documented below.
         */
        dataflowLaunchTemplateRequest?: outputs.dataflow.PipelineWorkloadDataflowLaunchTemplateRequest;
    }

    export interface PipelineWorkloadDataflowFlexTemplateRequest {
        /**
         * Parameter to launch a job from a Flex Template.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchflextemplateparameter
         * Structure is documented below.
         */
        launchParameter: outputs.dataflow.PipelineWorkloadDataflowFlexTemplateRequestLaunchParameter;
        /**
         * The regional endpoint to which to direct the request. For example, us-central1, us-west1.
         */
        location: string;
        /**
         * The ID of the Cloud Platform project that the job belongs to.
         */
        projectId: string;
        /**
         * If true, the request is validated but not actually executed. Defaults to false.
         */
        validateOnly?: boolean;
    }

    export interface PipelineWorkloadDataflowFlexTemplateRequestLaunchParameter {
        /**
         * Cloud Storage path to a file with a JSON-serialized ContainerSpec as content.
         */
        containerSpecGcsPath?: string;
        /**
         * The runtime environment for the Flex Template job.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#FlexTemplateRuntimeEnvironment
         * Structure is documented below.
         */
        environment?: outputs.dataflow.PipelineWorkloadDataflowFlexTemplateRequestLaunchParameterEnvironment;
        /**
         * The job name to use for the created job. For an update job request, the job name should be the same as the existing running job.
         */
        jobName: string;
        /**
         * Launch options for this Flex Template job. This is a common set of options across languages and templates. This should not be used to pass job parameters.
         * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
         */
        launchOptions?: {[key: string]: string};
        /**
         * 'The parameters for the Flex Template. Example: {"numWorkers":"5"}'
         * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
         */
        parameters?: {[key: string]: string};
        /**
         * 'Use this to pass transform name mappings for streaming update jobs. Example: {"oldTransformName":"newTransformName",...}'
         * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
         */
        transformNameMappings?: {[key: string]: string};
        /**
         * Set this to true if you are sending a request to update a running streaming job. When set, the job name should be the same as the running job.
         */
        update?: boolean;
    }

    export interface PipelineWorkloadDataflowFlexTemplateRequestLaunchParameterEnvironment {
        /**
         * Additional experiment flags for the job.
         */
        additionalExperiments?: string[];
        /**
         * Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs.
         * 'Example: { "name": "wrench", "mass": "1kg", "count": "3" }.'
         * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
         */
        additionalUserLabels?: {[key: string]: string};
        /**
         * Whether to enable Streaming Engine for the job.
         */
        enableStreamingEngine?: boolean;
        /**
         * Set FlexRS goal for the job. https://cloud.google.com/dataflow/docs/guides/flexrs
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#FlexResourceSchedulingGoal
         * Possible values are: `FLEXRS_UNSPECIFIED`, `FLEXRS_SPEED_OPTIMIZED`, `FLEXRS_COST_OPTIMIZED`.
         */
        flexrsGoal?: string;
        /**
         * Configuration for VM IPs.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#WorkerIPAddressConfiguration
         * Possible values are: `WORKER_IP_UNSPECIFIED`, `WORKER_IP_PUBLIC`, `WORKER_IP_PRIVATE`.
         */
        ipConfiguration?: string;
        /**
         * 'Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/'
         */
        kmsKeyName?: string;
        /**
         * The machine type to use for the job. Defaults to the value from the template if not specified.
         */
        machineType?: string;
        /**
         * The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000.
         */
        maxWorkers?: number;
        /**
         * Network to which VMs will be assigned. If empty or unspecified, the service will use the network "default".
         */
        network?: string;
        /**
         * The initial number of Compute Engine instances for the job.
         */
        numWorkers?: number;
        /**
         * The email address of the service account to run the job as.
         */
        serviceAccountEmail?: string;
        /**
         * Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form "https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK" or "regions/REGION/subnetworks/SUBNETWORK". If the subnetwork is located in a Shared VPC network, you must use the complete URL.
         */
        subnetwork?: string;
        /**
         * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
         */
        tempLocation?: string;
        /**
         * The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1". Mutually exclusive with workerZone. If neither workerRegion nor workerZone is specified, default to the control plane's region.
         */
        workerRegion?: string;
        /**
         * The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1-a". Mutually exclusive with workerRegion. If neither workerRegion nor workerZone is specified, a zone in the control plane's region is chosen based on available capacity. If both workerZone and zone are set, workerZone takes precedence.
         */
        workerZone?: string;
        /**
         * The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, workerZone will take precedence.
         */
        zone?: string;
    }

    export interface PipelineWorkloadDataflowLaunchTemplateRequest {
        /**
         * A Cloud Storage path to the template from which to create the job. Must be a valid Cloud Storage URL, beginning with 'gs://'.
         */
        gcsPath?: string;
        /**
         * The parameters of the template to launch. This should be part of the body of the POST request.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchtemplateparameters
         * Structure is documented below.
         */
        launchParameters?: outputs.dataflow.PipelineWorkloadDataflowLaunchTemplateRequestLaunchParameters;
        /**
         * The regional endpoint to which to direct the request.
         */
        location?: string;
        /**
         * The ID of the Cloud Platform project that the job belongs to.
         */
        projectId: string;
        /**
         * (Optional)
         */
        validateOnly?: boolean;
    }

    export interface PipelineWorkloadDataflowLaunchTemplateRequestLaunchParameters {
        /**
         * The runtime environment for the job.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#RuntimeEnvironment
         * Structure is documented below.
         */
        environment?: outputs.dataflow.PipelineWorkloadDataflowLaunchTemplateRequestLaunchParametersEnvironment;
        /**
         * The job name to use for the created job.
         */
        jobName: string;
        /**
         * The runtime parameters to pass to the job.
         * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
         */
        parameters?: {[key: string]: string};
        /**
         * Map of transform name prefixes of the job to be replaced to the corresponding name prefixes of the new job. Only applicable when updating a pipeline.
         * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
         */
        transformNameMapping?: {[key: string]: string};
        /**
         * If set, replace the existing pipeline with the name specified by jobName with this pipeline, preserving state.
         */
        update?: boolean;
    }

    export interface PipelineWorkloadDataflowLaunchTemplateRequestLaunchParametersEnvironment {
        /**
         * Additional experiment flags for the job.
         */
        additionalExperiments?: string[];
        /**
         * Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs.
         * 'Example: { "name": "wrench", "mass": "1kg", "count": "3" }.'
         * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
         */
        additionalUserLabels?: {[key: string]: string};
        /**
         * Whether to bypass the safety checks for the job's temporary directory. Use with caution.
         */
        bypassTempDirValidation?: boolean;
        /**
         * Whether to enable Streaming Engine for the job.
         */
        enableStreamingEngine?: boolean;
        /**
         * Configuration for VM IPs.
         * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#WorkerIPAddressConfiguration
         * Possible values are: `WORKER_IP_UNSPECIFIED`, `WORKER_IP_PUBLIC`, `WORKER_IP_PRIVATE`.
         */
        ipConfiguration?: string;
        /**
         * 'Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/'
         */
        kmsKeyName?: string;
        /**
         * The machine type to use for the job. Defaults to the value from the template if not specified.
         */
        machineType?: string;
        /**
         * The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000.
         */
        maxWorkers?: number;
        /**
         * Network to which VMs will be assigned. If empty or unspecified, the service will use the network "default".
         */
        network: string;
        /**
         * The initial number of Compute Engine instances for the job.
         */
        numWorkers?: number;
        /**
         * The email address of the service account to run the job as.
         */
        serviceAccountEmail?: string;
        /**
         * Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form "https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK" or "regions/REGION/subnetworks/SUBNETWORK". If the subnetwork is located in a Shared VPC network, you must use the complete URL.
         */
        subnetwork?: string;
        /**
         * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
         */
        tempLocation?: string;
        /**
         * The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1". Mutually exclusive with workerZone. If neither workerRegion nor workerZone is specified, default to the control plane's region.
         */
        workerRegion?: string;
        /**
         * The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1-a". Mutually exclusive with workerRegion. If neither workerRegion nor workerZone is specified, a zone in the control plane's region is chosen based on available capacity. If both workerZone and zone are set, workerZone takes precedence.
         */
        workerZone?: string;
        /**
         * The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, workerZone will take precedence.
         */
        zone?: string;
    }

}

export namespace dataform {
    export interface RepositoryGitRemoteSettings {
        /**
         * The name of the Secret Manager secret version to use as an authentication token for Git operations. Must be in the format projects/*&#47;secrets/*&#47;versions/*.
         */
        authenticationTokenSecretVersion: string;
        /**
         * The Git remote's default branch name.
         */
        defaultBranch: string;
        /**
         * (Output)
         * Indicates the status of the Git access token. https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories#TokenStatus
         */
        tokenStatus: string;
        /**
         * The Git remote's URL.
         */
        url: string;
    }

    export interface RepositoryReleaseConfigCodeCompilationConfig {
        /**
         * Optional. The default schema (BigQuery dataset ID) for assertions.
         */
        assertionSchema?: string;
        /**
         * Optional. The suffix that should be appended to all database (Google Cloud project ID) names.
         */
        databaseSuffix?: string;
        /**
         * Optional. The default database (Google Cloud project ID).
         */
        defaultDatabase?: string;
        /**
         * Optional. The default BigQuery location to use. Defaults to "US".
         * See the BigQuery docs for a full list of locations: https://cloud.google.com/bigquery/docs/locations.
         */
        defaultLocation?: string;
        /**
         * Optional. The default schema (BigQuery dataset ID).
         */
        defaultSchema?: string;
        /**
         * Optional. The suffix that should be appended to all schema (BigQuery dataset ID) names.
         */
        schemaSuffix?: string;
        /**
         * Optional. The prefix that should be prepended to all table names.
         */
        tablePrefix?: string;
        /**
         * Optional. User-defined variables that are made available to project code during compilation.
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        vars?: {[key: string]: string};
    }

    export interface RepositoryReleaseConfigRecentScheduledReleaseRecord {
        /**
         * (Output)
         * The name of the created compilation result, if one was successfully created. Must be in the format projects/*&#47;locations/*&#47;repositories/*&#47;compilationResults/*.
         */
        compilationResult: string;
        /**
         * (Output)
         * The error status encountered upon this attempt to create the compilation result, if the attempt was unsuccessful.
         * Structure is documented below.
         */
        errorStatuses: outputs.dataform.RepositoryReleaseConfigRecentScheduledReleaseRecordErrorStatus[];
        /**
         * (Output)
         * The timestamp of this release attempt.
         */
        releaseTime: string;
    }

    export interface RepositoryReleaseConfigRecentScheduledReleaseRecordErrorStatus {
        /**
         * (Output)
         * The status code, which should be an enum value of google.rpc.Code.
         */
        code: number;
        /**
         * (Output)
         * A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
         */
        message: string;
    }

    export interface RepositoryWorkflowConfigInvocationConfig {
        /**
         * Optional. When set to true, any incremental tables will be fully refreshed.
         */
        fullyRefreshIncrementalTablesEnabled?: boolean;
        /**
         * Optional. The set of tags to include.
         */
        includedTags?: string[];
        /**
         * Optional. The set of action identifiers to include.
         * Structure is documented below.
         */
        includedTargets?: outputs.dataform.RepositoryWorkflowConfigInvocationConfigIncludedTarget[];
        /**
         * Optional. The service account to run workflow invocations under.
         */
        serviceAccount?: string;
        /**
         * Optional. When set to true, transitive dependencies of included actions will be executed.
         */
        transitiveDependenciesIncluded?: boolean;
        /**
         * Optional. When set to true, transitive dependents of included actions will be executed.
         */
        transitiveDependentsIncluded?: boolean;
    }

    export interface RepositoryWorkflowConfigInvocationConfigIncludedTarget {
        /**
         * The action's database (Google Cloud project ID).
         */
        database?: string;
        /**
         * The action's name, within database and schema.
         */
        name?: string;
        /**
         * The action's schema (BigQuery dataset ID), within database.
         */
        schema?: string;
    }

    export interface RepositoryWorkflowConfigRecentScheduledExecutionRecord {
        /**
         * (Output)
         * The error status encountered upon this attempt to create the workflow invocation, if the attempt was unsuccessful.
         * Structure is documented below.
         */
        errorStatuses: outputs.dataform.RepositoryWorkflowConfigRecentScheduledExecutionRecordErrorStatus[];
        /**
         * (Output)
         * The timestamp of this workflow attempt.
         */
        executionTime: string;
        /**
         * (Output)
         * The name of the created workflow invocation, if one was successfully created. In the format projects/*&#47;locations/*&#47;repositories/*&#47;workflowInvocations/*.
         */
        workflowInvocation: string;
    }

    export interface RepositoryWorkflowConfigRecentScheduledExecutionRecordErrorStatus {
        /**
         * (Output)
         * The status code, which should be an enum value of google.rpc.Code.
         */
        code: number;
        /**
         * (Output)
         * A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
         */
        message: string;
    }

    export interface RepositoryWorkspaceCompilationOverrides {
        /**
         * Optional. The default database (Google Cloud project ID).
         */
        defaultDatabase?: string;
        /**
         * Optional. The suffix that should be appended to all schema (BigQuery dataset ID) names.
         */
        schemaSuffix?: string;
        /**
         * Optional. The prefix that should be prepended to all table names.
         */
        tablePrefix?: string;
    }

}

export namespace datafusion {
    export interface InstanceAccelerator {
        /**
         * The type of an accelator for a CDF instance.
         * Possible values are: `CDC`, `HEALTHCARE`, `CCAI_INSIGHTS`.
         */
        acceleratorType: string;
        /**
         * The type of an accelator for a CDF instance.
         * Possible values are: `ENABLED`, `DISABLED`.
         */
        state: string;
    }

    export interface InstanceCryptoKeyConfig {
        /**
         * The name of the key which is used to encrypt/decrypt customer data. For key in Cloud KMS, the key should be in the format of projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*.
         */
        keyReference: string;
    }

    export interface InstanceEventPublishConfig {
        /**
         * Option to enable Event Publishing.
         */
        enabled: boolean;
        /**
         * The resource name of the Pub/Sub topic. Format: projects/{projectId}/topics/{topic_id}
         */
        topic: string;
    }

    export interface InstanceNetworkConfig {
        /**
         * The IP range in CIDR notation to use for the managed Data Fusion instance
         * nodes. This range must not overlap with any other ranges used in the Data Fusion instance network.
         */
        ipAllocation: string;
        /**
         * Name of the network in the project with which the tenant project
         * will be peered for executing pipelines. In case of shared VPC where the network resides in another host
         * project the network should specified in the form of projects/{host-project-id}/global/networks/{network}
         */
        network: string;
    }

}

export namespace dataloss {
    export interface PreventionDeidentifyTemplateDeidentifyConfig {
        /**
         * Treat the dataset as an image and redact.
         * Structure is documented below.
         */
        imageTransformations?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformations;
        /**
         * Treat the dataset as free-form text and apply the same free text transformation everywhere
         * Structure is documented below.
         */
        infoTypeTransformations?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformations;
        /**
         * Treat the dataset as structured. Transformations can be applied to specific locations within structured datasets, such as transforming a column within a table.
         * Structure is documented below.
         */
        recordTransformations?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformations;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformations {
        /**
         * For determination of how redaction of images should occur.
         * Structure is documented below.
         */
        transforms: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransform[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransform {
        /**
         * Apply transformation to all findings not specified in other ImageTransformation's selectedInfoTypes.
         */
        allInfoTypes?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllInfoTypes;
        /**
         * Apply transformation to all text that doesn't match an infoType.
         */
        allText?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllText;
        /**
         * The color to use when redacting content from an image. If not specified, the default is black.
         * Structure is documented below.
         */
        redactionColor?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformRedactionColor;
        /**
         * Apply transformation to the selected infoTypes.
         * Structure is documented below.
         */
        selectedInfoTypes?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypes;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllInfoTypes {
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllText {
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformRedactionColor {
        /**
         * The amount of blue in the color as a value in the interval [0, 1].
         */
        blue?: number;
        /**
         * The amount of green in the color as a value in the interval [0, 1].
         */
        green?: number;
        /**
         * The amount of red in the color as a value in the interval [0, 1].
         */
        red?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypes {
        /**
         * InfoTypes to apply the transformation to. Leaving this empty will apply the transformation to apply to
         * all findings that correspond to infoTypes that were requested in InspectConfig.
         * Structure is documented below.
         */
        infoTypes: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoType[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoType {
        /**
         * Name of the information type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformations {
        /**
         * Transformation for each infoType. Cannot specify more than one for a given infoType.
         * Structure is documented below.
         */
        transformations: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformation[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformation {
        /**
         * InfoTypes to apply the transformation to. Leaving this empty will apply the transformation to apply to
         * all findings that correspond to infoTypes that were requested in InspectConfig.
         * Structure is documented below.
         */
        infoTypes?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoType[];
        /**
         * Apply the transformation to the entire field.
         * The `primitiveTransformation` block must only contain one argument, corresponding to the type of transformation.
         * Structure is documented below.
         */
        primitiveTransformation: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformation;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoType {
        /**
         * Name of the information type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformation {
        /**
         * Generalization function that buckets values based on ranges. The ranges and replacement values are dynamically provided by the user for custom behavior, such as 1-30 > LOW 31-65 > MEDIUM 66-100 > HIGH
         * This can be used on data of type: number, long, string, timestamp.
         * If the provided value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
         * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
         * Structure is documented below.
         */
        bucketingConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig;
        /**
         * Partially mask a string by replacing a given number of characters with a fixed character. Masking can start from the beginning or end of the string. This can be used on data of any type (numbers, longs, and so on) and when de-identifying structured data we'll attempt to preserve the original data's type. (This allows you to take a long like 123 and modify it to a string like **3).
         * Structure is documented below.
         */
        characterMaskConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig;
        /**
         * Pseudonymization method that generates deterministic encryption for the given input. Outputs a base64 encoded representation of the encrypted output. Uses AES-SIV based on the RFC [https://tools.ietf.org/html/rfc5297](https://tools.ietf.org/html/rfc5297).
         * Structure is documented below.
         */
        cryptoDeterministicConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig;
        /**
         * Pseudonymization method that generates surrogates via cryptographic hashing. Uses SHA-256. The key size must be either 32 or 64 bytes.
         * Outputs a base64 encoded representation of the hashed output (for example, L7k0BHmF1ha5U3NfGykjro4xWi1MPVQPjhMAZbSV9mM=).
         * Currently, only string and integer values can be hashed.
         * See https://cloud.google.com/dlp/docs/pseudonymization to learn more.
         * Structure is documented below.
         */
        cryptoHashConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig;
        /**
         * Replaces an identifier with a surrogate using Format Preserving Encryption (FPE) with the FFX mode of operation; however when used in the `content.reidentify` API method, it serves the opposite function by reversing the surrogate back into the original identifier. The identifier must be encoded as ASCII. For a given crypto key and context, the same identifier will be replaced with the same surrogate. Identifiers must be at least two characters long. In the case that the identifier is the empty string, it will be skipped. See [https://cloud.google.com/dlp/docs/pseudonymization](https://cloud.google.com/dlp/docs/pseudonymization) to learn more.
         * Note: We recommend using CryptoDeterministicConfig for all use cases which do not require preserving the input alphabet space and size, plus warrant referential integrity.
         * Structure is documented below.
         */
        cryptoReplaceFfxFpeConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig;
        /**
         * Shifts dates by random number of days, with option to be consistent for the same context. See https://cloud.google.com/dlp/docs/concepts-date-shifting to learn more.
         * Structure is documented below.
         */
        dateShiftConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig;
        /**
         * Buckets values based on fixed size ranges. The Bucketing transformation can provide all of this functionality, but requires more configuration. This message is provided as a convenience to the user for simple bucketing strategies.
         * The transformed value will be a hyphenated string of {lower_bound}-{upper_bound}. For example, if lowerBound = 10 and upperBound = 20, all values that are within this bucket will be replaced with "10-20".
         * This can be used on data of type: double, long.
         * If the bound Value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
         * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
         * Structure is documented below.
         */
        fixedSizeBucketingConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig;
        /**
         * Redact a given value. For example, if used with an InfoTypeTransformation transforming PHONE_NUMBER, and input 'My phone number is 206-555-0123', the output would be 'My phone number is '.
         */
        redactConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig;
        /**
         * Replace each input value with a given value.
         * Structure is documented below.
         */
        replaceConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig;
        /**
         * Replace with a value randomly drawn (with replacement) from a dictionary.
         * Structure is documented below.
         */
        replaceDictionaryConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig;
        /**
         * Replace each matching finding with the name of the info type.
         */
        replaceWithInfoTypeConfig?: boolean;
        /**
         * For use with Date, Timestamp, and TimeOfDay, extract or preserve a portion of the value.
         * Structure is documented below.
         */
        timePartConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig {
        /**
         * Set of buckets. Ranges must be non-overlapping.
         * Bucket is represented as a range, along with replacement values.
         * Structure is documented below.
         */
        buckets?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket {
        /**
         * Upper bound of the range, exclusive; type must match min.
         * The `max` block must only contain one argument. See the `bucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        max?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax;
        /**
         * Lower bound of the range, inclusive. Type should be the same as max if used.
         * The `min` block must only contain one argument. See the `bucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        min?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin;
        /**
         * Replacement value for this bucket.
         * The `replacementValue` block must only contain one argument.
         * Structure is documented below.
         */
        replacementValue: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax {
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin {
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue {
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig {
        /**
         * Characters to skip when doing de-identification of a value. These will be left alone and skipped.
         * Structure is documented below.
         */
        charactersToIgnores?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore[];
        /**
         * is *
         */
        maskingCharacter?: string;
        /**
         * is -4
         */
        numberToMask?: number;
        /**
         * Mask characters in reverse order. For example, if maskingCharacter is 0, numberToMask is 14, and reverseOrder is `false`, then the
         * input string `1234-5678-9012-3456` is masked as `00000000000000-3456`.
         */
        reverseOrder?: boolean;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore {
        /**
         * Characters to not transform when masking. Only one of this or `commonCharactersToIgnore` must be specified.
         */
        charactersToSkip?: string;
        /**
         * Common characters to not transform when masking. Useful to avoid removing punctuation. Only one of this or `charactersToSkip` must be specified.
         * Possible values are: `NUMERIC`, `ALPHA_UPPER_CASE`, `ALPHA_LOWER_CASE`, `PUNCTUATION`, `WHITESPACE`.
         */
        commonCharactersToIgnore?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig {
        /**
         * A context may be used for higher security and maintaining referential integrity such that the same identifier in two different contexts will be given a distinct surrogate. The context is appended to plaintext value being encrypted. On decryption the provided context is validated against the value used during encryption. If a context was provided during encryption, same context must be provided during decryption as well.
         * If the context is not set, plaintext would be used as is for encryption. If the context is set but:
         * 1. there is no record present when transforming a given value or
         * 2. the field is not present when transforming a given value,
         * plaintext would be used as is for encryption.
         * Note that case (1) is expected when an InfoTypeTransformation is applied to both structured and unstructured ContentItems.
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext;
        /**
         * The key used by the encryption function. For deterministic encryption using AES-SIV, the provided key is internally expanded to 64 bytes prior to use.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey;
        /**
         * The custom info type to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom info type followed by the number of characters comprising the surrogate. The following scheme defines the format: {info type name}({surrogate character count}):{surrogate}
         * For example, if the name of custom info type is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
         * This annotation identifies the surrogate when inspecting content using the custom info type 'Surrogate'. This facilitates reversal of the surrogate when it occurs in free text.
         * Note: For record transformations where the entire cell in a table is being transformed, surrogates are not mandatory. Surrogates are used to denote the location of the token and are necessary for re-identification in free form text.
         * In order for inspection to work properly, the name of this info type must not occur naturally anywhere in your data; otherwise, inspection may either
         * *   reverse a surrogate that does not correspond to an actual identifier
         * *   be unable to parse the surrogate and result in an error
         * Therefore, choose your custom info type name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: MY\_TOKEN\_TYPE.
         * Structure is documented below.
         */
        surrogateInfoType?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern `[A-Za-z0-9$-_]{1,64}`.
         */
        name?: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore;
        /**
         * Optional version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig {
        /**
         * The key used by the encryption function.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig {
        /**
         * Common alphabets. Only one of this, `customAlphabet` or `radix` must be specified.
         * Possible values are: `NUMERIC`, `HEXADECIMAL`, `UPPER_CASE_ALPHA_NUMERIC`, `ALPHA_NUMERIC`.
         */
        commonAlphabet?: string;
        /**
         * The 'tweak', a context may be used for higher security since the same identifier in two different contexts won't be given the same surrogate. If the context is not set, a default tweak will be used.
         * If the context is set but:
         * 1.  there is no record present when transforming a given value or
         * 2.  the field is not present when transforming a given value,
         * a default tweak will be used.
         * Note that case (1) is expected when an `InfoTypeTransformation` is applied to both structured and non-structured `ContentItem`s. Currently, the referenced field may be of value type integer or string.
         * The tweak is constructed as a sequence of bytes in big endian byte order such that:
         * *   a 64 bit integer is encoded followed by a single byte of value 1
         * *   a string is encoded in UTF-8 format followed by a single byte of value 2
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext;
        /**
         * The key used by the encryption algorithm.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey;
        /**
         * This is supported by mapping these to the alphanumeric characters that the FFX mode natively supports. This happens before/after encryption/decryption. Each character listed must appear only once. Number of characters must be in the range \[2, 95\]. This must be encoded as ASCII. The order of characters does not matter. The full list of allowed characters is:
         * ``0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ~`!@#$%^&*()_-+={[}]|:;"'<,>.?/``. Only one of this, `commonAlphabet` or `radix` must be specified.
         */
        customAlphabet?: string;
        /**
         * The native way to select the alphabet. Must be in the range \[2, 95\]. Only one of this, `customAlphabet` or `commonAlphabet` must be specified.
         */
        radix?: number;
        /**
         * The custom infoType to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom infoType followed by the number of characters comprising the surrogate. The following scheme defines the format: info\_type\_name(surrogate\_character\_count):surrogate
         * For example, if the name of custom infoType is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
         * This annotation identifies the surrogate when inspecting content using the custom infoType [`SurrogateType`](https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#surrogatetype). This facilitates reversal of the surrogate when it occurs in free text.
         * In order for inspection to work properly, the name of this infoType must not occur naturally anywhere in your data; otherwise, inspection may find a surrogate that does not correspond to an actual identifier. Therefore, choose your custom infoType name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: MY\_TOKEN\_TYPE
         * Structure is documented below.
         */
        surrogateInfoType?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern `[A-Za-z0-9$-_]{1,64}`.
         */
        name?: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore;
        /**
         * Optional version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig {
        /**
         * Points to the field that contains the context, for example, an entity id.
         * If set, must also set cryptoKey. If set, shift will be consistent for the given context.
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext;
        /**
         * Causes the shift to be computed based on this key and the context. This results in the same shift for the same context and cryptoKey. If set, must also set context. Can only be applied to table items.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey;
        /**
         * For example, -5 means shift date to at most 5 days back in the past.
         */
        lowerBoundDays: number;
        /**
         * Range of shift in days. Actual shift will be selected at random within this range (inclusive ends). Negative means shift to earlier in time. Must not be more than 365250 days (1000 years) each direction.
         * For example, 3 means shift date to at most 3 days into the future.
         */
        upperBoundDays: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext {
        /**
         * Name describing the field.
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig {
        /**
         * Size of each bucket (except for minimum and maximum buckets).
         * So if lowerBound = 10, upperBound = 89, and bucketSize = 10, then the following buckets would be used: -10, 10-20, 20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-89, 89+.
         * Precision up to 2 decimals works.
         */
        bucketSize: number;
        /**
         * Lower bound value of buckets.
         * All values less than lowerBound are grouped together into a single bucket; for example if lowerBound = 10, then all values less than 10 are replaced with the value "-10".
         * The `lowerBound` block must only contain one argument. See the `fixedSizeBucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        lowerBound: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound;
        /**
         * Upper bound value of buckets.
         * All values greater than upperBound are grouped together into a single bucket; for example if upperBound = 89, then all values greater than 89 are replaced with the value "89+".
         * The `upperBound` block must only contain one argument. See the `fixedSizeBucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        upperBound: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound {
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound {
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig {
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig {
        /**
         * Replace each input value with a given value.
         * The `newValue` block must only contain one argument. For example when replacing the contents of a string-type field, only `stringValue` should be set.
         * Structure is documented below.
         */
        newValue: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: number;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.
         * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig {
        /**
         * A list of words to select from for random replacement. The [limits](https://cloud.google.com/dlp/limits) page contains details about the size limits of dictionaries.
         * Structure is documented below.
         */
        wordList: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig {
        /**
         * The part of the time to keep.
         * Possible values are: `YEAR`, `MONTH`, `DAY_OF_MONTH`, `DAY_OF_WEEK`, `WEEK_OF_YEAR`, `HOUR_OF_DAY`.
         */
        partToExtract?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformations {
        /**
         * Transform the record by applying various field transformations.
         * Structure is documented below.
         */
        fieldTransformations?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformation[];
        /**
         * Configuration defining which records get suppressed entirely. Records that match any suppression rule are omitted from the output.
         * Structure is documented below.
         */
        recordSuppressions?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppression[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformation {
        /**
         * Only apply the transformation if the condition evaluates to true for the given RecordCondition. The conditions are allowed to reference fields that are not used in the actual transformation.
         * Example Use Cases:
         * - Apply a different bucket transformation to an age column if the zip code column for the same record is within a specific range.
         * - Redact a field if the date of birth field is greater than 85.
         * Structure is documented below.
         */
        condition?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationCondition;
        /**
         * Input field(s) to apply the transformation to. When you have columns that reference their position within a list, omit the index from the FieldId.
         * FieldId name matching ignores the index. For example, instead of "contact.nums[0].type", use "contact.nums.type".
         * Structure is documented below.
         */
        fields: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationField[];
        /**
         * Treat the contents of the field as free text, and selectively transform content that matches an InfoType.
         * Only one of `primitiveTransformation` or `infoTypeTransformations` must be specified.
         * Structure is documented below.
         */
        infoTypeTransformations?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformations;
        /**
         * Apply the transformation to the entire field.
         * The `primitiveTransformation` block must only contain one argument, corresponding to the type of transformation.
         * Only one of `primitiveTransformation` or `infoTypeTransformations` must be specified.
         * Structure is documented below.
         */
        primitiveTransformation?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformation;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationCondition {
        /**
         * An expression, consisting of an operator and conditions.
         * Structure is documented below.
         */
        expressions?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressions;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressions {
        /**
         * Conditions to apply to the expression.
         * Structure is documented below.
         */
        conditions?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditions;
        /**
         * The operator to apply to the result of conditions. Default and currently only supported value is AND.
         * Default value is `AND`.
         * Possible values are: `AND`.
         */
        logicalOperator?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditions {
        /**
         * A collection of conditions.
         * Structure is documented below.
         */
        conditions?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsCondition[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsCondition {
        /**
         * Field within the record this condition is evaluated against.
         * Structure is documented below.
         */
        field: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionField;
        /**
         * Operator used to compare the field or infoType to the value.
         * Possible values are: `EQUAL_TO`, `NOT_EQUAL_TO`, `GREATER_THAN`, `LESS_THAN`, `GREATER_THAN_OR_EQUALS`, `LESS_THAN_OR_EQUALS`, `EXISTS`.
         */
        operator: string;
        /**
         * Value to compare against. [Mandatory, except for EXISTS tests.]
         * Structure is documented below.
         */
        value?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionField {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValue {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationField {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformations {
        /**
         * Transformation for each infoType. Cannot specify more than one for a given infoType.
         * Structure is documented below.
         */
        transformations: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformation[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformation {
        /**
         * InfoTypes to apply the transformation to. Leaving this empty will apply the transformation to apply to
         * all findings that correspond to infoTypes that were requested in InspectConfig.
         * Structure is documented below.
         */
        infoTypes?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoType[];
        /**
         * Apply the transformation to the entire field.
         * The `primitiveTransformation` block must only contain one argument, corresponding to the type of transformation.
         * Structure is documented below.
         */
        primitiveTransformation: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformation;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoType {
        /**
         * Name of the information type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformation {
        /**
         * Generalization function that buckets values based on ranges. The ranges and replacement values are dynamically provided by the user for custom behavior, such as 1-30 > LOW 31-65 > MEDIUM 66-100 > HIGH
         * This can be used on data of type: number, long, string, timestamp.
         * If the provided value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
         * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
         * Structure is documented below.
         */
        bucketingConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig;
        /**
         * Partially mask a string by replacing a given number of characters with a fixed character. Masking can start from the beginning or end of the string. This can be used on data of any type (numbers, longs, and so on) and when de-identifying structured data we'll attempt to preserve the original data's type. (This allows you to take a long like 123 and modify it to a string like **3).
         * Structure is documented below.
         */
        characterMaskConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig;
        /**
         * Pseudonymization method that generates deterministic encryption for the given input. Outputs a base64 encoded representation of the encrypted output. Uses AES-SIV based on the RFC [https://tools.ietf.org/html/rfc5297](https://tools.ietf.org/html/rfc5297).
         * Structure is documented below.
         */
        cryptoDeterministicConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig;
        /**
         * Pseudonymization method that generates surrogates via cryptographic hashing. Uses SHA-256. The key size must be either 32 or 64 bytes.
         * Outputs a base64 encoded representation of the hashed output (for example, L7k0BHmF1ha5U3NfGykjro4xWi1MPVQPjhMAZbSV9mM=).
         * Currently, only string and integer values can be hashed.
         * See https://cloud.google.com/dlp/docs/pseudonymization to learn more.
         * Structure is documented below.
         */
        cryptoHashConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig;
        /**
         * Replaces an identifier with a surrogate using Format Preserving Encryption (FPE) with the FFX mode of operation; however when used in the `content.reidentify` API method, it serves the opposite function by reversing the surrogate back into the original identifier. The identifier must be encoded as ASCII. For a given crypto key and context, the same identifier will be replaced with the same surrogate. Identifiers must be at least two characters long. In the case that the identifier is the empty string, it will be skipped. See [https://cloud.google.com/dlp/docs/pseudonymization](https://cloud.google.com/dlp/docs/pseudonymization) to learn more.
         * Note: We recommend using CryptoDeterministicConfig for all use cases which do not require preserving the input alphabet space and size, plus warrant referential integrity.
         * Structure is documented below.
         */
        cryptoReplaceFfxFpeConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig;
        /**
         * Shifts dates by random number of days, with option to be consistent for the same context. See https://cloud.google.com/dlp/docs/concepts-date-shifting to learn more.
         * Structure is documented below.
         */
        dateShiftConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig;
        /**
         * Buckets values based on fixed size ranges. The Bucketing transformation can provide all of this functionality, but requires more configuration. This message is provided as a convenience to the user for simple bucketing strategies.
         * The transformed value will be a hyphenated string of {lower_bound}-{upper_bound}. For example, if lowerBound = 10 and upperBound = 20, all values that are within this bucket will be replaced with "10-20".
         * This can be used on data of type: double, long.
         * If the bound Value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
         * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
         * Structure is documented below.
         */
        fixedSizeBucketingConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig;
        /**
         * Redact a given value. For example, if used with an InfoTypeTransformation transforming PHONE_NUMBER, and input 'My phone number is 206-555-0123', the output would be 'My phone number is '.
         */
        redactConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig;
        /**
         * Replace each input value with a given value.
         * Structure is documented below.
         */
        replaceConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig;
        /**
         * Replace with a value randomly drawn (with replacement) from a dictionary.
         * Structure is documented below.
         */
        replaceDictionaryConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig;
        /**
         * Replace each matching finding with the name of the info type.
         */
        replaceWithInfoTypeConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceWithInfoTypeConfig;
        /**
         * For use with Date, Timestamp, and TimeOfDay, extract or preserve a portion of the value.
         * Structure is documented below.
         */
        timePartConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig {
        /**
         * Set of buckets. Ranges must be non-overlapping.
         * Bucket is represented as a range, along with replacement values.
         * Structure is documented below.
         */
        buckets: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket {
        /**
         * Upper bound of the range, exclusive; type must match min.
         * The `max` block must only contain one argument. See the `bucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        max?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax;
        /**
         * Lower bound of the range, inclusive. Type should be the same as max if used.
         * The `min` block must only contain one argument. See the `bucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        min?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin;
        /**
         * Replacement value for this bucket.
         * The `replacementValue` block must only contain one argument.
         * Structure is documented below.
         */
        replacementValue: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax {
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin {
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue {
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig {
        /**
         * Characters to skip when doing de-identification of a value. These will be left alone and skipped.
         * Structure is documented below.
         */
        charactersToIgnores?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore[];
        /**
         * is *
         */
        maskingCharacter?: string;
        /**
         * is -4
         */
        numberToMask?: number;
        /**
         * Mask characters in reverse order. For example, if maskingCharacter is 0, numberToMask is 14, and reverseOrder is `false`, then the
         * input string `1234-5678-9012-3456` is masked as `00000000000000-3456`.
         */
        reverseOrder?: boolean;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore {
        /**
         * Characters to not transform when masking. Only one of this or `commonCharactersToIgnore` must be specified.
         */
        charactersToSkip?: string;
        /**
         * Common characters to not transform when masking. Useful to avoid removing punctuation. Only one of this or `charactersToSkip` must be specified.
         * Possible values are: `NUMERIC`, `ALPHA_UPPER_CASE`, `ALPHA_LOWER_CASE`, `PUNCTUATION`, `WHITESPACE`.
         */
        commonCharactersToIgnore?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig {
        /**
         * A context may be used for higher security and maintaining referential integrity such that the same identifier in two different contexts will be given a distinct surrogate. The context is appended to plaintext value being encrypted. On decryption the provided context is validated against the value used during encryption. If a context was provided during encryption, same context must be provided during decryption as well.
         * If the context is not set, plaintext would be used as is for encryption. If the context is set but:
         * 1. there is no record present when transforming a given value or
         * 2. the field is not present when transforming a given value,
         * plaintext would be used as is for encryption.
         * Note that case (1) is expected when an InfoTypeTransformation is applied to both structured and unstructured ContentItems.
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext;
        /**
         * The key used by the encryption function. For deterministic encryption using AES-SIV, the provided key is internally expanded to 64 bytes prior to use.
         * Structure is documented below.
         */
        cryptoKey: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey;
        /**
         * The custom info type to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom info type followed by the number of characters comprising the surrogate. The following scheme defines the format: {info type name}({surrogate character count}):{surrogate}
         * For example, if the name of custom info type is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
         * This annotation identifies the surrogate when inspecting content using the custom info type 'Surrogate'. This facilitates reversal of the surrogate when it occurs in free text.
         * Note: For record transformations where the entire cell in a table is being transformed, surrogates are not mandatory. Surrogates are used to denote the location of the token and are necessary for re-identification in free form text.
         * In order for inspection to work properly, the name of this info type must not occur naturally anywhere in your data; otherwise, inspection may either
         * *   reverse a surrogate that does not correspond to an actual identifier
         * *   be unable to parse the surrogate and result in an error
         * Therefore, choose your custom info type name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: MY\_TOKEN\_TYPE.
         * Structure is documented below.
         */
        surrogateInfoType: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext {
        /**
         * Name describing the field.
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern `[A-Za-z0-9$-_]{1,64}`.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore;
        /**
         * Optional version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig {
        /**
         * The key used by the encryption function.
         * Structure is documented below.
         */
        cryptoKey: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig {
        /**
         * Common alphabets. Only one of this, `customAlphabet` or `radix` must be specified.
         * Possible values are: `NUMERIC`, `HEXADECIMAL`, `UPPER_CASE_ALPHA_NUMERIC`, `ALPHA_NUMERIC`.
         */
        commonAlphabet?: string;
        /**
         * The 'tweak', a context may be used for higher security since the same identifier in two different contexts won't be given the same surrogate. If the context is not set, a default tweak will be used.
         * If the context is set but:
         * 1.  there is no record present when transforming a given value or
         * 2.  the field is not present when transforming a given value,
         * a default tweak will be used.
         * Note that case (1) is expected when an `InfoTypeTransformation` is applied to both structured and non-structured `ContentItem`s. Currently, the referenced field may be of value type integer or string.
         * The tweak is constructed as a sequence of bytes in big endian byte order such that:
         * *   a 64 bit integer is encoded followed by a single byte of value 1
         * *   a string is encoded in UTF-8 format followed by a single byte of value 2
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext;
        /**
         * The key used by the encryption algorithm.
         * Structure is documented below.
         */
        cryptoKey: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey;
        /**
         * This is supported by mapping these to the alphanumeric characters that the FFX mode natively supports. This happens before/after encryption/decryption. Each character listed must appear only once. Number of characters must be in the range \[2, 95\]. This must be encoded as ASCII. The order of characters does not matter. The full list of allowed characters is:
         * ``0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ~`!@#$%^&*()_-+={[}]|:;"'<,>.?/``. Only one of this, `commonAlphabet` or `radix` must be specified.
         */
        customAlphabet?: string;
        /**
         * The native way to select the alphabet. Must be in the range \[2, 95\]. Only one of this, `customAlphabet` or `commonAlphabet` must be specified.
         */
        radix?: number;
        /**
         * The custom infoType to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom infoType followed by the number of characters comprising the surrogate. The following scheme defines the format: info\_type\_name(surrogate\_character\_count):surrogate
         * For example, if the name of custom infoType is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
         * This annotation identifies the surrogate when inspecting content using the custom infoType [`SurrogateType`](https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#surrogatetype). This facilitates reversal of the surrogate when it occurs in free text.
         * In order for inspection to work properly, the name of this infoType must not occur naturally anywhere in your data; otherwise, inspection may find a surrogate that does not correspond to an actual identifier. Therefore, choose your custom infoType name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: MY\_TOKEN\_TYPE
         * Structure is documented below.
         */
        surrogateInfoType?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext {
        /**
         * Name describing the field.
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern `[A-Za-z0-9$-_]{1,64}`.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore;
        /**
         * Optional version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig {
        /**
         * Points to the field that contains the context, for example, an entity id.
         * If set, must also set cryptoKey. If set, shift will be consistent for the given context.
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext;
        /**
         * Causes the shift to be computed based on this key and the context. This results in the same shift for the same context and cryptoKey. If set, must also set context. Can only be applied to table items.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey;
        /**
         * For example, -5 means shift date to at most 5 days back in the past.
         */
        lowerBoundDays: number;
        /**
         * Range of shift in days. Actual shift will be selected at random within this range (inclusive ends). Negative means shift to earlier in time. Must not be more than 365250 days (1000 years) each direction.
         * For example, 3 means shift date to at most 3 days into the future.
         */
        upperBoundDays: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext {
        /**
         * Name describing the field.
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig {
        /**
         * Size of each bucket (except for minimum and maximum buckets).
         * So if lowerBound = 10, upperBound = 89, and bucketSize = 10, then the following buckets would be used: -10, 10-20, 20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-89, 89+.
         * Precision up to 2 decimals works.
         */
        bucketSize: number;
        /**
         * Lower bound value of buckets.
         * All values less than lowerBound are grouped together into a single bucket; for example if lowerBound = 10, then all values less than 10 are replaced with the value "-10".
         * The `lowerBound` block must only contain one argument. See the `fixedSizeBucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        lowerBound: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound;
        /**
         * Upper bound value of buckets.
         * All values greater than upperBound are grouped together into a single bucket; for example if upperBound = 89, then all values greater than 89 are replaced with the value "89+".
         * The `upperBound` block must only contain one argument. See the `fixedSizeBucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        upperBound: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound {
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound {
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig {
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig {
        /**
         * Replace each input value with a given value.
         * The `newValue` block must only contain one argument. For example when replacing the contents of a string-type field, only `stringValue` should be set.
         * Structure is documented below.
         */
        newValue: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.
         * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig {
        /**
         * A list of words to select from for random replacement. The [limits](https://cloud.google.com/dlp/limits) page contains details about the size limits of dictionaries.
         * Structure is documented below.
         */
        wordList: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceWithInfoTypeConfig {
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig {
        /**
         * The part of the time to keep.
         * Possible values are: `YEAR`, `MONTH`, `DAY_OF_MONTH`, `DAY_OF_WEEK`, `WEEK_OF_YEAR`, `HOUR_OF_DAY`.
         */
        partToExtract: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformation {
        /**
         * Generalization function that buckets values based on ranges. The ranges and replacement values are dynamically provided by the user for custom behavior, such as 1-30 > LOW 31-65 > MEDIUM 66-100 > HIGH
         * This can be used on data of type: number, long, string, timestamp.
         * If the provided value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
         * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
         * Structure is documented below.
         */
        bucketingConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfig;
        /**
         * Partially mask a string by replacing a given number of characters with a fixed character. Masking can start from the beginning or end of the string. This can be used on data of any type (numbers, longs, and so on) and when de-identifying structured data we'll attempt to preserve the original data's type. (This allows you to take a long like 123 and modify it to a string like **3).
         * Structure is documented below.
         */
        characterMaskConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfig;
        /**
         * Pseudonymization method that generates deterministic encryption for the given input. Outputs a base64 encoded representation of the encrypted output. Uses AES-SIV based on the RFC [https://tools.ietf.org/html/rfc5297](https://tools.ietf.org/html/rfc5297).
         * Structure is documented below.
         */
        cryptoDeterministicConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfig;
        /**
         * Pseudonymization method that generates surrogates via cryptographic hashing. Uses SHA-256. The key size must be either 32 or 64 bytes.
         * Outputs a base64 encoded representation of the hashed output (for example, L7k0BHmF1ha5U3NfGykjro4xWi1MPVQPjhMAZbSV9mM=).
         * Currently, only string and integer values can be hashed.
         * See https://cloud.google.com/dlp/docs/pseudonymization to learn more.
         * Structure is documented below.
         */
        cryptoHashConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfig;
        /**
         * Replaces an identifier with a surrogate using Format Preserving Encryption (FPE) with the FFX mode of operation; however when used in the `content.reidentify` API method, it serves the opposite function by reversing the surrogate back into the original identifier. The identifier must be encoded as ASCII. For a given crypto key and context, the same identifier will be replaced with the same surrogate. Identifiers must be at least two characters long. In the case that the identifier is the empty string, it will be skipped. See [https://cloud.google.com/dlp/docs/pseudonymization](https://cloud.google.com/dlp/docs/pseudonymization) to learn more.
         * Note: We recommend using CryptoDeterministicConfig for all use cases which do not require preserving the input alphabet space and size, plus warrant referential integrity.
         * Structure is documented below.
         */
        cryptoReplaceFfxFpeConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig;
        /**
         * Shifts dates by random number of days, with option to be consistent for the same context. See https://cloud.google.com/dlp/docs/concepts-date-shifting to learn more.
         * Structure is documented below.
         */
        dateShiftConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfig;
        /**
         * Buckets values based on fixed size ranges. The Bucketing transformation can provide all of this functionality, but requires more configuration. This message is provided as a convenience to the user for simple bucketing strategies.
         * The transformed value will be a hyphenated string of {lower_bound}-{upper_bound}. For example, if lowerBound = 10 and upperBound = 20, all values that are within this bucket will be replaced with "10-20".
         * This can be used on data of type: double, long.
         * If the bound Value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
         * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
         * Structure is documented below.
         */
        fixedSizeBucketingConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfig;
        /**
         * Redact a given value. For example, if used with an InfoTypeTransformation transforming PHONE_NUMBER, and input 'My phone number is 206-555-0123', the output would be 'My phone number is '.
         */
        redactConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationRedactConfig;
        /**
         * Replace each input value with a given value.
         * Structure is documented below.
         */
        replaceConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfig;
        /**
         * Replace with a value randomly drawn (with replacement) from a dictionary.
         * Structure is documented below.
         */
        replaceDictionaryConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfig;
        /**
         * For use with Date, Timestamp, and TimeOfDay, extract or preserve a portion of the value.
         * Structure is documented below.
         */
        timePartConfig?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationTimePartConfig;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfig {
        /**
         * Set of buckets. Ranges must be non-overlapping.
         * Bucket is represented as a range, along with replacement values.
         * Structure is documented below.
         */
        buckets?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucket[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucket {
        /**
         * Upper bound of the range, exclusive; type must match min.
         * The `max` block must only contain one argument. See the `bucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        max?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMax;
        /**
         * Lower bound of the range, inclusive. Type should be the same as max if used.
         * The `min` block must only contain one argument. See the `bucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        min?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMin;
        /**
         * Replacement value for this bucket.
         * The `replacementValue` block must only contain one argument.
         * Structure is documented below.
         */
        replacementValue: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMax {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMin {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfig {
        /**
         * Characters to skip when doing de-identification of a value. These will be left alone and skipped.
         * Structure is documented below.
         */
        charactersToIgnores?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore[];
        /**
         * is *
         */
        maskingCharacter?: string;
        /**
         * is -4
         */
        numberToMask?: number;
        /**
         * Mask characters in reverse order. For example, if maskingCharacter is 0, numberToMask is 14, and reverseOrder is `false`, then the
         * input string `1234-5678-9012-3456` is masked as `00000000000000-3456`.
         */
        reverseOrder?: boolean;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore {
        /**
         * Characters to not transform when masking. Only one of this or `commonCharactersToIgnore` must be specified.
         */
        charactersToSkip?: string;
        /**
         * Common characters to not transform when masking. Useful to avoid removing punctuation. Only one of this or `charactersToSkip` must be specified.
         * Possible values are: `NUMERIC`, `ALPHA_UPPER_CASE`, `ALPHA_LOWER_CASE`, `PUNCTUATION`, `WHITESPACE`.
         */
        commonCharactersToIgnore?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfig {
        /**
         * A context may be used for higher security and maintaining referential integrity such that the same identifier in two different contexts will be given a distinct surrogate. The context is appended to plaintext value being encrypted. On decryption the provided context is validated against the value used during encryption. If a context was provided during encryption, same context must be provided during decryption as well.
         * If the context is not set, plaintext would be used as is for encryption. If the context is set but:
         * 1. there is no record present when transforming a given value or
         * 2. the field is not present when transforming a given value,
         * plaintext would be used as is for encryption.
         * Note that case (1) is expected when an InfoTypeTransformation is applied to both structured and unstructured ContentItems.
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigContext;
        /**
         * The key used by the encryption function. For deterministic encryption using AES-SIV, the provided key is internally expanded to 64 bytes prior to use.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey;
        /**
         * The custom info type to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom info type followed by the number of characters comprising the surrogate. The following scheme defines the format: {info type name}({surrogate character count}):{surrogate}
         * For example, if the name of custom info type is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
         * This annotation identifies the surrogate when inspecting content using the custom info type 'Surrogate'. This facilitates reversal of the surrogate when it occurs in free text.
         * Note: For record transformations where the entire cell in a table is being transformed, surrogates are not mandatory. Surrogates are used to denote the location of the token and are necessary for re-identification in free form text.
         * In order for inspection to work properly, the name of this info type must not occur naturally anywhere in your data; otherwise, inspection may either
         * *   reverse a surrogate that does not correspond to an actual identifier
         * *   be unable to parse the surrogate and result in an error
         * Therefore, choose your custom info type name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: MY\_TOKEN\_TYPE.
         * Structure is documented below.
         */
        surrogateInfoType?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigContext {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern `[A-Za-z0-9$-_]{1,64}`.
         */
        name?: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore;
        /**
         * Optional version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfig {
        /**
         * The key used by the encryption function.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKey;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig {
        /**
         * Common alphabets. Only one of this, `customAlphabet` or `radix` must be specified.
         * Possible values are: `NUMERIC`, `HEXADECIMAL`, `UPPER_CASE_ALPHA_NUMERIC`, `ALPHA_NUMERIC`.
         */
        commonAlphabet?: string;
        /**
         * The 'tweak', a context may be used for higher security since the same identifier in two different contexts won't be given the same surrogate. If the context is not set, a default tweak will be used.
         * If the context is set but:
         * 1.  there is no record present when transforming a given value or
         * 2.  the field is not present when transforming a given value,
         * a default tweak will be used.
         * Note that case (1) is expected when an `InfoTypeTransformation` is applied to both structured and non-structured `ContentItem`s. Currently, the referenced field may be of value type integer or string.
         * The tweak is constructed as a sequence of bytes in big endian byte order such that:
         * *   a 64 bit integer is encoded followed by a single byte of value 1
         * *   a string is encoded in UTF-8 format followed by a single byte of value 2
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext;
        /**
         * The key used by the encryption algorithm.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey;
        /**
         * This is supported by mapping these to the alphanumeric characters that the FFX mode natively supports. This happens before/after encryption/decryption. Each character listed must appear only once. Number of characters must be in the range \[2, 95\]. This must be encoded as ASCII. The order of characters does not matter. The full list of allowed characters is:
         * ``0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ~`!@#$%^&*()_-+={[}]|:;"'<,>.?/``. Only one of this, `commonAlphabet` or `radix` must be specified.
         */
        customAlphabet?: string;
        /**
         * The native way to select the alphabet. Must be in the range \[2, 95\]. Only one of this, `customAlphabet` or `commonAlphabet` must be specified.
         */
        radix?: number;
        /**
         * The custom infoType to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom infoType followed by the number of characters comprising the surrogate. The following scheme defines the format: info\_type\_name(surrogate\_character\_count):surrogate
         * For example, if the name of custom infoType is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
         * This annotation identifies the surrogate when inspecting content using the custom infoType [`SurrogateType`](https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#surrogatetype). This facilitates reversal of the surrogate when it occurs in free text.
         * In order for inspection to work properly, the name of this infoType must not occur naturally anywhere in your data; otherwise, inspection may find a surrogate that does not correspond to an actual identifier. Therefore, choose your custom infoType name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: MY\_TOKEN\_TYPE
         * Structure is documented below.
         */
        surrogateInfoType?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern `[A-Za-z0-9$-_]{1,64}`.
         */
        name?: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore;
        /**
         * Optional version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfig {
        /**
         * Points to the field that contains the context, for example, an entity id.
         * If set, must also set cryptoKey. If set, shift will be consistent for the given context.
         * Structure is documented below.
         */
        context?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigContext;
        /**
         * Causes the shift to be computed based on this key and the context. This results in the same shift for the same context and cryptoKey. If set, must also set context. Can only be applied to table items.
         * Structure is documented below.
         */
        cryptoKey?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKey;
        /**
         * For example, -5 means shift date to at most 5 days back in the past.
         */
        lowerBoundDays: number;
        /**
         * Range of shift in days. Actual shift will be selected at random within this range (inclusive ends). Negative means shift to earlier in time. Must not be more than 365250 days (1000 years) each direction.
         * For example, 3 means shift date to at most 3 days into the future.
         */
        upperBoundDays: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigContext {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKey {
        /**
         * KMS wrapped key.
         * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
         * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, `transient` or `unwrapped` must be specified.
         * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
         * Structure is documented below.
         */
        kmsWrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped;
        /**
         * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, `unwrapped` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        transient?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient;
        /**
         * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, `transient` or `kmsWrapped` must be specified.
         * Structure is documented below.
         */
        unwrapped?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped {
        /**
         * The resource name of the KMS CryptoKey to use for unwrapping.
         */
        cryptoKeyName: string;
        /**
         * The wrapped data crypto key.
         * A base64-encoded string.
         */
        wrappedKey: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient {
        /**
         * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate `TransientCryptoKey` protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
         */
        name: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped {
        /**
         * A 128/192/256 bit key.
         * A base64-encoded string.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        key: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfig {
        /**
         * Size of each bucket (except for minimum and maximum buckets).
         * So if lowerBound = 10, upperBound = 89, and bucketSize = 10, then the following buckets would be used: -10, 10-20, 20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-89, 89+.
         * Precision up to 2 decimals works.
         */
        bucketSize: number;
        /**
         * Lower bound value of buckets.
         * All values less than lowerBound are grouped together into a single bucket; for example if lowerBound = 10, then all values less than 10 are replaced with the value "-10".
         * The `lowerBound` block must only contain one argument. See the `fixedSizeBucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        lowerBound: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound;
        /**
         * Upper bound value of buckets.
         * All values greater than upperBound are grouped together into a single bucket; for example if upperBound = 89, then all values greater than 89 are replaced with the value "89+".
         * The `upperBound` block must only contain one argument. See the `fixedSizeBucketingConfig` block description for more information about choosing a data type.
         * Structure is documented below.
         */
        upperBound: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationRedactConfig {
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfig {
        /**
         * Replace each input value with a given value.
         * The `newValue` block must only contain one argument. For example when replacing the contents of a string-type field, only `stringValue` should be set.
         * Structure is documented below.
         */
        newValue: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValue {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.
         * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfig {
        /**
         * A list of words to select from for random replacement. The [limits](https://cloud.google.com/dlp/limits) page contains details about the size limits of dictionaries.
         * Structure is documented below.
         */
        wordList?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfigWordList;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfigWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationTimePartConfig {
        /**
         * The part of the time to keep.
         * Possible values are: `YEAR`, `MONTH`, `DAY_OF_MONTH`, `DAY_OF_WEEK`, `WEEK_OF_YEAR`, `HOUR_OF_DAY`.
         */
        partToExtract?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppression {
        /**
         * A condition that when it evaluates to true will result in the record being evaluated to be suppressed from the transformed content.
         * Structure is documented below.
         */
        condition?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionCondition;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionCondition {
        /**
         * An expression, consisting of an operator and conditions.
         * Structure is documented below.
         */
        expressions?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressions;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressions {
        /**
         * Conditions to apply to the expression.
         * Structure is documented below.
         */
        conditions?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditions;
        /**
         * The operator to apply to the result of conditions. Default and currently only supported value is AND.
         * Default value is `AND`.
         * Possible values are: `AND`.
         */
        logicalOperator?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditions {
        /**
         * A collection of conditions.
         * Structure is documented below.
         */
        conditions?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsCondition[];
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsCondition {
        /**
         * Field within the record this condition is evaluated against.
         * Structure is documented below.
         */
        field: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionField;
        /**
         * Operator used to compare the field or infoType to the value.
         * Possible values are: `EQUAL_TO`, `NOT_EQUAL_TO`, `GREATER_THAN`, `LESS_THAN`, `GREATER_THAN_OR_EQUALS`, `LESS_THAN_OR_EQUALS`, `EXISTS`.
         */
        operator: string;
        /**
         * Value to compare against. [Mandatory, except for EXISTS tests.]
         * Structure is documented below.
         */
        value?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValue;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionField {
        /**
         * Name describing the field.
         */
        name?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValue {
        /**
         * A boolean value.
         */
        booleanValue?: boolean;
        /**
         * Represents a whole or partial calendar date.
         * Structure is documented below.
         */
        dateValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueDateValue;
        /**
         * Represents a day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeekValue?: string;
        /**
         * A float value.
         */
        floatValue?: number;
        /**
         * An integer value (int64 format)
         */
        integerValue?: string;
        /**
         * A string value.
         */
        stringValue?: string;
        /**
         * Represents a time of day.
         * Structure is documented below.
         */
        timeValue?: outputs.dataloss.PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueTimeValue;
        /**
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        timestampValue?: string;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueDateValue {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
         *
         * - - -
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
         */
        year?: number;
    }

    export interface PreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueTimeValue {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PreventionInspectTemplateInspectConfig {
        /**
         * List of options defining data content to scan. If empty, text, images, and other content will be included.
         * Each value may be one of: `CONTENT_TEXT`, `CONTENT_IMAGE`.
         */
        contentOptions?: string[];
        /**
         * Custom info types to be used. See https://cloud.google.com/dlp/docs/creating-custom-infotypes to learn more.
         * Structure is documented below.
         */
        customInfoTypes?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoType[];
        /**
         * When true, excludes type information of the findings.
         */
        excludeInfoTypes?: boolean;
        /**
         * When true, a contextual quote from the data that triggered a finding is included in the response.
         */
        includeQuote?: boolean;
        /**
         * Restricts what infoTypes to look for. The values must correspond to InfoType values returned by infoTypes.list
         * or listed at https://cloud.google.com/dlp/docs/infotypes-reference.
         * When no InfoTypes or CustomInfoTypes are specified in a request, the system may automatically choose what detectors to run.
         * By default this may be all types, but may change over time as detectors are updated.
         * Structure is documented below.
         */
        infoTypes?: outputs.dataloss.PreventionInspectTemplateInspectConfigInfoType[];
        /**
         * Configuration to control the number of findings returned.
         * Structure is documented below.
         */
        limits?: outputs.dataloss.PreventionInspectTemplateInspectConfigLimits;
        /**
         * Only returns findings equal or above this threshold. See https://cloud.google.com/dlp/docs/likelihood for more info
         * Default value is `POSSIBLE`.
         * Possible values are: `VERY_UNLIKELY`, `UNLIKELY`, `POSSIBLE`, `LIKELY`, `VERY_LIKELY`.
         */
        minLikelihood?: string;
        /**
         * Set of rules to apply to the findings for this InspectConfig. Exclusion rules, contained in the set are executed in the end,
         * other rules are executed in the order they are specified for each info type.
         * Structure is documented below.
         */
        ruleSets?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSet[];
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoType {
        /**
         * Dictionary which defines the rule.
         * Structure is documented below.
         */
        dictionary?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeDictionary;
        /**
         * If set to EXCLUSION_TYPE_EXCLUDE this infoType will not cause a finding to be returned. It still can be used for rules matching.
         * Possible values are: `EXCLUSION_TYPE_EXCLUDE`.
         */
        exclusionType?: string;
        /**
         * CustomInfoType can either be a new infoType, or an extension of built-in infoType, when the name matches one of existing
         * infoTypes and that infoType is specified in `infoTypes` field. Specifying the latter adds findings to the
         * one detected by the system. If built-in info type is not specified in `infoTypes` list then the name is
         * treated as a custom info type.
         * Structure is documented below.
         */
        infoType: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeInfoType;
        /**
         * Likelihood to return for this CustomInfoType. This base value can be altered by a detection rule if the finding meets the criteria
         * specified by the rule.
         * Default value is `VERY_LIKELY`.
         * Possible values are: `VERY_UNLIKELY`, `UNLIKELY`, `POSSIBLE`, `LIKELY`, `VERY_LIKELY`.
         */
        likelihood?: string;
        /**
         * Regular expression which defines the rule.
         * Structure is documented below.
         */
        regex?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeRegex;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeSensitivityScore;
        /**
         * A reference to a StoredInfoType to use with scanning.
         * Structure is documented below.
         */
        storedType?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeStoredType;
        /**
         * Message for detecting output from deidentification transformations that support reversing.
         */
        surrogateType?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeSurrogateType;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeDictionary {
        /**
         * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
         * Structure is documented below.
         */
        cloudStoragePath?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryCloudStoragePath;
        /**
         * List of words or phrases to search for.
         * Structure is documented below.
         */
        wordList?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryWordList;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryCloudStoragePath {
        /**
         * A url representing a file or path (no wildcards) in Cloud Storage. Example: `gs://[BUCKET_NAME]/dictionary.txt`
         */
        path: string;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one
         * phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names
         * listed at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionInspectTemplateInspectConfigCustomInfoTypeInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeRegex {
        /**
         * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression.
         * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern: string;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeStoredType {
        /**
         * Resource name of the requested StoredInfoType, for example `organizations/433245324/storedInfoTypes/432452342`
         * or `projects/project-id/storedInfoTypes/432452342`.
         */
        name: string;
    }

    export interface PreventionInspectTemplateInspectConfigCustomInfoTypeSurrogateType {
    }

    export interface PreventionInspectTemplateInspectConfigInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
         * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionInspectTemplateInspectConfigInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionInspectTemplateInspectConfigInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionInspectTemplateInspectConfigLimits {
        /**
         * Configuration of findings limit given for specified infoTypes.
         * Structure is documented below.
         */
        maxFindingsPerInfoTypes?: outputs.dataloss.PreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoType[];
        /**
         * Max number of findings that will be returned for each item scanned. The maximum returned is 2000.
         */
        maxFindingsPerItem: number;
        /**
         * Max number of findings that will be returned per request/job. The maximum returned is 2000.
         */
        maxFindingsPerRequest: number;
    }

    export interface PreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoType {
        /**
         * Type of information the findings limit applies to. Only one limit per infoType should be provided. If InfoTypeLimit does
         * not have an infoType, the DLP API applies the limit against all infoTypes that are found but not
         * specified in another InfoTypeLimit.
         * Structure is documented below.
         */
        infoType: outputs.dataloss.PreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoType;
        /**
         * Max findings limit for the given infoType.
         */
        maxFindings: number;
    }

    export interface PreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names
         * listed at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSet {
        /**
         * List of infoTypes this rule set is applied to.
         * Structure is documented below.
         */
        infoTypes: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetInfoType[];
        /**
         * Set of rules to be applied to infoTypes. The rules are applied in order.
         * Structure is documented below.
         */
        rules: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRule[];
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
         * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRule {
        /**
         * The rule that specifies conditions when findings of infoTypes specified in InspectionRuleSet are removed from results.
         * Structure is documented below.
         */
        exclusionRule?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRule;
        /**
         * Hotword-based detection rule.
         * Structure is documented below.
         */
        hotwordRule?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRule;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRule {
        /**
         * Dictionary which defines the rule.
         * Structure is documented below.
         */
        dictionary?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionary;
        /**
         * Drop if the hotword rule is contained in the proximate context.
         * For tabular data, the context includes the column name.
         * Structure is documented below.
         */
        excludeByHotword?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotword;
        /**
         * Set of infoTypes for which findings would affect this rule.
         * Structure is documented below.
         */
        excludeInfoTypes?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes;
        /**
         * How the rule is applied. See the documentation for more information: https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#MatchingType
         * Possible values are: `MATCHING_TYPE_FULL_MATCH`, `MATCHING_TYPE_PARTIAL_MATCH`, `MATCHING_TYPE_INVERSE_MATCH`.
         */
        matchingType: string;
        /**
         * Regular expression which defines the rule.
         * Structure is documented below.
         */
        regex?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleRegex;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionary {
        /**
         * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
         * Structure is documented below.
         */
        cloudStoragePath?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath;
        /**
         * List of words or phrases to search for.
         * Structure is documented below.
         */
        wordList?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryWordList;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath {
        /**
         * A url representing a file or path (no wildcards) in Cloud Storage. Example: `gs://[BUCKET_NAME]/dictionary.txt`
         */
        path: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one
         * phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotword {
        /**
         * Regular expression pattern defining what qualifies as a hotword.
         * Structure is documented below.
         */
        hotwordRegex: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex;
        /**
         * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
         * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
         * used to match substrings of the finding itself. For example, the certainty of a phone number regex
         * `(\d{3}) \d{3}-\d{4}` could be adjusted upwards if the area code is known to be the local area code of a company
         * office using the hotword regex `(xxx)`, where `xxx` is the area code in question.
         * Structure is documented below.
         */
        proximity: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex {
        /**
         * The index of the submatch to extract as findings. When not specified,
         * the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression. Its syntax
         * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity {
        /**
         * Number of characters after the finding to consider.
         */
        windowAfter?: number;
        /**
         * Number of characters before the finding to consider.
         */
        windowBefore?: number;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes {
        /**
         * If a finding is matched by any of the infoType detectors listed here, the finding will be excluded from the scan results.
         * Structure is documented below.
         */
        infoTypes: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType[];
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
         * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore;
        /**
         * Version name for this InfoType.
         */
        version?: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleRegex {
        /**
         * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression.
         * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRule {
        /**
         * Regular expression pattern defining what qualifies as a hotword.
         * Structure is documented below.
         */
        hotwordRegex: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleHotwordRegex;
        /**
         * Likelihood adjustment to apply to all matching findings.
         * Structure is documented below.
         */
        likelihoodAdjustment: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment;
        /**
         * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
         * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
         * used to match substrings of the finding itself. For example, the certainty of a phone number regex
         * `(\d{3}) \d{3}-\d{4}` could be adjusted upwards if the area code is known to be the local area code of a company
         * office using the hotword regex `(xxx)`, where `xxx` is the area code in question.
         * Structure is documented below.
         */
        proximity: outputs.dataloss.PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleProximity;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleHotwordRegex {
        /**
         * The index of the submatch to extract as findings. When not specified,
         * the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression. Its syntax
         * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern: string;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment {
        /**
         * Set the likelihood of a finding to a fixed value. Either this or relativeLikelihood can be set.
         * Possible values are: `VERY_UNLIKELY`, `UNLIKELY`, `POSSIBLE`, `LIKELY`, `VERY_LIKELY`.
         */
        fixedLikelihood?: string;
        /**
         * Increase or decrease the likelihood by the specified number of levels. For example,
         * if a finding would be POSSIBLE without the detection rule and relativeLikelihood is 1,
         * then it is upgraded to LIKELY, while a value of -1 would downgrade it to UNLIKELY.
         * Likelihood may never drop below VERY_UNLIKELY or exceed VERY_LIKELY, so applying an
         * adjustment of 1 followed by an adjustment of -1 when base likelihood is VERY_LIKELY
         * will result in a final likelihood of LIKELY. Either this or fixedLikelihood can be set.
         */
        relativeLikelihood?: number;
    }

    export interface PreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleProximity {
        /**
         * Number of characters after the finding to consider.
         */
        windowAfter?: number;
        /**
         * Number of characters before the finding to consider.
         */
        windowBefore?: number;
    }

    export interface PreventionJobTriggerInspectJob {
        /**
         * A task to execute on the completion of a job.
         * Structure is documented below.
         */
        actions?: outputs.dataloss.PreventionJobTriggerInspectJobAction[];
        /**
         * The core content of the template.
         * Structure is documented below.
         */
        inspectConfig?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfig;
        /**
         * The name of the template to run when this job is triggered.
         */
        inspectTemplateName?: string;
        /**
         * Information on where to inspect
         * Structure is documented below.
         */
        storageConfig: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfig;
    }

    export interface PreventionJobTriggerInspectJobAction {
        /**
         * Create a de-identified copy of the requested table or files.
         * Structure is documented below.
         */
        deidentify?: outputs.dataloss.PreventionJobTriggerInspectJobActionDeidentify;
        /**
         * Sends an email when the job completes. The email goes to IAM project owners and technical Essential Contacts.
         */
        jobNotificationEmails?: outputs.dataloss.PreventionJobTriggerInspectJobActionJobNotificationEmails;
        /**
         * Publish a message into a given Pub/Sub topic when the job completes.
         * Structure is documented below.
         */
        pubSub?: outputs.dataloss.PreventionJobTriggerInspectJobActionPubSub;
        /**
         * Publish findings of a DlpJob to Data Catalog.
         */
        publishFindingsToCloudDataCatalog?: outputs.dataloss.PreventionJobTriggerInspectJobActionPublishFindingsToCloudDataCatalog;
        /**
         * Publish the result summary of a DlpJob to the Cloud Security Command Center.
         */
        publishSummaryToCscc?: outputs.dataloss.PreventionJobTriggerInspectJobActionPublishSummaryToCscc;
        /**
         * Enable Stackdriver metric dlp.googleapis.com/findingCount.
         */
        publishToStackdriver?: outputs.dataloss.PreventionJobTriggerInspectJobActionPublishToStackdriver;
        /**
         * If set, the detailed findings will be persisted to the specified OutputStorageConfig. Only a single instance of this action can be specified. Compatible with: Inspect, Risk
         * Structure is documented below.
         */
        saveFindings?: outputs.dataloss.PreventionJobTriggerInspectJobActionSaveFindings;
    }

    export interface PreventionJobTriggerInspectJobActionDeidentify {
        /**
         * User settable Cloud Storage bucket and folders to store de-identified files.
         * This field must be set for cloud storage deidentification.
         * The output Cloud Storage bucket must be different from the input bucket.
         * De-identified files will overwrite files in the output path.
         * Form of: gs://bucket/folder/ or gs://bucket
         */
        cloudStorageOutput: string;
        /**
         * List of user-specified file type groups to transform. If specified, only the files with these filetypes will be transformed.
         * If empty, all supported files will be transformed. Supported types may be automatically added over time.
         * If a file type is set in this field that isn't supported by the Deidentify action then the job will fail and will not be successfully created/started.
         * Each value may be one of: `IMAGE`, `TEXT_FILE`, `CSV`, `TSV`.
         */
        fileTypesToTransforms?: string[];
        /**
         * User specified deidentify templates and configs for structured, unstructured, and image files.
         * Structure is documented below.
         */
        transformationConfig?: outputs.dataloss.PreventionJobTriggerInspectJobActionDeidentifyTransformationConfig;
        /**
         * Config for storing transformation details.
         * Structure is documented below.
         */
        transformationDetailsStorageConfig?: outputs.dataloss.PreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfig;
    }

    export interface PreventionJobTriggerInspectJobActionDeidentifyTransformationConfig {
        /**
         * If this template is specified, it will serve as the default de-identify template.
         */
        deidentifyTemplate?: string;
        /**
         * If this template is specified, it will serve as the de-identify template for images.
         */
        imageRedactTemplate?: string;
        /**
         * If this template is specified, it will serve as the de-identify template for structured content such as delimited files and tables.
         */
        structuredDeidentifyTemplate?: string;
    }

    export interface PreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfig {
        /**
         * The BigQuery table in which to store the output.
         * Structure is documented below.
         */
        table: outputs.dataloss.PreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfigTable;
    }

    export interface PreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfigTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The ID of the table. The ID must contain only letters (a-z,
         * A-Z), numbers (0-9), or underscores (_). The maximum length
         * is 1,024 characters.
         */
        tableId?: string;
    }

    export interface PreventionJobTriggerInspectJobActionJobNotificationEmails {
    }

    export interface PreventionJobTriggerInspectJobActionPubSub {
        /**
         * Cloud Pub/Sub topic to send notifications to.
         */
        topic: string;
    }

    export interface PreventionJobTriggerInspectJobActionPublishFindingsToCloudDataCatalog {
    }

    export interface PreventionJobTriggerInspectJobActionPublishSummaryToCscc {
    }

    export interface PreventionJobTriggerInspectJobActionPublishToStackdriver {
    }

    export interface PreventionJobTriggerInspectJobActionSaveFindings {
        /**
         * Information on where to store output
         * Structure is documented below.
         */
        outputConfig: outputs.dataloss.PreventionJobTriggerInspectJobActionSaveFindingsOutputConfig;
    }

    export interface PreventionJobTriggerInspectJobActionSaveFindingsOutputConfig {
        /**
         * Schema used for writing the findings for Inspect jobs. This field is only used for
         * Inspect and must be unspecified for Risk jobs. Columns are derived from the Finding
         * object. If appending to an existing table, any columns from the predefined schema
         * that are missing will be added. No columns in the existing table will be deleted.
         * If unspecified, then all available columns will be used for a new table or an (existing)
         * table with no schema, and no changes will be made to an existing table that has a schema.
         * Only for use with external storage.
         * Possible values are: `BASIC_COLUMNS`, `GCS_COLUMNS`, `DATASTORE_COLUMNS`, `BIG_QUERY_COLUMNS`, `ALL_COLUMNS`.
         */
        outputSchema?: string;
        /**
         * Information on the location of the target BigQuery Table.
         * Structure is documented below.
         */
        table: outputs.dataloss.PreventionJobTriggerInspectJobActionSaveFindingsOutputConfigTable;
    }

    export interface PreventionJobTriggerInspectJobActionSaveFindingsOutputConfigTable {
        /**
         * The ID of the dataset containing this table.
         */
        datasetId: string;
        /**
         * The ID of the project containing this table.
         */
        projectId: string;
        /**
         * The ID of the table. The ID must contain only letters (a-z,
         * A-Z), numbers (0-9), or underscores (_). The maximum length
         * is 1,024 characters.
         */
        tableId?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfig {
        /**
         * Custom info types to be used. See https://cloud.google.com/dlp/docs/creating-custom-infotypes to learn more.
         * Structure is documented below.
         */
        customInfoTypes?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoType[];
        /**
         * When true, excludes type information of the findings.
         */
        excludeInfoTypes?: boolean;
        /**
         * When true, a contextual quote from the data that triggered a finding is included in the response.
         */
        includeQuote?: boolean;
        /**
         * Restricts what infoTypes to look for. The values must correspond to InfoType values returned by infoTypes.list
         * or listed at https://cloud.google.com/dlp/docs/infotypes-reference.
         * When no InfoTypes or CustomInfoTypes are specified in a request, the system may automatically choose what detectors to run.
         * By default this may be all types, but may change over time as detectors are updated.
         * Structure is documented below.
         */
        infoTypes?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigInfoType[];
        /**
         * Configuration to control the number of findings returned.
         * Structure is documented below.
         */
        limits?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigLimits;
        /**
         * Only returns findings equal or above this threshold. See https://cloud.google.com/dlp/docs/likelihood for more info
         * Default value is `POSSIBLE`.
         * Possible values are: `VERY_UNLIKELY`, `UNLIKELY`, `POSSIBLE`, `LIKELY`, `VERY_LIKELY`.
         */
        minLikelihood?: string;
        /**
         * Set of rules to apply to the findings for this InspectConfig. Exclusion rules, contained in the set are executed in the end,
         * other rules are executed in the order they are specified for each info type.
         * Structure is documented below.
         */
        ruleSets?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSet[];
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoType {
        /**
         * Dictionary which defines the rule.
         * Structure is documented below.
         */
        dictionary?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionary;
        /**
         * If set to EXCLUSION_TYPE_EXCLUDE this infoType will not cause a finding to be returned. It still can be used for rules matching.
         * Possible values are: `EXCLUSION_TYPE_EXCLUDE`.
         */
        exclusionType?: string;
        /**
         * CustomInfoType can either be a new infoType, or an extension of built-in infoType, when the name matches one of existing
         * infoTypes and that infoType is specified in `infoTypes` field. Specifying the latter adds findings to the
         * one detected by the system. If built-in info type is not specified in `infoTypes` list then the name is
         * treated as a custom info type.
         * Structure is documented below.
         */
        infoType: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoType;
        /**
         * Likelihood to return for this CustomInfoType. This base value can be altered by a detection rule if the finding meets the criteria
         * specified by the rule.
         * Default value is `VERY_LIKELY`.
         * Possible values are: `VERY_UNLIKELY`, `UNLIKELY`, `POSSIBLE`, `LIKELY`, `VERY_LIKELY`.
         */
        likelihood?: string;
        /**
         * Regular expression which defines the rule.
         * Structure is documented below.
         */
        regex?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeRegex;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSensitivityScore;
        /**
         * A reference to a StoredInfoType to use with scanning.
         * Structure is documented below.
         */
        storedType?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeStoredType;
        /**
         * Message for detecting output from deidentification transformations that support reversing.
         */
        surrogateType?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSurrogateType;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionary {
        /**
         * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
         * Structure is documented below.
         */
        cloudStoragePath?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryCloudStoragePath;
        /**
         * List of words or phrases to search for.
         * Structure is documented below.
         */
        wordList?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryWordList;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryCloudStoragePath {
        /**
         * A url representing a file or path (no wildcards) in Cloud Storage. Example: `gs://[BUCKET_NAME]/dictionary.txt`
         */
        path: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one
         * phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names
         * listed at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoTypeSensitivityScore;
        /**
         * Version of the information type to use. By default, the version is set to stable.
         */
        version?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeRegex {
        /**
         * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression.
         * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeStoredType {
        /**
         * (Output)
         * The creation timestamp of an inspectTemplate. Set by the server.
         */
        createTime: string;
        /**
         * Resource name of the requested StoredInfoType, for example `organizations/433245324/storedInfoTypes/432452342`
         * or `projects/project-id/storedInfoTypes/432452342`.
         */
        name: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSurrogateType {
    }

    export interface PreventionJobTriggerInspectJobInspectConfigInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
         * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigInfoTypeSensitivityScore;
        /**
         * Version of the information type to use. By default, the version is set to stable.
         */
        version?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigLimits {
        /**
         * Configuration of findings limit given for specified infoTypes.
         * Structure is documented below.
         */
        maxFindingsPerInfoTypes?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoType[];
        /**
         * Max number of findings that will be returned for each item scanned. The maximum returned is 2000.
         */
        maxFindingsPerItem?: number;
        /**
         * Max number of findings that will be returned per request/job. The maximum returned is 2000.
         */
        maxFindingsPerRequest?: number;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoType {
        /**
         * Type of information the findings limit applies to. Only one limit per infoType should be provided. If InfoTypeLimit does
         * not have an infoType, the DLP API applies the limit against all infoTypes that are found but not
         * specified in another InfoTypeLimit.
         * Structure is documented below.
         */
        infoType?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoType;
        /**
         * Max findings limit for the given infoType.
         */
        maxFindings?: number;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names
         * listed at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore;
        /**
         * Version of the information type to use. By default, the version is set to stable.
         */
        version?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSet {
        /**
         * List of infoTypes this rule set is applied to.
         * Structure is documented below.
         */
        infoTypes?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetInfoType[];
        /**
         * Set of rules to be applied to infoTypes. The rules are applied in order.
         * Structure is documented below.
         */
        rules: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRule[];
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
         * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetInfoTypeSensitivityScore;
        /**
         * Version of the information type to use. By default, the version is set to stable.
         */
        version?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRule {
        /**
         * The rule that specifies conditions when findings of infoTypes specified in InspectionRuleSet are removed from results.
         * Structure is documented below.
         */
        exclusionRule?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRule;
        /**
         * Hotword-based detection rule.
         * Structure is documented below.
         */
        hotwordRule?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRule;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRule {
        /**
         * Dictionary which defines the rule.
         * Structure is documented below.
         */
        dictionary?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionary;
        /**
         * Drop if the hotword rule is contained in the proximate context.
         * Structure is documented below.
         */
        excludeByHotword?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotword;
        /**
         * Set of infoTypes for which findings would affect this rule.
         * Structure is documented below.
         */
        excludeInfoTypes?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes;
        /**
         * How the rule is applied. See the documentation for more information: https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#MatchingType
         * Possible values are: `MATCHING_TYPE_FULL_MATCH`, `MATCHING_TYPE_PARTIAL_MATCH`, `MATCHING_TYPE_INVERSE_MATCH`.
         */
        matchingType: string;
        /**
         * Regular expression which defines the rule.
         * Structure is documented below.
         */
        regex?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleRegex;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionary {
        /**
         * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
         * Structure is documented below.
         */
        cloudStoragePath?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath;
        /**
         * List of words or phrases to search for.
         * Structure is documented below.
         */
        wordList?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryWordList;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath {
        /**
         * A url representing a file or path (no wildcards) in Cloud Storage. Example: `gs://[BUCKET_NAME]/dictionary.txt`
         */
        path: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one
         * phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotword {
        /**
         * Regular expression pattern defining what qualifies as a hotword.
         * Structure is documented below.
         */
        hotwordRegex?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex;
        /**
         * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
         * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
         * used to match substrings of the finding itself. For example, the certainty of a phone number regex
         * `(\d{3}) \d{3}-\d{4}` could be adjusted upwards if the area code is known to be the local area code of a company
         * office using the hotword regex `(xxx)`, where `xxx` is the area code in question.
         * Structure is documented below.
         */
        proximity?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex {
        /**
         * The index of the submatch to extract as findings. When not specified,
         * the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression. Its syntax
         * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity {
        /**
         * Number of characters after the finding to consider. Either this or windowBefore must be specified
         */
        windowAfter?: number;
        /**
         * Number of characters before the finding to consider. Either this or windowAfter must be specified
         */
        windowBefore?: number;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes {
        /**
         * If a finding is matched by any of the infoType detectors listed here, the finding will be excluded from the scan results.
         * Structure is documented below.
         */
        infoTypes: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType[];
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType {
        /**
         * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
         * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
         */
        name: string;
        /**
         * Optional custom sensitivity for this InfoType. This only applies to data profiling.
         * Structure is documented below.
         */
        sensitivityScore?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore;
        /**
         * Version of the information type to use. By default, the version is set to stable.
         */
        version?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore {
        /**
         * The sensitivity score applied to the resource.
         * Possible values are: `SENSITIVITY_LOW`, `SENSITIVITY_MODERATE`, `SENSITIVITY_HIGH`.
         */
        score: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleRegex {
        /**
         * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression.
         * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRule {
        /**
         * Regular expression pattern defining what qualifies as a hotword.
         * Structure is documented below.
         */
        hotwordRegex?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleHotwordRegex;
        /**
         * Likelihood adjustment to apply to all matching findings.
         * Structure is documented below.
         */
        likelihoodAdjustment?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment;
        /**
         * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
         * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
         * used to match substrings of the finding itself. For example, the certainty of a phone number regex
         * `(\d{3}) \d{3}-\d{4}` could be adjusted upwards if the area code is known to be the local area code of a company
         * office using the hotword regex `(xxx)`, where `xxx` is the area code in question.
         * Structure is documented below.
         */
        proximity?: outputs.dataloss.PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleProximity;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleHotwordRegex {
        /**
         * The index of the submatch to extract as findings. When not specified,
         * the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression. Its syntax
         * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern?: string;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment {
        /**
         * Set the likelihood of a finding to a fixed value. Either this or relativeLikelihood can be set.
         * Possible values are: `VERY_UNLIKELY`, `UNLIKELY`, `POSSIBLE`, `LIKELY`, `VERY_LIKELY`.
         */
        fixedLikelihood?: string;
        /**
         * Increase or decrease the likelihood by the specified number of levels. For example,
         * if a finding would be POSSIBLE without the detection rule and relativeLikelihood is 1,
         * then it is upgraded to LIKELY, while a value of -1 would downgrade it to UNLIKELY.
         * Likelihood may never drop below VERY_UNLIKELY or exceed VERY_LIKELY, so applying an
         * adjustment of 1 followed by an adjustment of -1 when base likelihood is VERY_LIKELY
         * will result in a final likelihood of LIKELY. Either this or fixedLikelihood can be set.
         */
        relativeLikelihood?: number;
    }

    export interface PreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleProximity {
        /**
         * Number of characters after the finding to consider. Either this or windowBefore must be specified
         */
        windowAfter?: number;
        /**
         * Number of characters before the finding to consider. Either this or windowAfter must be specified
         */
        windowBefore?: number;
    }

    export interface PreventionJobTriggerInspectJobStorageConfig {
        /**
         * Options defining BigQuery table and row identifiers.
         * Structure is documented below.
         */
        bigQueryOptions?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigBigQueryOptions;
        /**
         * Options defining a file or a set of files within a Google Cloud Storage bucket.
         * Structure is documented below.
         */
        cloudStorageOptions?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigCloudStorageOptions;
        /**
         * Options defining a data set within Google Cloud Datastore.
         * Structure is documented below.
         */
        datastoreOptions?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigDatastoreOptions;
        /**
         * Configuration to control jobs where the content being inspected is outside of Google Cloud Platform.
         * Structure is documented below.
         */
        hybridOptions?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigHybridOptions;
        /**
         * Information on where to inspect
         * Structure is documented below.
         */
        timespanConfig?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigTimespanConfig;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigBigQueryOptions {
        /**
         * References to fields excluded from scanning.
         * This allows you to skip inspection of entire columns which you know have no findings.
         * Structure is documented below.
         */
        excludedFields?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsExcludedField[];
        /**
         * Specifies the BigQuery fields that will be returned with findings.
         * If not specified, no identifying fields will be returned for findings.
         * Structure is documented below.
         */
        identifyingFields?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIdentifyingField[];
        /**
         * Limit scanning only to these fields.
         * Structure is documented below.
         */
        includedFields?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIncludedField[];
        /**
         * Max number of rows to scan. If the table has more rows than this value, the rest of the rows are omitted.
         * If not set, or if set to 0, all rows will be scanned. Only one of rowsLimit and rowsLimitPercent can be
         * specified. Cannot be used in conjunction with TimespanConfig.
         */
        rowsLimit?: number;
        /**
         * Max percentage of rows to scan. The rest are omitted. The number of rows scanned is rounded down.
         * Must be between 0 and 100, inclusively. Both 0 and 100 means no limit. Defaults to 0. Only one of
         * rowsLimit and rowsLimitPercent can be specified. Cannot be used in conjunction with TimespanConfig.
         */
        rowsLimitPercent?: number;
        /**
         * How to sample rows if not all rows are scanned. Meaningful only when used in conjunction with either
         * rowsLimit or rowsLimitPercent. If not specified, rows are scanned in the order BigQuery reads them.
         * Default value is `TOP`.
         * Possible values are: `TOP`, `RANDOM_START`.
         */
        sampleMethod?: string;
        /**
         * Set of files to scan.
         * Structure is documented below.
         */
        tableReference: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsTableReference;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsExcludedField {
        /**
         * Name describing the field excluded from scanning.
         */
        name: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIdentifyingField {
        /**
         * Name describing the field.
         */
        name: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIncludedField {
        /**
         * Name describing the field to which scanning is limited.
         */
        name: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigBigQueryOptionsTableReference {
        /**
         * The dataset ID of the table.
         */
        datasetId: string;
        /**
         * The Google Cloud Platform project ID of the project containing the table.
         */
        projectId: string;
        /**
         * The name of the table.
         */
        tableId: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigCloudStorageOptions {
        /**
         * Max number of bytes to scan from a file. If a scanned file's size is bigger than this value
         * then the rest of the bytes are omitted.
         */
        bytesLimitPerFile?: number;
        /**
         * Max percentage of bytes to scan from a file. The rest are omitted. The number of bytes scanned is rounded down.
         * Must be between 0 and 100, inclusively. Both 0 and 100 means no limit.
         */
        bytesLimitPerFilePercent?: number;
        /**
         * Set of files to scan.
         * Structure is documented below.
         */
        fileSet: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSet;
        /**
         * List of file type groups to include in the scan. If empty, all files are scanned and available data
         * format processors are applied. In addition, the binary content of the selected files is always scanned as well.
         * Images are scanned only as binary if the specified region does not support image inspection and no fileTypes were specified.
         * Each value may be one of: `BINARY_FILE`, `TEXT_FILE`, `IMAGE`, `WORD`, `PDF`, `AVRO`, `CSV`, `TSV`, `POWERPOINT`, `EXCEL`.
         */
        fileTypes?: string[];
        /**
         * Limits the number of files to scan to this percentage of the input FileSet. Number of files scanned is rounded down.
         * Must be between 0 and 100, inclusively. Both 0 and 100 means no limit.
         */
        filesLimitPercent?: number;
        /**
         * How to sample bytes if not all bytes are scanned. Meaningful only when used in conjunction with bytesLimitPerFile.
         * If not specified, scanning would start from the top.
         * Possible values are: `TOP`, `RANDOM_START`.
         */
        sampleMethod?: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSet {
        /**
         * The regex-filtered set of files to scan.
         * Structure is documented below.
         */
        regexFileSet?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSetRegexFileSet;
        /**
         * The Cloud Storage url of the file(s) to scan, in the format `gs://<bucket>/<path>`. Trailing wildcard
         * in the path is allowed.
         * If the url ends in a trailing slash, the bucket or directory represented by the url will be scanned
         * non-recursively (content in sub-directories will not be scanned). This means that `gs://mybucket/` is
         * equivalent to `gs://mybucket/*`, and `gs://mybucket/directory/` is equivalent to `gs://mybucket/directory/*`.
         */
        url?: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSetRegexFileSet {
        /**
         * The name of a Cloud Storage bucket.
         */
        bucketName: string;
        /**
         * A list of regular expressions matching file paths to exclude. All files in the bucket that match at
         * least one of these regular expressions will be excluded from the scan.
         */
        excludeRegexes?: string[];
        /**
         * A list of regular expressions matching file paths to include. All files in the bucket
         * that match at least one of these regular expressions will be included in the set of files,
         * except for those that also match an item in excludeRegex. Leaving this field empty will
         * match all files by default (this is equivalent to including .* in the list)
         */
        includeRegexes?: string[];
    }

    export interface PreventionJobTriggerInspectJobStorageConfigDatastoreOptions {
        /**
         * A representation of a Datastore kind.
         * Structure is documented below.
         */
        kind: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigDatastoreOptionsKind;
        /**
         * Datastore partition ID. A partition ID identifies a grouping of entities. The grouping
         * is always by project and namespace, however the namespace ID may be empty.
         * Structure is documented below.
         */
        partitionId: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigDatastoreOptionsPartitionId;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigDatastoreOptionsKind {
        /**
         * The name of the Datastore kind.
         */
        name: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigDatastoreOptionsPartitionId {
        /**
         * If not empty, the ID of the namespace to which the entities belong.
         */
        namespaceId?: string;
        /**
         * The ID of the project to which the entities belong.
         */
        projectId: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigHybridOptions {
        /**
         * A short description of where the data is coming from. Will be stored once in the job. 256 max length.
         */
        description?: string;
        /**
         * To organize findings, these labels will be added to each finding.
         * Label keys must be between 1 and 63 characters long and must conform to the following regular expression: `a-z?`.
         * Label values must be between 0 and 63 characters long and must conform to the regular expression `(a-z?)?`.
         * No more than 10 labels can be associated with a given finding.
         * Examples:
         * * `"environment" : "production"`
         * * `"pipeline" : "etl"`
         */
        labels?: {[key: string]: string};
        /**
         * These are labels that each inspection request must include within their 'finding_labels' map. Request
         * may contain others, but any missing one of these will be rejected.
         * Label keys must be between 1 and 63 characters long and must conform to the following regular expression: `a-z?`.
         * No more than 10 keys can be required.
         */
        requiredFindingLabelKeys?: string[];
        /**
         * If the container is a table, additional information to make findings meaningful such as the columns that are primary keys.
         * Structure is documented below.
         */
        tableOptions?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptions;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptions {
        /**
         * The columns that are the primary keys for table objects included in ContentItem. A copy of this
         * cell's value will stored alongside alongside each finding so that the finding can be traced to
         * the specific row it came from. No more than 3 may be provided.
         * Structure is documented below.
         */
        identifyingFields?: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptionsIdentifyingField[];
    }

    export interface PreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptionsIdentifyingField {
        /**
         * Name describing the field.
         */
        name: string;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigTimespanConfig {
        /**
         * When the job is started by a JobTrigger we will automatically figure out a valid startTime to avoid
         * scanning files that have not been modified since the last time the JobTrigger executed. This will
         * be based on the time of the execution of the last run of the JobTrigger.
         */
        enableAutoPopulationOfTimespanConfig?: boolean;
        /**
         * Exclude files or rows newer than this value. If set to zero, no upper time limit is applied.
         */
        endTime?: string;
        /**
         * Exclude files or rows older than this value.
         */
        startTime?: string;
        /**
         * Information on where to inspect
         * Structure is documented below.
         */
        timestampField: outputs.dataloss.PreventionJobTriggerInspectJobStorageConfigTimespanConfigTimestampField;
    }

    export interface PreventionJobTriggerInspectJobStorageConfigTimespanConfigTimestampField {
        /**
         * Specification of the field containing the timestamp of scanned items. Used for data sources like Datastore and BigQuery.
         * For BigQuery: Required to filter out rows based on the given start and end times. If not specified and the table was
         * modified between the given start and end times, the entire table will be scanned. The valid data types of the timestamp
         * field are: INTEGER, DATE, TIMESTAMP, or DATETIME BigQuery column.
         * For Datastore. Valid data types of the timestamp field are: TIMESTAMP. Datastore entity will be scanned if the
         * timestamp property does not exist or its value is empty or invalid.
         */
        name: string;
    }

    export interface PreventionJobTriggerTrigger {
        /**
         * For use with hybrid jobs. Jobs must be manually created and finished.
         */
        manual?: outputs.dataloss.PreventionJobTriggerTriggerManual;
        /**
         * Schedule for triggered jobs
         * Structure is documented below.
         */
        schedule?: outputs.dataloss.PreventionJobTriggerTriggerSchedule;
    }

    export interface PreventionJobTriggerTriggerManual {
    }

    export interface PreventionJobTriggerTriggerSchedule {
        /**
         * With this option a job is started a regular periodic basis. For example: every day (86400 seconds).
         * A scheduled start time will be skipped if the previous execution has not ended when its scheduled time occurs.
         * This value must be set to a time duration greater than or equal to 1 day and can be no longer than 60 days.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         *
         * - - -
         */
        recurrencePeriodDuration?: string;
    }

    export interface PreventionStoredInfoTypeDictionary {
        /**
         * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
         * Structure is documented below.
         */
        cloudStoragePath?: outputs.dataloss.PreventionStoredInfoTypeDictionaryCloudStoragePath;
        /**
         * List of words or phrases to search for.
         * Structure is documented below.
         */
        wordList?: outputs.dataloss.PreventionStoredInfoTypeDictionaryWordList;
    }

    export interface PreventionStoredInfoTypeDictionaryCloudStoragePath {
        /**
         * A url representing a file or path (no wildcards) in Cloud Storage. Example: `gs://[BUCKET_NAME]/dictionary.txt`
         */
        path: string;
    }

    export interface PreventionStoredInfoTypeDictionaryWordList {
        /**
         * Words or phrases defining the dictionary. The dictionary must contain at least one
         * phrase and every phrase must contain at least 2 characters that are letters or digits.
         */
        words: string[];
    }

    export interface PreventionStoredInfoTypeLargeCustomDictionary {
        /**
         * Field in a BigQuery table where each cell represents a dictionary phrase.
         * Structure is documented below.
         */
        bigQueryField?: outputs.dataloss.PreventionStoredInfoTypeLargeCustomDictionaryBigQueryField;
        /**
         * Set of files containing newline-delimited lists of dictionary phrases.
         * Structure is documented below.
         */
        cloudStorageFileSet?: outputs.dataloss.PreventionStoredInfoTypeLargeCustomDictionaryCloudStorageFileSet;
        /**
         * Location to store dictionary artifacts in Google Cloud Storage. These files will only be accessible by project owners and the DLP API.
         * If any of these artifacts are modified, the dictionary is considered invalid and can no longer be used.
         * Structure is documented below.
         */
        outputPath: outputs.dataloss.PreventionStoredInfoTypeLargeCustomDictionaryOutputPath;
    }

    export interface PreventionStoredInfoTypeLargeCustomDictionaryBigQueryField {
        /**
         * Designated field in the BigQuery table.
         * Structure is documented below.
         */
        field: outputs.dataloss.PreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldField;
        /**
         * Field in a BigQuery table where each cell represents a dictionary phrase.
         * Structure is documented below.
         */
        table: outputs.dataloss.PreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldTable;
    }

    export interface PreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldField {
        /**
         * Name describing the field.
         */
        name: string;
    }

    export interface PreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldTable {
        /**
         * The dataset ID of the table.
         */
        datasetId: string;
        /**
         * The Google Cloud Platform project ID of the project containing the table.
         */
        projectId: string;
        /**
         * The name of the table.
         */
        tableId: string;
    }

    export interface PreventionStoredInfoTypeLargeCustomDictionaryCloudStorageFileSet {
        /**
         * The url, in the format `gs://<bucket>/<path>`. Trailing wildcard in the path is allowed.
         */
        url: string;
    }

    export interface PreventionStoredInfoTypeLargeCustomDictionaryOutputPath {
        /**
         * A url representing a file or path (no wildcards) in Cloud Storage. Example: `gs://[BUCKET_NAME]/dictionary.txt`
         */
        path: string;
    }

    export interface PreventionStoredInfoTypeRegex {
        /**
         * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
         */
        groupIndexes?: number[];
        /**
         * Pattern defining the regular expression.
         * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
         */
        pattern: string;
    }

}

export namespace dataplex {
    export interface AssetDiscoverySpec {
        /**
         * Optional. Configuration for CSV data.
         */
        csvOptions: outputs.dataplex.AssetDiscoverySpecCsvOptions;
        /**
         * Required. Whether discovery is enabled.
         */
        enabled: boolean;
        /**
         * Optional. The list of patterns to apply for selecting data to exclude during discovery. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
         */
        excludePatterns?: string[];
        /**
         * Optional. The list of patterns to apply for selecting data to include during discovery if only a subset of the data should considered. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
         */
        includePatterns?: string[];
        /**
         * Optional. Configuration for Json data.
         */
        jsonOptions: outputs.dataplex.AssetDiscoverySpecJsonOptions;
        /**
         * Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running discovery periodically. Successive discovery runs must be scheduled at least 60 minutes apart. The default value is to run discovery every 60 minutes. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
         */
        schedule?: string;
    }

    export interface AssetDiscoverySpecCsvOptions {
        /**
         * Optional. The delimiter being used to separate values. This defaults to ','.
         */
        delimiter?: string;
        /**
         * Optional. Whether to disable the inference of data type for CSV data. If true, all columns will be registered as strings.
         */
        disableTypeInference?: boolean;
        /**
         * Optional. The character encoding of the data. The default is UTF-8.
         */
        encoding?: string;
        /**
         * Optional. The number of rows to interpret as header rows that should be skipped when reading data rows.
         */
        headerRows?: number;
    }

    export interface AssetDiscoverySpecJsonOptions {
        /**
         * Optional. Whether to disable the inference of data type for Json data. If true, all columns will be registered as their primitive types (strings, number or boolean).
         */
        disableTypeInference?: boolean;
        /**
         * Optional. The character encoding of the data. The default is UTF-8.
         */
        encoding?: string;
    }

    export interface AssetDiscoveryStatus {
        lastRunDuration: string;
        lastRunTime: string;
        message: string;
        /**
         * Output only. Current state of the asset. Possible values: STATE_UNSPECIFIED, ACTIVE, CREATING, DELETING, ACTION_REQUIRED
         */
        state: string;
        stats: outputs.dataplex.AssetDiscoveryStatusStat[];
        /**
         * Output only. The time when the asset was last updated.
         */
        updateTime: string;
    }

    export interface AssetDiscoveryStatusStat {
        dataItems: number;
        dataSize: number;
        filesets: number;
        tables: number;
    }

    export interface AssetIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AssetIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AssetResourceSpec {
        /**
         * Immutable. Relative name of the cloud resource that contains the data that is being managed within a lake. For example: `projects/{project_number}/buckets/{bucket_id}` `projects/{project_number}/datasets/{dataset_id}`
         */
        name?: string;
        /**
         * Optional. Determines how read permissions are handled for each asset and their associated tables. Only available to storage buckets assets. Possible values: DIRECT, MANAGED
         */
        readAccessMode: string;
        /**
         * Required. Immutable. Type of resource. Possible values: STORAGE_BUCKET, BIGQUERY_DATASET
         *
         * - - -
         */
        type: string;
    }

    export interface AssetResourceStatus {
        message: string;
        /**
         * Output only. Current state of the asset. Possible values: STATE_UNSPECIFIED, ACTIVE, CREATING, DELETING, ACTION_REQUIRED
         */
        state: string;
        /**
         * Output only. The time when the asset was last updated.
         */
        updateTime: string;
    }

    export interface AssetSecurityStatus {
        message: string;
        /**
         * Output only. Current state of the asset. Possible values: STATE_UNSPECIFIED, ACTIVE, CREATING, DELETING, ACTION_REQUIRED
         */
        state: string;
        /**
         * Output only. The time when the asset was last updated.
         */
        updateTime: string;
    }

    export interface DatascanData {
        /**
         * The Dataplex entity that represents the data source(e.g. BigQuery table) for Datascan.
         */
        entity?: string;
        /**
         * The service-qualified full resource name of the cloud resource for a DataScan job to scan against. The field could be:
         * (Cloud Storage bucket for DataDiscoveryScan)BigQuery table of type "TABLE" for DataProfileScan/DataQualityScan.
         */
        resource?: string;
    }

    export interface DatascanDataProfileResult {
        /**
         * Profile information for the corresponding field.
         * Structure is documented below.
         */
        profiles: outputs.dataplex.DatascanDataProfileResultProfile[];
        /**
         * The count of rows scanned.
         */
        rowCount?: string;
        /**
         * (Output)
         * The data scanned for this result.
         * Structure is documented below.
         */
        scannedDatas: outputs.dataplex.DatascanDataProfileResultScannedData[];
    }

    export interface DatascanDataProfileResultProfile {
        /**
         * List of fields with structural and profile information for each field.
         * Structure is documented below.
         */
        fields?: outputs.dataplex.DatascanDataProfileResultProfileField[];
    }

    export interface DatascanDataProfileResultProfileField {
        /**
         * The mode of the field. Possible values include:
         * 1. REQUIRED, if it is a required field.
         * 2. NULLABLE, if it is an optional field.
         * 3. REPEATED, if it is a repeated field.
         */
        mode?: string;
        /**
         * A mutable name for the rule.
         * The name must contain only letters (a-z, A-Z), numbers (0-9), or hyphens (-).
         * The maximum length is 63 characters.
         * Must start with a letter.
         * Must end with a number or a letter.
         */
        name?: string;
        /**
         * Profile information for the corresponding field.
         * Structure is documented below.
         */
        profile?: outputs.dataplex.DatascanDataProfileResultProfileFieldProfile;
        /**
         * The field data type.
         */
        type?: string;
    }

    export interface DatascanDataProfileResultProfileFieldProfile {
        /**
         * Ratio of rows with distinct values against total scanned rows. Not available for complex non-groupable field type RECORD and fields with REPEATABLE mode.
         */
        distinctRatio?: number;
        /**
         * (Output)
         * Double type field information.
         * Structure is documented below.
         */
        doubleProfiles: outputs.dataplex.DatascanDataProfileResultProfileFieldProfileDoubleProfile[];
        /**
         * (Output)
         * Integer type field information.
         * Structure is documented below.
         */
        integerProfiles: outputs.dataplex.DatascanDataProfileResultProfileFieldProfileIntegerProfile[];
        /**
         * (Output)
         * Ratio of rows with null value against total scanned rows.
         */
        nullRatio: number;
        /**
         * (Output)
         * String type field information.
         * Structure is documented below.
         */
        stringProfiles: outputs.dataplex.DatascanDataProfileResultProfileFieldProfileStringProfile[];
        /**
         * The list of top N non-null values and number of times they occur in the scanned data. N is 10 or equal to the number of distinct values in the field, whichever is smaller. Not available for complex non-groupable field type RECORD and fields with REPEATABLE mode.
         * Structure is documented below.
         */
        topNValues?: outputs.dataplex.DatascanDataProfileResultProfileFieldProfileTopNValues;
    }

    export interface DatascanDataProfileResultProfileFieldProfileDoubleProfile {
        /**
         * Average of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        average?: number;
        /**
         * Maximum of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        max?: string;
        /**
         * Minimum of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        min?: string;
        /**
         * A quartile divides the number of data points into four parts, or quarters, of more-or-less equal size. Three main quartiles used are: The first quartile (Q1) splits off the lowest 25% of data from the highest 75%. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. The second quartile (Q2) is the median of a data set. So, 50% of the data lies below this point. The third quartile (Q3) splits off the highest 25% of data from the lowest 75%. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Here, the quartiles is provided as an ordered list of quartile values for the scanned data, occurring in order Q1, median, Q3.
         */
        quartiles?: string;
        /**
         * Standard deviation of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        standardDeviation?: number;
    }

    export interface DatascanDataProfileResultProfileFieldProfileIntegerProfile {
        /**
         * Average of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        average?: number;
        /**
         * Maximum of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        max?: string;
        /**
         * Minimum of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        min?: string;
        /**
         * A quartile divides the number of data points into four parts, or quarters, of more-or-less equal size. Three main quartiles used are: The first quartile (Q1) splits off the lowest 25% of data from the highest 75%. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. The second quartile (Q2) is the median of a data set. So, 50% of the data lies below this point. The third quartile (Q3) splits off the highest 25% of data from the lowest 75%. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Here, the quartiles is provided as an ordered list of quartile values for the scanned data, occurring in order Q1, median, Q3.
         */
        quartiles?: string;
        /**
         * Standard deviation of non-null values in the scanned data. NaN, if the field has a NaN.
         */
        standardDeviation?: number;
    }

    export interface DatascanDataProfileResultProfileFieldProfileStringProfile {
        /**
         * Average length of non-null values in the scanned data.
         */
        averageLength?: number;
        /**
         * Maximum length of non-null values in the scanned data.
         */
        maxLength?: string;
        /**
         * Minimum length of non-null values in the scanned data.
         */
        minLength?: string;
    }

    export interface DatascanDataProfileResultProfileFieldProfileTopNValues {
        /**
         * Count of the corresponding value in the scanned data.
         */
        count?: string;
        /**
         * String value of a top N non-null value.
         */
        value?: string;
    }

    export interface DatascanDataProfileResultScannedData {
        /**
         * The range denoted by values of an incremental field
         * Structure is documented below.
         */
        incrementalField?: outputs.dataplex.DatascanDataProfileResultScannedDataIncrementalField;
    }

    export interface DatascanDataProfileResultScannedDataIncrementalField {
        /**
         * Value that marks the end of the range.
         */
        end?: string;
        /**
         * The unnested field (of type Date or Timestamp) that contains values which monotonically increase over time. If not specified, a data scan will run for all data in the table.
         */
        field?: string;
        /**
         * Value that marks the start of the range.
         */
        start?: string;
    }

    export interface DatascanDataProfileSpec {
        /**
         * The fields to exclude from data profile.
         * If specified, the fields will be excluded from data profile, regardless of `includeFields` value.
         * Structure is documented below.
         */
        excludeFields?: outputs.dataplex.DatascanDataProfileSpecExcludeFields;
        /**
         * The fields to include in data profile.
         * If not specified, all fields at the time of profile scan job execution are included, except for ones listed in `excludeFields`.
         * Structure is documented below.
         */
        includeFields?: outputs.dataplex.DatascanDataProfileSpecIncludeFields;
        /**
         * Actions to take upon job completion.
         * Structure is documented below.
         */
        postScanActions?: outputs.dataplex.DatascanDataProfileSpecPostScanActions;
        /**
         * A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10
         */
        rowFilter?: string;
        /**
         * The percentage of the records to be selected from the dataset for DataScan.
         * Value can range between 0.0 and 100.0 with up to 3 significant decimal digits.
         * Sampling is not applied if `samplingPercent` is not specified, 0 or 100.
         */
        samplingPercent?: number;
    }

    export interface DatascanDataProfileSpecExcludeFields {
        /**
         * Expected input is a list of fully qualified names of fields as in the schema.
         * Only top-level field names for nested fields are supported.
         * For instance, if 'x' is of nested field type, listing 'x' is supported but 'x.y.z' is not supported. Here 'y' and 'y.z' are nested fields of 'x'.
         */
        fieldNames?: string[];
    }

    export interface DatascanDataProfileSpecIncludeFields {
        /**
         * Expected input is a list of fully qualified names of fields as in the schema.
         * Only top-level field names for nested fields are supported.
         * For instance, if 'x' is of nested field type, listing 'x' is supported but 'x.y.z' is not supported. Here 'y' and 'y.z' are nested fields of 'x'.
         */
        fieldNames?: string[];
    }

    export interface DatascanDataProfileSpecPostScanActions {
        /**
         * If set, results will be exported to the provided BigQuery table.
         * Structure is documented below.
         */
        bigqueryExport?: outputs.dataplex.DatascanDataProfileSpecPostScanActionsBigqueryExport;
    }

    export interface DatascanDataProfileSpecPostScanActionsBigqueryExport {
        /**
         * The BigQuery table to export DataProfileScan results to.
         * Format://bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET_ID/tables/TABLE_ID
         */
        resultsTable?: string;
    }

    export interface DatascanDataQualityResult {
        /**
         * A list of results at the dimension level.
         * Structure is documented below.
         */
        dimensions?: outputs.dataplex.DatascanDataQualityResultDimension[];
        /**
         * (Output)
         * Whether the rule passed or failed.
         */
        passed: boolean;
        /**
         * The count of rows scanned.
         */
        rowCount: string;
        /**
         * The list of rules to evaluate against a data source. At least one rule is required.
         * Structure is documented below.
         */
        rules: outputs.dataplex.DatascanDataQualityResultRule[];
        /**
         * (Output)
         * The data scanned for this result.
         * Structure is documented below.
         */
        scannedDatas: outputs.dataplex.DatascanDataQualityResultScannedData[];
    }

    export interface DatascanDataQualityResultDimension {
        /**
         * (Output)
         * Whether the rule passed or failed.
         */
        passed?: boolean;
    }

    export interface DatascanDataQualityResultRule {
        /**
         * (Output)
         * The number of rows a rule was evaluated against. This field is only valid for ColumnMap type rules.
         * Evaluated count can be configured to either
         * 1. include all rows (default) - with null rows automatically failing rule evaluation, or
         * 2. exclude null rows from the evaluatedCount, by setting ignoreNulls = true.
         */
        evaluatedCount: string;
        /**
         * (Output)
         * The query to find rows that did not pass this rule. Only applies to ColumnMap and RowCondition rules.
         */
        failingRowsQuery: string;
        /**
         * (Output)
         * The number of rows with null values in the specified column.
         */
        nullCount: string;
        /**
         * (Output)
         * The ratio of passedCount / evaluatedCount. This field is only valid for ColumnMap type rules.
         */
        passRatio: number;
        /**
         * (Output)
         * Whether the rule passed or failed.
         */
        passed: boolean;
        /**
         * (Output)
         * The number of rows which passed a rule evaluation. This field is only valid for ColumnMap type rules.
         */
        passedCount: string;
        /**
         * (Output)
         * The rule specified in the DataQualitySpec, as is.
         * Structure is documented below.
         */
        rules: outputs.dataplex.DatascanDataQualityResultRuleRule[];
    }

    export interface DatascanDataQualityResultRuleRule {
        /**
         * The unnested column which this rule is evaluated against.
         */
        column?: string;
        /**
         * The dimension a rule belongs to. Results are also aggregated at the dimension level. Supported dimensions are ["COMPLETENESS", "ACCURACY", "CONSISTENCY", "VALIDITY", "UNIQUENESS", "INTEGRITY"]
         */
        dimension?: string;
        /**
         * Rows with null values will automatically fail a rule, unless ignoreNull is true. In that case, such null rows are trivially considered passing. Only applicable to ColumnMap rules.
         */
        ignoreNull?: boolean;
        /**
         * ColumnMap rule which evaluates whether each column value is null.
         */
        nonNullExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleNonNullExpectation[];
        /**
         * ColumnMap rule which evaluates whether each column value lies between a specified range.
         * Structure is documented below.
         */
        rangeExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleRangeExpectation[];
        /**
         * ColumnMap rule which evaluates whether each column value matches a specified regex.
         * Structure is documented below.
         */
        regexExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleRegexExpectation[];
        /**
         * Table rule which evaluates whether each row passes the specified condition.
         * Structure is documented below.
         */
        rowConditionExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleRowConditionExpectation[];
        /**
         * ColumnMap rule which evaluates whether each column value is contained by a specified set.
         * Structure is documented below.
         */
        setExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleSetExpectation[];
        /**
         * ColumnAggregate rule which evaluates whether the column aggregate statistic lies between a specified range.
         * Structure is documented below.
         */
        statisticRangeExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleStatisticRangeExpectation[];
        /**
         * Table rule which evaluates whether the provided expression is true.
         * Structure is documented below.
         */
        tableConditionExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleTableConditionExpectation[];
        /**
         * The minimum ratio of passingRows / totalRows required to pass this rule, with a range of [0.0, 1.0]. 0 indicates default value (i.e. 1.0).
         */
        threshold?: number;
        /**
         * Row-level rule which evaluates whether each column value is unique.
         */
        uniquenessExpectations: outputs.dataplex.DatascanDataQualityResultRuleRuleUniquenessExpectation[];
    }

    export interface DatascanDataQualityResultRuleRuleNonNullExpectation {
    }

    export interface DatascanDataQualityResultRuleRuleRangeExpectation {
        /**
         * The maximum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.
         */
        maxValue?: string;
        /**
         * The minimum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.
         */
        minValue?: string;
        /**
         * Whether each value needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
         * Only relevant if a maxValue has been defined. Default = false.
         */
        strictMaxEnabled?: boolean;
        /**
         * Whether each value needs to be strictly greater than ('>') the minimum, or if equality is allowed.
         * Only relevant if a minValue has been defined. Default = false.
         */
        strictMinEnabled?: boolean;
    }

    export interface DatascanDataQualityResultRuleRuleRegexExpectation {
        /**
         * A regular expression the column value is expected to match.
         */
        regex?: string;
    }

    export interface DatascanDataQualityResultRuleRuleRowConditionExpectation {
        /**
         * The SQL expression.
         */
        sqlExpression?: string;
    }

    export interface DatascanDataQualityResultRuleRuleSetExpectation {
        /**
         * Expected values for the column value.
         */
        values?: string[];
    }

    export interface DatascanDataQualityResultRuleRuleStatisticRangeExpectation {
        /**
         * The maximum column statistic value allowed for a row to pass this validation.
         * At least one of minValue and maxValue need to be provided.
         */
        maxValue?: string;
        /**
         * The minimum column statistic value allowed for a row to pass this validation.
         * At least one of minValue and maxValue need to be provided.
         */
        minValue?: string;
        /**
         * column statistics.
         * Possible values are: `STATISTIC_UNDEFINED`, `MEAN`, `MIN`, `MAX`.
         */
        statistic?: string;
        /**
         * Whether column statistic needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
         * Only relevant if a maxValue has been defined. Default = false.
         */
        strictMaxEnabled?: boolean;
        /**
         * Whether column statistic needs to be strictly greater than ('>') the minimum, or if equality is allowed.
         * Only relevant if a minValue has been defined. Default = false.
         */
        strictMinEnabled?: boolean;
    }

    export interface DatascanDataQualityResultRuleRuleTableConditionExpectation {
        /**
         * The SQL expression.
         */
        sqlExpression?: string;
    }

    export interface DatascanDataQualityResultRuleRuleUniquenessExpectation {
    }

    export interface DatascanDataQualityResultScannedData {
        /**
         * The range denoted by values of an incremental field
         * Structure is documented below.
         */
        incrementalField?: outputs.dataplex.DatascanDataQualityResultScannedDataIncrementalField;
    }

    export interface DatascanDataQualityResultScannedDataIncrementalField {
        /**
         * Value that marks the end of the range.
         */
        end?: string;
        /**
         * The unnested field (of type Date or Timestamp) that contains values which monotonically increase over time. If not specified, a data scan will run for all data in the table.
         */
        field?: string;
        /**
         * Value that marks the start of the range.
         */
        start?: string;
    }

    export interface DatascanDataQualitySpec {
        /**
         * Actions to take upon job completion.
         * Structure is documented below.
         */
        postScanActions?: outputs.dataplex.DatascanDataQualitySpecPostScanActions;
        /**
         * A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10
         */
        rowFilter?: string;
        /**
         * The list of rules to evaluate against a data source. At least one rule is required.
         * Structure is documented below.
         */
        rules?: outputs.dataplex.DatascanDataQualitySpecRule[];
        /**
         * The percentage of the records to be selected from the dataset for DataScan.
         * Value can range between 0.0 and 100.0 with up to 3 significant decimal digits.
         * Sampling is not applied if `samplingPercent` is not specified, 0 or 100.
         */
        samplingPercent?: number;
    }

    export interface DatascanDataQualitySpecPostScanActions {
        /**
         * If set, results will be exported to the provided BigQuery table.
         * Structure is documented below.
         */
        bigqueryExport?: outputs.dataplex.DatascanDataQualitySpecPostScanActionsBigqueryExport;
    }

    export interface DatascanDataQualitySpecPostScanActionsBigqueryExport {
        /**
         * The BigQuery table to export DataProfileScan results to.
         * Format://bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET_ID/tables/TABLE_ID
         */
        resultsTable?: string;
    }

    export interface DatascanDataQualitySpecRule {
        /**
         * The unnested column which this rule is evaluated against.
         */
        column?: string;
        /**
         * Description of the rule.
         * The maximum length is 1,024 characters.
         */
        description?: string;
        /**
         * The dimension a rule belongs to. Results are also aggregated at the dimension level. Supported dimensions are ["COMPLETENESS", "ACCURACY", "CONSISTENCY", "VALIDITY", "UNIQUENESS", "INTEGRITY"]
         */
        dimension: string;
        /**
         * Rows with null values will automatically fail a rule, unless ignoreNull is true. In that case, such null rows are trivially considered passing. Only applicable to ColumnMap rules.
         */
        ignoreNull?: boolean;
        /**
         * A mutable name for the rule.
         * The name must contain only letters (a-z, A-Z), numbers (0-9), or hyphens (-).
         * The maximum length is 63 characters.
         * Must start with a letter.
         * Must end with a number or a letter.
         */
        name?: string;
        /**
         * ColumnMap rule which evaluates whether each column value is null.
         */
        nonNullExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleNonNullExpectation;
        /**
         * ColumnMap rule which evaluates whether each column value lies between a specified range.
         * Structure is documented below.
         */
        rangeExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleRangeExpectation;
        /**
         * ColumnMap rule which evaluates whether each column value matches a specified regex.
         * Structure is documented below.
         */
        regexExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleRegexExpectation;
        /**
         * Table rule which evaluates whether each row passes the specified condition.
         * Structure is documented below.
         */
        rowConditionExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleRowConditionExpectation;
        /**
         * ColumnMap rule which evaluates whether each column value is contained by a specified set.
         * Structure is documented below.
         */
        setExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleSetExpectation;
        /**
         * ColumnAggregate rule which evaluates whether the column aggregate statistic lies between a specified range.
         * Structure is documented below.
         */
        statisticRangeExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleStatisticRangeExpectation;
        /**
         * Table rule which evaluates whether the provided expression is true.
         * Structure is documented below.
         */
        tableConditionExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleTableConditionExpectation;
        /**
         * The minimum ratio of passingRows / totalRows required to pass this rule, with a range of [0.0, 1.0]. 0 indicates default value (i.e. 1.0).
         */
        threshold?: number;
        /**
         * Row-level rule which evaluates whether each column value is unique.
         */
        uniquenessExpectation?: outputs.dataplex.DatascanDataQualitySpecRuleUniquenessExpectation;
    }

    export interface DatascanDataQualitySpecRuleNonNullExpectation {
    }

    export interface DatascanDataQualitySpecRuleRangeExpectation {
        /**
         * The maximum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.
         */
        maxValue?: string;
        /**
         * The minimum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.
         */
        minValue?: string;
        /**
         * Whether each value needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
         * Only relevant if a maxValue has been defined. Default = false.
         */
        strictMaxEnabled?: boolean;
        /**
         * Whether each value needs to be strictly greater than ('>') the minimum, or if equality is allowed.
         * Only relevant if a minValue has been defined. Default = false.
         */
        strictMinEnabled?: boolean;
    }

    export interface DatascanDataQualitySpecRuleRegexExpectation {
        /**
         * A regular expression the column value is expected to match.
         */
        regex: string;
    }

    export interface DatascanDataQualitySpecRuleRowConditionExpectation {
        /**
         * The SQL expression.
         */
        sqlExpression: string;
    }

    export interface DatascanDataQualitySpecRuleSetExpectation {
        /**
         * Expected values for the column value.
         */
        values: string[];
    }

    export interface DatascanDataQualitySpecRuleStatisticRangeExpectation {
        /**
         * The maximum column statistic value allowed for a row to pass this validation.
         * At least one of minValue and maxValue need to be provided.
         */
        maxValue?: string;
        /**
         * The minimum column statistic value allowed for a row to pass this validation.
         * At least one of minValue and maxValue need to be provided.
         */
        minValue?: string;
        /**
         * column statistics.
         * Possible values are: `STATISTIC_UNDEFINED`, `MEAN`, `MIN`, `MAX`.
         */
        statistic: string;
        /**
         * Whether column statistic needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
         * Only relevant if a maxValue has been defined. Default = false.
         */
        strictMaxEnabled?: boolean;
        /**
         * Whether column statistic needs to be strictly greater than ('>') the minimum, or if equality is allowed.
         * Only relevant if a minValue has been defined. Default = false.
         */
        strictMinEnabled?: boolean;
    }

    export interface DatascanDataQualitySpecRuleTableConditionExpectation {
        /**
         * The SQL expression.
         */
        sqlExpression: string;
    }

    export interface DatascanDataQualitySpecRuleUniquenessExpectation {
    }

    export interface DatascanExecutionSpec {
        /**
         * The unnested field (of type Date or Timestamp) that contains values which monotonically increase over time. If not specified, a data scan will run for all data in the table.
         */
        field?: string;
        /**
         * Spec related to how often and when a scan should be triggered.
         * Structure is documented below.
         */
        trigger: outputs.dataplex.DatascanExecutionSpecTrigger;
    }

    export interface DatascanExecutionSpecTrigger {
        /**
         * The scan runs once via dataScans.run API.
         */
        onDemand?: outputs.dataplex.DatascanExecutionSpecTriggerOnDemand;
        /**
         * The scan is scheduled to run periodically.
         * Structure is documented below.
         */
        schedule?: outputs.dataplex.DatascanExecutionSpecTriggerSchedule;
    }

    export interface DatascanExecutionSpecTriggerOnDemand {
    }

    export interface DatascanExecutionSpecTriggerSchedule {
        /**
         * Cron schedule for running scans periodically. This field is required for Schedule scans.
         *
         * - - -
         */
        cron: string;
    }

    export interface DatascanExecutionStatus {
        /**
         * (Output)
         * The time when the latest DataScanJob started.
         */
        latestJobEndTime: string;
        /**
         * (Output)
         * The time when the latest DataScanJob ended.
         */
        latestJobStartTime: string;
    }

    export interface DatascanIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DatascanIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface LakeAssetStatus {
        activeAssets: number;
        securityPolicyApplyingAssets: number;
        /**
         * Output only. The time when the lake was last updated.
         */
        updateTime: string;
    }

    export interface LakeIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface LakeIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface LakeMetastore {
        /**
         * Optional. A relative reference to the Dataproc Metastore (https://cloud.google.com/dataproc-metastore/docs) service associated with the lake: `projects/{project_id}/locations/{location_id}/services/{service_id}`
         */
        service?: string;
    }

    export interface LakeMetastoreStatus {
        endpoint: string;
        message: string;
        /**
         * Output only. Current state of the lake. Possible values: STATE_UNSPECIFIED, ACTIVE, CREATING, DELETING, ACTION_REQUIRED
         */
        state: string;
        /**
         * Output only. The time when the lake was last updated.
         */
        updateTime: string;
    }

    export interface TaskExecutionSpec {
        /**
         * The arguments to pass to the task. The args can use placeholders of the format ${placeholder} as part of key/value string. These will be interpolated before passing the args to the driver. Currently supported placeholders: - ${taskId} - ${job_time} To pass positional args, set the key as TASK_ARGS. The value should be a comma-separated string of all the positional arguments. To use a delimiter other than comma, refer to https://cloud.google.com/sdk/gcloud/reference/topic/escaping. In case of other keys being present in the args, then TASK_ARGS will be passed as the last argument. An object containing a list of 'key': value pairs. Example: { 'name': 'wrench', 'mass': '1.3kg', 'count': '3' }.
         */
        args?: {[key: string]: string};
        /**
         * The Cloud KMS key to use for encryption, of the form: projects/{project_number}/locations/{locationId}/keyRings/{key-ring-name}/cryptoKeys/{key-name}.
         *
         * - - -
         */
        kmsKey?: string;
        /**
         * The maximum duration after which the job execution is expired. A duration in seconds with up to nine fractional digits, ending with 's'. Example: '3.5s'.
         */
        maxJobExecutionLifetime?: string;
        /**
         * The ID of the project in which the resource belongs.
         * If it is not provided, the provider project is used.
         */
        project?: string;
        /**
         * Service account to use to execute a task. If not provided, the default Compute service account for the project is used.
         */
        serviceAccount: string;
    }

    export interface TaskExecutionStatus {
        /**
         * (Output)
         * latest job execution.
         * Structure is documented below.
         */
        latestJobs: outputs.dataplex.TaskExecutionStatusLatestJob[];
        /**
         * (Output)
         * Last update time of the status.
         */
        updateTime: string;
    }

    export interface TaskExecutionStatusLatestJob {
        /**
         * (Output)
         * The time when the job ended.
         */
        endTime: string;
        /**
         * (Output)
         * Additional information about the current state.
         */
        message: string;
        /**
         * (Output)
         * The relative resource name of the job, of the form: projects/{project_number}/locations/{locationId}/lakes/{lakeId}/tasks/{taskId}/jobs/{jobId}.
         */
        name: string;
        /**
         * (Output)
         * The number of times the job has been retried (excluding the initial attempt).
         */
        retryCount: number;
        /**
         * (Output)
         * The underlying service running a job.
         */
        service: string;
        /**
         * (Output)
         * The full resource name for the job run under a particular service.
         */
        serviceJob: string;
        /**
         * The first run of the task will be after this time. If not specified, the task will run shortly after being submitted if ON_DEMAND and based on the schedule if RECURRING.
         */
        startTime: string;
        /**
         * (Output)
         * Execution state for the job.
         */
        state: string;
        /**
         * (Output)
         * System generated globally unique ID for the job.
         */
        uid: string;
    }

    export interface TaskIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TaskIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TaskNotebook {
        /**
         * Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * Cloud Storage URIs of files to be placed in the working directory of each executor.
         */
        fileUris?: string[];
        /**
         * Infrastructure specification for the execution.
         * Structure is documented below.
         */
        infrastructureSpec?: outputs.dataplex.TaskNotebookInfrastructureSpec;
        /**
         * Path to input notebook. This can be the Cloud Storage URI of the notebook file or the path to a Notebook Content. The execution args are accessible as environment variables (TASK_key=value).
         */
        notebook: string;
    }

    export interface TaskNotebookInfrastructureSpec {
        /**
         * Compute resources needed for a Task when using Dataproc Serverless.
         * Structure is documented below.
         */
        batch?: outputs.dataplex.TaskNotebookInfrastructureSpecBatch;
        /**
         * Container Image Runtime Configuration.
         * Structure is documented below.
         */
        containerImage?: outputs.dataplex.TaskNotebookInfrastructureSpecContainerImage;
        /**
         * Vpc network.
         * Structure is documented below.
         */
        vpcNetwork?: outputs.dataplex.TaskNotebookInfrastructureSpecVpcNetwork;
    }

    export interface TaskNotebookInfrastructureSpecBatch {
        /**
         * Total number of job executors. Executor Count should be between 2 and 100. [Default=2]
         */
        executorsCount?: number;
        /**
         * Max configurable executors. If maxExecutorsCount > executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]
         */
        maxExecutorsCount?: number;
    }

    export interface TaskNotebookInfrastructureSpecContainerImage {
        /**
         * Container image to use.
         */
        image?: string;
        /**
         * A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar
         */
        javaJars?: string[];
        /**
         * Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.
         */
        properties?: {[key: string]: string};
        /**
         * A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz
         */
        pythonPackages?: string[];
    }

    export interface TaskNotebookInfrastructureSpecVpcNetwork {
        /**
         * The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.
         */
        network?: string;
        /**
         * List of network tags to apply to the job.
         */
        networkTags?: string[];
        /**
         * The Cloud VPC sub-network in which the job is run.
         */
        subNetwork?: string;
    }

    export interface TaskSpark {
        /**
         * Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * Cloud Storage URIs of files to be placed in the working directory of each executor.
         */
        fileUris?: string[];
        /**
         * Infrastructure specification for the execution.
         * Structure is documented below.
         */
        infrastructureSpec?: outputs.dataplex.TaskSparkInfrastructureSpec;
        /**
         * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris. The execution args are passed in as a sequence of named process arguments (--key=value).
         */
        mainClass?: string;
        /**
         * The Cloud Storage URI of the jar file that contains the main class. The execution args are passed in as a sequence of named process arguments (--key=value).
         */
        mainJarFileUri?: string;
        /**
         * The Gcloud Storage URI of the main Python file to use as the driver. Must be a .py file. The execution args are passed in as a sequence of named process arguments (--key=value).
         */
        pythonScriptFile?: string;
        /**
         * The query text. The execution args are used to declare a set of script variables (set key='value';).
         */
        sqlScript?: string;
        /**
         * A reference to a query file. This can be the Cloud Storage URI of the query file or it can the path to a SqlScript Content. The execution args are used to declare a set of script variables (set key='value';).
         */
        sqlScriptFile?: string;
    }

    export interface TaskSparkInfrastructureSpec {
        /**
         * Compute resources needed for a Task when using Dataproc Serverless.
         * Structure is documented below.
         */
        batch?: outputs.dataplex.TaskSparkInfrastructureSpecBatch;
        /**
         * Container Image Runtime Configuration.
         * Structure is documented below.
         */
        containerImage?: outputs.dataplex.TaskSparkInfrastructureSpecContainerImage;
        /**
         * Vpc network.
         * Structure is documented below.
         */
        vpcNetwork?: outputs.dataplex.TaskSparkInfrastructureSpecVpcNetwork;
    }

    export interface TaskSparkInfrastructureSpecBatch {
        /**
         * Total number of job executors. Executor Count should be between 2 and 100. [Default=2]
         */
        executorsCount?: number;
        /**
         * Max configurable executors. If maxExecutorsCount > executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]
         */
        maxExecutorsCount?: number;
    }

    export interface TaskSparkInfrastructureSpecContainerImage {
        /**
         * Container image to use.
         */
        image?: string;
        /**
         * A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar
         */
        javaJars?: string[];
        /**
         * Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.
         */
        properties?: {[key: string]: string};
        /**
         * A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz
         */
        pythonPackages?: string[];
    }

    export interface TaskSparkInfrastructureSpecVpcNetwork {
        /**
         * The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.
         */
        network?: string;
        /**
         * List of network tags to apply to the job.
         */
        networkTags?: string[];
        /**
         * The Cloud VPC sub-network in which the job is run.
         */
        subNetwork?: string;
    }

    export interface TaskTriggerSpec {
        /**
         * Prevent the task from executing. This does not cancel already running tasks. It is intended to temporarily disable RECURRING tasks.
         */
        disabled?: boolean;
        /**
         * Number of retry attempts before aborting. Set to zero to never attempt to retry a failed task.
         */
        maxRetries?: number;
        /**
         * Cron schedule (https://en.wikipedia.org/wiki/Cron) for running tasks periodically. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: 'CRON_TZ=${IANA_TIME_ZONE}' or 'TZ=${IANA_TIME_ZONE}'. The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, CRON_TZ=America/New_York 1 * * * *, or TZ=America/New_York 1 * * * *. This field is required for RECURRING tasks.
         */
        schedule?: string;
        /**
         * The first run of the task will be after this time. If not specified, the task will run shortly after being submitted if ON_DEMAND and based on the schedule if RECURRING.
         */
        startTime?: string;
        /**
         * Trigger type of the user-specified Task
         * Possible values are: `ON_DEMAND`, `RECURRING`.
         */
        type: string;
    }

    export interface ZoneAssetStatus {
        activeAssets: number;
        securityPolicyApplyingAssets: number;
        /**
         * Output only. The time when the zone was last updated.
         */
        updateTime: string;
    }

    export interface ZoneDiscoverySpec {
        /**
         * Optional. Configuration for CSV data.
         */
        csvOptions: outputs.dataplex.ZoneDiscoverySpecCsvOptions;
        /**
         * Required. Whether discovery is enabled.
         */
        enabled: boolean;
        /**
         * Optional. The list of patterns to apply for selecting data to exclude during discovery. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
         */
        excludePatterns?: string[];
        /**
         * Optional. The list of patterns to apply for selecting data to include during discovery if only a subset of the data should considered. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
         */
        includePatterns?: string[];
        /**
         * Optional. Configuration for Json data.
         */
        jsonOptions: outputs.dataplex.ZoneDiscoverySpecJsonOptions;
        /**
         * Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running discovery periodically. Successive discovery runs must be scheduled at least 60 minutes apart. The default value is to run discovery every 60 minutes. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
         */
        schedule: string;
    }

    export interface ZoneDiscoverySpecCsvOptions {
        /**
         * Optional. The delimiter being used to separate values. This defaults to ','.
         */
        delimiter?: string;
        /**
         * Optional. Whether to disable the inference of data type for CSV data. If true, all columns will be registered as strings.
         */
        disableTypeInference?: boolean;
        /**
         * Optional. The character encoding of the data. The default is UTF-8.
         */
        encoding?: string;
        /**
         * Optional. The number of rows to interpret as header rows that should be skipped when reading data rows.
         */
        headerRows?: number;
    }

    export interface ZoneDiscoverySpecJsonOptions {
        /**
         * Optional. Whether to disable the inference of data type for Json data. If true, all columns will be registered as their primitive types (strings, number or boolean).
         */
        disableTypeInference?: boolean;
        /**
         * Optional. The character encoding of the data. The default is UTF-8.
         */
        encoding?: string;
    }

    export interface ZoneIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ZoneIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ZoneResourceSpec {
        /**
         * Required. Immutable. The location type of the resources that are allowed to be attached to the assets within this zone. Possible values: LOCATION_TYPE_UNSPECIFIED, SINGLE_REGION, MULTI_REGION
         *
         * - - -
         */
        locationType: string;
    }

}

export namespace dataproc {
    export interface AutoscalingPolicyBasicAlgorithm {
        /**
         * Duration between scaling events. A scaling period starts after the
         * update operation from the previous event has completed.
         * Bounds: [2m, 1d]. Default: 2m.
         */
        cooldownPeriod?: string;
        /**
         * YARN autoscaling configuration.
         * Structure is documented below.
         */
        yarnConfig: outputs.dataproc.AutoscalingPolicyBasicAlgorithmYarnConfig;
    }

    export interface AutoscalingPolicyBasicAlgorithmYarnConfig {
        /**
         * Timeout for YARN graceful decommissioning of Node Managers. Specifies the
         * duration to wait for jobs to complete before forcefully removing workers
         * (and potentially interrupting jobs). Only applicable to downscaling operations.
         * Bounds: [0s, 1d].
         */
        gracefulDecommissionTimeout: string;
        /**
         * Fraction of average pending memory in the last cooldown period for which to
         * remove workers. A scale-down factor of 1 will result in scaling down so that there
         * is no available memory remaining after the update (more aggressive scaling).
         * A scale-down factor of 0 disables removing workers, which can be beneficial for
         * autoscaling a single job.
         * Bounds: [0.0, 1.0].
         */
        scaleDownFactor: number;
        /**
         * Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
         * For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
         * recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
         * means the autoscaler will scale down on any recommended change.
         * Bounds: [0.0, 1.0]. Default: 0.0.
         */
        scaleDownMinWorkerFraction?: number;
        /**
         * Fraction of average pending memory in the last cooldown period for which to
         * add workers. A scale-up factor of 1.0 will result in scaling up so that there
         * is no pending memory remaining after the update (more aggressive scaling).
         * A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
         * (less aggressive scaling).
         * Bounds: [0.0, 1.0].
         */
        scaleUpFactor: number;
        /**
         * Minimum scale-up threshold as a fraction of total cluster size before scaling
         * occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
         * must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
         * 0 means the autoscaler will scale up on any recommended change.
         * Bounds: [0.0, 1.0]. Default: 0.0.
         */
        scaleUpMinWorkerFraction?: number;
    }

    export interface AutoscalingPolicyIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AutoscalingPolicyIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AutoscalingPolicySecondaryWorkerConfig {
        /**
         * Maximum number of instances for this group. Note that by default, clusters will not use
         * secondary workers. Required for secondary workers if the minimum secondary instances is set.
         * Bounds: [minInstances, ). Defaults to 0.
         */
        maxInstances?: number;
        /**
         * Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
         */
        minInstances?: number;
        /**
         * Weight for the instance group, which is used to determine the fraction of total workers
         * in the cluster from this instance group. For example, if primary workers have weight 2,
         * and secondary workers have weight 1, the cluster will have approximately 2 primary workers
         * for each secondary worker.
         * The cluster may not reach the specified balance if constrained by min/max bounds or other
         * autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
         * primary workers will be added. The cluster can also be out of balance when created.
         * If weight is not set on any instance group, the cluster will default to equal weight for
         * all groups: the cluster will attempt to maintain an equal number of workers in each group
         * within the configured size bounds for each group. If weight is set for one group only,
         * the cluster will default to zero weight on the unset group. For example if weight is set
         * only on primary workers, the cluster will use primary workers only and no secondary workers.
         */
        weight?: number;
    }

    export interface AutoscalingPolicyWorkerConfig {
        /**
         * Maximum number of instances for this group.
         */
        maxInstances: number;
        /**
         * Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
         */
        minInstances?: number;
        /**
         * Weight for the instance group, which is used to determine the fraction of total workers
         * in the cluster from this instance group. For example, if primary workers have weight 2,
         * and secondary workers have weight 1, the cluster will have approximately 2 primary workers
         * for each secondary worker.
         * The cluster may not reach the specified balance if constrained by min/max bounds or other
         * autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
         * primary workers will be added. The cluster can also be out of balance when created.
         * If weight is not set on any instance group, the cluster will default to equal weight for
         * all groups: the cluster will attempt to maintain an equal number of workers in each group
         * within the configured size bounds for each group. If weight is set for one group only,
         * the cluster will default to zero weight on the unset group. For example if weight is set
         * only on primary workers, the cluster will use primary workers only and no secondary workers.
         */
        weight?: number;
    }

    export interface ClusterClusterConfig {
        /**
         * The autoscaling policy config associated with the cluster.
         * Note that once set, if `autoscalingConfig` is the only field set in `clusterConfig`, it can
         * only be removed by setting `policyUri = ""`, rather than removing the whole block.
         * Structure defined below.
         */
        autoscalingConfig?: outputs.dataproc.ClusterClusterConfigAutoscalingConfig;
        bucket: string;
        /**
         * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
         * Structure defined below.
         */
        dataprocMetricConfig?: outputs.dataproc.ClusterClusterConfigDataprocMetricConfig;
        /**
         * The Customer managed encryption keys settings for the cluster.
         * Structure defined below.
         */
        encryptionConfig?: outputs.dataproc.ClusterClusterConfigEncryptionConfig;
        /**
         * The config settings for port access on the cluster.
         * Structure defined below.
         */
        endpointConfig: outputs.dataproc.ClusterClusterConfigEndpointConfig;
        /**
         * Common config settings for resources of Google Compute Engine cluster
         * instances, applicable to all instances in the cluster. Structure defined below.
         */
        gceClusterConfig: outputs.dataproc.ClusterClusterConfigGceClusterConfig;
        /**
         * Commands to execute on each node after config is completed.
         * You can specify multiple versions of these. Structure defined below.
         */
        initializationActions?: outputs.dataproc.ClusterClusterConfigInitializationAction[];
        /**
         * The settings for auto deletion cluster schedule.
         * Structure defined below.
         */
        lifecycleConfig?: outputs.dataproc.ClusterClusterConfigLifecycleConfig;
        /**
         * The Google Compute Engine config settings for the master instances
         * in a cluster. Structure defined below.
         */
        masterConfig: outputs.dataproc.ClusterClusterConfigMasterConfig;
        /**
         * The config setting for metastore service with the cluster.
         * Structure defined below.
         * - - -
         */
        metastoreConfig?: outputs.dataproc.ClusterClusterConfigMetastoreConfig;
        /**
         * The Google Compute Engine config settings for the additional
         * instances in a cluster. Structure defined below.
         * * **NOTE** : `preemptibleWorkerConfig` is
         * an alias for the api's [secondaryWorkerConfig](https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig#InstanceGroupConfig). The name doesn't necessarily mean it is preemptible and is named as
         * such for legacy/compatibility reasons.
         */
        preemptibleWorkerConfig: outputs.dataproc.ClusterClusterConfigPreemptibleWorkerConfig;
        /**
         * Security related configuration. Structure defined below.
         */
        securityConfig?: outputs.dataproc.ClusterClusterConfigSecurityConfig;
        /**
         * The config settings for software inside the cluster.
         * Structure defined below.
         */
        softwareConfig: outputs.dataproc.ClusterClusterConfigSoftwareConfig;
        /**
         * The Cloud Storage staging bucket used to stage files,
         * such as Hadoop jars, between client machines and the cluster.
         * Note: If you don't explicitly specify a `stagingBucket`
         * then GCP will auto create / assign one for you. However, you are not guaranteed
         * an auto generated bucket which is solely dedicated to your cluster; it may be shared
         * with other clusters in the same region/zone also choosing to use the auto generation
         * option.
         */
        stagingBucket?: string;
        /**
         * The Cloud Storage temp bucket used to store ephemeral cluster
         * and jobs data, such as Spark and MapReduce history files.
         * Note: If you don't explicitly specify a `tempBucket` then GCP will auto create / assign one for you.
         */
        tempBucket: string;
        /**
         * The Google Compute Engine config settings for the worker instances
         * in a cluster. Structure defined below.
         */
        workerConfig: outputs.dataproc.ClusterClusterConfigWorkerConfig;
    }

    export interface ClusterClusterConfigAutoscalingConfig {
        /**
         * The autoscaling policy used by the cluster.
         *
         * Only resource names including projectid and location (region) are valid. Examples:
         *
         * `https://www.googleapis.com/compute/v1/projects/[projectId]/locations/[dataprocRegion]/autoscalingPolicies/[policyId]`
         * `projects/[projectId]/locations/[dataprocRegion]/autoscalingPolicies/[policyId]`
         * Note that the policy must be in the same project and Cloud Dataproc region.
         *
         * - - -
         */
        policyUri: string;
    }

    export interface ClusterClusterConfigDataprocMetricConfig {
        /**
         * Metrics sources to enable.
         */
        metrics: outputs.dataproc.ClusterClusterConfigDataprocMetricConfigMetric[];
    }

    export interface ClusterClusterConfigDataprocMetricConfigMetric {
        /**
         * One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
         *
         * - - -
         */
        metricOverrides?: string[];
        /**
         * A source for the collection of Dataproc OSS metrics (see [available OSS metrics](https://cloud.google.com//dataproc/docs/guides/monitoring#available_oss_metrics)).
         */
        metricSource: string;
    }

    export interface ClusterClusterConfigEncryptionConfig {
        /**
         * The Cloud KMS key name to use for PD disk encryption for
         * all instances in the cluster.
         *
         * - - -
         */
        kmsKeyName: string;
    }

    export interface ClusterClusterConfigEndpointConfig {
        /**
         * The flag to enable http access to specific ports
         * on the cluster from external sources (aka Component Gateway). Defaults to false.
         */
        enableHttpPortAccess: boolean;
        httpPorts: {[key: string]: any};
    }

    export interface ClusterClusterConfigGceClusterConfig {
        /**
         * By default, clusters are not restricted to internal IP addresses,
         * and will have ephemeral external IP addresses assigned to each instance. If set to true, all
         * instances in the cluster will only have internal IP addresses. Note: Private Google Access
         * (also known as `privateIpGoogleAccess`) must be enabled on the subnetwork that the cluster
         * will be launched in.
         */
        internalIpOnly?: boolean;
        /**
         * A map of the Compute Engine metadata entries to add to all instances
         * (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
         */
        metadata?: {[key: string]: string};
        /**
         * The name or selfLink of the Google Compute Engine
         * network to the cluster will be part of. Conflicts with `subnetwork`.
         * If neither is specified, this defaults to the "default" network.
         */
        network: string;
        /**
         * Node Group Affinity for sole-tenant clusters.
         */
        nodeGroupAffinity: outputs.dataproc.ClusterClusterConfigGceClusterConfigNodeGroupAffinity;
        /**
         * Reservation Affinity for consuming zonal reservation.
         */
        reservationAffinity: outputs.dataproc.ClusterClusterConfigGceClusterConfigReservationAffinity;
        /**
         * The service account to be used by the Node VMs.
         * If not specified, the "default" service account is used.
         */
        serviceAccount?: string;
        /**
         * The set of Google API scopes
         * to be made available on all of the node VMs under the `serviceAccount`
         * specified. Both OAuth2 URLs and gcloud
         * short names are supported. To allow full access to all Cloud APIs, use the
         * `cloud-platform` scope. See a complete list of scopes [here](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes).
         */
        serviceAccountScopes: string[];
        /**
         * Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).
         *
         * - - -
         */
        shieldedInstanceConfig: outputs.dataproc.ClusterClusterConfigGceClusterConfigShieldedInstanceConfig;
        /**
         * The name or selfLink of the Google Compute Engine
         * subnetwork the cluster will be part of. Conflicts with `network`.
         */
        subnetwork?: string;
        /**
         * The list of instance tags applied to instances in the cluster.
         * Tags are used to identify valid sources or targets for network firewalls.
         */
        tags?: string[];
        /**
         * The GCP zone where your data is stored and used (i.e. where
         * the master and the worker nodes will be created in). If `region` is set to 'global' (default)
         * then `zone` is mandatory, otherwise GCP is able to make use of [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/auto-zone)
         * to determine this automatically for you.
         * Note: This setting additionally determines and restricts
         * which computing resources are available for use with other configs such as
         * `cluster_config.master_config.machine_type` and `cluster_config.worker_config.machine_type`.
         */
        zone: string;
    }

    export interface ClusterClusterConfigGceClusterConfigNodeGroupAffinity {
        /**
         * The URI of a sole-tenant node group resource that the cluster will be created on.
         */
        nodeGroupUri: string;
    }

    export interface ClusterClusterConfigGceClusterConfigReservationAffinity {
        /**
         * Corresponds to the type of reservation consumption.
         */
        consumeReservationType?: string;
        /**
         * Corresponds to the label key of reservation resource.
         */
        key?: string;
        /**
         * Corresponds to the label values of reservation resource.
         */
        values?: string[];
    }

    export interface ClusterClusterConfigGceClusterConfigShieldedInstanceConfig {
        /**
         * Defines whether instances have integrity monitoring enabled.
         *
         * - - -
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Defines whether instances have Secure Boot enabled.
         */
        enableSecureBoot?: boolean;
        /**
         * Defines whether instances have the [vTPM](https://cloud.google.com/security/shielded-cloud/shielded-vm#vtpm) enabled.
         */
        enableVtpm?: boolean;
    }

    export interface ClusterClusterConfigInitializationAction {
        /**
         * The script to be executed during initialization of the cluster.
         * The script must be a GCS file with a gs:// prefix.
         */
        script: string;
        /**
         * The maximum duration (in seconds) which `script` is
         * allowed to take to execute its action. GCP will default to a predetermined
         * computed value if not set (currently 300).
         *
         * - - -
         */
        timeoutSec?: number;
    }

    export interface ClusterClusterConfigLifecycleConfig {
        /**
         * The time when cluster will be auto-deleted.
         * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
         * Example: "2014-10-02T15:01:23.045123456Z".
         *
         * - - -
         */
        autoDeleteTime?: string;
        /**
         * The duration to keep the cluster alive while idling
         * (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
         */
        idleDeleteTtl?: string;
        idleStartTime: string;
    }

    export interface ClusterClusterConfigMasterConfig {
        /**
         * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
         */
        accelerators?: outputs.dataproc.ClusterClusterConfigMasterConfigAccelerator[];
        /**
         * Disk Config
         */
        diskConfig: outputs.dataproc.ClusterClusterConfigMasterConfigDiskConfig;
        /**
         * The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
         * for more information.
         */
        imageUri: string;
        instanceNames: string[];
        /**
         * The name of a Google Compute Engine machine type
         * to create for the master. If not specified, GCP will default to a predetermined
         * computed value (currently `n1-standard-4`).
         */
        machineType: string;
        /**
         * The name of a minimum generation of CPU family
         * for the master. If not specified, GCP will default to a predetermined computed value
         * for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
         * for details about which CPU families are available (and defaulted) for each zone.
         */
        minCpuPlatform: string;
        /**
         * Specifies the number of master nodes to create.
         * If not specified, GCP will default to a predetermined computed value (currently 1).
         */
        numInstances: number;
    }

    export interface ClusterClusterConfigMasterConfigAccelerator {
        /**
         * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
         *
         * > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
         * zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
         * if you are trying to use accelerators in a given zone.
         *
         * - - -
         *
         *
         * > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
         * zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
         * if you are trying to use accelerators in a given zone.
         *
         * - - -
         */
        acceleratorCount: number;
        /**
         * The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
         */
        acceleratorType: string;
    }

    export interface ClusterClusterConfigMasterConfigDiskConfig {
        /**
         * Size of the primary disk attached to each node, specified
         * in GB. The primary disk contains the boot volume and system libraries, and the
         * smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         *
         * in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         *
         * in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         */
        bootDiskSizeGb: number;
        /**
         * The disk type of the primary disk attached to each node.
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         *
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         *
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         */
        bootDiskType?: string;
        /**
         * The amount of local SSD disks that will be
         * attached to each master cluster node. Defaults to 0.
         *
         * attached to each worker cluster node. Defaults to 0.
         *
         * attached to each preemptible worker node. Defaults to 0.
         *
         * - - -
         */
        numLocalSsds: number;
    }

    export interface ClusterClusterConfigMetastoreConfig {
        /**
         * Resource name of an existing Dataproc Metastore service.
         *
         * Only resource names including projectid and location (region) are valid. Examples:
         *
         * `projects/[projectId]/locations/[dataprocRegion]/services/[service-name]`
         */
        dataprocMetastoreService: string;
    }

    export interface ClusterClusterConfigPreemptibleWorkerConfig {
        /**
         * Disk Config
         */
        diskConfig: outputs.dataproc.ClusterClusterConfigPreemptibleWorkerConfigDiskConfig;
        instanceNames: string[];
        /**
         * Specifies the number of preemptible nodes to create.
         * Defaults to 0.
         */
        numInstances: number;
        /**
         * Specifies the preemptibility of the secondary workers. The default value is `PREEMPTIBLE`
         * Accepted values are:
         * * PREEMPTIBILITY_UNSPECIFIED
         * * NON_PREEMPTIBLE
         * * PREEMPTIBLE
         */
        preemptibility?: string;
    }

    export interface ClusterClusterConfigPreemptibleWorkerConfigDiskConfig {
        /**
         * Size of the primary disk attached to each node, specified
         * in GB. The primary disk contains the boot volume and system libraries, and the
         * smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         *
         * in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         *
         * in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         */
        bootDiskSizeGb: number;
        /**
         * The disk type of the primary disk attached to each node.
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         *
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         *
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         */
        bootDiskType?: string;
        /**
         * The amount of local SSD disks that will be
         * attached to each master cluster node. Defaults to 0.
         *
         * attached to each worker cluster node. Defaults to 0.
         *
         * attached to each preemptible worker node. Defaults to 0.
         *
         * - - -
         */
        numLocalSsds: number;
    }

    export interface ClusterClusterConfigSecurityConfig {
        /**
         * Kerberos Configuration
         */
        kerberosConfig: outputs.dataproc.ClusterClusterConfigSecurityConfigKerberosConfig;
    }

    export interface ClusterClusterConfigSecurityConfigKerberosConfig {
        /**
         * The admin server (IP or hostname) for the
         * remote trusted realm in a cross realm trust relationship.
         */
        crossRealmTrustAdminServer?: string;
        /**
         * The KDC (IP or hostname) for the
         * remote trusted realm in a cross realm trust relationship.
         */
        crossRealmTrustKdc?: string;
        /**
         * The remote realm the Dataproc on-cluster KDC will
         * trust, should the user enable cross realm trust.
         */
        crossRealmTrustRealm?: string;
        /**
         * The Cloud Storage URI of a KMS
         * encrypted file containing the shared password between the on-cluster Kerberos realm
         * and the remote trusted realm, in a cross realm trust relationship.
         */
        crossRealmTrustSharedPasswordUri?: string;
        /**
         * Flag to indicate whether to Kerberize the cluster.
         */
        enableKerberos?: boolean;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing
         * the master key of the KDC database.
         */
        kdcDbKeyUri?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing
         * the password to the user provided key. For the self-signed certificate, this password
         * is generated by Dataproc.
         */
        keyPasswordUri?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing
         * the password to the user provided keystore. For the self-signed certificated, the password
         * is generated by Dataproc.
         */
        keystorePasswordUri?: string;
        /**
         * The Cloud Storage URI of the keystore file used for SSL encryption.
         * If not provided, Dataproc will provide a self-signed certificate.
         */
        keystoreUri?: string;
        /**
         * The URI of the KMS key used to encrypt various sensitive files.
         */
        kmsKeyUri: string;
        /**
         * The name of the on-cluster Kerberos realm. If not specified, the
         * uppercased domain of hostnames will be the realm.
         */
        realm?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file
         * containing the root principal password.
         */
        rootPrincipalPasswordUri: string;
        /**
         * The lifetime of the ticket granting ticket, in hours.
         */
        tgtLifetimeHours?: number;
        /**
         * The Cloud Storage URI of a KMS encrypted file
         * containing the password to the user provided truststore. For the self-signed
         * certificate, this password is generated by Dataproc.
         */
        truststorePasswordUri?: string;
        /**
         * The Cloud Storage URI of the truststore file used for
         * SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
         *
         * - - -
         */
        truststoreUri?: string;
    }

    export interface ClusterClusterConfigSoftwareConfig {
        /**
         * The Cloud Dataproc image version to use
         * for the cluster - this controls the sets of software versions
         * installed onto the nodes when you create clusters. If not specified, defaults to the
         * latest version. For a list of valid versions see
         * [Cloud Dataproc versions](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions)
         */
        imageVersion: string;
        /**
         * The set of optional components to activate on the cluster. See [Available Optional Components](https://cloud.google.com/dataproc/docs/concepts/components/overview#available_optional_components).
         *
         * - - -
         */
        optionalComponents?: string[];
        /**
         * A list of override and additional properties (key/value pairs)
         * used to modify various aspects of the common configuration files used when creating
         * a cluster. For a list of valid properties please see
         * [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties)
         */
        overrideProperties?: {[key: string]: string};
        /**
         * The properties to set on daemon config files. Property keys are specified in prefix:property format, 
         * for example spark:spark.kubernetes.container.image.
         */
        properties: {[key: string]: any};
    }

    export interface ClusterClusterConfigWorkerConfig {
        /**
         * The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
         */
        accelerators?: outputs.dataproc.ClusterClusterConfigWorkerConfigAccelerator[];
        /**
         * Disk Config
         */
        diskConfig: outputs.dataproc.ClusterClusterConfigWorkerConfigDiskConfig;
        /**
         * The URI for the image to use for this worker.  See [the guide](https://cloud.google.com/dataproc/docs/guides/dataproc-images)
         * for more information.
         */
        imageUri: string;
        instanceNames: string[];
        /**
         * The name of a Google Compute Engine machine type
         * to create for the worker nodes. If not specified, GCP will default to a predetermined
         * computed value (currently `n1-standard-4`).
         */
        machineType: string;
        /**
         * The name of a minimum generation of CPU family
         * for the master. If not specified, GCP will default to a predetermined computed value
         * for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
         * for details about which CPU families are available (and defaulted) for each zone.
         */
        minCpuPlatform: string;
        /**
         * Specifies the number of worker nodes to create.
         * If not specified, GCP will default to a predetermined computed value (currently 2).
         * There is currently a beta feature which allows you to run a
         * [Single Node Cluster](https://cloud.google.com/dataproc/docs/concepts/single-node-clusters).
         * In order to take advantage of this you need to set
         * `"dataproc:dataproc.allow.zero.workers" = "true"` in
         * `cluster_config.software_config.properties`
         */
        numInstances: number;
    }

    export interface ClusterClusterConfigWorkerConfigAccelerator {
        /**
         * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of `1`, `2`, `4`, or `8`.
         *
         * > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
         * zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
         * if you are trying to use accelerators in a given zone.
         *
         * - - -
         *
         *
         * > The Cloud Dataproc API can return unintuitive error messages when using accelerators; even when you have defined an accelerator, Auto Zone Placement does not exclusively select
         * zones that have that accelerator available. If you get a 400 error that the accelerator can't be found, this is a likely cause. Make sure you check [accelerator availability by zone](https://cloud.google.com/compute/docs/reference/rest/v1/acceleratorTypes/list)
         * if you are trying to use accelerators in a given zone.
         *
         * - - -
         */
        acceleratorCount: number;
        /**
         * The short name of the accelerator type to expose to this instance. For example, `nvidia-tesla-k80`.
         */
        acceleratorType: string;
    }

    export interface ClusterClusterConfigWorkerConfigDiskConfig {
        /**
         * Size of the primary disk attached to each node, specified
         * in GB. The primary disk contains the boot volume and system libraries, and the
         * smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         *
         * in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         *
         * in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined
         * computed value if not set (currently 500GB). Note: If SSDs are not
         * attached, it also contains the HDFS data blocks and Hadoop working directories.
         */
        bootDiskSizeGb: number;
        /**
         * The disk type of the primary disk attached to each node.
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         *
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         *
         * One of `"pd-ssd"` or `"pd-standard"`. Defaults to `"pd-standard"`.
         */
        bootDiskType?: string;
        /**
         * The amount of local SSD disks that will be
         * attached to each master cluster node. Defaults to 0.
         *
         * attached to each worker cluster node. Defaults to 0.
         *
         * attached to each preemptible worker node. Defaults to 0.
         *
         * - - -
         */
        numLocalSsds: number;
    }

    export interface ClusterIAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ClusterIAMMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ClusterVirtualClusterConfig {
        /**
         * Configuration of auxiliary services used by this cluster. 
         * Structure defined below.
         */
        auxiliaryServicesConfig: outputs.dataproc.ClusterVirtualClusterConfigAuxiliaryServicesConfig;
        /**
         * The configuration for running the Dataproc cluster on Kubernetes.
         * Structure defined below.
         * - - -
         */
        kubernetesClusterConfig: outputs.dataproc.ClusterVirtualClusterConfigKubernetesClusterConfig;
        /**
         * The Cloud Storage staging bucket used to stage files,
         * such as Hadoop jars, between client machines and the cluster.
         * Note: If you don't explicitly specify a `stagingBucket`
         * then GCP will auto create / assign one for you. However, you are not guaranteed
         * an auto generated bucket which is solely dedicated to your cluster; it may be shared
         * with other clusters in the same region/zone also choosing to use the auto generation
         * option.
         */
        stagingBucket?: string;
    }

    export interface ClusterVirtualClusterConfigAuxiliaryServicesConfig {
        /**
         * The Hive Metastore configuration for this workload.
         */
        metastoreConfig?: outputs.dataproc.ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig;
        /**
         * The Spark History Server configuration for the workload.
         */
        sparkHistoryServerConfig?: outputs.dataproc.ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig;
    }

    export interface ClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
        /**
         * Resource name of an existing Dataproc Metastore service.
         *
         * Only resource names including projectid and location (region) are valid. Examples:
         *
         * `projects/[projectId]/locations/[dataprocRegion]/services/[service-name]`
         */
        dataprocMetastoreService?: string;
    }

    export interface ClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
        /**
         * Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
         * - - -
         */
        dataprocCluster?: string;
    }

    export interface ClusterVirtualClusterConfigKubernetesClusterConfig {
        /**
         * The configuration for running the Dataproc cluster on GKE.
         */
        gkeClusterConfig: outputs.dataproc.ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig;
        /**
         * A namespace within the Kubernetes cluster to deploy into. 
         * If this namespace does not exist, it is created.
         * If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
         * If not specified, the name of the Dataproc Cluster is used.
         */
        kubernetesNamespace?: string;
        /**
         * The software configuration for this Dataproc cluster running on Kubernetes.
         */
        kubernetesSoftwareConfig: outputs.dataproc.ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig;
    }

    export interface ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
        /**
         * A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster 
         * (the GKE cluster can be zonal or regional)
         */
        gkeClusterTarget?: string;
        /**
         * GKE node pools where workloads will be scheduled. At least one node pool must be assigned the `DEFAULT` 
         * GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a `DEFAULT` GkeNodePoolTarget.
         * Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
         */
        nodePoolTargets?: outputs.dataproc.ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget[];
    }

    export interface ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
        /**
         * The target GKE node pool.
         */
        nodePool: string;
        /**
         * The configuration for the GKE node pool. 
         * If specified, Dataproc attempts to create a node pool with the specified shape.
         * If one with the same name already exists, it is verified against all specified fields.
         * If a field differs, the virtual cluster creation will fail.
         */
        nodePoolConfig: outputs.dataproc.ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig;
        /**
         * The roles associated with the GKE node pool. 
         * One of `"DEFAULT"`, `"CONTROLLER"`, `"SPARK_DRIVER"` or `"SPARK_EXECUTOR"`.
         */
        roles: string[];
    }

    export interface ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
        /**
         * The autoscaler configuration for this node pool. 
         * The autoscaler is enabled only when a valid configuration is present.
         */
        autoscaling: outputs.dataproc.ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling;
        /**
         * The node pool configuration.
         */
        config: outputs.dataproc.ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig;
        /**
         * The list of Compute Engine zones where node pool nodes associated 
         * with a Dataproc on GKE virtual cluster will be located.
         * - - -
         */
        locations: string[];
    }

    export interface ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
        /**
         * The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
         */
        maxNodeCount?: number;
        /**
         * The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
         */
        minNodeCount?: number;
    }

    export interface ClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
        /**
         * The number of local SSD disks to attach to the node, 
         * which is limited by the maximum number of disks allowable per zone.
         */
        localSsdCount?: number;
        /**
         * The name of a Compute Engine machine type.
         *
         * to create for the master. If not specified, GCP will default to a predetermined
         * computed value (currently `n1-standard-4`).
         *
         * to create for the worker nodes. If not specified, GCP will default to a predetermined
         * computed value (currently `n1-standard-4`).
         */
        machineType?: string;
        /**
         * Minimum CPU platform to be used by this instance. 
         * The instance may be scheduled on the specified or a newer CPU platform.
         * Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
         *
         * for the master. If not specified, GCP will default to a predetermined computed value
         * for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
         * for details about which CPU families are available (and defaulted) for each zone.
         *
         * for the master. If not specified, GCP will default to a predetermined computed value
         * for each zone. See [the guide](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
         * for details about which CPU families are available (and defaulted) for each zone.
         */
        minCpuPlatform?: string;
        /**
         * Whether the nodes are created as preemptible VM instances. 
         * Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
         * CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
         */
        preemptible?: boolean;
        /**
         * Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
         */
        spot?: boolean;
    }

    export interface ClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
        /**
         * The components that should be installed in this Dataproc cluster. The key must be a string from the   
         * KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
         * * **NOTE** : `component_version[SPARK]` is mandatory to set, or the creation of the cluster will fail.
         */
        componentVersion: {[key: string]: string};
        /**
         * The properties to set on daemon config files. Property keys are specified in prefix:property format, 
         * for example spark:spark.kubernetes.container.image.
         */
        properties: {[key: string]: string};
    }

    export interface JobHadoopConfig {
        /**
         * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
         */
        args?: string[];
        /**
         * HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
         */
        fileUris?: string[];
        /**
         * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
         */
        jarFileUris?: string[];
        loggingConfig: outputs.dataproc.JobHadoopConfigLoggingConfig;
        /**
         * The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jarFileUris`. Conflicts with `mainJarFileUri`
         */
        mainClass?: string;
        /**
         * The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with `mainClass`
         */
        mainJarFileUri?: string;
        /**
         * A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site` and classes in user code..
         *
         * * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        properties?: {[key: string]: string};
    }

    export interface JobHadoopConfigLoggingConfig {
        driverLogLevels: {[key: string]: string};
    }

    export interface JobHiveConfig {
        /**
         * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
         */
        continueOnFailure?: boolean;
        /**
         * HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
         */
        jarFileUris?: string[];
        /**
         * A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/hive/conf/hive-site.xml`, and classes in user code..
         */
        properties?: {[key: string]: string};
        /**
         * HCFS URI of file containing Hive script to execute as the job.
         * Conflicts with `queryList`
         */
        queryFileUri?: string;
        /**
         * The list of Hive queries or statements to execute as part of the job.
         * Conflicts with `queryFileUri`
         */
        queryLists?: string[];
        /**
         * Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
         */
        scriptVariables?: {[key: string]: string};
    }

    export interface JobIAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface JobIAMMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface JobPigConfig {
        /**
         * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
         */
        continueOnFailure?: boolean;
        /**
         * HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
         *
         * * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        jarFileUris?: string[];
        loggingConfig: outputs.dataproc.JobPigConfigLoggingConfig;
        /**
         * A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`, `/etc/pig/conf/pig.properties`, and classes in user code.
         */
        properties?: {[key: string]: string};
        /**
         * HCFS URI of file containing Hive script to execute as the job.
         * Conflicts with `queryList`
         */
        queryFileUri?: string;
        /**
         * The list of Hive queries or statements to execute as part of the job.
         * Conflicts with `queryFileUri`
         */
        queryLists?: string[];
        /**
         * Mapping of query variable names to values (equivalent to the Pig command: `name=[value]`).
         */
        scriptVariables?: {[key: string]: string};
    }

    export interface JobPigConfigLoggingConfig {
        driverLogLevels: {[key: string]: string};
    }

    export interface JobPlacement {
        clusterName: string;
        clusterUuid: string;
    }

    export interface JobPrestoConfig {
        /**
         * Presto client tags to attach to this query.
         */
        clientTags?: string[];
        /**
         * Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
         */
        continueOnFailure?: boolean;
        loggingConfig: outputs.dataproc.JobPrestoConfigLoggingConfig;
        /**
         * The format in which query output will be displayed. See the Presto documentation for supported output formats.
         *
         * * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        outputFormat?: string;
        /**
         * A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
         */
        properties?: {[key: string]: string};
        /**
         * The HCFS URI of the script that contains SQL queries.
         * Conflicts with `queryList`
         */
        queryFileUri?: string;
        /**
         * The list of SQL queries or statements to execute as part of the job.
         * Conflicts with `queryFileUri`
         */
        queryLists?: string[];
    }

    export interface JobPrestoConfigLoggingConfig {
        driverLogLevels: {[key: string]: string};
    }

    export interface JobPysparkConfig {
        /**
         * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * The arguments to pass to the driver.
         */
        args?: string[];
        /**
         * HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
         */
        fileUris?: string[];
        /**
         * HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
         */
        jarFileUris?: string[];
        loggingConfig: outputs.dataproc.JobPysparkConfigLoggingConfig;
        /**
         * The HCFS URI of the main Python file to use as the driver. Must be a .py file.
         */
        mainPythonFileUri: string;
        /**
         * A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.
         *
         * * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        properties?: {[key: string]: string};
        /**
         * HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
         */
        pythonFileUris?: string[];
    }

    export interface JobPysparkConfigLoggingConfig {
        driverLogLevels: {[key: string]: string};
    }

    export interface JobReference {
        jobId: string;
    }

    export interface JobScheduling {
        maxFailuresPerHour: number;
        maxFailuresTotal: number;
    }

    export interface JobSparkConfig {
        /**
         * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * The arguments to pass to the driver.
         */
        args?: string[];
        /**
         * HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
         */
        fileUris?: string[];
        /**
         * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
         */
        jarFileUris?: string[];
        loggingConfig: outputs.dataproc.JobSparkConfigLoggingConfig;
        /**
         * The class containing the main method of the driver. Must be in a
         * provided jar or jar that is already on the classpath. Conflicts with `mainJarFileUri`
         */
        mainClass?: string;
        /**
         * The HCFS URI of jar file containing
         * the driver jar. Conflicts with `mainClass`
         */
        mainJarFileUri?: string;
        /**
         * A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in `/etc/spark/conf/spark-defaults.conf` and classes in user code.
         *
         * * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        properties?: {[key: string]: string};
    }

    export interface JobSparkConfigLoggingConfig {
        driverLogLevels: {[key: string]: string};
    }

    export interface JobSparksqlConfig {
        /**
         * HCFS URIs of jar files to be added to the Spark CLASSPATH.
         *
         * * `logging_config.driver_log_levels`- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        jarFileUris?: string[];
        loggingConfig: outputs.dataproc.JobSparksqlConfigLoggingConfig;
        /**
         * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
         */
        properties?: {[key: string]: string};
        /**
         * The HCFS URI of the script that contains SQL queries.
         * Conflicts with `queryList`
         */
        queryFileUri?: string;
        /**
         * The list of SQL queries or statements to execute as part of the job.
         * Conflicts with `queryFileUri`
         */
        queryLists?: string[];
        /**
         * Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
         */
        scriptVariables?: {[key: string]: string};
    }

    export interface JobSparksqlConfigLoggingConfig {
        driverLogLevels: {[key: string]: string};
    }

    export interface JobStatus {
        details: string;
        state: string;
        stateStartTime: string;
        substate: string;
    }

    export interface MetastoreFederationBackendMetastore {
        /**
         * The type of the backend metastore.
         * Possible values are: `METASTORE_TYPE_UNSPECIFIED`, `DATAPROC_METASTORE`, `BIGQUERY`.
         *
         * - - -
         */
        metastoreType: string;
        /**
         * The relative resource name of the metastore that is being federated. The formats of the relative resource names for the currently supported metastores are listed below: Dataplex: projects/{projectId}/locations/{location}/lakes/{lake_id} BigQuery: projects/{projectId} Dataproc Metastore: projects/{projectId}/locations/{location}/services/{serviceId}
         */
        name: string;
        /**
         * The identifier for this object. Format specified above.
         */
        rank: string;
    }

    export interface MetastoreFederationIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface MetastoreFederationIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface MetastoreServiceEncryptionConfig {
        /**
         * The fully qualified customer provided Cloud KMS key name to use for customer data encryption.
         * Use the following format: `projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)`
         */
        kmsKey: string;
    }

    export interface MetastoreServiceHiveMetastoreConfig {
        auxiliaryVersions?: outputs.dataproc.MetastoreServiceHiveMetastoreConfigAuxiliaryVersion[];
        /**
         * A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml).
         * The mappings override system defaults (some keys cannot be overridden)
         */
        configOverrides: {[key: string]: string};
        endpointProtocol?: string;
        /**
         * Information used to configure the Hive metastore service as a service principal in a Kerberos realm.
         * Structure is documented below.
         */
        kerberosConfig?: outputs.dataproc.MetastoreServiceHiveMetastoreConfigKerberosConfig;
        /**
         * The Hive metastore schema version.
         */
        version: string;
    }

    export interface MetastoreServiceHiveMetastoreConfigAuxiliaryVersion {
        /**
         * A mapping of Hive metastore configuration key-value pairs to apply to the auxiliary Hive metastore (configured in hive-site.xml) in addition to the primary version's overrides.
         * If keys are present in both the auxiliary version's overrides and the primary version's overrides, the value from the auxiliary version's overrides takes precedence.
         */
        configOverrides?: {[key: string]: string};
        /**
         * The identifier for this object. Format specified above.
         */
        key: string;
        /**
         * The Hive metastore version of the auxiliary service. It must be less than the primary Hive metastore service's version.
         */
        version: string;
    }

    export interface MetastoreServiceHiveMetastoreConfigKerberosConfig {
        /**
         * A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC).
         * Structure is documented below.
         */
        keytab: outputs.dataproc.MetastoreServiceHiveMetastoreConfigKerberosConfigKeytab;
        /**
         * A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
         */
        krb5ConfigGcsUri: string;
        /**
         * A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
         */
        principal: string;
    }

    export interface MetastoreServiceHiveMetastoreConfigKerberosConfigKeytab {
        /**
         * The relative resource name of a Secret Manager secret version, in the following form:
         * "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
         */
        cloudSecret: string;
    }

    export interface MetastoreServiceIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface MetastoreServiceIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface MetastoreServiceMaintenanceWindow {
        /**
         * The day of week, when the window starts.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeek: string;
        /**
         * The hour of day (0-23) when the window starts.
         */
        hourOfDay: number;
    }

    export interface MetastoreServiceMetadataIntegration {
        /**
         * The integration config for the Data Catalog service.
         * Structure is documented below.
         */
        dataCatalogConfig: outputs.dataproc.MetastoreServiceMetadataIntegrationDataCatalogConfig;
    }

    export interface MetastoreServiceMetadataIntegrationDataCatalogConfig {
        /**
         * Defines whether the metastore metadata should be synced to Data Catalog. The default value is to disable syncing metastore metadata to Data Catalog.
         */
        enabled: boolean;
    }

    export interface MetastoreServiceNetworkConfig {
        /**
         * The consumer-side network configuration for the Dataproc Metastore instance.
         * Structure is documented below.
         */
        consumers: outputs.dataproc.MetastoreServiceNetworkConfigConsumer[];
    }

    export interface MetastoreServiceNetworkConfigConsumer {
        /**
         * (Output)
         * The URI of the endpoint used to access the metastore service.
         */
        endpointUri: string;
        /**
         * The subnetwork of the customer project from which an IP address is reserved and used as the Dataproc Metastore service's endpoint.
         * It is accessible to hosts in the subnet and to all hosts in a subnet in the same region and same network.
         * There must be at least one IP address available in the subnet's primary range. The subnet is specified in the following form:
         * `projects/{projectNumber}/regions/{region_id}/subnetworks/{subnetwork_id}
         */
        subnetwork: string;
    }

    export interface MetastoreServiceScalingConfig {
        /**
         * Metastore instance sizes.
         * Possible values are: `EXTRA_SMALL`, `SMALL`, `MEDIUM`, `LARGE`, `EXTRA_LARGE`.
         */
        instanceSize?: string;
        /**
         * Scaling factor, in increments of 0.1 for values less than 1.0, and increments of 1.0 for values greater than 1.0.
         */
        scalingFactor?: number;
    }

    export interface MetastoreServiceTelemetryConfig {
        /**
         * The output format of the Dataproc Metastore service's logs.
         * Default value is `JSON`.
         * Possible values are: `LEGACY`, `JSON`.
         */
        logFormat?: string;
    }

    export interface WorkflowTemplateJob {
        /**
         * Job is a Hadoop job.
         */
        hadoopJob?: outputs.dataproc.WorkflowTemplateJobHadoopJob;
        /**
         * Job is a Hive job.
         */
        hiveJob?: outputs.dataproc.WorkflowTemplateJobHiveJob;
        /**
         * The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
         */
        labels?: {[key: string]: string};
        /**
         * Job is a Pig job.
         */
        pigJob?: outputs.dataproc.WorkflowTemplateJobPigJob;
        /**
         * The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
         */
        prerequisiteStepIds?: string[];
        /**
         * Job is a Presto job.
         */
        prestoJob?: outputs.dataproc.WorkflowTemplateJobPrestoJob;
        /**
         * Job is a PySpark job.
         */
        pysparkJob?: outputs.dataproc.WorkflowTemplateJobPysparkJob;
        /**
         * Job scheduling configuration.
         */
        scheduling?: outputs.dataproc.WorkflowTemplateJobScheduling;
        /**
         * Job is a Spark job.
         */
        sparkJob?: outputs.dataproc.WorkflowTemplateJobSparkJob;
        /**
         * Job is a SparkR job.
         */
        sparkRJob?: outputs.dataproc.WorkflowTemplateJobSparkRJob;
        /**
         * Job is a SparkSql job.
         */
        sparkSqlJob?: outputs.dataproc.WorkflowTemplateJobSparkSqlJob;
        /**
         * Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job `goog-dataproc-workflow-step-id` label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
         */
        stepId: string;
    }

    export interface WorkflowTemplateJobHadoopJob {
        /**
         * HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
         */
        archiveUris?: string[];
        /**
         * The arguments to pass to the driver. Do not include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
         */
        args?: string[];
        /**
         * HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
         */
        fileUris?: string[];
        /**
         * Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
         */
        jarFileUris?: string[];
        /**
         * The runtime log config for job execution.
         */
        loggingConfig?: outputs.dataproc.WorkflowTemplateJobHadoopJobLoggingConfig;
        /**
         * The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jarFileUris`.
         */
        mainClass?: string;
        /**
         * The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
         */
        mainJarFileUri?: string;
        /**
         * A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
         */
        properties?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobHadoopJobLoggingConfig {
        /**
         * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        driverLogLevels?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobHiveJob {
        /**
         * Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
         */
        continueOnFailure?: boolean;
        /**
         * HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
         */
        jarFileUris?: string[];
        /**
         * A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
         */
        properties?: {[key: string]: string};
        /**
         * The HCFS URI of the script that contains Hive queries.
         */
        queryFileUri?: string;
        /**
         * A list of queries.
         */
        queryList?: outputs.dataproc.WorkflowTemplateJobHiveJobQueryList;
        /**
         * Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
         */
        scriptVariables?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobHiveJobQueryList {
        /**
         * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
         */
        queries: string[];
    }

    export interface WorkflowTemplateJobPigJob {
        /**
         * Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
         */
        continueOnFailure?: boolean;
        /**
         * HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
         */
        jarFileUris?: string[];
        /**
         * The runtime log config for job execution.
         */
        loggingConfig?: outputs.dataproc.WorkflowTemplateJobPigJobLoggingConfig;
        /**
         * A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
         */
        properties?: {[key: string]: string};
        /**
         * The HCFS URI of the script that contains the Pig queries.
         */
        queryFileUri?: string;
        /**
         * A list of queries.
         */
        queryList?: outputs.dataproc.WorkflowTemplateJobPigJobQueryList;
        /**
         * Mapping of query variable names to values (equivalent to the Pig command: `name=`).
         */
        scriptVariables?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobPigJobLoggingConfig {
        /**
         * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        driverLogLevels?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobPigJobQueryList {
        /**
         * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
         */
        queries: string[];
    }

    export interface WorkflowTemplateJobPrestoJob {
        /**
         * Presto client tags to attach to this query
         */
        clientTags?: string[];
        /**
         * Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
         */
        continueOnFailure?: boolean;
        /**
         * The runtime log config for job execution.
         */
        loggingConfig?: outputs.dataproc.WorkflowTemplateJobPrestoJobLoggingConfig;
        /**
         * The format in which query output will be displayed. See the Presto documentation for supported output formats
         */
        outputFormat?: string;
        /**
         * A mapping of property names to values. Used to set Presto (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
         */
        properties?: {[key: string]: string};
        /**
         * The HCFS URI of the script that contains SQL queries.
         */
        queryFileUri?: string;
        /**
         * A list of queries.
         */
        queryList?: outputs.dataproc.WorkflowTemplateJobPrestoJobQueryList;
    }

    export interface WorkflowTemplateJobPrestoJobLoggingConfig {
        /**
         * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        driverLogLevels?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobPrestoJobQueryList {
        /**
         * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
         */
        queries: string[];
    }

    export interface WorkflowTemplateJobPysparkJob {
        /**
         * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
         */
        args?: string[];
        /**
         * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
         */
        fileUris?: string[];
        /**
         * HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
         */
        jarFileUris?: string[];
        /**
         * The runtime log config for job execution.
         */
        loggingConfig?: outputs.dataproc.WorkflowTemplateJobPysparkJobLoggingConfig;
        /**
         * Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
         */
        mainPythonFileUri: string;
        /**
         * A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
         */
        properties?: {[key: string]: string};
        /**
         * HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
         */
        pythonFileUris?: string[];
    }

    export interface WorkflowTemplateJobPysparkJobLoggingConfig {
        /**
         * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        driverLogLevels?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobScheduling {
        /**
         * Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
         */
        maxFailuresPerHour?: number;
        /**
         * Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
         */
        maxFailuresTotal?: number;
    }

    export interface WorkflowTemplateJobSparkJob {
        /**
         * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
         */
        args?: string[];
        /**
         * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
         */
        fileUris?: string[];
        /**
         * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
         */
        jarFileUris?: string[];
        /**
         * The runtime log config for job execution.
         */
        loggingConfig?: outputs.dataproc.WorkflowTemplateJobSparkJobLoggingConfig;
        /**
         * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in `jarFileUris`.
         */
        mainClass?: string;
        /**
         * The HCFS URI of the jar file that contains the main class.
         */
        mainJarFileUri?: string;
        /**
         * A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
         */
        properties?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobSparkJobLoggingConfig {
        /**
         * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        driverLogLevels?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobSparkRJob {
        /**
         * HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
         */
        archiveUris?: string[];
        /**
         * The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
         */
        args?: string[];
        /**
         * HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
         */
        fileUris?: string[];
        /**
         * The runtime log config for job execution.
         */
        loggingConfig?: outputs.dataproc.WorkflowTemplateJobSparkRJobLoggingConfig;
        /**
         * Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
         */
        mainRFileUri: string;
        /**
         * A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
         */
        properties?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobSparkRJobLoggingConfig {
        /**
         * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        driverLogLevels?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobSparkSqlJob {
        /**
         * HCFS URIs of jar files to be added to the Spark CLASSPATH.
         */
        jarFileUris?: string[];
        /**
         * The runtime log config for job execution.
         */
        loggingConfig?: outputs.dataproc.WorkflowTemplateJobSparkSqlJobLoggingConfig;
        /**
         * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
         */
        properties?: {[key: string]: string};
        /**
         * The HCFS URI of the script that contains SQL queries.
         */
        queryFileUri?: string;
        /**
         * A list of queries.
         */
        queryList?: outputs.dataproc.WorkflowTemplateJobSparkSqlJobQueryList;
        /**
         * Mapping of query variable names to values (equivalent to the Spark SQL command: SET `name="value";`).
         */
        scriptVariables?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobSparkSqlJobLoggingConfig {
        /**
         * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
         */
        driverLogLevels?: {[key: string]: string};
    }

    export interface WorkflowTemplateJobSparkSqlJobQueryList {
        /**
         * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
         */
        queries: string[];
    }

    export interface WorkflowTemplateParameter {
        /**
         * Brief description of the parameter. Must not exceed 1024 characters.
         */
        description?: string;
        /**
         * Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
         */
        fields: string[];
        /**
         * Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
         */
        name: string;
        /**
         * Validation rules to be applied to this parameter's value.
         */
        validation?: outputs.dataproc.WorkflowTemplateParameterValidation;
    }

    export interface WorkflowTemplateParameterValidation {
        /**
         * Validation based on regular expressions.
         */
        regex?: outputs.dataproc.WorkflowTemplateParameterValidationRegex;
        /**
         * Validation based on a list of allowed values.
         */
        values?: outputs.dataproc.WorkflowTemplateParameterValidationValues;
    }

    export interface WorkflowTemplateParameterValidationRegex {
        /**
         * Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
         */
        regexes: string[];
    }

    export interface WorkflowTemplateParameterValidationValues {
        /**
         * Required. List of allowed values for the parameter.
         */
        values: string[];
    }

    export interface WorkflowTemplatePlacement {
        /**
         * A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
         */
        clusterSelector?: outputs.dataproc.WorkflowTemplatePlacementClusterSelector;
        /**
         * A cluster that is managed by the workflow.
         */
        managedCluster?: outputs.dataproc.WorkflowTemplatePlacementManagedCluster;
    }

    export interface WorkflowTemplatePlacementClusterSelector {
        /**
         * Required. The cluster labels. Cluster must have all labels to match.
         */
        clusterLabels: {[key: string]: string};
        /**
         * The zone where workflow process executes. This parameter does not affect the selection of the cluster. If unspecified, the zone of the first cluster matching the selector is used.
         */
        zone: string;
    }

    export interface WorkflowTemplatePlacementManagedCluster {
        /**
         * Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
         */
        clusterName: string;
        /**
         * Required. The cluster configuration.
         */
        config: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfig;
        /**
         * The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
         */
        labels?: {[key: string]: string};
    }

    export interface WorkflowTemplatePlacementManagedClusterConfig {
        /**
         * Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
         */
        autoscalingConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfig;
        /**
         * Encryption settings for the cluster.
         */
        encryptionConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigEncryptionConfig;
        /**
         * Port/endpoint configuration for this cluster
         */
        endpointConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigEndpointConfig;
        /**
         * The shared Compute Engine config settings for all instances in a cluster.
         */
        gceClusterConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigGceClusterConfig;
        /**
         * The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as `gceClusterConfig`, `masterConfig`, `workerConfig`, `secondaryWorkerConfig`, and `autoscalingConfig`.
         */
        gkeClusterConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfig;
        /**
         * Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's `role` metadata to run an executable on a master or worker node, as shown below using `curl` (you can also use `wget`): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
         */
        initializationActions?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigInitializationAction[];
        /**
         * Lifecycle setting for the cluster.
         */
        lifecycleConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigLifecycleConfig;
        /**
         * The Compute Engine config settings for additional worker instances in a cluster.
         */
        masterConfig: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigMasterConfig;
        /**
         * Metastore configuration.
         */
        metastoreConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigMetastoreConfig;
        /**
         * The Compute Engine config settings for additional worker instances in a cluster.
         */
        secondaryWorkerConfig: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfig;
        /**
         * Security settings for the cluster.
         */
        securityConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigSecurityConfig;
        /**
         * The config settings for software inside the cluster.
         */
        softwareConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigSoftwareConfig;
        /**
         * A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
         */
        stagingBucket?: string;
        /**
         * A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
         */
        tempBucket?: string;
        /**
         * The Compute Engine config settings for additional worker instances in a cluster.
         *
         * - - -
         */
        workerConfig: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigWorkerConfig;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigAutoscalingConfig {
        /**
         * The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` Note that the policy must be in the same project and Dataproc region.
         */
        policy?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigEncryptionConfig {
        /**
         * The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
         */
        gcePdKmsKeyName?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigEndpointConfig {
        /**
         * If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
         */
        enableHttpPortAccess?: boolean;
        /**
         * Output only. The map of port descriptions to URLs. Will only be populated if enableHttpPortAccess is true.
         */
        httpPorts: {[key: string]: string};
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigGceClusterConfig {
        /**
         * If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This `internalIpOnly` restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
         */
        internalIpOnly: boolean;
        /**
         * The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
         */
        metadata?: {[key: string]: string};
        /**
         * The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither `networkUri` nor `subnetworkUri` is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default` * `default`
         */
        network?: string;
        /**
         * Node Group Affinity for sole-tenant clusters.
         */
        nodeGroupAffinity?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity;
        /**
         * The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
         */
        privateIpv6GoogleAccess?: string;
        /**
         * Reservation Affinity for consuming Zonal reservation.
         */
        reservationAffinity?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinity;
        /**
         * The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
         */
        serviceAccount?: string;
        /**
         * The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
         */
        serviceAccountScopes?: string[];
        /**
         * Shielded Instance Config for clusters using [Compute Engine Shielded VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm). Structure defined below.
         */
        shieldedInstanceConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig;
        /**
         * The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0` * `sub0`
         */
        subnetwork?: string;
        /**
         * The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
         */
        tags?: string[];
        /**
         * The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/` * `us-central1-f`
         */
        zone: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
        /**
         * Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1` * `node-group-1`
         */
        nodeGroup: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinity {
        /**
         * Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
         */
        consumeReservationType?: string;
        /**
         * Corresponds to the label key of reservation resource.
         */
        key?: string;
        /**
         * Corresponds to the label values of reservation resource.
         */
        values?: string[];
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
        /**
         * Defines whether instances have [Integrity Monitoring](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#integrity-monitoring) enabled.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Defines whether instances have [Secure Boot](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot) enabled.
         */
        enableSecureBoot?: boolean;
        /**
         * Defines whether instances have the [vTPM](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#vtpm) enabled.
         */
        enableVtpm?: boolean;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfig {
        /**
         * A target for the deployment.
         */
        namespacedGkeDeploymentTarget?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTarget;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigGkeClusterConfigNamespacedGkeDeploymentTarget {
        /**
         * A namespace within the GKE cluster to deploy into.
         */
        clusterNamespace?: string;
        /**
         * The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
         */
        targetGkeCluster?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigInitializationAction {
        /**
         * Required. Cloud Storage URI of executable file.
         */
        executableFile?: string;
        /**
         * Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
         */
        executionTimeout?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigLifecycleConfig {
        /**
         * The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
         */
        autoDeleteTime?: string;
        /**
         * The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
         */
        autoDeleteTtl?: string;
        /**
         * The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
         */
        idleDeleteTtl?: string;
        /**
         * Output only. The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
         */
        idleStartTime: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigMasterConfig {
        /**
         * The Compute Engine accelerator configuration for these instances.
         */
        accelerators: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigMasterConfigAccelerator[];
        /**
         * Disk option config settings.
         */
        diskConfig: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfig;
        /**
         * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
         */
        image?: string;
        /**
         * Output only. The list of instance names. Dataproc derives the names from `clusterName`, `numInstances`, and the instance group.
         */
        instanceNames: string[];
        /**
         * Output only. Specifies that this instance group contains preemptible instances.
         */
        isPreemptible: boolean;
        /**
         * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
         */
        machineType?: string;
        /**
         * Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
         */
        managedGroupConfigs: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfig[];
        /**
         * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
         */
        minCpuPlatform: string;
        /**
         * The number of VM instances in the instance group. For master instance groups, must be set to 1.
         */
        numInstances?: number;
        /**
         * Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
         */
        preemptibility?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigMasterConfigAccelerator {
        /**
         * The number of the accelerator cards of this type exposed to this instance.
         */
        acceleratorCount?: number;
        /**
         * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
         */
        acceleratorType?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfig {
        /**
         * Size in GB of the boot disk (default is 500GB).
         */
        bootDiskSizeGb?: number;
        /**
         * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
         */
        bootDiskType?: string;
        /**
         * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
         */
        numLocalSsds: number;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfig {
        instanceGroupManagerName: string;
        instanceTemplateName: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigMetastoreConfig {
        /**
         * Required. Resource name of an existing Dataproc Metastore service. Example: * `projects/`
         */
        dataprocMetastoreService: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfig {
        /**
         * The Compute Engine accelerator configuration for these instances.
         */
        accelerators: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAccelerator[];
        /**
         * Disk option config settings.
         */
        diskConfig: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig;
        /**
         * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
         */
        image?: string;
        /**
         * Output only. The list of instance names. Dataproc derives the names from `clusterName`, `numInstances`, and the instance group.
         */
        instanceNames: string[];
        /**
         * Output only. Specifies that this instance group contains preemptible instances.
         */
        isPreemptible: boolean;
        /**
         * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
         */
        machineType?: string;
        /**
         * Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
         */
        managedGroupConfigs: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfig[];
        /**
         * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
         */
        minCpuPlatform: string;
        /**
         * The number of VM instances in the instance group. For master instance groups, must be set to 1.
         */
        numInstances?: number;
        /**
         * Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
         */
        preemptibility?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAccelerator {
        /**
         * The number of the accelerator cards of this type exposed to this instance.
         */
        acceleratorCount?: number;
        /**
         * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
         */
        acceleratorType?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
        /**
         * Size in GB of the boot disk (default is 500GB).
         */
        bootDiskSizeGb?: number;
        /**
         * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
         */
        bootDiskType?: string;
        /**
         * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
         */
        numLocalSsds: number;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfig {
        instanceGroupManagerName: string;
        instanceTemplateName: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigSecurityConfig {
        /**
         * Kerberos related configuration.
         */
        kerberosConfig?: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfig;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfig {
        /**
         * The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
         */
        crossRealmTrustAdminServer?: string;
        /**
         * The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
         */
        crossRealmTrustKdc?: string;
        /**
         * The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
         */
        crossRealmTrustRealm?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
         */
        crossRealmTrustSharedPassword?: string;
        /**
         * Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
         */
        enableKerberos?: boolean;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
         */
        kdcDbKey?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
         */
        keyPassword?: string;
        /**
         * The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
         */
        keystore?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
         */
        keystorePassword?: string;
        /**
         * The uri of the KMS key used to encrypt various sensitive files.
         */
        kmsKey?: string;
        /**
         * The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
         */
        realm?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing the root principal password.
         */
        rootPrincipalPassword?: string;
        /**
         * The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
         */
        tgtLifetimeHours?: number;
        /**
         * The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
         */
        truststore?: string;
        /**
         * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
         */
        truststorePassword?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigSoftwareConfig {
        /**
         * The version of software inside the cluster. It must be one of the supported [Dataproc Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the ["preview" version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
         */
        imageVersion?: string;
        /**
         * The set of components to activate on the cluster.
         */
        optionalComponents?: string[];
        /**
         * The properties to set on daemon config files.
         *
         * Property keys are specified in `prefix:property` format, for example `core:hadoop.tmp.dir`. The following are supported prefixes and their mappings:
         *
         * * capacity-scheduler: `capacity-scheduler.xml`
         * * core: `core-site.xml`
         * * distcp: `distcp-default.xml`
         * * hdfs: `hdfs-site.xml`
         * * hive: `hive-site.xml`
         * * mapred: `mapred-site.xml`
         * * pig: `pig.properties`
         * * spark: `spark-defaults.conf`
         * * yarn: `yarn-site.xml`
         *
         *
         * For more information, see [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
         */
        properties?: {[key: string]: string};
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigWorkerConfig {
        /**
         * The Compute Engine accelerator configuration for these instances.
         */
        accelerators: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAccelerator[];
        /**
         * Disk option config settings.
         */
        diskConfig: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfig;
        /**
         * The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
         */
        image?: string;
        /**
         * Output only. The list of instance names. Dataproc derives the names from `clusterName`, `numInstances`, and the instance group.
         */
        instanceNames: string[];
        /**
         * Output only. Specifies that this instance group contains preemptible instances.
         */
        isPreemptible: boolean;
        /**
         * The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
         */
        machineType?: string;
        /**
         * Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
         */
        managedGroupConfigs: outputs.dataproc.WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfig[];
        /**
         * Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
         */
        minCpuPlatform: string;
        /**
         * The number of VM instances in the instance group. For master instance groups, must be set to 1.
         */
        numInstances?: number;
        /**
         * Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
         */
        preemptibility?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigWorkerConfigAccelerator {
        /**
         * The number of the accelerator cards of this type exposed to this instance.
         */
        acceleratorCount?: number;
        /**
         * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
         */
        acceleratorType?: string;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfig {
        /**
         * Size in GB of the boot disk (default is 500GB).
         */
        bootDiskSizeGb?: number;
        /**
         * Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
         */
        bootDiskType?: string;
        /**
         * Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
         */
        numLocalSsds: number;
    }

    export interface WorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfig {
        instanceGroupManagerName: string;
        instanceTemplateName: string;
    }

}

export namespace datastore {
    export interface DataStoreIndexProperty {
        /**
         * The direction the index should optimize for sorting.
         * Possible values are: `ASCENDING`, `DESCENDING`.
         */
        direction: string;
        /**
         * The property name to index.
         */
        name: string;
    }

}

export namespace datastream {
    export interface ConnectionProfileBigqueryProfile {
    }

    export interface ConnectionProfileForwardSshConnectivity {
        /**
         * Hostname for the SSH tunnel.
         */
        hostname: string;
        /**
         * SSH password.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password?: string;
        /**
         * Port for the SSH tunnel.
         */
        port?: number;
        /**
         * SSH private key.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        privateKey?: string;
        /**
         * Username for the SSH tunnel.
         */
        username: string;
    }

    export interface ConnectionProfileGcsProfile {
        /**
         * The Cloud Storage bucket name.
         */
        bucket: string;
        /**
         * The root path inside the Cloud Storage bucket.
         */
        rootPath?: string;
    }

    export interface ConnectionProfileMysqlProfile {
        /**
         * Hostname for the MySQL connection.
         */
        hostname: string;
        /**
         * Password for the MySQL connection.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * Port for the MySQL connection.
         */
        port?: number;
        /**
         * SSL configuration for the MySQL connection.
         * Structure is documented below.
         */
        sslConfig?: outputs.datastream.ConnectionProfileMysqlProfileSslConfig;
        /**
         * Username for the MySQL connection.
         */
        username: string;
    }

    export interface ConnectionProfileMysqlProfileSslConfig {
        /**
         * PEM-encoded certificate of the CA that signed the source database
         * server's certificate.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        caCertificate?: string;
        /**
         * (Output)
         * Indicates whether the clientKey field is set.
         */
        caCertificateSet: boolean;
        /**
         * PEM-encoded certificate that will be used by the replica to
         * authenticate against the source database server. If this field
         * is used then the 'clientKey' and the 'caCertificate' fields are
         * mandatory.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        clientCertificate?: string;
        /**
         * (Output)
         * Indicates whether the clientCertificate field is set.
         */
        clientCertificateSet: boolean;
        /**
         * PEM-encoded private key associated with the Client Certificate.
         * If this field is used then the 'client_certificate' and the
         * 'ca_certificate' fields are mandatory.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        clientKey?: string;
        /**
         * (Output)
         * Indicates whether the clientKey field is set.
         */
        clientKeySet: boolean;
    }

    export interface ConnectionProfileOracleProfile {
        /**
         * Connection string attributes
         */
        connectionAttributes?: {[key: string]: string};
        /**
         * Database for the Oracle connection.
         */
        databaseService: string;
        /**
         * Hostname for the Oracle connection.
         */
        hostname: string;
        /**
         * Password for the Oracle connection.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * Port for the Oracle connection.
         */
        port?: number;
        /**
         * Username for the Oracle connection.
         */
        username: string;
    }

    export interface ConnectionProfilePostgresqlProfile {
        /**
         * Database for the PostgreSQL connection.
         */
        database: string;
        /**
         * Hostname for the PostgreSQL connection.
         */
        hostname: string;
        /**
         * Password for the PostgreSQL connection.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * Port for the PostgreSQL connection.
         */
        port?: number;
        /**
         * Username for the PostgreSQL connection.
         */
        username: string;
    }

    export interface ConnectionProfilePrivateConnectivity {
        /**
         * A reference to a private connection resource. Format: `projects/{project}/locations/{location}/privateConnections/{name}`
         */
        privateConnection: string;
    }

    export interface PrivateConnectionError {
        /**
         * A list of messages that carry the error details.
         */
        details?: {[key: string]: string};
        /**
         * A message containing more information about the error that occurred.
         */
        message?: string;
    }

    export interface PrivateConnectionVpcPeeringConfig {
        /**
         * A free subnet for peering. (CIDR of /29)
         *
         * - - -
         */
        subnet: string;
        /**
         * Fully qualified name of the VPC that Datastream will peer to.
         * Format: projects/{project}/global/{networks}/{name}
         */
        vpc: string;
    }

    export interface StreamBackfillAll {
        /**
         * MySQL data source objects to avoid backfilling.
         * Structure is documented below.
         */
        mysqlExcludedObjects?: outputs.datastream.StreamBackfillAllMysqlExcludedObjects;
        /**
         * PostgreSQL data source objects to avoid backfilling.
         * Structure is documented below.
         */
        oracleExcludedObjects?: outputs.datastream.StreamBackfillAllOracleExcludedObjects;
        /**
         * PostgreSQL data source objects to avoid backfilling.
         * Structure is documented below.
         */
        postgresqlExcludedObjects?: outputs.datastream.StreamBackfillAllPostgresqlExcludedObjects;
    }

    export interface StreamBackfillAllMysqlExcludedObjects {
        /**
         * MySQL databases on the server
         * Structure is documented below.
         */
        mysqlDatabases: outputs.datastream.StreamBackfillAllMysqlExcludedObjectsMysqlDatabase[];
    }

    export interface StreamBackfillAllMysqlExcludedObjectsMysqlDatabase {
        /**
         * Database name.
         */
        database: string;
        /**
         * Tables in the database.
         * Structure is documented below.
         */
        mysqlTables?: outputs.datastream.StreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTable[];
    }

    export interface StreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTable {
        /**
         * MySQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        mysqlColumns?: outputs.datastream.StreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTableMysqlColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTableMysqlColumn {
        /**
         * Column collation.
         */
        collation?: string;
        /**
         * Column name.
         */
        column?: string;
        /**
         * The MySQL data type. Full data types list can be found here:
         * https://dev.mysql.com/doc/refman/8.0/en/data-types.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * Whether or not the column can accept a null value.
         */
        nullable?: boolean;
        /**
         * The ordinal position of the column in the table.
         */
        ordinalPosition?: number;
        /**
         * Whether or not the column represents a primary key.
         */
        primaryKey?: boolean;
    }

    export interface StreamBackfillAllOracleExcludedObjects {
        /**
         * Oracle schemas/databases in the database server
         * Structure is documented below.
         */
        oracleSchemas: outputs.datastream.StreamBackfillAllOracleExcludedObjectsOracleSchema[];
    }

    export interface StreamBackfillAllOracleExcludedObjectsOracleSchema {
        /**
         * Tables in the database.
         * Structure is documented below.
         */
        oracleTables?: outputs.datastream.StreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTable[];
        /**
         * Schema name.
         */
        schema: string;
    }

    export interface StreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTable {
        /**
         * Oracle columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        oracleColumns?: outputs.datastream.StreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTableOracleColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTableOracleColumn {
        /**
         * Column name.
         */
        column?: string;
        /**
         * The Oracle data type. Full data types list can be found here:
         * https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/Data-Types.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column encoding.
         */
        encoding: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * (Output)
         * Whether or not the column can accept a null value.
         */
        nullable: boolean;
        /**
         * (Output)
         * The ordinal position of the column in the table.
         */
        ordinalPosition: number;
        /**
         * (Output)
         * Column precision.
         */
        precision: number;
        /**
         * (Output)
         * Whether or not the column represents a primary key.
         */
        primaryKey: boolean;
        /**
         * (Output)
         * Column scale.
         */
        scale: number;
    }

    export interface StreamBackfillAllPostgresqlExcludedObjects {
        /**
         * PostgreSQL schemas on the server
         * Structure is documented below.
         */
        postgresqlSchemas: outputs.datastream.StreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchema[];
    }

    export interface StreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchema {
        /**
         * Tables in the schema.
         * Structure is documented below.
         */
        postgresqlTables?: outputs.datastream.StreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTable[];
        /**
         * Database name.
         */
        schema: string;
    }

    export interface StreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTable {
        /**
         * PostgreSQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        postgresqlColumns?: outputs.datastream.StreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn {
        /**
         * Column name.
         */
        column?: string;
        /**
         * The PostgreSQL data type. Full data types list can be found here:
         * https://www.postgresql.org/docs/current/datatype.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * Whether or not the column can accept a null value.
         */
        nullable?: boolean;
        /**
         * The ordinal position of the column in the table.
         */
        ordinalPosition?: number;
        /**
         * (Output)
         * Column precision.
         */
        precision: number;
        /**
         * Whether or not the column represents a primary key.
         */
        primaryKey?: boolean;
        /**
         * (Output)
         * Column scale.
         */
        scale: number;
    }

    export interface StreamBackfillNone {
    }

    export interface StreamDestinationConfig {
        /**
         * A configuration for how data should be loaded to Cloud Storage.
         * Structure is documented below.
         */
        bigqueryDestinationConfig?: outputs.datastream.StreamDestinationConfigBigqueryDestinationConfig;
        /**
         * Destination connection profile resource. Format: projects/{project}/locations/{location}/connectionProfiles/{name}
         */
        destinationConnectionProfile: string;
        /**
         * A configuration for how data should be loaded to Cloud Storage.
         * Structure is documented below.
         */
        gcsDestinationConfig?: outputs.datastream.StreamDestinationConfigGcsDestinationConfig;
    }

    export interface StreamDestinationConfigBigqueryDestinationConfig {
        /**
         * The guaranteed data freshness (in seconds) when querying tables created by the stream.
         * Editing this field will only affect new tables created in the future, but existing tables
         * will not be impacted. Lower values mean that queries will return fresher data, but may result in higher cost.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.
         */
        dataFreshness?: string;
        /**
         * A single target dataset to which all data will be streamed.
         * Structure is documented below.
         */
        singleTargetDataset?: outputs.datastream.StreamDestinationConfigBigqueryDestinationConfigSingleTargetDataset;
        /**
         * Destination datasets are created so that hierarchy of the destination data objects matches the source hierarchy.
         * Structure is documented below.
         */
        sourceHierarchyDatasets?: outputs.datastream.StreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasets;
    }

    export interface StreamDestinationConfigBigqueryDestinationConfigSingleTargetDataset {
        /**
         * Dataset ID in the format projects/{project}/datasets/{dataset_id} or
         * {project}:{dataset_id}
         */
        datasetId: string;
    }

    export interface StreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasets {
        /**
         * Dataset template used for dynamic dataset creation.
         * Structure is documented below.
         */
        datasetTemplate: outputs.datastream.StreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasetsDatasetTemplate;
    }

    export interface StreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasetsDatasetTemplate {
        /**
         * If supplied, every created dataset will have its name prefixed by the provided value.
         * The prefix and name will be separated by an underscore. i.e. _.
         */
        datasetIdPrefix?: string;
        /**
         * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery
         * table. The BigQuery Service Account associated with your project requires access to this
         * encryption key. i.e. projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{cryptoKey}.
         * See https://cloud.google.com/bigquery/docs/customer-managed-encryption for more information.
         *
         * - - -
         */
        kmsKeyName?: string;
        /**
         * The geographic location where the dataset should reside.
         * See https://cloud.google.com/bigquery/docs/locations for supported locations.
         */
        location: string;
    }

    export interface StreamDestinationConfigGcsDestinationConfig {
        /**
         * AVRO file format configuration.
         */
        avroFileFormat?: outputs.datastream.StreamDestinationConfigGcsDestinationConfigAvroFileFormat;
        /**
         * The maximum duration for which new events are added before a file is closed and a new file is created.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.
         */
        fileRotationInterval: string;
        /**
         * The maximum file size to be saved in the bucket.
         */
        fileRotationMb: number;
        /**
         * JSON file format configuration.
         * Structure is documented below.
         */
        jsonFileFormat?: outputs.datastream.StreamDestinationConfigGcsDestinationConfigJsonFileFormat;
        /**
         * Path inside the Cloud Storage bucket to write data to.
         */
        path?: string;
    }

    export interface StreamDestinationConfigGcsDestinationConfigAvroFileFormat {
    }

    export interface StreamDestinationConfigGcsDestinationConfigJsonFileFormat {
        /**
         * Compression of the loaded JSON file.
         * Possible values are: `NO_COMPRESSION`, `GZIP`.
         */
        compression?: string;
        /**
         * The schema file format along JSON data files.
         * Possible values are: `NO_SCHEMA_FILE`, `AVRO_SCHEMA_FILE`.
         */
        schemaFileFormat?: string;
    }

    export interface StreamSourceConfig {
        /**
         * MySQL data source configuration.
         * Structure is documented below.
         */
        mysqlSourceConfig?: outputs.datastream.StreamSourceConfigMysqlSourceConfig;
        /**
         * MySQL data source configuration.
         * Structure is documented below.
         */
        oracleSourceConfig?: outputs.datastream.StreamSourceConfigOracleSourceConfig;
        /**
         * PostgreSQL data source configuration.
         * Structure is documented below.
         */
        postgresqlSourceConfig?: outputs.datastream.StreamSourceConfigPostgresqlSourceConfig;
        /**
         * Source connection profile resource. Format: projects/{project}/locations/{location}/connectionProfiles/{name}
         */
        sourceConnectionProfile: string;
    }

    export interface StreamSourceConfigMysqlSourceConfig {
        /**
         * MySQL objects to exclude from the stream.
         * Structure is documented below.
         */
        excludeObjects?: outputs.datastream.StreamSourceConfigMysqlSourceConfigExcludeObjects;
        /**
         * MySQL objects to retrieve from the source.
         * Structure is documented below.
         */
        includeObjects?: outputs.datastream.StreamSourceConfigMysqlSourceConfigIncludeObjects;
        /**
         * Maximum number of concurrent backfill tasks. The number should be non negative.
         * If not set (or set to 0), the system's default value will be used.
         */
        maxConcurrentBackfillTasks: number;
        /**
         * Maximum number of concurrent CDC tasks. The number should be non negative.
         * If not set (or set to 0), the system's default value will be used.
         */
        maxConcurrentCdcTasks: number;
    }

    export interface StreamSourceConfigMysqlSourceConfigExcludeObjects {
        /**
         * MySQL databases on the server
         * Structure is documented below.
         */
        mysqlDatabases: outputs.datastream.StreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabase[];
    }

    export interface StreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabase {
        /**
         * Database name.
         */
        database: string;
        /**
         * Tables in the database.
         * Structure is documented below.
         */
        mysqlTables?: outputs.datastream.StreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTable[];
    }

    export interface StreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTable {
        /**
         * MySQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        mysqlColumns?: outputs.datastream.StreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTableMysqlColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTableMysqlColumn {
        /**
         * Column collation.
         */
        collation?: string;
        /**
         * Column name.
         */
        column?: string;
        /**
         * The MySQL data type. Full data types list can be found here:
         * https://dev.mysql.com/doc/refman/8.0/en/data-types.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * Whether or not the column can accept a null value.
         */
        nullable?: boolean;
        /**
         * The ordinal position of the column in the table.
         */
        ordinalPosition?: number;
        /**
         * Whether or not the column represents a primary key.
         */
        primaryKey?: boolean;
    }

    export interface StreamSourceConfigMysqlSourceConfigIncludeObjects {
        /**
         * MySQL databases on the server
         * Structure is documented below.
         */
        mysqlDatabases: outputs.datastream.StreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabase[];
    }

    export interface StreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabase {
        /**
         * Database name.
         */
        database: string;
        /**
         * Tables in the database.
         * Structure is documented below.
         */
        mysqlTables?: outputs.datastream.StreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTable[];
    }

    export interface StreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTable {
        /**
         * MySQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        mysqlColumns?: outputs.datastream.StreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTableMysqlColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTableMysqlColumn {
        /**
         * Column collation.
         */
        collation?: string;
        /**
         * Column name.
         */
        column?: string;
        /**
         * The MySQL data type. Full data types list can be found here:
         * https://dev.mysql.com/doc/refman/8.0/en/data-types.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * Whether or not the column can accept a null value.
         */
        nullable?: boolean;
        /**
         * The ordinal position of the column in the table.
         */
        ordinalPosition?: number;
        /**
         * Whether or not the column represents a primary key.
         */
        primaryKey?: boolean;
    }

    export interface StreamSourceConfigOracleSourceConfig {
        /**
         * Configuration to drop large object values.
         */
        dropLargeObjects?: outputs.datastream.StreamSourceConfigOracleSourceConfigDropLargeObjects;
        /**
         * Oracle objects to exclude from the stream.
         * Structure is documented below.
         */
        excludeObjects?: outputs.datastream.StreamSourceConfigOracleSourceConfigExcludeObjects;
        /**
         * Oracle objects to retrieve from the source.
         * Structure is documented below.
         */
        includeObjects?: outputs.datastream.StreamSourceConfigOracleSourceConfigIncludeObjects;
        /**
         * Maximum number of concurrent backfill tasks. The number should be non negative.
         * If not set (or set to 0), the system's default value will be used.
         */
        maxConcurrentBackfillTasks: number;
        /**
         * Maximum number of concurrent CDC tasks. The number should be non negative.
         * If not set (or set to 0), the system's default value will be used.
         */
        maxConcurrentCdcTasks: number;
        /**
         * Configuration to drop large object values.
         */
        streamLargeObjects?: outputs.datastream.StreamSourceConfigOracleSourceConfigStreamLargeObjects;
    }

    export interface StreamSourceConfigOracleSourceConfigDropLargeObjects {
    }

    export interface StreamSourceConfigOracleSourceConfigExcludeObjects {
        /**
         * Oracle schemas/databases in the database server
         * Structure is documented below.
         */
        oracleSchemas: outputs.datastream.StreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchema[];
    }

    export interface StreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchema {
        /**
         * Tables in the database.
         * Structure is documented below.
         */
        oracleTables?: outputs.datastream.StreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTable[];
        /**
         * Schema name.
         */
        schema: string;
    }

    export interface StreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTable {
        /**
         * Oracle columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        oracleColumns?: outputs.datastream.StreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTableOracleColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTableOracleColumn {
        /**
         * Column name.
         */
        column?: string;
        /**
         * The Oracle data type. Full data types list can be found here:
         * https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/Data-Types.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column encoding.
         */
        encoding: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * (Output)
         * Whether or not the column can accept a null value.
         */
        nullable: boolean;
        /**
         * (Output)
         * The ordinal position of the column in the table.
         */
        ordinalPosition: number;
        /**
         * (Output)
         * Column precision.
         */
        precision: number;
        /**
         * (Output)
         * Whether or not the column represents a primary key.
         */
        primaryKey: boolean;
        /**
         * (Output)
         * Column scale.
         */
        scale: number;
    }

    export interface StreamSourceConfigOracleSourceConfigIncludeObjects {
        /**
         * Oracle schemas/databases in the database server
         * Structure is documented below.
         */
        oracleSchemas: outputs.datastream.StreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchema[];
    }

    export interface StreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchema {
        /**
         * Tables in the database.
         * Structure is documented below.
         */
        oracleTables?: outputs.datastream.StreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTable[];
        /**
         * Schema name.
         */
        schema: string;
    }

    export interface StreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTable {
        /**
         * Oracle columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        oracleColumns?: outputs.datastream.StreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTableOracleColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTableOracleColumn {
        /**
         * Column name.
         */
        column?: string;
        /**
         * The Oracle data type. Full data types list can be found here:
         * https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/Data-Types.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column encoding.
         */
        encoding: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * (Output)
         * Whether or not the column can accept a null value.
         */
        nullable: boolean;
        /**
         * (Output)
         * The ordinal position of the column in the table.
         */
        ordinalPosition: number;
        /**
         * (Output)
         * Column precision.
         */
        precision: number;
        /**
         * (Output)
         * Whether or not the column represents a primary key.
         */
        primaryKey: boolean;
        /**
         * (Output)
         * Column scale.
         */
        scale: number;
    }

    export interface StreamSourceConfigOracleSourceConfigStreamLargeObjects {
    }

    export interface StreamSourceConfigPostgresqlSourceConfig {
        /**
         * PostgreSQL objects to exclude from the stream.
         * Structure is documented below.
         */
        excludeObjects?: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigExcludeObjects;
        /**
         * PostgreSQL objects to retrieve from the source.
         * Structure is documented below.
         */
        includeObjects?: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigIncludeObjects;
        /**
         * Maximum number of concurrent backfill tasks. The number should be non
         * negative. If not set (or set to 0), the system's default value will be used.
         */
        maxConcurrentBackfillTasks: number;
        /**
         * The name of the publication that includes the set of all tables
         * that are defined in the stream's include_objects.
         */
        publication: string;
        /**
         * The name of the logical replication slot that's configured with
         * the pgoutput plugin.
         */
        replicationSlot: string;
    }

    export interface StreamSourceConfigPostgresqlSourceConfigExcludeObjects {
        /**
         * PostgreSQL schemas on the server
         * Structure is documented below.
         */
        postgresqlSchemas: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchema[];
    }

    export interface StreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchema {
        /**
         * Tables in the schema.
         * Structure is documented below.
         */
        postgresqlTables?: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTable[];
        /**
         * Database name.
         */
        schema: string;
    }

    export interface StreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTable {
        /**
         * PostgreSQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        postgresqlColumns?: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn {
        /**
         * Column name.
         */
        column?: string;
        /**
         * The PostgreSQL data type. Full data types list can be found here:
         * https://www.postgresql.org/docs/current/datatype.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * Whether or not the column can accept a null value.
         */
        nullable?: boolean;
        /**
         * The ordinal position of the column in the table.
         */
        ordinalPosition?: number;
        /**
         * (Output)
         * Column precision.
         */
        precision: number;
        /**
         * Whether or not the column represents a primary key.
         */
        primaryKey?: boolean;
        /**
         * (Output)
         * Column scale.
         */
        scale: number;
    }

    export interface StreamSourceConfigPostgresqlSourceConfigIncludeObjects {
        /**
         * PostgreSQL schemas on the server
         * Structure is documented below.
         */
        postgresqlSchemas: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchema[];
    }

    export interface StreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchema {
        /**
         * Tables in the schema.
         * Structure is documented below.
         */
        postgresqlTables?: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTable[];
        /**
         * Database name.
         */
        schema: string;
    }

    export interface StreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTable {
        /**
         * PostgreSQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
         * Structure is documented below.
         */
        postgresqlColumns?: outputs.datastream.StreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn[];
        /**
         * Table name.
         */
        table: string;
    }

    export interface StreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn {
        /**
         * Column name.
         */
        column?: string;
        /**
         * The PostgreSQL data type. Full data types list can be found here:
         * https://www.postgresql.org/docs/current/datatype.html
         */
        dataType?: string;
        /**
         * (Output)
         * Column length.
         */
        length: number;
        /**
         * Whether or not the column can accept a null value.
         */
        nullable?: boolean;
        /**
         * The ordinal position of the column in the table.
         */
        ordinalPosition?: number;
        /**
         * (Output)
         * Column precision.
         */
        precision: number;
        /**
         * Whether or not the column represents a primary key.
         */
        primaryKey?: boolean;
        /**
         * (Output)
         * Column scale.
         */
        scale: number;
    }

}

export namespace deploymentmanager {
    export interface DeploymentLabel {
        /**
         * Key for label.
         */
        key?: string;
        /**
         * Value of label.
         */
        value?: string;
    }

    export interface DeploymentTarget {
        /**
         * The root configuration file to use for this deployment.
         * Structure is documented below.
         */
        config: outputs.deploymentmanager.DeploymentTargetConfig;
        /**
         * Specifies import files for this configuration. This can be
         * used to import templates or other files. For example, you might
         * import a text file in order to use the file in a template.
         * Structure is documented below.
         */
        imports?: outputs.deploymentmanager.DeploymentTargetImport[];
    }

    export interface DeploymentTargetConfig {
        /**
         * The full YAML contents of your configuration file.
         */
        content: string;
    }

    export interface DeploymentTargetImport {
        /**
         * The full contents of the template that you want to import.
         */
        content?: string;
        /**
         * The name of the template to import, as declared in the YAML
         * configuration.
         *
         * - - -
         */
        name?: string;
    }

}

export namespace diagflow {
    export interface CxAgentSpeechToTextSettings {
        /**
         * Whether to use speech adaptation for speech recognition.
         */
        enableSpeechAdaptation?: boolean;
    }

    export interface CxEntityTypeEntity {
        /**
         * A collection of value synonyms. For example, if the entity type is vegetable, and value is scallions, a synonym could be green onions.
         * For KIND_LIST entity types: This collection must contain exactly one synonym equal to value.
         *
         * - - -
         */
        synonyms?: string[];
        /**
         * The primary value associated with this entity entry. For example, if the entity type is vegetable, the value could be scallions.
         * For KIND_MAP entity types: A canonical value to be used in place of synonyms.
         * For KIND_LIST entity types: A string that can contain references to other entity types (with or without aliases).
         */
        value?: string;
    }

    export interface CxEntityTypeExcludedPhrase {
        /**
         * The word or phrase to be excluded.
         */
        value?: string;
    }

    export interface CxEnvironmentVersionConfig {
        /**
         * Format: projects/{{project}}/locations/{{location}}/agents/{{agent}}/flows/{{flow}}/versions/{{version}}.
         *
         * - - -
         */
        version: string;
    }

    export interface CxFlowEventHandler {
        /**
         * The name of the event to handle.
         */
        event?: string;
        /**
         * (Output)
         * The unique identifier of this event handler.
         */
        name: string;
        /**
         * The target flow to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
         */
        targetFlow?: string;
        /**
         * The target page to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        targetPage?: string;
        /**
         * The fulfillment to call when the event occurs. Handling webhook errors with a fulfillment enabled with webhook could cause infinite loop. It is invalid to specify such fulfillment for a handler handling webhooks.
         * Structure is documented below.
         */
        triggerFulfillment?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillment;
    }

    export interface CxFlowEventHandlerTriggerFulfillment {
        /**
         * Conditional cases for this fulfillment.
         * Structure is documented below.
         */
        conditionalCases?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentConditionalCase[];
        /**
         * The list of rich message responses to present to the user.
         * Structure is documented below.
         */
        messages?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentMessage[];
        /**
         * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
         */
        returnPartialResponses?: boolean;
        /**
         * Set parameter values before executing the webhook.
         * Structure is documented below.
         */
        setParameterActions?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentSetParameterAction[];
        /**
         * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
         */
        tag?: string;
        /**
         * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
         */
        webhook?: string;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentConditionalCase {
        /**
         * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
         * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
         */
        cases?: string;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentMessage {
        /**
         * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
         */
        channel?: string;
        /**
         * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
         * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
         * * In a webhook response when you determine that you handled the customer issue.
         * Structure is documented below.
         */
        conversationSuccess?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentMessageConversationSuccess;
        /**
         * Indicates that the conversation should be handed off to a live agent.
         * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
         * * In a webhook response when you determine that the customer issue can only be handled by a human.
         * Structure is documented below.
         */
        liveAgentHandoff?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentMessageLiveAgentHandoff;
        /**
         * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
         * Structure is documented below.
         */
        outputAudioText?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentMessageOutputAudioText;
        /**
         * A custom, platform-specific payload.
         */
        payload?: string;
        /**
         * Specifies an audio clip to be played by the client as part of the response.
         * Structure is documented below.
         */
        playAudio?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentMessagePlayAudio;
        /**
         * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
         * Structure is documented below.
         */
        telephonyTransferCall?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentMessageTelephonyTransferCall;
        /**
         * The text response message.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxFlowEventHandlerTriggerFulfillmentMessageText;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentMessageConversationSuccess {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentMessageLiveAgentHandoff {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentMessageOutputAudioText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * The SSML text to be synthesized. For more information, see SSML.
         */
        ssml?: string;
        /**
         * The raw text to be synthesized.
         */
        text?: string;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentMessagePlayAudio {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
         */
        audioUri: string;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentMessageTelephonyTransferCall {
        /**
         * Transfer the call to a phone number in E.164 format.
         */
        phoneNumber: string;
    }

    export interface CxFlowEventHandlerTriggerFulfillmentMessageText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxFlowEventHandlerTriggerFulfillmentSetParameterAction {
        /**
         * Display name of the parameter.
         */
        parameter?: string;
        /**
         * The new JSON-encoded value of the parameter. A null value clears the parameter.
         */
        value?: string;
    }

    export interface CxFlowNluSettings {
        /**
         * To filter out false positive results and still get variety in matched natural language inputs for your agent, you can tune the machine learning classification threshold.
         * If the returned score value is less than the threshold value, then a no-match event will be triggered. The score values range from 0.0 (completely uncertain) to 1.0 (completely certain). If set to 0.0, the default of 0.3 is used.
         */
        classificationThreshold?: number;
        /**
         * Indicates NLU model training mode.
         * * MODEL_TRAINING_MODE_AUTOMATIC: NLU model training is automatically triggered when a flow gets modified. User can also manually trigger model training in this mode.
         * * MODEL_TRAINING_MODE_MANUAL: User needs to manually trigger NLU model training. Best for large flows whose models take long time to train.
         * Possible values are: `MODEL_TRAINING_MODE_AUTOMATIC`, `MODEL_TRAINING_MODE_MANUAL`.
         */
        modelTrainingMode?: string;
        /**
         * Indicates the type of NLU model.
         * * MODEL_TYPE_STANDARD: Use standard NLU model.
         * * MODEL_TYPE_ADVANCED: Use advanced NLU model.
         * Possible values are: `MODEL_TYPE_STANDARD`, `MODEL_TYPE_ADVANCED`.
         */
        modelType?: string;
    }

    export interface CxFlowTransitionRoute {
        /**
         * The condition to evaluate against form parameters or session parameters.
         * At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
         */
        condition?: string;
        /**
         * The unique identifier of an Intent.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>. Indicates that the transition can only happen when the given intent is matched. At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
         */
        intent?: string;
        /**
         * (Output)
         * The unique identifier of this transition route.
         */
        name: string;
        /**
         * The target flow to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
         */
        targetFlow?: string;
        /**
         * The target page to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        targetPage?: string;
        /**
         * The fulfillment to call when the condition is satisfied. At least one of triggerFulfillment and target must be specified. When both are defined, triggerFulfillment is executed first.
         * Structure is documented below.
         */
        triggerFulfillment?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillment;
    }

    export interface CxFlowTransitionRouteTriggerFulfillment {
        /**
         * Conditional cases for this fulfillment.
         * Structure is documented below.
         */
        conditionalCases?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentConditionalCase[];
        /**
         * The list of rich message responses to present to the user.
         * Structure is documented below.
         */
        messages?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentMessage[];
        /**
         * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
         */
        returnPartialResponses?: boolean;
        /**
         * Set parameter values before executing the webhook.
         * Structure is documented below.
         */
        setParameterActions?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentSetParameterAction[];
        /**
         * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
         */
        tag?: string;
        /**
         * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
         */
        webhook?: string;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentConditionalCase {
        /**
         * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
         * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
         */
        cases?: string;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentMessage {
        /**
         * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
         */
        channel?: string;
        /**
         * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
         * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
         * * In a webhook response when you determine that you handled the customer issue.
         * Structure is documented below.
         */
        conversationSuccess?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentMessageConversationSuccess;
        /**
         * Indicates that the conversation should be handed off to a live agent.
         * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
         * * In a webhook response when you determine that the customer issue can only be handled by a human.
         * Structure is documented below.
         */
        liveAgentHandoff?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff;
        /**
         * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
         * Structure is documented below.
         */
        outputAudioText?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentMessageOutputAudioText;
        /**
         * A custom, platform-specific payload.
         */
        payload?: string;
        /**
         * Specifies an audio clip to be played by the client as part of the response.
         * Structure is documented below.
         */
        playAudio?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentMessagePlayAudio;
        /**
         * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
         * Structure is documented below.
         */
        telephonyTransferCall?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall;
        /**
         * The text response message.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxFlowTransitionRouteTriggerFulfillmentMessageText;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentMessageConversationSuccess {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentMessageOutputAudioText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * The SSML text to be synthesized. For more information, see SSML.
         */
        ssml?: string;
        /**
         * The raw text to be synthesized.
         */
        text?: string;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentMessagePlayAudio {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
         */
        audioUri: string;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall {
        /**
         * Transfer the call to a phone number in E.164 format.
         */
        phoneNumber: string;
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentMessageText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxFlowTransitionRouteTriggerFulfillmentSetParameterAction {
        /**
         * Display name of the parameter.
         */
        parameter?: string;
        /**
         * The new JSON-encoded value of the parameter. A null value clears the parameter.
         */
        value?: string;
    }

    export interface CxIntentParameter {
        /**
         * The entity type of the parameter.
         * Format: projects/-/locations/-/agents/-/entityTypes/<System Entity Type ID> for system entity types (for example, projects/-/locations/-/agents/-/entityTypes/sys.date), or projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/entityTypes/<Entity Type ID> for developer entity types.
         */
        entityType: string;
        /**
         * The unique identifier of the parameter. This field is used by training phrases to annotate their parts.
         */
        id: string;
        /**
         * Indicates whether the parameter represents a list of values.
         */
        isList?: boolean;
        /**
         * Indicates whether the parameter content should be redacted in log. If redaction is enabled, the parameter content will be replaced by parameter name during logging.
         * Note: the parameter content is subject to redaction if either parameter level redaction or entity type level redaction is enabled.
         */
        redact?: boolean;
    }

    export interface CxIntentTrainingPhrase {
        /**
         * (Output)
         * The unique identifier of the training phrase.
         */
        id: string;
        /**
         * The ordered list of training phrase parts. The parts are concatenated in order to form the training phrase.
         * Note: The API does not automatically annotate training phrases like the Dialogflow Console does.
         * Note: Do not forget to include whitespace at part boundaries, so the training phrase is well formatted when the parts are concatenated.
         * If the training phrase does not need to be annotated with parameters, you just need a single part with only the Part.text field set.
         * If you want to annotate the training phrase, you must create multiple parts, where the fields of each part are populated in one of two ways:
         * Part.text is set to a part of the phrase that has no parameters.
         * Part.text is set to a part of the phrase that you want to annotate, and the parameterId field is set.
         * Structure is documented below.
         */
        parts: outputs.diagflow.CxIntentTrainingPhrasePart[];
        /**
         * Indicates how many times this example was added to the intent.
         */
        repeatCount?: number;
    }

    export interface CxIntentTrainingPhrasePart {
        /**
         * The parameter used to annotate this part of the training phrase. This field is required for annotated parts of the training phrase.
         */
        parameterId?: string;
        /**
         * The text for this part.
         */
        text: string;
    }

    export interface CxPageEntryFulfillment {
        /**
         * Conditional cases for this fulfillment.
         * Structure is documented below.
         */
        conditionalCases?: outputs.diagflow.CxPageEntryFulfillmentConditionalCase[];
        /**
         * The list of rich message responses to present to the user.
         * Structure is documented below.
         */
        messages?: outputs.diagflow.CxPageEntryFulfillmentMessage[];
        /**
         * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
         */
        returnPartialResponses?: boolean;
        /**
         * Set parameter values before executing the webhook.
         * Structure is documented below.
         */
        setParameterActions?: outputs.diagflow.CxPageEntryFulfillmentSetParameterAction[];
        /**
         * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
         */
        tag?: string;
        /**
         * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
         */
        webhook?: string;
    }

    export interface CxPageEntryFulfillmentConditionalCase {
        /**
         * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
         * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
         */
        cases?: string;
    }

    export interface CxPageEntryFulfillmentMessage {
        /**
         * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
         */
        channel?: string;
        /**
         * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
         * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
         * * In a webhook response when you determine that you handled the customer issue.
         * Structure is documented below.
         */
        conversationSuccess?: outputs.diagflow.CxPageEntryFulfillmentMessageConversationSuccess;
        /**
         * Indicates that the conversation should be handed off to a live agent.
         * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
         * * In a webhook response when you determine that the customer issue can only be handled by a human.
         * Structure is documented below.
         */
        liveAgentHandoff?: outputs.diagflow.CxPageEntryFulfillmentMessageLiveAgentHandoff;
        /**
         * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
         * Structure is documented below.
         */
        outputAudioText?: outputs.diagflow.CxPageEntryFulfillmentMessageOutputAudioText;
        /**
         * A custom, platform-specific payload.
         */
        payload?: string;
        /**
         * Specifies an audio clip to be played by the client as part of the response.
         * Structure is documented below.
         */
        playAudio?: outputs.diagflow.CxPageEntryFulfillmentMessagePlayAudio;
        /**
         * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
         * Structure is documented below.
         */
        telephonyTransferCall?: outputs.diagflow.CxPageEntryFulfillmentMessageTelephonyTransferCall;
        /**
         * The text response message.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxPageEntryFulfillmentMessageText;
    }

    export interface CxPageEntryFulfillmentMessageConversationSuccess {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageEntryFulfillmentMessageLiveAgentHandoff {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageEntryFulfillmentMessageOutputAudioText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * The SSML text to be synthesized. For more information, see SSML.
         */
        ssml?: string;
        /**
         * The raw text to be synthesized.
         */
        text?: string;
    }

    export interface CxPageEntryFulfillmentMessagePlayAudio {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
         */
        audioUri: string;
    }

    export interface CxPageEntryFulfillmentMessageTelephonyTransferCall {
        /**
         * Transfer the call to a phone number in E.164 format.
         */
        phoneNumber: string;
    }

    export interface CxPageEntryFulfillmentMessageText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxPageEntryFulfillmentSetParameterAction {
        /**
         * Display name of the parameter.
         */
        parameter?: string;
        /**
         * The new JSON-encoded value of the parameter. A null value clears the parameter.
         */
        value?: string;
    }

    export interface CxPageEventHandler {
        /**
         * The name of the event to handle.
         */
        event?: string;
        /**
         * (Output)
         * The unique identifier of this event handler.
         */
        name: string;
        /**
         * The target flow to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
         */
        targetFlow?: string;
        /**
         * The target page to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        targetPage?: string;
        /**
         * The fulfillment to call when the event occurs. Handling webhook errors with a fulfillment enabled with webhook could cause infinite loop. It is invalid to specify such fulfillment for a handler handling webhooks.
         * Structure is documented below.
         */
        triggerFulfillment?: outputs.diagflow.CxPageEventHandlerTriggerFulfillment;
    }

    export interface CxPageEventHandlerTriggerFulfillment {
        /**
         * Conditional cases for this fulfillment.
         * Structure is documented below.
         */
        conditionalCases?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentConditionalCase[];
        /**
         * The list of rich message responses to present to the user.
         * Structure is documented below.
         */
        messages?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentMessage[];
        /**
         * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
         */
        returnPartialResponses?: boolean;
        /**
         * Set parameter values before executing the webhook.
         * Structure is documented below.
         */
        setParameterActions?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentSetParameterAction[];
        /**
         * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
         */
        tag?: string;
        /**
         * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
         */
        webhook?: string;
    }

    export interface CxPageEventHandlerTriggerFulfillmentConditionalCase {
        /**
         * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
         * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
         */
        cases?: string;
    }

    export interface CxPageEventHandlerTriggerFulfillmentMessage {
        /**
         * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
         */
        channel?: string;
        /**
         * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
         * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
         * * In a webhook response when you determine that you handled the customer issue.
         * Structure is documented below.
         */
        conversationSuccess?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentMessageConversationSuccess;
        /**
         * Indicates that the conversation should be handed off to a live agent.
         * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
         * * In a webhook response when you determine that the customer issue can only be handled by a human.
         * Structure is documented below.
         */
        liveAgentHandoff?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentMessageLiveAgentHandoff;
        /**
         * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
         * Structure is documented below.
         */
        outputAudioText?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentMessageOutputAudioText;
        /**
         * A custom, platform-specific payload.
         */
        payload?: string;
        /**
         * Specifies an audio clip to be played by the client as part of the response.
         * Structure is documented below.
         */
        playAudio?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentMessagePlayAudio;
        /**
         * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
         * Structure is documented below.
         */
        telephonyTransferCall?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentMessageTelephonyTransferCall;
        /**
         * The text response message.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxPageEventHandlerTriggerFulfillmentMessageText;
    }

    export interface CxPageEventHandlerTriggerFulfillmentMessageConversationSuccess {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageEventHandlerTriggerFulfillmentMessageLiveAgentHandoff {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageEventHandlerTriggerFulfillmentMessageOutputAudioText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * The SSML text to be synthesized. For more information, see SSML.
         */
        ssml?: string;
        /**
         * The raw text to be synthesized.
         */
        text?: string;
    }

    export interface CxPageEventHandlerTriggerFulfillmentMessagePlayAudio {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
         */
        audioUri: string;
    }

    export interface CxPageEventHandlerTriggerFulfillmentMessageTelephonyTransferCall {
        /**
         * Transfer the call to a phone number in E.164 format.
         */
        phoneNumber: string;
    }

    export interface CxPageEventHandlerTriggerFulfillmentMessageText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxPageEventHandlerTriggerFulfillmentSetParameterAction {
        /**
         * Display name of the parameter.
         */
        parameter?: string;
        /**
         * The new JSON-encoded value of the parameter. A null value clears the parameter.
         */
        value?: string;
    }

    export interface CxPageForm {
        /**
         * Parameters to collect from the user.
         * Structure is documented below.
         */
        parameters?: outputs.diagflow.CxPageFormParameter[];
    }

    export interface CxPageFormParameter {
        /**
         * The default value of an optional parameter. If the parameter is required, the default value will be ignored.
         */
        defaultValue?: string;
        /**
         * The human-readable name of the parameter, unique within the form.
         */
        displayName?: string;
        /**
         * The entity type of the parameter.
         * Format: projects/-/locations/-/agents/-/entityTypes/<System Entity Type ID> for system entity types (for example, projects/-/locations/-/agents/-/entityTypes/sys.date), or projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/entityTypes/<Entity Type ID> for developer entity types.
         */
        entityType?: string;
        /**
         * Defines fill behavior for the parameter.
         * Structure is documented below.
         */
        fillBehavior?: outputs.diagflow.CxPageFormParameterFillBehavior;
        /**
         * Indicates whether the parameter represents a list of values.
         */
        isList?: boolean;
        /**
         * Indicates whether the parameter content should be redacted in log.
         * If redaction is enabled, the parameter content will be replaced by parameter name during logging. Note: the parameter content is subject to redaction if either parameter level redaction or entity type level redaction is enabled.
         */
        redact?: boolean;
        /**
         * Indicates whether the parameter is required. Optional parameters will not trigger prompts; however, they are filled if the user specifies them.
         * Required parameters must be filled before form filling concludes.
         */
        required?: boolean;
    }

    export interface CxPageFormParameterFillBehavior {
        /**
         * The fulfillment to provide the initial prompt that the agent can present to the user in order to fill the parameter.
         * Structure is documented below.
         */
        initialPromptFulfillment?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillment;
        /**
         * The handlers for parameter-level events, used to provide reprompt for the parameter or transition to a different page/flow. The supported events are:
         * * sys.no-match-<N>, where N can be from 1 to 6
         * * sys.no-match-default
         * * sys.no-input-<N>, where N can be from 1 to 6
         * * sys.no-input-default
         * * sys.invalid-parameter
         * [initialPromptFulfillment][initialPromptFulfillment] provides the first prompt for the parameter.
         * If the user's response does not fill the parameter, a no-match/no-input event will be triggered, and the fulfillment associated with the sys.no-match-1/sys.no-input-1 handler (if defined) will be called to provide a prompt. The sys.no-match-2/sys.no-input-2 handler (if defined) will respond to the next no-match/no-input event, and so on.
         * A sys.no-match-default or sys.no-input-default handler will be used to handle all following no-match/no-input events after all numbered no-match/no-input handlers for the parameter are consumed.
         * A sys.invalid-parameter handler can be defined to handle the case where the parameter values have been invalidated by webhook. For example, if the user's response fill the parameter, however the parameter was invalidated by webhook, the fulfillment associated with the sys.invalid-parameter handler (if defined) will be called to provide a prompt.
         * If the event handler for the corresponding event can't be found on the parameter, initialPromptFulfillment will be re-prompted.
         * Structure is documented below.
         */
        repromptEventHandlers?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandler[];
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillment {
        /**
         * Conditional cases for this fulfillment.
         * Structure is documented below.
         */
        conditionalCases?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentConditionalCase[];
        /**
         * The list of rich message responses to present to the user.
         * Structure is documented below.
         */
        messages?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessage[];
        /**
         * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
         */
        returnPartialResponses?: boolean;
        /**
         * Set parameter values before executing the webhook.
         * Structure is documented below.
         */
        setParameterActions?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentSetParameterAction[];
        /**
         * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
         */
        tag?: string;
        /**
         * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
         */
        webhook?: string;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentConditionalCase {
        /**
         * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
         * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
         */
        cases?: string;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessage {
        /**
         * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
         */
        channel?: string;
        /**
         * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
         * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
         * * In a webhook response when you determine that you handled the customer issue.
         * Structure is documented below.
         */
        conversationSuccess?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageConversationSuccess;
        /**
         * Indicates that the conversation should be handed off to a live agent.
         * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
         * * In a webhook response when you determine that the customer issue can only be handled by a human.
         * Structure is documented below.
         */
        liveAgentHandoff?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageLiveAgentHandoff;
        /**
         * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
         * Structure is documented below.
         */
        outputAudioText?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageOutputAudioText;
        /**
         * A custom, platform-specific payload.
         */
        payload?: string;
        /**
         * Specifies an audio clip to be played by the client as part of the response.
         * Structure is documented below.
         */
        playAudio?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessagePlayAudio;
        /**
         * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
         * Structure is documented below.
         */
        telephonyTransferCall?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageTelephonyTransferCall;
        /**
         * The text response message.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageText;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageConversationSuccess {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageLiveAgentHandoff {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageOutputAudioText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * The SSML text to be synthesized. For more information, see SSML.
         */
        ssml?: string;
        /**
         * The raw text to be synthesized.
         */
        text?: string;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessagePlayAudio {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
         */
        audioUri: string;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageTelephonyTransferCall {
        /**
         * Transfer the call to a phone number in E.164 format.
         */
        phoneNumber: string;
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxPageFormParameterFillBehaviorInitialPromptFulfillmentSetParameterAction {
        /**
         * Display name of the parameter.
         */
        parameter?: string;
        /**
         * The new JSON-encoded value of the parameter. A null value clears the parameter.
         */
        value?: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandler {
        /**
         * The name of the event to handle.
         */
        event?: string;
        /**
         * (Output)
         * The unique identifier of this event handler.
         */
        name: string;
        /**
         * The target flow to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
         */
        targetFlow?: string;
        /**
         * The target page to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        targetPage?: string;
        /**
         * The fulfillment to call when the event occurs. Handling webhook errors with a fulfillment enabled with webhook could cause infinite loop. It is invalid to specify such fulfillment for a handler handling webhooks.
         * Structure is documented below.
         */
        triggerFulfillment?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillment;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillment {
        /**
         * Conditional cases for this fulfillment.
         * Structure is documented below.
         */
        conditionalCases?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentConditionalCase[];
        /**
         * The list of rich message responses to present to the user.
         * Structure is documented below.
         */
        messages?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessage[];
        /**
         * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
         */
        returnPartialResponses?: boolean;
        /**
         * Set parameter values before executing the webhook.
         * Structure is documented below.
         */
        setParameterActions?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentSetParameterAction[];
        /**
         * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
         */
        tag?: string;
        /**
         * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
         */
        webhook?: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentConditionalCase {
        /**
         * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
         * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
         */
        cases?: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessage {
        /**
         * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
         */
        channel?: string;
        /**
         * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
         * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
         * * In a webhook response when you determine that you handled the customer issue.
         * Structure is documented below.
         */
        conversationSuccess?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageConversationSuccess;
        /**
         * Indicates that the conversation should be handed off to a live agent.
         * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
         * * In a webhook response when you determine that the customer issue can only be handled by a human.
         * Structure is documented below.
         */
        liveAgentHandoff?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageLiveAgentHandoff;
        /**
         * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
         * Structure is documented below.
         */
        outputAudioText?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageOutputAudioText;
        /**
         * A custom, platform-specific payload.
         */
        payload?: string;
        /**
         * Specifies an audio clip to be played by the client as part of the response.
         * Structure is documented below.
         */
        playAudio?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessagePlayAudio;
        /**
         * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
         * Structure is documented below.
         */
        telephonyTransferCall?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageTelephonyTransferCall;
        /**
         * The text response message.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageText;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageConversationSuccess {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageLiveAgentHandoff {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageOutputAudioText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * The SSML text to be synthesized. For more information, see SSML.
         */
        ssml?: string;
        /**
         * The raw text to be synthesized.
         */
        text?: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessagePlayAudio {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
         */
        audioUri: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageTelephonyTransferCall {
        /**
         * Transfer the call to a phone number in E.164 format.
         */
        phoneNumber: string;
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentSetParameterAction {
        /**
         * Display name of the parameter.
         */
        parameter?: string;
        /**
         * The new JSON-encoded value of the parameter. A null value clears the parameter.
         */
        value?: string;
    }

    export interface CxPageTransitionRoute {
        /**
         * The condition to evaluate against form parameters or session parameters.
         * At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
         */
        condition?: string;
        /**
         * The unique identifier of an Intent.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>. Indicates that the transition can only happen when the given intent is matched. At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
         */
        intent?: string;
        /**
         * (Output)
         * The unique identifier of this transition route.
         */
        name: string;
        /**
         * The target flow to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
         */
        targetFlow?: string;
        /**
         * The target page to transition to.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        targetPage?: string;
        /**
         * The fulfillment to call when the condition is satisfied. At least one of triggerFulfillment and target must be specified. When both are defined, triggerFulfillment is executed first.
         * Structure is documented below.
         */
        triggerFulfillment?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillment;
    }

    export interface CxPageTransitionRouteTriggerFulfillment {
        /**
         * Conditional cases for this fulfillment.
         * Structure is documented below.
         */
        conditionalCases?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentConditionalCase[];
        /**
         * The list of rich message responses to present to the user.
         * Structure is documented below.
         */
        messages?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentMessage[];
        /**
         * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
         */
        returnPartialResponses?: boolean;
        /**
         * Set parameter values before executing the webhook.
         * Structure is documented below.
         */
        setParameterActions?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentSetParameterAction[];
        /**
         * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
         */
        tag?: string;
        /**
         * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
         */
        webhook?: string;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentConditionalCase {
        /**
         * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
         * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
         */
        cases?: string;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentMessage {
        /**
         * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
         */
        channel?: string;
        /**
         * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
         * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
         * * In a webhook response when you determine that you handled the customer issue.
         * Structure is documented below.
         */
        conversationSuccess?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentMessageConversationSuccess;
        /**
         * Indicates that the conversation should be handed off to a live agent.
         * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
         * You may set this, for example:
         * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
         * * In a webhook response when you determine that the customer issue can only be handled by a human.
         * Structure is documented below.
         */
        liveAgentHandoff?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff;
        /**
         * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
         * Structure is documented below.
         */
        outputAudioText?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentMessageOutputAudioText;
        /**
         * A custom, platform-specific payload.
         */
        payload?: string;
        /**
         * Specifies an audio clip to be played by the client as part of the response.
         * Structure is documented below.
         */
        playAudio?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentMessagePlayAudio;
        /**
         * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
         * Structure is documented below.
         */
        telephonyTransferCall?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall;
        /**
         * The text response message.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxPageTransitionRouteTriggerFulfillmentMessageText;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentMessageConversationSuccess {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff {
        /**
         * Custom metadata. Dialogflow doesn't impose any structure on this.
         */
        metadata?: string;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentMessageOutputAudioText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * The SSML text to be synthesized. For more information, see SSML.
         */
        ssml?: string;
        /**
         * The raw text to be synthesized.
         */
        text?: string;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentMessagePlayAudio {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
         */
        audioUri: string;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall {
        /**
         * Transfer the call to a phone number in E.164 format.
         */
        phoneNumber: string;
    }

    export interface CxPageTransitionRouteTriggerFulfillmentMessageText {
        /**
         * (Output)
         * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
         */
        allowPlaybackInterruption: boolean;
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxPageTransitionRouteTriggerFulfillmentSetParameterAction {
        /**
         * Display name of the parameter.
         */
        parameter?: string;
        /**
         * The new JSON-encoded value of the parameter. A null value clears the parameter.
         */
        value?: string;
    }

    export interface CxSecuritySettingsAudioExportSettings {
        /**
         * Filename pattern for exported audio.
         */
        audioExportPattern?: string;
        /**
         * File format for exported audio file. Currently only in telephony recordings.
         * * MULAW: G.711 mu-law PCM with 8kHz sample rate.
         * * MP3: MP3 file format.
         * * OGG: OGG Vorbis.
         * Possible values are: `MULAW`, `MP3`, `OGG`.
         */
        audioFormat?: string;
        /**
         * Enable audio redaction if it is true.
         */
        enableAudioRedaction?: boolean;
        /**
         * Cloud Storage bucket to export audio record to. Setting this field would grant the Storage Object Creator role to the Dialogflow Service Agent. API caller that tries to modify this field should have the permission of storage.buckets.setIamPolicy.
         */
        gcsBucket?: string;
    }

    export interface CxSecuritySettingsInsightsExportSettings {
        /**
         * If enabled, we will automatically exports conversations to Insights and Insights runs its analyzers.
         */
        enableInsightsExport: boolean;
    }

    export interface CxTestCaseLastTestResult {
        /**
         * The conversation turns uttered during the test case replay in chronological order.
         * Structure is documented below.
         */
        conversationTurns?: outputs.diagflow.CxTestCaseLastTestResultConversationTurn[];
        /**
         * Environment where the test was run. If not set, it indicates the draft environment.
         */
        environment?: string;
        /**
         * The unique identifier of the intent.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>.
         *
         * (Optional)
         * The unique identifier of the page.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        name?: string;
        /**
         * Whether the test case passed in the agent environment.
         * * PASSED: The test passed.
         * * FAILED: The test did not pass.
         * Possible values are: `PASSED`, `FAILED`.
         */
        testResult?: string;
        /**
         * The time that the test was run. A timestamp in RFC3339 text format.
         */
        testTime?: string;
    }

    export interface CxTestCaseLastTestResultConversationTurn {
        /**
         * The user input.
         * Structure is documented below.
         */
        userInput?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnUserInput;
        /**
         * The virtual agent output.
         * Structure is documented below.
         */
        virtualAgentOutput?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnVirtualAgentOutput;
    }

    export interface CxTestCaseLastTestResultConversationTurnUserInput {
        /**
         * Whether sentiment analysis is enabled.
         */
        enableSentimentAnalysis?: boolean;
        /**
         * Parameters that need to be injected into the conversation during intent detection.
         */
        injectedParameters?: string;
        /**
         * User input. Supports text input, event input, dtmf input in the test case.
         * Structure is documented below.
         */
        input?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnUserInputInput;
        /**
         * If webhooks should be allowed to trigger in response to the user utterance. Often if parameters are injected, webhooks should not be enabled.
         */
        isWebhookEnabled?: boolean;
    }

    export interface CxTestCaseLastTestResultConversationTurnUserInputInput {
        /**
         * The DTMF event to be handled.
         * Structure is documented below.
         */
        dtmf?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnUserInputInputDtmf;
        /**
         * The event to be triggered.
         * Structure is documented below.
         */
        event?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnUserInputInputEvent;
        /**
         * The language of the input. See [Language Support](https://cloud.google.com/dialogflow/cx/docs/reference/language) for a list of the currently supported language codes.
         * Note that queries in the same session do not necessarily need to specify the same language.
         */
        languageCode?: string;
        /**
         * The natural language text to be processed.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnUserInputInputText;
    }

    export interface CxTestCaseLastTestResultConversationTurnUserInputInputDtmf {
        /**
         * The dtmf digits.
         */
        digits?: string;
        /**
         * The finish digit (if any).
         */
        finishDigit?: string;
    }

    export interface CxTestCaseLastTestResultConversationTurnUserInputInputEvent {
        /**
         * Name of the event.
         */
        event: string;
    }

    export interface CxTestCaseLastTestResultConversationTurnUserInputInputText {
        /**
         * The natural language text to be processed. Text length must not exceed 256 characters.
         */
        text: string;
    }

    export interface CxTestCaseLastTestResultConversationTurnVirtualAgentOutput {
        /**
         * The [Page](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.flows.pages#Page) on which the utterance was spoken.
         * Structure is documented below.
         */
        currentPage?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnVirtualAgentOutputCurrentPage;
        /**
         * The list of differences between the original run and the replay for this output, if any.
         * Structure is documented below.
         */
        differences?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnVirtualAgentOutputDifference[];
        /**
         * The session parameters available to the bot at this point.
         */
        sessionParameters?: string;
        /**
         * Response error from the agent in the test result. If set, other output is empty.
         * Structure is documented below.
         */
        status?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnVirtualAgentOutputStatus;
        /**
         * The text responses from the agent for the turn.
         * Structure is documented below.
         */
        textResponses?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnVirtualAgentOutputTextResponse[];
        /**
         * The [Intent](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.intents#Intent) that triggered the response.
         * Structure is documented below.
         */
        triggeredIntent?: outputs.diagflow.CxTestCaseLastTestResultConversationTurnVirtualAgentOutputTriggeredIntent;
    }

    export interface CxTestCaseLastTestResultConversationTurnVirtualAgentOutputCurrentPage {
        /**
         * (Output)
         * The human-readable name of the page, unique within the flow.
         */
        displayName?: string;
        /**
         * The unique identifier of the page.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        name?: string;
    }

    export interface CxTestCaseLastTestResultConversationTurnVirtualAgentOutputDifference {
        /**
         * A human readable description of the diff, showing the actual output vs expected output.
         */
        description?: string;
        /**
         * The type of diff.
         * * INTENT: The intent.
         * * PAGE: The page.
         * * PARAMETERS: The parameters.
         * * UTTERANCE: The message utterance.
         * * FLOW: The flow.
         * Possible values are: `INTENT`, `PAGE`, `PARAMETERS`, `UTTERANCE`, `FLOW`.
         */
        type?: string;
    }

    export interface CxTestCaseLastTestResultConversationTurnVirtualAgentOutputStatus {
        /**
         * The status code, which should be an enum value of google.rpc.Code.
         */
        code?: number;
        /**
         * A JSON encoded list of messages that carry the error details.
         */
        details?: string;
        /**
         * A developer-facing error message.
         */
        message?: string;
    }

    export interface CxTestCaseLastTestResultConversationTurnVirtualAgentOutputTextResponse {
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxTestCaseLastTestResultConversationTurnVirtualAgentOutputTriggeredIntent {
        /**
         * (Output)
         * The human-readable name of the intent, unique within the agent.
         */
        displayName?: string;
        /**
         * The unique identifier of the intent.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>.
         */
        name?: string;
    }

    export interface CxTestCaseTestCaseConversationTurn {
        /**
         * The user input.
         * Structure is documented below.
         */
        userInput?: outputs.diagflow.CxTestCaseTestCaseConversationTurnUserInput;
        /**
         * The virtual agent output.
         * Structure is documented below.
         */
        virtualAgentOutput?: outputs.diagflow.CxTestCaseTestCaseConversationTurnVirtualAgentOutput;
    }

    export interface CxTestCaseTestCaseConversationTurnUserInput {
        /**
         * Whether sentiment analysis is enabled.
         */
        enableSentimentAnalysis?: boolean;
        /**
         * Parameters that need to be injected into the conversation during intent detection.
         */
        injectedParameters?: string;
        /**
         * User input. Supports text input, event input, dtmf input in the test case.
         * Structure is documented below.
         */
        input?: outputs.diagflow.CxTestCaseTestCaseConversationTurnUserInputInput;
        /**
         * If webhooks should be allowed to trigger in response to the user utterance. Often if parameters are injected, webhooks should not be enabled.
         */
        isWebhookEnabled?: boolean;
    }

    export interface CxTestCaseTestCaseConversationTurnUserInputInput {
        /**
         * The DTMF event to be handled.
         * Structure is documented below.
         */
        dtmf?: outputs.diagflow.CxTestCaseTestCaseConversationTurnUserInputInputDtmf;
        /**
         * The event to be triggered.
         * Structure is documented below.
         */
        event?: outputs.diagflow.CxTestCaseTestCaseConversationTurnUserInputInputEvent;
        /**
         * The language of the input. See [Language Support](https://cloud.google.com/dialogflow/cx/docs/reference/language) for a list of the currently supported language codes.
         * Note that queries in the same session do not necessarily need to specify the same language.
         */
        languageCode?: string;
        /**
         * The natural language text to be processed.
         * Structure is documented below.
         */
        text?: outputs.diagflow.CxTestCaseTestCaseConversationTurnUserInputInputText;
    }

    export interface CxTestCaseTestCaseConversationTurnUserInputInputDtmf {
        /**
         * The dtmf digits.
         */
        digits?: string;
        /**
         * The finish digit (if any).
         */
        finishDigit?: string;
    }

    export interface CxTestCaseTestCaseConversationTurnUserInputInputEvent {
        /**
         * Name of the event.
         */
        event: string;
    }

    export interface CxTestCaseTestCaseConversationTurnUserInputInputText {
        /**
         * The natural language text to be processed. Text length must not exceed 256 characters.
         */
        text: string;
    }

    export interface CxTestCaseTestCaseConversationTurnVirtualAgentOutput {
        /**
         * The [Page](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.flows.pages#Page) on which the utterance was spoken.
         * Structure is documented below.
         */
        currentPage?: outputs.diagflow.CxTestCaseTestCaseConversationTurnVirtualAgentOutputCurrentPage;
        /**
         * The session parameters available to the bot at this point.
         */
        sessionParameters?: string;
        /**
         * The text responses from the agent for the turn.
         * Structure is documented below.
         */
        textResponses?: outputs.diagflow.CxTestCaseTestCaseConversationTurnVirtualAgentOutputTextResponse[];
        /**
         * The [Intent](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.intents#Intent) that triggered the response.
         * Structure is documented below.
         */
        triggeredIntent?: outputs.diagflow.CxTestCaseTestCaseConversationTurnVirtualAgentOutputTriggeredIntent;
    }

    export interface CxTestCaseTestCaseConversationTurnVirtualAgentOutputCurrentPage {
        /**
         * (Output)
         * The human-readable name of the page, unique within the flow.
         */
        displayName: string;
        /**
         * The unique identifier of the page.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         */
        name?: string;
    }

    export interface CxTestCaseTestCaseConversationTurnVirtualAgentOutputTextResponse {
        /**
         * A collection of text responses.
         */
        texts?: string[];
    }

    export interface CxTestCaseTestCaseConversationTurnVirtualAgentOutputTriggeredIntent {
        /**
         * (Output)
         * The human-readable name of the intent, unique within the agent.
         */
        displayName: string;
        /**
         * The unique identifier of the intent.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>.
         */
        name?: string;
    }

    export interface CxTestCaseTestConfig {
        /**
         * Flow name to start the test case with.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
         * Only one of flow and page should be set to indicate the starting point of the test case. If neither is set, the test case will start with start page on the default start flow.
         */
        flow?: string;
        /**
         * The page to start the test case with.
         * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
         * Only one of flow and page should be set to indicate the starting point of the test case. If neither is set, the test case will start with start page on the default start flow.
         */
        page?: string;
        /**
         * Session parameters to be compared when calculating differences.
         */
        trackingParameters?: string[];
    }

    export interface CxVersionNluSetting {
        /**
         * To filter out false positive results and still get variety in matched natural language inputs for your agent, you can tune the machine learning classification threshold. If the returned score value is less than the threshold value, then a no-match event will be triggered.
         * The score values range from 0.0 (completely uncertain) to 1.0 (completely certain). If set to 0.0, the default of 0.3 is used.
         */
        classificationThreshold?: number;
        /**
         * Indicates NLU model training mode.
         * * MODEL_TRAINING_MODE_AUTOMATIC: NLU model training is automatically triggered when a flow gets modified. User can also manually trigger model training in this mode.
         * * MODEL_TRAINING_MODE_MANUAL: User needs to manually trigger NLU model training. Best for large flows whose models take long time to train.
         * Possible values are `MODEL_TRAINING_MODE_AUTOMATIC` and `MODEL_TRAINING_MODE_MANUAL`.
         */
        modelTrainingMode?: string;
        /**
         * Indicates the type of NLU model.
         * * MODEL_TYPE_STANDARD: Use standard NLU model.
         * * MODEL_TYPE_ADVANCED: Use advanced NLU model.
         * Possible values are `MODEL_TYPE_STANDARD` and `MODEL_TYPE_ADVANCED`.
         */
        modelType?: string;
    }

    export interface CxWebhookGenericWebService {
        /**
         * Specifies a list of allowed custom CA certificates (in DER format) for HTTPS verification.
         */
        allowedCaCerts?: string[];
        /**
         * The HTTP request headers to send together with webhook requests.
         */
        requestHeaders?: {[key: string]: string};
        /**
         * Whether to use speech adaptation for speech recognition.
         */
        uri: string;
    }

    export interface CxWebhookServiceDirectory {
        /**
         * The name of Service Directory service.
         * Structure is documented below.
         */
        genericWebService: outputs.diagflow.CxWebhookServiceDirectoryGenericWebService;
        /**
         * The name of Service Directory service.
         */
        service: string;
    }

    export interface CxWebhookServiceDirectoryGenericWebService {
        /**
         * Specifies a list of allowed custom CA certificates (in DER format) for HTTPS verification.
         */
        allowedCaCerts?: string[];
        /**
         * The HTTP request headers to send together with webhook requests.
         */
        requestHeaders?: {[key: string]: string};
        /**
         * Whether to use speech adaptation for speech recognition.
         */
        uri: string;
    }

    export interface EntityTypeEntity {
        /**
         * A collection of value synonyms. For example, if the entity type is vegetable, and value is scallions, a synonym
         * could be green onions.
         * For KIND_LIST entity types:
         * * This collection must contain exactly one synonym equal to value.
         */
        synonyms: string[];
        /**
         * The primary value associated with this entity entry. For example, if the entity type is vegetable, the value
         * could be scallions.
         * For KIND_MAP entity types:
         * * A reference value to be used in place of synonyms.
         * For KIND_LIST entity types:
         * * A string that can contain references to other entity types (with or without aliases).
         */
        value: string;
    }

    export interface FulfillmentFeature {
        /**
         * The type of the feature that enabled for fulfillment.
         * * SMALLTALK: Fulfillment is enabled for SmallTalk.
         * Possible values are: `SMALLTALK`.
         */
        type: string;
    }

    export interface FulfillmentGenericWebService {
        /**
         * The password for HTTP Basic authentication.
         */
        password?: string;
        /**
         * The HTTP request headers to send together with fulfillment requests.
         */
        requestHeaders?: {[key: string]: string};
        /**
         * The fulfillment URI for receiving POST requests. It must use https protocol.
         */
        uri: string;
        /**
         * The user name for HTTP Basic authentication.
         */
        username?: string;
    }

    export interface IntentFollowupIntentInfo {
        /**
         * The unique identifier of the followup intent.
         * Format: projects/<Project ID>/agent/intents/<Intent ID>.
         */
        followupIntentName?: string;
        /**
         * The unique identifier of the parent intent in the chain of followup intents.
         * Format: projects/<Project ID>/agent/intents/<Intent ID>.
         */
        parentFollowupIntentName?: string;
    }

}

export namespace dns {
    export interface DnsManagedZoneIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DnsManagedZoneIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface GetKeysKeySigningKey {
        /**
         * String mnemonic specifying the DNSSEC algorithm of this key. Immutable after creation time. Possible values are `ecdsap256sha256`, `ecdsap384sha384`, `rsasha1`, `rsasha256`, and `rsasha512`.
         */
        algorithm: string;
        /**
         * The time that this resource was created in the control plane. This is in RFC3339 text format.
         */
        creationTime: string;
        /**
         * A mutable string of at most 1024 characters associated with this resource for the user's convenience.
         */
        description: string;
        /**
         * A list of cryptographic hashes of the DNSKEY resource record associated with this DnsKey. These digests are needed to construct a DS record that points at this DNS key. Each contains:
         */
        digests: outputs.dns.GetKeysKeySigningKeyDigest[];
        /**
         * The DS record based on the KSK record. This is used when [delegating](https://cloud.google.com/dns/docs/dnssec-advanced#subdelegation) DNSSEC-signed subdomains.
         */
        dsRecord: string;
        /**
         * Unique identifier for the resource; defined by the server.
         */
        id: string;
        /**
         * Active keys will be used to sign subsequent changes to the ManagedZone. Inactive keys will still be present as DNSKEY Resource Records for the use of resolvers validating existing signatures.
         */
        isActive: boolean;
        /**
         * Length of the key in bits. Specified at creation time then immutable.
         */
        keyLength: number;
        /**
         * The key tag is a non-cryptographic hash of the a DNSKEY resource record associated with this DnsKey. The key tag can be used to identify a DNSKEY more quickly (but it is not a unique identifier). In particular, the key tag is used in a parent zone's DS record to point at the DNSKEY in this child ManagedZone. The key tag is a number in the range [0, 65535] and the algorithm to calculate it is specified in RFC4034 Appendix B.
         */
        keyTag: number;
        /**
         * Base64 encoded public half of this key.
         */
        publicKey: string;
    }

    export interface GetKeysKeySigningKeyDigest {
        /**
         * The base-16 encoded bytes of this digest. Suitable for use in a DS resource record.
         */
        digest: string;
        /**
         * Specifies the algorithm used to calculate this digest. Possible values are `sha1`, `sha256` and `sha384`
         */
        type: string;
    }

    export interface GetKeysZoneSigningKey {
        /**
         * String mnemonic specifying the DNSSEC algorithm of this key. Immutable after creation time. Possible values are `ecdsap256sha256`, `ecdsap384sha384`, `rsasha1`, `rsasha256`, and `rsasha512`.
         */
        algorithm: string;
        /**
         * The time that this resource was created in the control plane. This is in RFC3339 text format.
         */
        creationTime: string;
        /**
         * A mutable string of at most 1024 characters associated with this resource for the user's convenience.
         */
        description: string;
        /**
         * A list of cryptographic hashes of the DNSKEY resource record associated with this DnsKey. These digests are needed to construct a DS record that points at this DNS key. Each contains:
         */
        digests: outputs.dns.GetKeysZoneSigningKeyDigest[];
        /**
         * Unique identifier for the resource; defined by the server.
         */
        id: string;
        /**
         * Active keys will be used to sign subsequent changes to the ManagedZone. Inactive keys will still be present as DNSKEY Resource Records for the use of resolvers validating existing signatures.
         */
        isActive: boolean;
        /**
         * Length of the key in bits. Specified at creation time then immutable.
         */
        keyLength: number;
        /**
         * The key tag is a non-cryptographic hash of the a DNSKEY resource record associated with this DnsKey. The key tag can be used to identify a DNSKEY more quickly (but it is not a unique identifier). In particular, the key tag is used in a parent zone's DS record to point at the DNSKEY in this child ManagedZone. The key tag is a number in the range [0, 65535] and the algorithm to calculate it is specified in RFC4034 Appendix B.
         */
        keyTag: number;
        /**
         * Base64 encoded public half of this key.
         */
        publicKey: string;
    }

    export interface GetKeysZoneSigningKeyDigest {
        /**
         * The base-16 encoded bytes of this digest. Suitable for use in a DS resource record.
         */
        digest: string;
        /**
         * Specifies the algorithm used to calculate this digest. Possible values are `sha1`, `sha256` and `sha384`
         */
        type: string;
    }

    export interface ManagedZoneCloudLoggingConfig {
        /**
         * If set, enable query logging for this ManagedZone. False by default, making logging opt-in.
         */
        enableLogging: boolean;
    }

    export interface ManagedZoneDnssecConfig {
        /**
         * Specifies parameters that will be used for generating initial DnsKeys
         * for this ManagedZone. If you provide a spec for keySigning or zoneSigning,
         * you must also provide one for the other.
         * defaultKeySpecs can only be updated when the state is `off`.
         * Structure is documented below.
         */
        defaultKeySpecs: outputs.dns.ManagedZoneDnssecConfigDefaultKeySpec[];
        /**
         * Identifies what kind of resource this is
         */
        kind?: string;
        /**
         * Specifies the mechanism used to provide authenticated denial-of-existence responses.
         * nonExistence can only be updated when the state is `off`.
         * Possible values are: `nsec`, `nsec3`.
         */
        nonExistence: string;
        /**
         * Specifies whether DNSSEC is enabled, and what mode it is in
         * Possible values are: `off`, `on`, `transfer`.
         */
        state?: string;
    }

    export interface ManagedZoneDnssecConfigDefaultKeySpec {
        /**
         * String mnemonic specifying the DNSSEC algorithm of this key
         * Possible values are: `ecdsap256sha256`, `ecdsap384sha384`, `rsasha1`, `rsasha256`, `rsasha512`.
         */
        algorithm?: string;
        /**
         * Length of the keys in bits
         */
        keyLength?: number;
        /**
         * Specifies whether this is a key signing key (KSK) or a zone
         * signing key (ZSK). Key signing keys have the Secure Entry
         * Point flag set and, when active, will only be used to sign
         * resource record sets of type DNSKEY. Zone signing keys do
         * not have the Secure Entry Point flag set and will be used
         * to sign all other types of resource record sets.
         * Possible values are: `keySigning`, `zoneSigning`.
         */
        keyType?: string;
        /**
         * Identifies what kind of resource this is
         */
        kind?: string;
    }

    export interface ManagedZoneForwardingConfig {
        /**
         * List of target name servers to forward to. Cloud DNS will
         * select the best available name server if more than
         * one target is given.
         * Structure is documented below.
         */
        targetNameServers: outputs.dns.ManagedZoneForwardingConfigTargetNameServer[];
    }

    export interface ManagedZoneForwardingConfigTargetNameServer {
        /**
         * Forwarding path for this TargetNameServer. If unset or `default` Cloud DNS will make forwarding
         * decision based on address ranges, i.e. RFC1918 addresses go to the VPC, Non-RFC1918 addresses go
         * to the Internet. When set to `private`, Cloud DNS will always send queries through VPC for this target
         * Possible values are: `default`, `private`.
         */
        forwardingPath?: string;
        /**
         * IPv4 address of a target name server.
         */
        ipv4Address: string;
    }

    export interface ManagedZonePeeringConfig {
        /**
         * The network with which to peer.
         * Structure is documented below.
         */
        targetNetwork: outputs.dns.ManagedZonePeeringConfigTargetNetwork;
    }

    export interface ManagedZonePeeringConfigTargetNetwork {
        /**
         * The id or fully qualified URL of the VPC network to forward queries to.
         * This should be formatted like `projects/{project}/global/networks/{network}` or
         * `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`
         */
        networkUrl: string;
    }

    export interface ManagedZonePrivateVisibilityConfig {
        /**
         * The list of Google Kubernetes Engine clusters that can see this zone.
         * Structure is documented below.
         */
        gkeClusters?: outputs.dns.ManagedZonePrivateVisibilityConfigGkeCluster[];
        networks?: outputs.dns.ManagedZonePrivateVisibilityConfigNetwork[];
    }

    export interface ManagedZonePrivateVisibilityConfigGkeCluster {
        /**
         * The resource name of the cluster to bind this ManagedZone to.
         * This should be specified in the format like
         * `projects/*&#47;locations/*&#47;clusters/*`
         */
        gkeClusterName: string;
    }

    export interface ManagedZonePrivateVisibilityConfigNetwork {
        /**
         * The id or fully qualified URL of the VPC network to bind to.
         * This should be formatted like `projects/{project}/global/networks/{network}` or
         * `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`
         */
        networkUrl: string;
    }

    export interface ManagedZoneServiceDirectoryConfig {
        /**
         * The namespace associated with the zone.
         * Structure is documented below.
         */
        namespace: outputs.dns.ManagedZoneServiceDirectoryConfigNamespace;
    }

    export interface ManagedZoneServiceDirectoryConfigNamespace {
        /**
         * The fully qualified or partial URL of the service directory namespace that should be
         * associated with the zone. This should be formatted like
         * `https://servicedirectory.googleapis.com/v1/projects/{project}/locations/{location}/namespaces/{namespace_id}`
         * or simply `projects/{project}/locations/{location}/namespaces/{namespace_id}`
         * Ignored for `public` visibility zones.
         */
        namespaceUrl: string;
    }

    export interface PolicyAlternativeNameServerConfig {
        /**
         * Sets an alternative name server for the associated networks. When specified,
         * all DNS queries are forwarded to a name server that you choose. Names such as .internal
         * are not available when an alternative name server is specified.
         * Structure is documented below.
         */
        targetNameServers: outputs.dns.PolicyAlternativeNameServerConfigTargetNameServer[];
    }

    export interface PolicyAlternativeNameServerConfigTargetNameServer {
        /**
         * Forwarding path for this TargetNameServer. If unset or `default` Cloud DNS will make forwarding
         * decision based on address ranges, i.e. RFC1918 addresses go to the VPC, Non-RFC1918 addresses go
         * to the Internet. When set to `private`, Cloud DNS will always send queries through VPC for this target
         * Possible values are: `default`, `private`.
         */
        forwardingPath?: string;
        /**
         * IPv4 address to forward to.
         */
        ipv4Address: string;
    }

    export interface PolicyNetwork {
        /**
         * The id or fully qualified URL of the VPC network to forward queries to.
         * This should be formatted like `projects/{project}/global/networks/{network}` or
         * `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`
         */
        networkUrl: string;
    }

    export interface RecordSetRoutingPolicy {
        /**
         * Specifies whether to enable fencing for geo queries.
         */
        enableGeoFencing?: boolean;
        /**
         * The configuration for Geolocation based routing policy.
         * Structure is document below.
         */
        geos?: outputs.dns.RecordSetRoutingPolicyGeo[];
        /**
         * The configuration for a primary-backup policy with global to regional failover. Queries are responded to with the global primary targets, but if none of the primary targets are healthy, then we fallback to a regional failover policy.
         * Structure is document below.
         */
        primaryBackup?: outputs.dns.RecordSetRoutingPolicyPrimaryBackup;
        /**
         * The configuration for Weighted Round Robin based routing policy.
         * Structure is document below.
         */
        wrrs?: outputs.dns.RecordSetRoutingPolicyWrr[];
    }

    export interface RecordSetRoutingPolicyGeo {
        /**
         * For A and AAAA types only. The list of targets to be health checked. These can be specified along with `rrdatas` within this item.
         * Structure is document below.
         */
        healthCheckedTargets?: outputs.dns.RecordSetRoutingPolicyGeoHealthCheckedTargets;
        /**
         * The location name defined in Google Cloud.
         */
        location: string;
        /**
         * Same as `rrdatas` above.
         */
        rrdatas?: string[];
    }

    export interface RecordSetRoutingPolicyGeoHealthCheckedTargets {
        /**
         * The list of internal load balancers to health check.
         * Structure is document below.
         */
        internalLoadBalancers: outputs.dns.RecordSetRoutingPolicyGeoHealthCheckedTargetsInternalLoadBalancer[];
    }

    export interface RecordSetRoutingPolicyGeoHealthCheckedTargetsInternalLoadBalancer {
        /**
         * The frontend IP address of the load balancer.
         */
        ipAddress: string;
        /**
         * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
         */
        ipProtocol: string;
        /**
         * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
         */
        loadBalancerType: string;
        /**
         * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `projects/{project}/global/networks/{network}` or `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
         */
        networkUrl: string;
        /**
         * The configured port of the load balancer.
         */
        port: string;
        /**
         * The ID of the project in which the load balancer belongs.
         */
        project: string;
        /**
         * The region of the load balancer. Only needed for regional load balancers.
         */
        region?: string;
    }

    export interface RecordSetRoutingPolicyPrimaryBackup {
        /**
         * The backup geo targets, which provide a regional failover policy for the otherwise global primary targets.
         * Structure is document above.
         */
        backupGeos: outputs.dns.RecordSetRoutingPolicyPrimaryBackupBackupGeo[];
        /**
         * Specifies whether to enable fencing for backup geo queries.
         */
        enableGeoFencingForBackups?: boolean;
        /**
         * The list of global primary targets to be health checked.
         * Structure is document below.
         */
        primary: outputs.dns.RecordSetRoutingPolicyPrimaryBackupPrimary;
        /**
         * Specifies the percentage of traffic to send to the backup targets even when the primary targets are healthy.
         */
        trickleRatio?: number;
    }

    export interface RecordSetRoutingPolicyPrimaryBackupBackupGeo {
        /**
         * The list of targets to be health checked. Note that if DNSSEC is enabled for this zone, only one of `rrdatas` or `healthCheckedTargets` can be set.
         * Structure is document below.
         *
         * Structure is document below.
         */
        healthCheckedTargets?: outputs.dns.RecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargets;
        /**
         * The location name defined in Google Cloud.
         */
        location: string;
        rrdatas?: string[];
    }

    export interface RecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargets {
        /**
         * The list of internal load balancers to health check.
         * Structure is document below.
         */
        internalLoadBalancers: outputs.dns.RecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargetsInternalLoadBalancer[];
    }

    export interface RecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargetsInternalLoadBalancer {
        /**
         * The frontend IP address of the load balancer.
         */
        ipAddress: string;
        /**
         * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
         */
        ipProtocol: string;
        /**
         * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
         */
        loadBalancerType: string;
        /**
         * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `projects/{project}/global/networks/{network}` or `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
         */
        networkUrl: string;
        /**
         * The configured port of the load balancer.
         */
        port: string;
        /**
         * The ID of the project in which the load balancer belongs.
         */
        project: string;
        /**
         * The region of the load balancer. Only needed for regional load balancers.
         */
        region?: string;
    }

    export interface RecordSetRoutingPolicyPrimaryBackupPrimary {
        /**
         * The list of internal load balancers to health check.
         * Structure is document below.
         */
        internalLoadBalancers: outputs.dns.RecordSetRoutingPolicyPrimaryBackupPrimaryInternalLoadBalancer[];
    }

    export interface RecordSetRoutingPolicyPrimaryBackupPrimaryInternalLoadBalancer {
        /**
         * The frontend IP address of the load balancer.
         */
        ipAddress: string;
        /**
         * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
         */
        ipProtocol: string;
        /**
         * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
         */
        loadBalancerType: string;
        /**
         * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `projects/{project}/global/networks/{network}` or `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
         */
        networkUrl: string;
        /**
         * The configured port of the load balancer.
         */
        port: string;
        /**
         * The ID of the project in which the load balancer belongs.
         */
        project: string;
        /**
         * The region of the load balancer. Only needed for regional load balancers.
         */
        region?: string;
    }

    export interface RecordSetRoutingPolicyWrr {
        /**
         * The list of targets to be health checked. Note that if DNSSEC is enabled for this zone, only one of `rrdatas` or `healthCheckedTargets` can be set.
         * Structure is document below.
         */
        healthCheckedTargets?: outputs.dns.RecordSetRoutingPolicyWrrHealthCheckedTargets;
        /**
         * Same as `rrdatas` above.
         */
        rrdatas?: string[];
        /**
         * The ratio of traffic routed to the target.
         */
        weight: number;
    }

    export interface RecordSetRoutingPolicyWrrHealthCheckedTargets {
        /**
         * The list of internal load balancers to health check.
         * Structure is document below.
         */
        internalLoadBalancers: outputs.dns.RecordSetRoutingPolicyWrrHealthCheckedTargetsInternalLoadBalancer[];
    }

    export interface RecordSetRoutingPolicyWrrHealthCheckedTargetsInternalLoadBalancer {
        /**
         * The frontend IP address of the load balancer.
         */
        ipAddress: string;
        /**
         * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
         */
        ipProtocol: string;
        /**
         * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
         */
        loadBalancerType: string;
        /**
         * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `projects/{project}/global/networks/{network}` or `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
         */
        networkUrl: string;
        /**
         * The configured port of the load balancer.
         */
        port: string;
        /**
         * The ID of the project in which the load balancer belongs.
         */
        project: string;
        /**
         * The region of the load balancer. Only needed for regional load balancers.
         */
        region?: string;
    }

    export interface ResponsePolicyGkeCluster {
        /**
         * The resource name of the cluster to bind this ManagedZone to.
         * This should be specified in the format like
         * `projects/*&#47;locations/*&#47;clusters/*`
         */
        gkeClusterName: string;
    }

    export interface ResponsePolicyNetwork {
        /**
         * The fully qualified URL of the VPC network to bind to.
         * This should be formatted like
         * `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`
         */
        networkUrl: string;
    }

    export interface ResponsePolicyRuleLocalData {
        /**
         * All resource record sets for this selector, one per resource record type. The name must match the dns_name.
         * Structure is documented below.
         */
        localDatas: outputs.dns.ResponsePolicyRuleLocalDataLocalData[];
    }

    export interface ResponsePolicyRuleLocalDataLocalData {
        /**
         * For example, www.example.com.
         */
        name: string;
        /**
         * As defined in RFC 1035 (section 5) and RFC 1034 (section 3.6.1)
         */
        rrdatas?: string[];
        /**
         * Number of seconds that this ResourceRecordSet can be cached by
         * resolvers.
         */
        ttl?: number;
        /**
         * One of valid DNS resource types.
         * Possible values are: `A`, `AAAA`, `CAA`, `CNAME`, `DNSKEY`, `DS`, `HTTPS`, `IPSECVPNKEY`, `MX`, `NAPTR`, `NS`, `PTR`, `SOA`, `SPF`, `SRV`, `SSHFP`, `SVCB`, `TLSA`, `TXT`.
         */
        type: string;
    }

}

export namespace endpoints {
    export interface ConsumersIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ConsumersIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServiceApi {
        /**
         * A list of Method objects; structure is documented below.
         */
        methods: outputs.endpoints.ServiceApiMethod[];
        /**
         * The simple name of the endpoint as described in the config.
         */
        name: string;
        /**
         * `SYNTAX_PROTO2` or `SYNTAX_PROTO3`.
         */
        syntax: string;
        /**
         * A version string for this api. If specified, will have the form major-version.minor-version, e.g. `1.10`.
         */
        version: string;
    }

    export interface ServiceApiMethod {
        /**
         * The simple name of the endpoint as described in the config.
         */
        name: string;
        /**
         * The type URL for the request to this API.
         */
        requestType: string;
        /**
         * The type URL for the response from this API.
         */
        responseType: string;
        /**
         * `SYNTAX_PROTO2` or `SYNTAX_PROTO3`.
         */
        syntax: string;
    }

    export interface ServiceEndpoint {
        /**
         * The FQDN of the endpoint as described in the config.
         */
        address: string;
        /**
         * The simple name of the endpoint as described in the config.
         */
        name: string;
    }

    export interface ServiceIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServiceIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace essentialcontacts {
    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinition {
        /**
         * Date time property. Not supported by CMEK compliant deployment.
         */
        dateTimeTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionDateTimeTypeOptions;
        /**
         * The display-name for the property, used for front-end.
         */
        displayName?: string;
        /**
         * Enum/categorical property.
         * Structure is documented below.
         */
        enumTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionEnumTypeOptions;
        /**
         * Float property.
         */
        floatTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionFloatTypeOptions;
        /**
         * Integer property.
         */
        integerTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionIntegerTypeOptions;
        /**
         * Whether the property can be filtered. If this is a sub-property, all the parent properties must be marked filterable.
         */
        isFilterable?: boolean;
        /**
         * Whether the property is user supplied metadata.
         */
        isMetadata?: boolean;
        /**
         * Whether the property can have multiple values.
         */
        isRepeatable?: boolean;
        /**
         * Whether the property is mandatory.
         */
        isRequired?: boolean;
        /**
         * Indicates that the property should be included in a global search.
         */
        isSearchable?: boolean;
        /**
         * Map property.
         */
        mapTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionMapTypeOptions;
        /**
         * The name of the metadata property.
         */
        name: string;
        /**
         * Nested structured data property.
         * Structure is documented below.
         */
        propertyTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptions;
        /**
         * Stores the retrieval importance.
         * Possible values are: `HIGHEST`, `HIGHER`, `HIGH`, `MEDIUM`, `LOW`, `LOWEST`.
         */
        retrievalImportance?: string;
        /**
         * The schema source information.
         * Structure is documented below.
         */
        schemaSources?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionSchemaSource[];
        /**
         * Text property.
         */
        textTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionTextTypeOptions;
        /**
         * Timestamp property. Not supported by CMEK compliant deployment.
         */
        timestampTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionTimestampTypeOptions;
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionDateTimeTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionEnumTypeOptions {
        /**
         * List of possible enum values.
         */
        possibleValues: string[];
        /**
         * Make sure the enum property value provided in the document is in the possile value list during document creation. The validation check runs by default.
         *
         * - - -
         */
        validationCheckDisabled?: boolean;
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionFloatTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionIntegerTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionMapTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptions {
        /**
         * Defines the metadata for a schema property.
         * Structure is documented below.
         */
        propertyDefinitions: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinition[];
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinition {
        /**
         * Date time property. Not supported by CMEK compliant deployment.
         */
        dateTimeTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionDateTimeTypeOptions;
        /**
         * The display-name for the property, used for front-end.
         */
        displayName?: string;
        /**
         * Enum/categorical property.
         * Structure is documented below.
         */
        enumTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionEnumTypeOptions;
        /**
         * Float property.
         */
        floatTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionFloatTypeOptions;
        /**
         * Integer property.
         */
        integerTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionIntegerTypeOptions;
        /**
         * Whether the property can be filtered. If this is a sub-property, all the parent properties must be marked filterable.
         */
        isFilterable?: boolean;
        /**
         * Whether the property is user supplied metadata.
         */
        isMetadata?: boolean;
        /**
         * Whether the property can have multiple values.
         */
        isRepeatable?: boolean;
        /**
         * Whether the property is mandatory.
         */
        isRequired?: boolean;
        /**
         * Indicates that the property should be included in a global search.
         */
        isSearchable?: boolean;
        /**
         * Map property.
         */
        mapTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionMapTypeOptions;
        /**
         * The name of the metadata property.
         */
        name: string;
        /**
         * Stores the retrieval importance.
         * Possible values are: `HIGHEST`, `HIGHER`, `HIGH`, `MEDIUM`, `LOW`, `LOWEST`.
         */
        retrievalImportance?: string;
        /**
         * The schema source information.
         * Structure is documented below.
         */
        schemaSources?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionSchemaSource[];
        /**
         * Text property.
         */
        textTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTextTypeOptions;
        /**
         * Timestamp property. Not supported by CMEK compliant deployment.
         */
        timestampTypeOptions?: outputs.essentialcontacts.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTimestampTypeOptions;
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionDateTimeTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionEnumTypeOptions {
        /**
         * List of possible enum values.
         */
        possibleValues: string[];
        /**
         * Make sure the enum property value provided in the document is in the possile value list during document creation. The validation check runs by default.
         *
         * - - -
         */
        validationCheckDisabled?: boolean;
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionFloatTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionIntegerTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionMapTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionSchemaSource {
        /**
         * The schema name in the source.
         */
        name?: string;
        /**
         * The Doc AI processor type name.
         */
        processorType?: string;
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTextTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTimestampTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionSchemaSource {
        /**
         * The schema name in the source.
         */
        name?: string;
        /**
         * The Doc AI processor type name.
         */
        processorType?: string;
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionTextTypeOptions {
    }

    export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionTimestampTypeOptions {
    }

}

export namespace eventarc {
    export interface TriggerDestination {
        /**
         * [WARNING] Configuring a Cloud Function in Trigger is not supported as of today. The Cloud Function resource name. Format: projects/{project}/locations/{location}/functions/{function}
         */
        cloudFunction?: string;
        /**
         * Cloud Run fully-managed service that receives the events. The service should be running in the same project of the trigger.
         */
        cloudRunService?: outputs.eventarc.TriggerDestinationCloudRunService;
        /**
         * A GKE service capable of receiving events. The service should be running in the same project as the trigger.
         */
        gke?: outputs.eventarc.TriggerDestinationGke;
        /**
         * The resource name of the Workflow whose Executions are triggered by the events. The Workflow resource should be deployed in the same project as the trigger. Format: `projects/{project}/locations/{location}/workflows/{workflow}`
         */
        workflow?: string;
    }

    export interface TriggerDestinationCloudRunService {
        /**
         * Optional. The relative path on the Cloud Run service the events should be sent to. The value must conform to the definition of URI path segment (section 3.3 of RFC2396). Examples: "/route", "route", "route/subroute".
         */
        path?: string;
        /**
         * Required. The region the Cloud Run service is deployed in.
         */
        region: string;
        /**
         * Required. The name of the Cloud Run service being addressed. See https://cloud.google.com/run/docs/reference/rest/v1/namespaces.services. Only services located in the same project of the trigger object can be addressed.
         */
        service: string;
    }

    export interface TriggerDestinationGke {
        /**
         * Required. The name of the cluster the GKE service is running in. The cluster must be running in the same project as the trigger being created.
         */
        cluster: string;
        /**
         * Required. The name of the Google Compute Engine in which the cluster resides, which can either be compute zone (for example, us-central1-a) for the zonal clusters or region (for example, us-central1) for regional clusters.
         */
        location: string;
        /**
         * Required. The namespace the GKE service is running in.
         */
        namespace: string;
        /**
         * Optional. The relative path on the GKE service the events should be sent to. The value must conform to the definition of a URI path segment (section 3.3 of RFC2396). Examples: "/route", "route", "route/subroute".
         */
        path?: string;
        /**
         * Required. Name of the GKE service.
         */
        service: string;
    }

    export interface TriggerMatchingCriteria {
        /**
         * Required. The name of a CloudEvents attribute. Currently, only a subset of attributes are supported for filtering. All triggers MUST provide a filter for the 'type' attribute.
         */
        attribute: string;
        /**
         * Optional. The operator used for matching the events with the value of the filter. If not specified, only events that have an exact key-value pair specified in the filter are matched. The only allowed value is `match-path-pattern`.
         */
        operator?: string;
        /**
         * Required. The value for the attribute. See https://cloud.google.com/eventarc/docs/creating-triggers#trigger-gcloud for available values.
         *
         * - - -
         */
        value: string;
    }

    export interface TriggerTransport {
        /**
         * The Pub/Sub topic and subscription used by Eventarc as delivery intermediary.
         */
        pubsubs?: outputs.eventarc.TriggerTransportPubsub[];
    }

    export interface TriggerTransportPubsub {
        /**
         * Output only. The name of the Pub/Sub subscription created and managed by Eventarc system as a transport for the event delivery. Format: `projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}`.
         */
        subscription: string;
        /**
         * Optional. The name of the Pub/Sub topic created and managed by Eventarc system as a transport for the event delivery. Format: `projects/{PROJECT_ID}/topics/{TOPIC_NAME}. You may set an existing topic for triggers of the type google.cloud.pubsub.topic.v1.messagePublished` only. The topic you provide here will not be deleted by Eventarc at trigger deletion.
         */
        topic?: string;
    }

}

export namespace filestore {
    export interface InstanceFileShares {
        /**
         * File share capacity in GiB. This must be at least 1024 GiB
         * for the standard tier, or 2560 GiB for the premium tier.
         */
        capacityGb: number;
        /**
         * The name of the fileshare (16 characters or less)
         */
        name: string;
        /**
         * Nfs Export Options. There is a limit of 10 export options per file share.
         * Structure is documented below.
         */
        nfsExportOptions?: outputs.filestore.InstanceFileSharesNfsExportOption[];
        /**
         * (Output)
         * The resource name of the backup, in the format
         * projects/{projectId}/locations/{locationId}/backups/{backupId},
         * that this file share has been restored from.
         */
        sourceBackup: string;
    }

    export interface InstanceFileSharesNfsExportOption {
        /**
         * Either READ_ONLY, for allowing only read requests on the exported directory,
         * or READ_WRITE, for allowing both read and write requests. The default is READ_WRITE.
         * Default value is `READ_WRITE`.
         * Possible values are: `READ_ONLY`, `READ_WRITE`.
         */
        accessMode?: string;
        /**
         * An integer representing the anonymous group id with a default value of 65534.
         * Anon_gid may only be set with squashMode of ROOT_SQUASH. An error will be returned
         * if this field is specified for other squashMode settings.
         */
        anonGid?: number;
        /**
         * An integer representing the anonymous user id with a default value of 65534.
         * Anon_uid may only be set with squashMode of ROOT_SQUASH. An error will be returned
         * if this field is specified for other squashMode settings.
         */
        anonUid?: number;
        /**
         * List of either IPv4 addresses, or ranges in CIDR notation which may mount the file share.
         * Overlapping IP ranges are not allowed, both within and across NfsExportOptions. An error will be returned.
         * The limit is 64 IP ranges/addresses for each FileShareConfig among all NfsExportOptions.
         */
        ipRanges?: string[];
        /**
         * Either NO_ROOT_SQUASH, for allowing root access on the exported directory, or ROOT_SQUASH,
         * for not allowing root access. The default is NO_ROOT_SQUASH.
         * Default value is `NO_ROOT_SQUASH`.
         * Possible values are: `NO_ROOT_SQUASH`, `ROOT_SQUASH`.
         */
        squashMode?: string;
    }

    export interface InstanceNetwork {
        /**
         * The network connect mode of the Filestore instance.
         * If not provided, the connect mode defaults to
         * DIRECT_PEERING.
         * Default value is `DIRECT_PEERING`.
         * Possible values are: `DIRECT_PEERING`, `PRIVATE_SERVICE_ACCESS`.
         *
         * - - -
         */
        connectMode?: string;
        /**
         * (Output)
         * A list of IPv4 or IPv6 addresses.
         */
        ipAddresses: string[];
        /**
         * IP versions for which the instance has
         * IP addresses assigned.
         * Each value may be one of: `ADDRESS_MODE_UNSPECIFIED`, `MODE_IPV4`, `MODE_IPV6`.
         */
        modes: string[];
        /**
         * The name of the GCE VPC network to which the
         * instance is connected.
         */
        network: string;
        /**
         * A /29 CIDR block that identifies the range of IP
         * addresses reserved for this instance.
         */
        reservedIpRange: string;
    }

}

export namespace firebase {
    export interface ExtensionsInstanceConfig {
        /**
         * List of extension events selected by consumer that extension is allowed to
         * emit, identified by their types.
         */
        allowedEventTypes?: string[];
        /**
         * (Output)
         * The time at which the Extension Instance Config was created.
         */
        createTime: string;
        /**
         * Fully qualified Eventarc resource name that consumers should use for event triggers.
         */
        eventarcChannel: string;
        /**
         * The ref of the Extension from the Registry (e.g. publisher-id/awesome-extension)
         */
        extensionRef: string;
        /**
         * The version of the Extension from the Registry (e.g. 1.0.3). If left blank, latest is assumed.
         */
        extensionVersion: string;
        /**
         * (Output)
         * The unique identifier for this configuration.
         */
        name: string;
        /**
         * Environment variables that may be configured for the Extension
         */
        params: {[key: string]: string};
        /**
         * (Output)
         * Postinstall instructions to be shown for this Extension, with
         * template strings representing function and parameter values substituted
         * with actual values. These strings include: ${param:FOO},
         * ${function:myFunc.url},
         * ${function:myFunc.name}, and ${function:myFunc.location}
         *
         * - - -
         */
        populatedPostinstallContent: string;
        /**
         * Params whose values are only available at deployment time.
         * Unlike other params, these will not be set as environment variables on
         * functions.
         */
        systemParams: {[key: string]: string};
    }

    export interface ExtensionsInstanceErrorStatus {
        /**
         * The status code, which should be an enum value of google.rpc.Code.
         */
        code?: number;
        /**
         * A list of messages that carry the error details.
         */
        details?: {[key: string]: any}[];
        /**
         * A developer-facing error message, which should be in English.
         */
        message?: string;
    }

    export interface ExtensionsInstanceRuntimeData {
        /**
         * The fatal error state for the extension instance
         * Structure is documented below.
         */
        fatalError?: outputs.firebase.ExtensionsInstanceRuntimeDataFatalError;
        /**
         * The processing state for the extension instance
         * Structure is documented below.
         */
        processingState?: outputs.firebase.ExtensionsInstanceRuntimeDataProcessingState;
        /**
         * The time of the last state update.
         */
        stateUpdateTime?: string;
    }

    export interface ExtensionsInstanceRuntimeDataFatalError {
        /**
         * The error message. This is set by the extension developer to give
         * more detail on why the extension is unusable and must be re-installed
         * or reconfigured.
         */
        errorMessage?: string;
    }

    export interface ExtensionsInstanceRuntimeDataProcessingState {
        /**
         * Details about the processing. e.g. This could include the type of
         * processing in progress or it could list errors or failures.
         * This information will be shown in the console on the detail page
         * for the extension instance.
         */
        detailMessage?: string;
        /**
         * The processing state of the extension instance.
         */
        state?: string;
    }

    export interface HostingVersionConfig {
        /**
         * An array of objects (called redirect rules), where each rule specifies a URL pattern that, if matched to the request URL path,
         * triggers Hosting to respond with a redirect to the specified destination path.
         * Structure is documented below.
         */
        redirects?: outputs.firebase.HostingVersionConfigRedirect[];
        /**
         * An array of objects (called rewrite rules), where each rule specifies a URL pattern that, if matched to the
         * request URL path, triggers Hosting to respond as if the service were given the specified destination URL.
         * Structure is documented below.
         */
        rewrites?: outputs.firebase.HostingVersionConfigRewrite[];
    }

    export interface HostingVersionConfigRedirect {
        /**
         * The user-supplied glob to match against the request URL path.
         */
        glob?: string;
        /**
         * The value to put in the HTTP location header of the response.
         * The location can contain capture group values from the pattern using a : prefix to identify
         * the segment and an optional * to capture the rest of the URL. For example:
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        location: string;
        /**
         * The user-supplied RE2 regular expression to match against the request URL path.
         */
        regex?: string;
        /**
         * The status HTTP code to return in the response. It must be a valid 3xx status code.
         */
        statusCode: number;
    }

    export interface HostingVersionConfigRewrite {
        /**
         * The function to proxy requests to. Must match the exported function name exactly.
         */
        function?: string;
        /**
         * The user-supplied glob to match against the request URL path.
         */
        glob?: string;
        /**
         * The user-supplied RE2 regular expression to match against the request URL path.
         */
        regex?: string;
        /**
         * The request will be forwarded to Cloud Run.
         * Structure is documented below.
         */
        run?: outputs.firebase.HostingVersionConfigRewriteRun;
    }

    export interface HostingVersionConfigRewriteRun {
        /**
         * Optional. User-provided region where the Cloud Run service is hosted. Defaults to `us-central1` if not supplied.
         */
        region?: string;
        /**
         * User-defined ID of the Cloud Run service.
         */
        serviceId: string;
    }

}

export namespace firebaserules {
    export interface RulesetMetadata {
        services: string[];
    }

    export interface RulesetSource {
        /**
         * `File` set constituting the `Source` bundle.
         */
        files: outputs.firebaserules.RulesetSourceFile[];
        /**
         * `Language` of the `Source` bundle. If unspecified, the language will default to `FIREBASE_RULES`. Possible values: LANGUAGE_UNSPECIFIED, FIREBASE_RULES, EVENT_FLOW_TRIGGERS
         */
        language?: string;
    }

    export interface RulesetSourceFile {
        /**
         * Textual Content.
         */
        content: string;
        /**
         * Fingerprint (e.g. github sha) associated with the `File`.
         */
        fingerprint?: string;
        /**
         * File name.
         *
         * - - -
         */
        name: string;
    }

}

export namespace firestore {
    export interface FieldIndexConfig {
        /**
         * The indexes to configure on the field. Order or array contains must be specified.
         * Structure is documented below.
         */
        indexes?: outputs.firestore.FieldIndexConfigIndex[];
    }

    export interface FieldIndexConfigIndex {
        /**
         * Indicates that this field supports operations on arrayValues. Only one of `order` and `arrayConfig` can
         * be specified.
         * Possible values are: `CONTAINS`.
         */
        arrayConfig?: string;
        /**
         * Indicates that this field supports ordering by the specified order or comparing using =, <, <=, >, >=, !=.
         * Only one of `order` and `arrayConfig` can be specified.
         * Possible values are: `ASCENDING`, `DESCENDING`.
         */
        order?: string;
        /**
         * The scope at which a query is run. Collection scoped queries require you specify
         * the collection at query time. Collection group scope allows queries across all
         * collections with the same id.
         * Default value is `COLLECTION`.
         * Possible values are: `COLLECTION`, `COLLECTION_GROUP`.
         */
        queryScope?: string;
    }

    export interface FieldTtlConfig {
        /**
         * (Output)
         * The state of the TTL configuration.
         */
        state: string;
    }

    export interface IndexField {
        /**
         * Indicates that this field supports operations on arrayValues. Only one of `order` and `arrayConfig` can
         * be specified.
         * Possible values are: `CONTAINS`.
         *
         * - - -
         */
        arrayConfig?: string;
        /**
         * Name of the field.
         */
        fieldPath?: string;
        /**
         * Indicates that this field supports ordering by the specified order or comparing using =, <, <=, >, >=.
         * Only one of `order` and `arrayConfig` can be specified.
         * Possible values are: `ASCENDING`, `DESCENDING`.
         */
        order?: string;
    }

}

export namespace folder {
    export interface AccessApprovalSettingsEnrolledService {
        /**
         * The product for which Access Approval will be enrolled. Allowed values are listed (case-sensitive):
         * * all
         * * App Engine
         * * BigQuery
         * * Cloud Bigtable
         * * Cloud Key Management Service
         * * Compute Engine
         * * Cloud Dataflow
         * * Cloud Identity and Access Management
         * * Cloud Pub/Sub
         * * Cloud Storage
         * * Persistent Disk
         * Note: These values are supported as input, but considered a legacy format:
         * * all
         * * appengine.googleapis.com
         * * bigquery.googleapis.com
         * * bigtable.googleapis.com
         * * cloudkms.googleapis.com
         * * compute.googleapis.com
         * * dataflow.googleapis.com
         * * iam.googleapis.com
         * * pubsub.googleapis.com
         * * storage.googleapis.com
         */
        cloudProduct: string;
        /**
         * The enrollment level of the service.
         * Default value is `BLOCK_ALL`.
         * Possible values are: `BLOCK_ALL`.
         *
         * - - -
         */
        enrollmentLevel?: string;
    }

    export interface GetOrganizationPolicyBooleanPolicy {
        enforced: boolean;
    }

    export interface GetOrganizationPolicyListPolicy {
        allows: outputs.folder.GetOrganizationPolicyListPolicyAllow[];
        denies: outputs.folder.GetOrganizationPolicyListPolicyDeny[];
        inheritFromParent: boolean;
        suggestedValue: string;
    }

    export interface GetOrganizationPolicyListPolicyAllow {
        all: boolean;
        values: string[];
    }

    export interface GetOrganizationPolicyListPolicyDeny {
        all: boolean;
        values: string[];
    }

    export interface GetOrganizationPolicyRestorePolicy {
        default: boolean;
    }

    export interface IAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface IAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface IamAuditConfigAuditLogConfig {
        /**
         * Identities that do not cause logging for this type of permission.  The format is the same as that for `members`.
         */
        exemptedMembers?: string[];
        /**
         * Permission type for which logging is to be configured.  Must be one of `DATA_READ`, `DATA_WRITE`, or `ADMIN_READ`.
         */
        logType: string;
    }

    export interface OrganizationPolicyBooleanPolicy {
        /**
         * If true, then the Policy is enforced. If false, then any configuration is acceptable.
         */
        enforced: boolean;
    }

    export interface OrganizationPolicyListPolicy {
        /**
         * or `deny` - (Optional) One or the other must be set.
         */
        allow?: outputs.folder.OrganizationPolicyListPolicyAllow;
        deny?: outputs.folder.OrganizationPolicyListPolicyDeny;
        /**
         * If set to true, the values from the effective Policy of the parent resource
         * are inherited, meaning the values set in this Policy are added to the values inherited up the hierarchy.
         *
         * The `allow` or `deny` blocks support:
         */
        inheritFromParent?: boolean;
        /**
         * The Google Cloud Console will try to default to a configuration that matches the value specified in this field.
         */
        suggestedValue: string;
    }

    export interface OrganizationPolicyListPolicyAllow {
        /**
         * The policy allows or denies all values.
         */
        all?: boolean;
        /**
         * The policy can define specific values that are allowed or denied.
         */
        values?: string[];
    }

    export interface OrganizationPolicyListPolicyDeny {
        /**
         * The policy allows or denies all values.
         */
        all?: boolean;
        /**
         * The policy can define specific values that are allowed or denied.
         */
        values?: string[];
    }

    export interface OrganizationPolicyRestorePolicy {
        /**
         * May only be set to true. If set, then the default Policy is restored.
         */
        default: boolean;
    }

}

export namespace gameservices {
    export interface GameServerClusterConnectionInfo {
        /**
         * Reference of the GKE cluster where the game servers are installed.
         * Structure is documented below.
         */
        gkeClusterReference: outputs.gameservices.GameServerClusterConnectionInfoGkeClusterReference;
        /**
         * Namespace designated on the game server cluster where the game server
         * instances will be created. The namespace existence will be validated
         * during creation.
         */
        namespace: string;
    }

    export interface GameServerClusterConnectionInfoGkeClusterReference {
        /**
         * The full or partial name of a GKE cluster, using one of the following
         * forms:
         * * `projects/{project_id}/locations/{location}/clusters/{cluster_id}`
         * * `locations/{location}/clusters/{cluster_id}`
         * * `{cluster_id}`
         * If project and location are not specified, the project and location of the
         * GameServerCluster resource are used to generate the full name of the
         * GKE cluster.
         *
         * - - -
         */
        cluster: string;
    }

    export interface GameServerConfigFleetConfig {
        /**
         * The fleet spec, which is sent to Agones to configure fleet.
         * The spec can be passed as inline json but it is recommended to use a file reference
         * instead. File references can contain the json or yaml format of the fleet spec. Eg:
         * * fleetSpec = jsonencode(yamldecode(file("fleet_configs.yaml")))
         * * fleetSpec = file("fleet_configs.json")
         * The format of the spec can be found :
         * `https://agones.dev/site/docs/reference/fleet/`.
         */
        fleetSpec: string;
        /**
         * The name of the FleetConfig.
         *
         * - - -
         */
        name: string;
    }

    export interface GameServerConfigScalingConfig {
        /**
         * Fleet autoscaler spec, which is sent to Agones.
         * Example spec can be found :
         * https://agones.dev/site/docs/reference/fleetautoscaler/
         */
        fleetAutoscalerSpec: string;
        /**
         * The name of the ScalingConfig
         */
        name: string;
        /**
         * The schedules to which this scaling config applies.
         * Structure is documented below.
         */
        schedules?: outputs.gameservices.GameServerConfigScalingConfigSchedule[];
        /**
         * Labels used to identify the clusters to which this scaling config
         * applies. A cluster is subject to this scaling config if its labels match
         * any of the selector entries.
         * Structure is documented below.
         */
        selectors?: outputs.gameservices.GameServerConfigScalingConfigSelector[];
    }

    export interface GameServerConfigScalingConfigSchedule {
        /**
         * The duration for the cron job event. The duration of the event is effective
         * after the cron job's start time.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        cronJobDuration?: string;
        /**
         * The cron definition of the scheduled event. See
         * https://en.wikipedia.org/wiki/Cron. Cron spec specifies the local time as
         * defined by the realm.
         */
        cronSpec?: string;
        /**
         * The end time of the event.
         * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
         */
        endTime?: string;
        /**
         * The start time of the event.
         * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
         */
        startTime?: string;
    }

    export interface GameServerConfigScalingConfigSelector {
        /**
         * Set of labels to group by.
         */
        labels?: {[key: string]: string};
    }

    export interface GameServerDeploymentRolloutGameServerConfigOverride {
        /**
         * Version of the configuration.
         */
        configVersion?: string;
        /**
         * Selection by realms.
         * Structure is documented below.
         */
        realmsSelector?: outputs.gameservices.GameServerDeploymentRolloutGameServerConfigOverrideRealmsSelector;
    }

    export interface GameServerDeploymentRolloutGameServerConfigOverrideRealmsSelector {
        /**
         * List of realms to match against.
         */
        realms?: string[];
    }

    export interface GetGameServerDeploymentRolloutGameServerConfigOverride {
        /**
         * Version of the configuration.
         */
        configVersion: string;
        /**
         * Selection by realms.  Structure is documented below.
         */
        realmsSelectors: outputs.gameservices.GetGameServerDeploymentRolloutGameServerConfigOverrideRealmsSelector[];
    }

    export interface GetGameServerDeploymentRolloutGameServerConfigOverrideRealmsSelector {
        /**
         * List of realms to match against.
         */
        realms: string[];
    }

}

export namespace gkebackup {
    export interface BackupPlanBackupConfig {
        /**
         * If True, include all namespaced resources.
         */
        allNamespaces?: boolean;
        /**
         * This defines a customer managed encryption key that will be used to encrypt the "config"
         * portion (the Kubernetes resources) of Backups created via this plan.
         * Structure is documented below.
         */
        encryptionKey?: outputs.gkebackup.BackupPlanBackupConfigEncryptionKey;
        /**
         * This flag specifies whether Kubernetes Secret resources should be included
         * when they fall into the scope of Backups.
         */
        includeSecrets: boolean;
        /**
         * This flag specifies whether volume data should be backed up when PVCs are
         * included in the scope of a Backup.
         */
        includeVolumeData: boolean;
        /**
         * A list of namespaced Kubernetes Resources.
         * Structure is documented below.
         */
        selectedApplications?: outputs.gkebackup.BackupPlanBackupConfigSelectedApplications;
        /**
         * If set, include just the resources in the listed namespaces.
         * Structure is documented below.
         */
        selectedNamespaces?: outputs.gkebackup.BackupPlanBackupConfigSelectedNamespaces;
    }

    export interface BackupPlanBackupConfigEncryptionKey {
        /**
         * Google Cloud KMS encryption key. Format: projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*
         */
        gcpKmsEncryptionKey: string;
    }

    export interface BackupPlanBackupConfigSelectedApplications {
        /**
         * A list of namespaced Kubernetes resources.
         * Structure is documented below.
         */
        namespacedNames: outputs.gkebackup.BackupPlanBackupConfigSelectedApplicationsNamespacedName[];
    }

    export interface BackupPlanBackupConfigSelectedApplicationsNamespacedName {
        /**
         * The name of a Kubernetes Resource.
         */
        name: string;
        /**
         * The namespace of a Kubernetes Resource.
         */
        namespace: string;
    }

    export interface BackupPlanBackupConfigSelectedNamespaces {
        /**
         * A list of Kubernetes Namespaces.
         */
        namespaces: string[];
    }

    export interface BackupPlanBackupSchedule {
        /**
         * A standard cron string that defines a repeating schedule for
         * creating Backups via this BackupPlan.
         * If this is defined, then backupRetainDays must also be defined.
         */
        cronSchedule?: string;
        /**
         * This flag denotes whether automatic Backup creation is paused for this BackupPlan.
         */
        paused: boolean;
    }

    export interface BackupPlanIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface BackupPlanIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface BackupPlanRetentionPolicy {
        /**
         * Minimum age for a Backup created via this BackupPlan (in days).
         * Must be an integer value between 0-90 (inclusive).
         * A Backup created under this BackupPlan will not be deletable
         * until it reaches Backup's (create time + backup_delete_lock_days).
         * Updating this field of a BackupPlan does not affect existing Backups.
         * Backups created after a successful update will inherit this new value.
         */
        backupDeleteLockDays: number;
        /**
         * The default maximum age of a Backup created via this BackupPlan.
         * This field MUST be an integer value >= 0 and <= 365. If specified,
         * a Backup created under this BackupPlan will be automatically deleted
         * after its age reaches (createTime + backupRetainDays).
         * If not specified, Backups created under this BackupPlan will NOT be
         * subject to automatic deletion. Updating this field does NOT affect
         * existing Backups under it. Backups created AFTER a successful update
         * will automatically pick up the new value.
         * NOTE: backupRetainDays must be >= backupDeleteLockDays.
         * If cronSchedule is defined, then this must be <= 360 * the creation interval.]
         */
        backupRetainDays: number;
        /**
         * This flag denotes whether the retention policy of this BackupPlan is locked.
         * If set to True, no further update is allowed on this policy, including
         * the locked field itself.
         */
        locked: boolean;
    }

    export interface RestorePlanIamBindingCondition {
        /**
         * The description is a user specified string description
         * of the transformation rule.
         *
         * (Optional)
         * User specified descriptive string for this RestorePlan.
         */
        description?: string;
        expression: string;
        title: string;
    }

    export interface RestorePlanIamMemberCondition {
        /**
         * The description is a user specified string description
         * of the transformation rule.
         *
         * (Optional)
         * User specified descriptive string for this RestorePlan.
         */
        description?: string;
        expression: string;
        title: string;
    }

    export interface RestorePlanRestoreConfig {
        /**
         * If True, restore all namespaced resources in the Backup.
         * Setting this field to False will result in an error.
         */
        allNamespaces?: boolean;
        /**
         * Defines the behavior for handling the situation where cluster-scoped resources
         * being restored already exist in the target cluster.
         * This MUST be set to a value other than `CLUSTER_RESOURCE_CONFLICT_POLICY_UNSPECIFIED`
         * if `clusterResourceRestoreScope` is anyting other than `noGroupKinds`.
         * See https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke/reference/rest/v1/RestoreConfig#clusterresourceconflictpolicy
         * for more information on each policy option.
         * Possible values are: `USE_EXISTING_VERSION`, `USE_BACKUP_VERSION`.
         */
        clusterResourceConflictPolicy?: string;
        /**
         * Identifies the cluster-scoped resources to restore from the Backup.
         * Structure is documented below.
         */
        clusterResourceRestoreScope?: outputs.gkebackup.RestorePlanRestoreConfigClusterResourceRestoreScope;
        /**
         * A list of selected namespaces excluded from restoration.
         * All namespaces except those in this list will be restored.
         * Structure is documented below.
         */
        excludedNamespaces?: outputs.gkebackup.RestorePlanRestoreConfigExcludedNamespaces;
        /**
         * Defines the behavior for handling the situation where sets of namespaced resources
         * being restored already exist in the target cluster.
         * This MUST be set to a value other than `NAMESPACED_RESOURCE_RESTORE_MODE_UNSPECIFIED`
         * if the `namespacedResourceRestoreScope` is anything other than `noNamespaces`.
         * See https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke/reference/rest/v1/RestoreConfig#namespacedresourcerestoremode
         * for more information on each mode.
         * Possible values are: `DELETE_AND_RESTORE`, `FAIL_ON_CONFLICT`.
         */
        namespacedResourceRestoreMode?: string;
        /**
         * Do not restore any namespaced resources if set to "True".
         * Specifying this field to "False" is not allowed.
         */
        noNamespaces?: boolean;
        /**
         * A list of selected ProtectedApplications to restore.
         * The listed ProtectedApplications and all the resources
         * to which they refer will be restored.
         * Structure is documented below.
         */
        selectedApplications?: outputs.gkebackup.RestorePlanRestoreConfigSelectedApplications;
        /**
         * A list of selected namespaces to restore from the Backup.
         * The listed Namespaces and all resources contained in them will be restored.
         * Structure is documented below.
         */
        selectedNamespaces?: outputs.gkebackup.RestorePlanRestoreConfigSelectedNamespaces;
        /**
         * A list of transformation rules to be applied against Kubernetes
         * resources as they are selected for restoration from a Backup.
         * Rules are executed in order defined - this order matters,
         * as changes made by a rule may impact the filtering logic of subsequent
         * rules. An empty list means no transformation will occur.
         * Structure is documented below.
         */
        transformationRules?: outputs.gkebackup.RestorePlanRestoreConfigTransformationRule[];
        /**
         * Specifies the mechanism to be used to restore volume data.
         * This should be set to a value other than `NAMESPACED_RESOURCE_RESTORE_MODE_UNSPECIFIED`
         * if the `namespacedResourceRestoreScope` is anything other than `noNamespaces`.
         * If not specified, it will be treated as `NO_VOLUME_DATA_RESTORATION`.
         * See https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke/reference/rest/v1/RestoreConfig#VolumeDataRestorePolicy
         * for more information on each policy option.
         * Possible values are: `RESTORE_VOLUME_DATA_FROM_BACKUP`, `REUSE_VOLUME_HANDLE_FROM_BACKUP`, `NO_VOLUME_DATA_RESTORATION`.
         */
        volumeDataRestorePolicy?: string;
    }

    export interface RestorePlanRestoreConfigClusterResourceRestoreScope {
        /**
         * If True, all valid cluster-scoped resources will be restored.
         * Mutually exclusive to any other field in `clusterResourceRestoreScope`.
         */
        allGroupKinds?: boolean;
        /**
         * A list of cluster-scoped resource group kinds to NOT restore from the backup.
         * If specified, all valid cluster-scoped resources will be restored except
         * for those specified in the list.
         * Mutually exclusive to any other field in `clusterResourceRestoreScope`.
         * Structure is documented below.
         */
        excludedGroupKinds?: outputs.gkebackup.RestorePlanRestoreConfigClusterResourceRestoreScopeExcludedGroupKind[];
        /**
         * If True, no cluster-scoped resources will be restored.
         * Mutually exclusive to any other field in `clusterResourceRestoreScope`.
         */
        noGroupKinds?: boolean;
        /**
         * A list of cluster-scoped resource group kinds to restore from the backup.
         * If specified, only the selected resources will be restored.
         * Mutually exclusive to any other field in the `clusterResourceRestoreScope`.
         * Structure is documented below.
         */
        selectedGroupKinds?: outputs.gkebackup.RestorePlanRestoreConfigClusterResourceRestoreScopeSelectedGroupKind[];
    }

    export interface RestorePlanRestoreConfigClusterResourceRestoreScopeExcludedGroupKind {
        /**
         * API Group string of a Kubernetes resource, e.g.
         * "apiextensions.k8s.io", "storage.k8s.io", etc.
         * Use empty string for core group.
         */
        resourceGroup?: string;
        /**
         * Kind of a Kubernetes resource, e.g.
         * "CustomResourceDefinition", "StorageClass", etc.
         */
        resourceKind?: string;
    }

    export interface RestorePlanRestoreConfigClusterResourceRestoreScopeSelectedGroupKind {
        /**
         * API Group string of a Kubernetes resource, e.g.
         * "apiextensions.k8s.io", "storage.k8s.io", etc.
         * Use empty string for core group.
         */
        resourceGroup?: string;
        /**
         * Kind of a Kubernetes resource, e.g.
         * "CustomResourceDefinition", "StorageClass", etc.
         */
        resourceKind?: string;
    }

    export interface RestorePlanRestoreConfigExcludedNamespaces {
        /**
         * A list of Kubernetes Namespaces.
         */
        namespaces: string[];
    }

    export interface RestorePlanRestoreConfigSelectedApplications {
        /**
         * A list of namespaced Kubernetes resources.
         * Structure is documented below.
         */
        namespacedNames: outputs.gkebackup.RestorePlanRestoreConfigSelectedApplicationsNamespacedName[];
    }

    export interface RestorePlanRestoreConfigSelectedApplicationsNamespacedName {
        /**
         * The name of a Kubernetes Resource.
         */
        name: string;
        /**
         * The namespace of a Kubernetes Resource.
         */
        namespace: string;
    }

    export interface RestorePlanRestoreConfigSelectedNamespaces {
        /**
         * A list of Kubernetes Namespaces.
         */
        namespaces: string[];
    }

    export interface RestorePlanRestoreConfigTransformationRule {
        /**
         * The description is a user specified string description
         * of the transformation rule.
         */
        description?: string;
        /**
         * A list of transformation rule actions to take against candidate
         * resources. Actions are executed in order defined - this order
         * matters, as they could potentially interfere with each other and
         * the first operation could affect the outcome of the second operation.
         * Structure is documented below.
         */
        fieldActions: outputs.gkebackup.RestorePlanRestoreConfigTransformationRuleFieldAction[];
        /**
         * This field is used to specify a set of fields that should be used to
         * determine which resources in backup should be acted upon by the
         * supplied transformation rule actions, and this will ensure that only
         * specific resources are affected by transformation rule actions.
         * Structure is documented below.
         */
        resourceFilter?: outputs.gkebackup.RestorePlanRestoreConfigTransformationRuleResourceFilter;
    }

    export interface RestorePlanRestoreConfigTransformationRuleFieldAction {
        /**
         * A string containing a JSON Pointer value that references the
         * location in the target document to move the value from.
         */
        fromPath?: string;
        /**
         * Specifies the operation to perform.
         * Possible values are: `REMOVE`, `MOVE`, `COPY`, `ADD`, `TEST`, `REPLACE`.
         */
        op: string;
        /**
         * A string containing a JSON-Pointer value that references a
         * location within the target document where the operation is performed.
         */
        path?: string;
        /**
         * A string that specifies the desired value in string format
         * to use for transformation.
         *
         * - - -
         */
        value?: string;
    }

    export interface RestorePlanRestoreConfigTransformationRuleResourceFilter {
        /**
         * (Filtering parameter) Any resource subject to transformation must
         * belong to one of the listed "types". If this field is not provided,
         * no type filtering will be performed
         * (all resources of all types matching previous filtering parameters
         * will be candidates for transformation).
         * Structure is documented below.
         */
        groupKinds?: outputs.gkebackup.RestorePlanRestoreConfigTransformationRuleResourceFilterGroupKind[];
        /**
         * This is a JSONPath expression that matches specific fields of
         * candidate resources and it operates as a filtering parameter
         * (resources that are not matched with this expression will not
         * be candidates for transformation).
         */
        jsonPath?: string;
        /**
         * (Filtering parameter) Any resource subject to transformation must
         * be contained within one of the listed Kubernetes Namespace in the
         * Backup. If this field is not provided, no namespace filtering will
         * be performed (all resources in all Namespaces, including all
         * cluster-scoped resources, will be candidates for transformation).
         * To mix cluster-scoped and namespaced resources in the same rule,
         * use an empty string ("") as one of the target namespaces.
         */
        namespaces?: string[];
    }

    export interface RestorePlanRestoreConfigTransformationRuleResourceFilterGroupKind {
        /**
         * API Group string of a Kubernetes resource, e.g.
         * "apiextensions.k8s.io", "storage.k8s.io", etc.
         * Use empty string for core group.
         */
        resourceGroup?: string;
        /**
         * Kind of a Kubernetes resource, e.g.
         * "CustomResourceDefinition", "StorageClass", etc.
         */
        resourceKind?: string;
    }

}

export namespace gkehub {
    export interface FeatureIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FeatureIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FeatureMembershipConfigmanagement {
        /**
         * Binauthz configuration for the cluster. Structure is documented below.
         */
        binauthz?: outputs.gkehub.FeatureMembershipConfigmanagementBinauthz;
        /**
         * Config Sync configuration for the cluster. Structure is documented below.
         */
        configSync?: outputs.gkehub.FeatureMembershipConfigmanagementConfigSync;
        /**
         * Hierarchy Controller configuration for the cluster. Structure is documented below.
         */
        hierarchyController?: outputs.gkehub.FeatureMembershipConfigmanagementHierarchyController;
        /**
         * Policy Controller configuration for the cluster. Structure is documented below.
         */
        policyController?: outputs.gkehub.FeatureMembershipConfigmanagementPolicyController;
        /**
         * Version of ACM installed.
         */
        version: string;
    }

    export interface FeatureMembershipConfigmanagementBinauthz {
        /**
         * Whether binauthz is enabled in this cluster.
         */
        enabled?: boolean;
    }

    export interface FeatureMembershipConfigmanagementConfigSync {
        /**
         * (Optional) Structure is documented below.
         */
        git?: outputs.gkehub.FeatureMembershipConfigmanagementConfigSyncGit;
        /**
         * (Optional) Supported from ACM versions 1.12.0 onwards. Structure is documented below.
         *
         * Use either `git` or `oci` config option.
         */
        oci?: outputs.gkehub.FeatureMembershipConfigmanagementConfigSyncOci;
        /**
         * Supported from ACM versions 1.10.0 onwards. Set to true to enable the Config Sync admission webhook to prevent drifts. If set to "false", disables the Config Sync admission webhook and does not prevent drifts.
         */
        preventDrift: boolean;
        /**
         * Specifies whether the Config Sync Repo is in "hierarchical" or "unstructured" mode.
         */
        sourceFormat?: string;
    }

    export interface FeatureMembershipConfigmanagementConfigSyncGit {
        /**
         * The GCP Service Account Email used for auth when secretType is gcpServiceAccount.
         */
        gcpServiceAccountEmail?: string;
        /**
         * URL for the HTTPS proxy to be used when communicating with the Git repo.
         */
        httpsProxy?: string;
        /**
         * The path within the Git repository that represents the top level of the repo to sync. Default: the root directory of the repository.
         */
        policyDir?: string;
        /**
         * Type of secret configured for access to the Git repo.
         */
        secretType?: string;
        /**
         * The branch of the repository to sync from. Default: master.
         */
        syncBranch?: string;
        /**
         * The URL of the Git repository to use as the source of truth.
         */
        syncRepo?: string;
        /**
         * Git revision (tag or hash) to check out. Default HEAD.
         */
        syncRev?: string;
        /**
         * Period in seconds between consecutive syncs. Default: 15.
         */
        syncWaitSecs?: string;
    }

    export interface FeatureMembershipConfigmanagementConfigSyncOci {
        /**
         * The GCP Service Account Email used for auth when secretType is gcpserviceaccount.
         */
        gcpServiceAccountEmail?: string;
        /**
         * The absolute path of the directory that contains the local resources. Default: the root directory of the image.
         */
        policyDir?: string;
        /**
         * Type of secret configured for access to the OCI Image. Must be one of gcenode, gcpserviceaccount or none.
         */
        secretType?: string;
        /**
         * The OCI image repository URL for the package to sync from. e.g. LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY_NAME/PACKAGE_NAME.
         */
        syncRepo?: string;
        /**
         * Period in seconds(int64 format) between consecutive syncs. Default: 15.
         */
        syncWaitSecs?: string;
    }

    export interface FeatureMembershipConfigmanagementHierarchyController {
        /**
         * Whether hierarchical resource quota is enabled in this cluster.
         */
        enableHierarchicalResourceQuota?: boolean;
        /**
         * Whether pod tree labels are enabled in this cluster.
         */
        enablePodTreeLabels?: boolean;
        /**
         * Whether Hierarchy Controller is enabled in this cluster.
         */
        enabled?: boolean;
    }

    export interface FeatureMembershipConfigmanagementPolicyController {
        /**
         * Sets the interval for Policy Controller Audit Scans (in seconds). When set to 0, this disables audit functionality altogether.
         */
        auditIntervalSeconds?: string;
        /**
         * Enables the installation of Policy Controller. If false, the rest of PolicyController fields take no effect.
         */
        enabled?: boolean;
        /**
         * The set of namespaces that are excluded from Policy Controller checks. Namespaces do not need to currently exist on the cluster.
         */
        exemptableNamespaces?: string[];
        /**
         * Logs all denies and dry run failures.
         */
        logDeniesEnabled?: boolean;
        /**
         * Specifies the backends Policy Controller should export metrics to. For example, to specify metrics should be exported to Cloud Monitoring and Prometheus, specify backends: [\"cloudmonitoring\", \"prometheus\"]. Default: [\"cloudmonitoring\", \"prometheus\"]
         */
        monitoring: outputs.gkehub.FeatureMembershipConfigmanagementPolicyControllerMonitoring;
        /**
         * Enables mutation in policy controller. If true, mutation CRDs, webhook, and controller deployment will be deployed to the cluster.
         */
        mutationEnabled?: boolean;
        /**
         * Enables the ability to use Constraint Templates that reference to objects other than the object currently being evaluated.
         */
        referentialRulesEnabled?: boolean;
        /**
         * Installs the default template library along with Policy Controller.
         */
        templateLibraryInstalled?: boolean;
    }

    export interface FeatureMembershipConfigmanagementPolicyControllerMonitoring {
        backends: string[];
    }

    export interface FeatureMembershipMesh {
        /**
         * @deprecated Deprecated in favor of the `management` field
         */
        controlPlane?: string;
        /**
         * Whether to automatically manage Service Mesh. Can either be `MANAGEMENT_AUTOMATIC` or `MANAGEMENT_MANUAL`.
         */
        management?: string;
    }

    export interface FeatureResourceState {
        /**
         * (Output)
         * Whether this Feature has outstanding resources that need to be cleaned up before it can be disabled.
         */
        hasResources: boolean;
        /**
         * (Output)
         * Output only. The "running state" of the Feature in this Hub.
         * Structure is documented below.
         */
        state: string;
    }

    export interface FeatureSpec {
        /**
         * Fleet Observability feature spec.
         * Structure is documented below.
         */
        fleetobservability?: outputs.gkehub.FeatureSpecFleetobservability;
        /**
         * Multicluster Ingress-specific spec.
         * Structure is documented below.
         */
        multiclusteringress?: outputs.gkehub.FeatureSpecMulticlusteringress;
    }

    export interface FeatureSpecFleetobservability {
        /**
         * Specified if fleet logging feature is enabled for the entire fleet. If UNSPECIFIED, fleet logging feature is disabled for the entire fleet.
         * Structure is documented below.
         */
        loggingConfig?: outputs.gkehub.FeatureSpecFleetobservabilityLoggingConfig;
    }

    export interface FeatureSpecFleetobservabilityLoggingConfig {
        /**
         * Specified if applying the default routing config to logs not specified in other configs.
         * Structure is documented below.
         */
        defaultConfig?: outputs.gkehub.FeatureSpecFleetobservabilityLoggingConfigDefaultConfig;
        /**
         * Specified if applying the routing config to all logs for all fleet scopes.
         * Structure is documented below.
         */
        fleetScopeLogsConfig?: outputs.gkehub.FeatureSpecFleetobservabilityLoggingConfigFleetScopeLogsConfig;
    }

    export interface FeatureSpecFleetobservabilityLoggingConfigDefaultConfig {
        /**
         * Specified if fleet logging feature is enabled.
         * Possible values are: `MODE_UNSPECIFIED`, `COPY`, `MOVE`.
         */
        mode?: string;
    }

    export interface FeatureSpecFleetobservabilityLoggingConfigFleetScopeLogsConfig {
        /**
         * Specified if fleet logging feature is enabled.
         * Possible values are: `MODE_UNSPECIFIED`, `COPY`, `MOVE`.
         */
        mode?: string;
    }

    export interface FeatureSpecMulticlusteringress {
        /**
         * Fully-qualified Membership name which hosts the MultiClusterIngress CRD. Example: `projects/foo-proj/locations/global/memberships/bar`
         */
        configMembership: string;
    }

    export interface FeatureState {
        /**
         * (Output)
         * Output only. The "running state" of the Feature in this Hub.
         * Structure is documented below.
         */
        states: outputs.gkehub.FeatureStateState[];
    }

    export interface FeatureStateState {
        /**
         * (Output)
         * The high-level, machine-readable status of this Feature.
         */
        code: string;
        /**
         * (Output)
         * A human-readable description of the current status.
         */
        description: string;
        /**
         * (Output)
         * The time this status and any related Feature-specific details were updated. A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z"
         */
        updateTime: string;
    }

    export interface MembershipAuthority {
        issuer: string;
    }

    export interface MembershipBindingState {
        /**
         * (Output)
         * Code describes the state of a MembershipBinding resource.
         */
        code: string;
    }

    export interface MembershipEndpoint {
        /**
         * If this Membership is a Kubernetes API server hosted on GKE, this is a self link to its GCP resource.
         * Structure is documented below.
         */
        gkeCluster?: outputs.gkehub.MembershipEndpointGkeCluster;
    }

    export interface MembershipEndpointGkeCluster {
        resourceLink: string;
    }

    export interface MembershipIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface MembershipIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface MembershipRbacRoleBindingRole {
        /**
         * PredefinedRole is an ENUM representation of the default Kubernetes Roles
         * Possible values are: `UNKNOWN`, `ADMIN`, `EDIT`, `VIEW`, `ANTHOS_SUPPORT`.
         *
         * - - -
         */
        predefinedRole: string;
    }

    export interface MembershipRbacRoleBindingState {
        /**
         * (Output)
         * Code describes the state of a RBAC Role Binding resource.
         */
        code: string;
    }

    export interface NamespaceState {
        /**
         * (Output)
         * Code describes the state of a Namespace resource.
         */
        code: string;
    }

    export interface ScopeIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ScopeIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ScopeRbacRoleBindingRole {
        /**
         * PredefinedRole is an ENUM representation of the default Kubernetes Roles
         * Possible values are: `UNKNOWN`, `ADMIN`, `EDIT`, `VIEW`.
         *
         * - - -
         */
        predefinedRole?: string;
    }

    export interface ScopeRbacRoleBindingState {
        /**
         * (Output)
         * Code describes the state of a RBAC Role Binding resource.
         */
        code: string;
    }

    export interface ScopeState {
        /**
         * (Output)
         * Code describes the state of a Scope resource.
         */
        code: string;
    }

}

export namespace gkeonprem {
    export interface BareMetalAdminClusterClusterOperations {
        /**
         * Whether collection of application logs/metrics should be enabled (in addition to system logs/metrics).
         */
        enableApplicationLogs?: boolean;
    }

    export interface BareMetalAdminClusterControlPlane {
        /**
         * Customizes the default API server args. Only a subset of
         * customized flags are supported. Please refer to the API server
         * documentation below to know the exact format:
         * https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
         * Structure is documented below.
         */
        apiServerArgs?: outputs.gkeonprem.BareMetalAdminClusterControlPlaneApiServerArg[];
        /**
         * Configures the node pool running the control plane. If specified the corresponding NodePool will be created for the cluster's control plane. The NodePool will have the same name and namespace as the cluster.
         * Structure is documented below.
         */
        controlPlaneNodePoolConfig: outputs.gkeonprem.BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfig;
    }

    export interface BareMetalAdminClusterControlPlaneApiServerArg {
        /**
         * The argument name as it appears on the API Server command line please make sure to remove the leading dashes.
         */
        argument: string;
        /**
         * The value of the arg as it will be passed to the API Server command line.
         */
        value: string;
    }

    export interface BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfig {
        /**
         * The generic configuration for a node pool running the control plane.
         * Structure is documented below.
         */
        nodePoolConfig: outputs.gkeonprem.BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig;
    }

    export interface BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * The list of machine addresses in the Bare Metal Node Pool.
         * Structure is documented below.
         */
        nodeConfigs?: outputs.gkeonprem.BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig[];
        /**
         * Specifies the nodes operating system (default: LINUX).
         */
        operatingSystem?: string;
        /**
         * The initial taints assigned to nodes of this node pool.
         * Structure is documented below.
         */
        taints?: outputs.gkeonprem.BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint[];
    }

    export interface BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * The default IPv4 address for SSH access and Kubernetes node.
         * Example: 192.168.0.1
         */
        nodeIp?: string;
    }

    export interface BareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint {
        /**
         * Specifies the nodes operating system (default: LINUX).
         * Possible values are: `EFFECT_UNSPECIFIED`, `PREFER_NO_SCHEDULE`, `NO_EXECUTE`.
         */
        effect?: string;
        /**
         * Key associated with the effect.
         */
        key?: string;
        /**
         * Value associated with the effect.
         */
        value?: string;
    }

    export interface BareMetalAdminClusterFleet {
        /**
         * (Output)
         * The name of the managed Hub Membership resource associated to this cluster.
         * Membership names are formatted as
         * `projects/<project-number>/locations/<location>/memberships/<cluster-id>`.
         */
        membership: string;
    }

    export interface BareMetalAdminClusterLoadBalancer {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        manualLbConfig?: outputs.gkeonprem.BareMetalAdminClusterLoadBalancerManualLbConfig;
        /**
         * Specifies the load balancer ports.
         * Structure is documented below.
         */
        portConfig: outputs.gkeonprem.BareMetalAdminClusterLoadBalancerPortConfig;
        /**
         * Specified the Bare Metal Load Balancer Config
         * Structure is documented below.
         */
        vipConfig: outputs.gkeonprem.BareMetalAdminClusterLoadBalancerVipConfig;
    }

    export interface BareMetalAdminClusterLoadBalancerManualLbConfig {
        /**
         * Whether manual load balancing is enabled.
         */
        enabled: boolean;
    }

    export interface BareMetalAdminClusterLoadBalancerPortConfig {
        /**
         * The port that control plane hosted load balancers will listen on.
         */
        controlPlaneLoadBalancerPort: number;
    }

    export interface BareMetalAdminClusterLoadBalancerVipConfig {
        /**
         * The VIP which you previously set aside for the Kubernetes API of this Bare Metal Admin Cluster.
         */
        controlPlaneVip: string;
    }

    export interface BareMetalAdminClusterMaintenanceConfig {
        /**
         * All IPv4 address from these ranges will be placed into maintenance mode.
         * Nodes in maintenance mode will be cordoned and drained. When both of these
         * are true, the "baremetal.cluster.gke.io/maintenance" annotation will be set
         * on the node resource.
         */
        maintenanceAddressCidrBlocks: string[];
    }

    export interface BareMetalAdminClusterNetworkConfig {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        islandModeCidr?: outputs.gkeonprem.BareMetalAdminClusterNetworkConfigIslandModeCidr;
    }

    export interface BareMetalAdminClusterNetworkConfigIslandModeCidr {
        /**
         * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
         */
        podAddressCidrBlocks: string[];
        /**
         * All services in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
         */
        serviceAddressCidrBlocks: string[];
    }

    export interface BareMetalAdminClusterNodeAccessConfig {
        /**
         * LoginUser is the user name used to access node machines.
         * It defaults to "root" if not set.
         */
        loginUser?: string;
    }

    export interface BareMetalAdminClusterNodeConfig {
        /**
         * The maximum number of pods a node can run. The size of the CIDR range
         * assigned to the node will be derived from this parameter.
         */
        maxPodsPerNode?: number;
    }

    export interface BareMetalAdminClusterProxy {
        /**
         * A list of IPs, hostnames, and domains that should skip the proxy.
         * Examples: ["127.0.0.1", "example.com", ".corp", "localhost"].
         */
        noProxies?: string[];
        /**
         * Specifies the address of your proxy server.
         * Examples: http://domain
         * WARNING: Do not provide credentials in the format
         * http://(username:password@)domain these will be rejected by the server.
         */
        uri: string;
    }

    export interface BareMetalAdminClusterSecurityConfig {
        /**
         * Configures user access to the Bare Metal User cluster.
         * Structure is documented below.
         */
        authorization?: outputs.gkeonprem.BareMetalAdminClusterSecurityConfigAuthorization;
    }

    export interface BareMetalAdminClusterSecurityConfigAuthorization {
        /**
         * Users that will be granted the cluster-admin role on the cluster, providing full access to the cluster.
         * Structure is documented below.
         */
        adminUsers: outputs.gkeonprem.BareMetalAdminClusterSecurityConfigAuthorizationAdminUser[];
    }

    export interface BareMetalAdminClusterSecurityConfigAuthorizationAdminUser {
        /**
         * The name of the user, e.g. `my-gcp-id@gmail.com`.
         */
        username: string;
    }

    export interface BareMetalAdminClusterStatus {
        /**
         * (Output)
         * ResourceConditions provide a standard mechanism for higher-level status reporting from admin cluster controller.
         * Structure is documented below.
         */
        conditions: outputs.gkeonprem.BareMetalAdminClusterStatusCondition[];
        /**
         * (Output)
         * Human-friendly representation of the error message from the admin cluster
         * controller. The error message can be temporary as the admin cluster
         * controller creates a cluster or node pool. If the error message persists
         * for a longer period of time, it can be used to surface error message to
         * indicate real problems requiring user intervention.
         */
        errorMessage: string;
    }

    export interface BareMetalAdminClusterStatusCondition {
        /**
         * (Output)
         * Last time the condition transit from one status to another.
         */
        lastTransitionTime: string;
        /**
         * Human-readable message indicating details about last transition.
         */
        message?: string;
        /**
         * (Output)
         * A human-readable message of the check failure.
         */
        reason?: string;
        /**
         * (Output)
         * The lifecycle state of the condition.
         */
        state: string;
        /**
         * Type of the condition.
         * (e.g., ClusterRunning, NodePoolRunning or ServerSidePreflightReady)
         */
        type?: string;
    }

    export interface BareMetalAdminClusterStorage {
        /**
         * Specifies the config for local PersistentVolumes backed
         * by mounted node disks. These disks need to be formatted and mounted by the
         * user, which can be done before or after cluster creation.
         * Structure is documented below.
         */
        lvpNodeMountsConfig: outputs.gkeonprem.BareMetalAdminClusterStorageLvpNodeMountsConfig;
        /**
         * Specifies the config for local PersistentVolumes backed by
         * subdirectories in a shared filesystem. These subdirectores are
         * automatically created during cluster creation.
         * Structure is documented below.
         */
        lvpShareConfig: outputs.gkeonprem.BareMetalAdminClusterStorageLvpShareConfig;
    }

    export interface BareMetalAdminClusterStorageLvpNodeMountsConfig {
        /**
         * The host machine path.
         */
        path: string;
        /**
         * The StorageClass name that PVs will be created with.
         */
        storageClass: string;
    }

    export interface BareMetalAdminClusterStorageLvpShareConfig {
        /**
         * Defines the machine path and storage class for the LVP Share.
         * Structure is documented below.
         */
        lvpConfig: outputs.gkeonprem.BareMetalAdminClusterStorageLvpShareConfigLvpConfig;
        /**
         * The number of subdirectories to create under path.
         */
        sharedPathPvCount?: number;
    }

    export interface BareMetalAdminClusterStorageLvpShareConfigLvpConfig {
        /**
         * The host machine path.
         */
        path: string;
        /**
         * The StorageClass name that PVs will be created with.
         */
        storageClass: string;
    }

    export interface BareMetalAdminClusterValidationCheck {
        /**
         * (Output)
         * Options used for the validation check.
         */
        options: string;
        /**
         * (Output)
         * The scenario when the preflight checks were run..
         */
        scenario: string;
        /**
         * (Output)
         * Specifies the detailed validation check status
         * Structure is documented below.
         */
        statuses: outputs.gkeonprem.BareMetalAdminClusterValidationCheckStatus[];
    }

    export interface BareMetalAdminClusterValidationCheckStatus {
        /**
         * (Output)
         * Individual checks which failed as part of the Preflight check execution.
         * Structure is documented below.
         */
        results: outputs.gkeonprem.BareMetalAdminClusterValidationCheckStatusResult[];
    }

    export interface BareMetalAdminClusterValidationCheckStatusResult {
        /**
         * (Output)
         * The category of the validation.
         */
        category: string;
        /**
         * A human readable description of this Bare Metal Admin Cluster.
         */
        description: string;
        /**
         * (Output)
         * Detailed failure information, which might be unformatted.
         */
        details: string;
        /**
         * (Output)
         * Options used for the validation check.
         */
        options: string;
        /**
         * (Output)
         * A human-readable message of the check failure.
         */
        reason: string;
    }

    export interface BareMetalClusterBinaryAuthorization {
        /**
         * Mode of operation for binauthz policy evaluation. If unspecified,
         * defaults to DISABLED.
         * Possible values are: `DISABLED`, `PROJECT_SINGLETON_POLICY_ENFORCE`.
         */
        evaluationMode?: string;
    }

    export interface BareMetalClusterClusterOperations {
        /**
         * Whether collection of application logs/metrics should be enabled (in addition to system logs/metrics).
         */
        enableApplicationLogs?: boolean;
    }

    export interface BareMetalClusterControlPlane {
        /**
         * Customizes the default API server args. Only a subset of
         * customized flags are supported. Please refer to the API server
         * documentation below to know the exact format:
         * https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
         * Structure is documented below.
         */
        apiServerArgs?: outputs.gkeonprem.BareMetalClusterControlPlaneApiServerArg[];
        /**
         * Configures the node pool running the control plane. If specified the corresponding NodePool will be created for the cluster's control plane. The NodePool will have the same name and namespace as the cluster.
         * Structure is documented below.
         */
        controlPlaneNodePoolConfig: outputs.gkeonprem.BareMetalClusterControlPlaneControlPlaneNodePoolConfig;
    }

    export interface BareMetalClusterControlPlaneApiServerArg {
        /**
         * The argument name as it appears on the API Server command line please make sure to remove the leading dashes.
         */
        argument: string;
        /**
         * The value of the arg as it will be passed to the API Server command line.
         */
        value: string;
    }

    export interface BareMetalClusterControlPlaneControlPlaneNodePoolConfig {
        /**
         * The generic configuration for a node pool running the control plane.
         * Structure is documented below.
         */
        nodePoolConfig: outputs.gkeonprem.BareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig;
    }

    export interface BareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels: {[key: string]: string};
        /**
         * The list of machine addresses in the Bare Metal Node Pool.
         * Structure is documented below.
         */
        nodeConfigs?: outputs.gkeonprem.BareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig[];
        /**
         * Specifies the nodes operating system (default: LINUX).
         */
        operatingSystem?: string;
        /**
         * The initial taints assigned to nodes of this node pool.
         * Structure is documented below.
         */
        taints: outputs.gkeonprem.BareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint[];
    }

    export interface BareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * The default IPv4 address for SSH access and Kubernetes node.
         * Example: 192.168.0.1
         */
        nodeIp?: string;
    }

    export interface BareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint {
        /**
         * Specifies the nodes operating system (default: LINUX).
         * Possible values are: `EFFECT_UNSPECIFIED`, `PREFER_NO_SCHEDULE`, `NO_EXECUTE`.
         */
        effect?: string;
        /**
         * Key associated with the effect.
         */
        key?: string;
        /**
         * Value associated with the effect.
         */
        value?: string;
    }

    export interface BareMetalClusterFleet {
        /**
         * (Output)
         * The name of the managed Hub Membership resource associated to this cluster.
         * Membership names are formatted as
         * `projects/<project-number>/locations/<location>/memberships/<cluster-id>`.
         */
        membership: string;
    }

    export interface BareMetalClusterLoadBalancer {
        /**
         * Configuration for BGP typed load balancers.
         * Structure is documented below.
         */
        bgpLbConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfig;
        /**
         * A nested object resource
         * Structure is documented below.
         */
        manualLbConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerManualLbConfig;
        /**
         * A nested object resource
         * Structure is documented below.
         */
        metalLbConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerMetalLbConfig;
        /**
         * Specifies the load balancer ports.
         * Structure is documented below.
         */
        portConfig: outputs.gkeonprem.BareMetalClusterLoadBalancerPortConfig;
        /**
         * Specified the Bare Metal Load Balancer Config
         * Structure is documented below.
         */
        vipConfig: outputs.gkeonprem.BareMetalClusterLoadBalancerVipConfig;
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfig {
        /**
         * AddressPools is a list of non-overlapping IP pools used by load balancer
         * typed services. All addresses must be routable to load balancer nodes.
         * IngressVIP must be included in the pools.
         * Structure is documented below.
         */
        addressPools: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfigAddressPool[];
        /**
         * BGP autonomous system number (ASN) of the cluster.
         * This field can be updated after cluster creation.
         */
        asn: number;
        /**
         * The list of BGP peers that the cluster will connect to.
         * At least one peer must be configured for each control plane node.
         * Control plane nodes will connect to these peers to advertise the control
         * plane VIP. The Services load balancer also uses these peers by default.
         * This field can be updated after cluster creation.
         * Structure is documented below.
         */
        bgpPeerConfigs: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfigBgpPeerConfig[];
        /**
         * Specifies the node pool running data plane load balancing. L2 connectivity
         * is required among nodes in this pool. If missing, the control plane node
         * pool is used for data plane load balancing.
         * Structure is documented below.
         */
        loadBalancerNodePoolConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfig;
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfigAddressPool {
        /**
         * The addresses that are part of this pool. Each address must be either in the CIDR form (1.2.3.0/24) or range form (1.2.3.1-1.2.3.5).
         */
        addresses: string[];
        /**
         * If true, avoid using IPs ending in .0 or .255.
         * This avoids buggy consumer devices mistakenly dropping IPv4 traffic for those special IP addresses.
         */
        avoidBuggyIps?: boolean;
        /**
         * If true, prevent IP addresses from being automatically assigned.
         */
        manualAssign?: string;
        /**
         * The name of the address pool.
         */
        pool: string;
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfigBgpPeerConfig {
        /**
         * BGP autonomous system number (ASN) for the network that contains the
         * external peer device.
         */
        asn: number;
        /**
         * The IP address of the control plane node that connects to the external
         * peer.
         * If you don't specify any control plane nodes, all control plane nodes
         * can connect to the external peer. If you specify one or more IP addresses,
         * only the nodes specified participate in peering sessions.
         */
        controlPlaneNodes?: string[];
        /**
         * The IP address of the external peer device.
         */
        ipAddress: string;
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfig {
        /**
         * The generic configuration for a node pool running a load balancer.
         * Structure is documented below.
         */
        nodePoolConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfig;
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfig {
        /**
         * The modifiable kubelet configurations for the baremetal machines.
         * Structure is documented below.
         */
        kubeletConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigKubeletConfig;
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * The list of machine addresses in the Bare Metal Node Pool.
         * Structure is documented below.
         */
        nodeConfigs?: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig[];
        /**
         * Specifies the nodes operating system (default: LINUX).
         */
        operatingSystem?: string;
        /**
         * The initial taints assigned to nodes of this node pool.
         * Structure is documented below.
         */
        taints?: outputs.gkeonprem.BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint[];
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigKubeletConfig {
        /**
         * The maximum size of bursty pulls, temporarily allows pulls to burst to this
         * number, while still not exceeding registry_pull_qps.
         * The value must not be a negative number.
         * Updating this field may impact scalability by changing the amount of
         * traffic produced by image pulls.
         * Defaults to 10.
         */
        registryBurst?: number;
        /**
         * The limit of registry pulls per second.
         * Setting this value to 0 means no limit.
         * Updating this field may impact scalability by changing the amount of
         * traffic produced by image pulls.
         * Defaults to 5.
         */
        registryPullQps?: number;
        /**
         * Prevents the Kubelet from pulling multiple images at a time.
         * We recommend *not* changing the default value on nodes that run docker
         * daemon with version  < 1.9 or an Another Union File System (Aufs) storage
         * backend. Issue https://github.com/kubernetes/kubernetes/issues/10959 has
         * more details.
         */
        serializeImagePullsDisabled?: boolean;
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * The default IPv4 address for SSH access and Kubernetes node.
         * Example: 192.168.0.1
         */
        nodeIp?: string;
    }

    export interface BareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint {
        /**
         * Specifies the nodes operating system (default: LINUX).
         * Possible values are: `EFFECT_UNSPECIFIED`, `PREFER_NO_SCHEDULE`, `NO_EXECUTE`.
         */
        effect?: string;
        /**
         * Key associated with the effect.
         */
        key?: string;
        /**
         * Value associated with the effect.
         */
        value?: string;
    }

    export interface BareMetalClusterLoadBalancerManualLbConfig {
        /**
         * Whether manual load balancing is enabled.
         */
        enabled: boolean;
    }

    export interface BareMetalClusterLoadBalancerMetalLbConfig {
        /**
         * AddressPools is a list of non-overlapping IP pools used by load balancer
         * typed services. All addresses must be routable to load balancer nodes.
         * IngressVIP must be included in the pools.
         * Structure is documented below.
         */
        addressPools: outputs.gkeonprem.BareMetalClusterLoadBalancerMetalLbConfigAddressPool[];
        /**
         * Specifies the load balancer's node pool configuration.
         * Structure is documented below.
         */
        loadBalancerNodePoolConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfig;
    }

    export interface BareMetalClusterLoadBalancerMetalLbConfigAddressPool {
        /**
         * The addresses that are part of this pool. Each address must be either in the CIDR form (1.2.3.0/24) or range form (1.2.3.1-1.2.3.5).
         */
        addresses: string[];
        /**
         * If true, avoid using IPs ending in .0 or .255.
         * This avoids buggy consumer devices mistakenly dropping IPv4 traffic for those special IP addresses.
         */
        avoidBuggyIps?: boolean;
        /**
         * If true, prevent IP addresses from being automatically assigned.
         */
        manualAssign?: boolean;
        /**
         * The name of the address pool.
         */
        pool: string;
    }

    export interface BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfig {
        /**
         * The generic configuration for a node pool running a load balancer.
         * Structure is documented below.
         */
        nodePoolConfig?: outputs.gkeonprem.BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfig;
    }

    export interface BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels: {[key: string]: string};
        /**
         * The list of machine addresses in the Bare Metal Node Pool.
         * Structure is documented below.
         */
        nodeConfigs?: outputs.gkeonprem.BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig[];
        /**
         * Specifies the nodes operating system (default: LINUX).
         */
        operatingSystem: string;
        /**
         * The initial taints assigned to nodes of this node pool.
         * Structure is documented below.
         */
        taints: outputs.gkeonprem.BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint[];
    }

    export interface BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * The default IPv4 address for SSH access and Kubernetes node.
         * Example: 192.168.0.1
         */
        nodeIp?: string;
    }

    export interface BareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint {
        /**
         * Specifies the nodes operating system (default: LINUX).
         * Possible values are: `EFFECT_UNSPECIFIED`, `PREFER_NO_SCHEDULE`, `NO_EXECUTE`.
         */
        effect?: string;
        /**
         * Key associated with the effect.
         */
        key?: string;
        /**
         * Value associated with the effect.
         */
        value?: string;
    }

    export interface BareMetalClusterLoadBalancerPortConfig {
        /**
         * The port that control plane hosted load balancers will listen on.
         */
        controlPlaneLoadBalancerPort: number;
    }

    export interface BareMetalClusterLoadBalancerVipConfig {
        /**
         * The VIP which you previously set aside for the Kubernetes API of this Bare Metal User Cluster.
         */
        controlPlaneVip: string;
        /**
         * The VIP which you previously set aside for ingress traffic into this Bare Metal User Cluster.
         */
        ingressVip: string;
    }

    export interface BareMetalClusterMaintenanceConfig {
        /**
         * All IPv4 address from these ranges will be placed into maintenance mode.
         * Nodes in maintenance mode will be cordoned and drained. When both of these
         * are true, the "baremetal.cluster.gke.io/maintenance" annotation will be set
         * on the node resource.
         */
        maintenanceAddressCidrBlocks: string[];
    }

    export interface BareMetalClusterNetworkConfig {
        /**
         * Enables the use of advanced Anthos networking features, such as Bundled
         * Load Balancing with BGP or the egress NAT gateway.
         * Setting configuration for advanced networking features will automatically
         * set this flag.
         */
        advancedNetworking?: boolean;
        /**
         * A nested object resource
         * Structure is documented below.
         */
        islandModeCidr?: outputs.gkeonprem.BareMetalClusterNetworkConfigIslandModeCidr;
        /**
         * Configuration for multiple network interfaces.
         * Structure is documented below.
         */
        multipleNetworkInterfacesConfig?: outputs.gkeonprem.BareMetalClusterNetworkConfigMultipleNetworkInterfacesConfig;
        /**
         * Configuration for SR-IOV.
         * Structure is documented below.
         */
        srIovConfig?: outputs.gkeonprem.BareMetalClusterNetworkConfigSrIovConfig;
    }

    export interface BareMetalClusterNetworkConfigIslandModeCidr {
        /**
         * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
         */
        podAddressCidrBlocks: string[];
        /**
         * All services in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
         */
        serviceAddressCidrBlocks: string[];
    }

    export interface BareMetalClusterNetworkConfigMultipleNetworkInterfacesConfig {
        /**
         * Whether to enable multiple network interfaces for your pods.
         * When set network_config.advanced_networking is automatically
         * set to true.
         */
        enabled?: boolean;
    }

    export interface BareMetalClusterNetworkConfigSrIovConfig {
        /**
         * Whether to install the SR-IOV operator.
         */
        enabled?: boolean;
    }

    export interface BareMetalClusterNodeAccessConfig {
        /**
         * LoginUser is the user name used to access node machines.
         * It defaults to "root" if not set.
         */
        loginUser: string;
    }

    export interface BareMetalClusterNodeConfig {
        /**
         * The available runtimes that can be used to run containers in a Bare Metal User Cluster.
         * Possible values are: `CONTAINER_RUNTIME_UNSPECIFIED`, `DOCKER`, `CONTAINERD`.
         */
        containerRuntime: string;
        /**
         * The maximum number of pods a node can run. The size of the CIDR range
         * assigned to the node will be derived from this parameter.
         */
        maxPodsPerNode: number;
    }

    export interface BareMetalClusterOsEnvironmentConfig {
        /**
         * Whether the package repo should not be included when initializing
         * bare metal machines.
         */
        packageRepoExcluded: boolean;
    }

    export interface BareMetalClusterProxy {
        /**
         * A list of IPs, hostnames, and domains that should skip the proxy.
         * Examples: ["127.0.0.1", "example.com", ".corp", "localhost"].
         */
        noProxies?: string[];
        /**
         * Specifies the address of your proxy server.
         * Examples: http://domain
         * WARNING: Do not provide credentials in the format
         * http://(username:password@)domain these will be rejected by the server.
         */
        uri: string;
    }

    export interface BareMetalClusterSecurityConfig {
        /**
         * Configures user access to the Bare Metal User cluster.
         * Structure is documented below.
         */
        authorization?: outputs.gkeonprem.BareMetalClusterSecurityConfigAuthorization;
    }

    export interface BareMetalClusterSecurityConfigAuthorization {
        /**
         * Users that will be granted the cluster-admin role on the cluster, providing full access to the cluster.
         * Structure is documented below.
         */
        adminUsers: outputs.gkeonprem.BareMetalClusterSecurityConfigAuthorizationAdminUser[];
    }

    export interface BareMetalClusterSecurityConfigAuthorizationAdminUser {
        /**
         * The name of the user, e.g. `my-gcp-id@gmail.com`.
         */
        username: string;
    }

    export interface BareMetalClusterStatus {
        /**
         * (Output)
         * ResourceConditions provide a standard mechanism for higher-level status reporting from user cluster controller.
         * Structure is documented below.
         */
        conditions: outputs.gkeonprem.BareMetalClusterStatusCondition[];
        /**
         * (Output)
         * Human-friendly representation of the error message from the user cluster
         * controller. The error message can be temporary as the user cluster
         * controller creates a cluster or node pool. If the error message persists
         * for a longer period of time, it can be used to surface error message to
         * indicate real problems requiring user intervention.
         */
        errorMessage: string;
    }

    export interface BareMetalClusterStatusCondition {
        /**
         * (Output)
         * Last time the condition transit from one status to another.
         */
        lastTransitionTime: string;
        /**
         * Human-readable message indicating details about last transition.
         */
        message?: string;
        /**
         * (Output)
         * A human-readable message of the check failure.
         */
        reason?: string;
        /**
         * (Output)
         * The lifecycle state of the condition.
         */
        state: string;
        /**
         * Type of the condition.
         * (e.g., ClusterRunning, NodePoolRunning or ServerSidePreflightReady)
         */
        type?: string;
    }

    export interface BareMetalClusterStorage {
        /**
         * Specifies the config for local PersistentVolumes backed
         * by mounted node disks. These disks need to be formatted and mounted by the
         * user, which can be done before or after cluster creation.
         * Structure is documented below.
         */
        lvpNodeMountsConfig: outputs.gkeonprem.BareMetalClusterStorageLvpNodeMountsConfig;
        /**
         * Specifies the config for local PersistentVolumes backed by
         * subdirectories in a shared filesystem. These subdirectores are
         * automatically created during cluster creation.
         * Structure is documented below.
         */
        lvpShareConfig: outputs.gkeonprem.BareMetalClusterStorageLvpShareConfig;
    }

    export interface BareMetalClusterStorageLvpNodeMountsConfig {
        /**
         * The host machine path.
         */
        path: string;
        /**
         * The StorageClass name that PVs will be created with.
         *
         * - - -
         */
        storageClass: string;
    }

    export interface BareMetalClusterStorageLvpShareConfig {
        /**
         * Defines the machine path and storage class for the LVP Share.
         * Structure is documented below.
         */
        lvpConfig: outputs.gkeonprem.BareMetalClusterStorageLvpShareConfigLvpConfig;
        /**
         * The number of subdirectories to create under path.
         */
        sharedPathPvCount?: number;
    }

    export interface BareMetalClusterStorageLvpShareConfigLvpConfig {
        /**
         * The host machine path.
         */
        path: string;
        /**
         * The StorageClass name that PVs will be created with.
         */
        storageClass: string;
    }

    export interface BareMetalClusterUpgradePolicy {
        /**
         * Specifies which upgrade policy to use.
         * Possible values are: `SERIAL`, `CONCURRENT`.
         */
        policy?: string;
    }

    export interface BareMetalClusterValidationCheck {
        /**
         * (Output)
         * Options used for the validation check.
         */
        options: string;
        /**
         * (Output)
         * The scenario when the preflight checks were run..
         */
        scenario: string;
        /**
         * (Output)
         * Specifies the detailed validation check status
         * Structure is documented below.
         */
        statuses: outputs.gkeonprem.BareMetalClusterValidationCheckStatus[];
    }

    export interface BareMetalClusterValidationCheckStatus {
        /**
         * (Output)
         * Individual checks which failed as part of the Preflight check execution.
         * Structure is documented below.
         */
        results: outputs.gkeonprem.BareMetalClusterValidationCheckStatusResult[];
    }

    export interface BareMetalClusterValidationCheckStatusResult {
        /**
         * (Output)
         * The category of the validation.
         */
        category: string;
        /**
         * A human readable description of this Bare Metal User Cluster.
         */
        description: string;
        /**
         * (Output)
         * Detailed failure information, which might be unformatted.
         */
        details: string;
        /**
         * (Output)
         * Options used for the validation check.
         */
        options: string;
        /**
         * (Output)
         * A human-readable message of the check failure.
         */
        reason: string;
    }

    export interface BareMetalNodePoolNodePoolConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels: {[key: string]: string};
        /**
         * The list of machine addresses in the Bare Metal Node Pool.
         * Structure is documented below.
         */
        nodeConfigs: outputs.gkeonprem.BareMetalNodePoolNodePoolConfigNodeConfig[];
        /**
         * Specifies the nodes operating system (default: LINUX).
         */
        operatingSystem: string;
        /**
         * The initial taints assigned to nodes of this node pool.
         * Structure is documented below.
         */
        taints: outputs.gkeonprem.BareMetalNodePoolNodePoolConfigTaint[];
    }

    export interface BareMetalNodePoolNodePoolConfigNodeConfig {
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to
         * each node. These will added in addition to any default label(s)
         * that Kubernetes may apply to the node. In case of conflict in
         * label keys, the applied set may differ depending on the Kubernetes
         * version -- it's best to assume the behavior is undefined and
         * conflicts should be avoided. For more information, including usage
         * and the valid values, see:
         * http://kubernetes.io/v1.1/docs/user-guide/labels.html
         * An object containing a list of "key": value pairs.
         * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
         */
        labels?: {[key: string]: string};
        /**
         * The default IPv4 address for SSH access and Kubernetes node.
         * Example: 192.168.0.1
         */
        nodeIp?: string;
    }

    export interface BareMetalNodePoolNodePoolConfigTaint {
        /**
         * Specifies the nodes operating system (default: LINUX).
         * Possible values are: `EFFECT_UNSPECIFIED`, `PREFER_NO_SCHEDULE`, `NO_EXECUTE`.
         *
         * - - -
         */
        effect?: string;
        /**
         * Key associated with the effect.
         */
        key?: string;
        /**
         * Value associated with the effect.
         */
        value?: string;
    }

    export interface BareMetalNodePoolStatus {
        /**
         * (Output)
         * ResourceConditions provide a standard mechanism for higher-level status reporting from user cluster controller.
         * Structure is documented below.
         */
        conditions: outputs.gkeonprem.BareMetalNodePoolStatusCondition[];
        /**
         * (Output)
         * Human-friendly representation of the error message from the user cluster
         * controller. The error message can be temporary as the user cluster
         * controller creates a cluster or node pool. If the error message persists
         * for a longer period of time, it can be used to surface error message to
         * indicate real problems requiring user intervention.
         */
        errorMessage: string;
    }

    export interface BareMetalNodePoolStatusCondition {
        /**
         * (Output)
         * Last time the condition transit from one status to another.
         */
        lastTransitionTime: string;
        /**
         * Human-readable message indicating details about last transition.
         */
        message?: string;
        /**
         * Machine-readable message indicating details about last transition.
         */
        reason?: string;
        /**
         * (Output)
         * The lifecycle state of the condition.
         */
        state: string;
        /**
         * Type of the condition.
         * (e.g., ClusterRunning, NodePoolRunning or ServerSidePreflightReady)
         */
        type?: string;
    }

    export interface VMwareClusterAntiAffinityGroups {
        /**
         * Spread nodes across at least three physical hosts (requires at least three
         * hosts).
         * Enabled by default.
         */
        aagConfigDisabled: boolean;
    }

    export interface VMwareClusterAuthorization {
        /**
         * Users that will be granted the cluster-admin role on the cluster, providing
         * full access to the cluster.
         * Structure is documented below.
         */
        adminUsers?: outputs.gkeonprem.VMwareClusterAuthorizationAdminUser[];
    }

    export interface VMwareClusterAuthorizationAdminUser {
        /**
         * The name of the user, e.g. `my-gcp-id@gmail.com`.
         */
        username: string;
    }

    export interface VMwareClusterAutoRepairConfig {
        /**
         * Whether auto repair is enabled.
         */
        enabled: boolean;
    }

    export interface VMwareClusterControlPlaneNode {
        /**
         * AutoResizeConfig provides auto resizing configurations.
         * Structure is documented below.
         */
        autoResizeConfig: outputs.gkeonprem.VMwareClusterControlPlaneNodeAutoResizeConfig;
        /**
         * The number of CPUs for each admin cluster node that serve as control planes
         * for this VMware User Cluster. (default: 4 CPUs)
         */
        cpus?: number;
        /**
         * The megabytes of memory for each admin cluster node that serves as a
         * control plane for this VMware User Cluster (default: 8192 MB memory).
         */
        memory?: number;
        /**
         * The number of control plane nodes for this VMware User Cluster.
         * (default: 1 replica).
         */
        replicas?: number;
        /**
         * (Output)
         * Vsphere-specific config.
         * Structure is documented below.
         */
        vsphereConfigs: outputs.gkeonprem.VMwareClusterControlPlaneNodeVsphereConfig[];
    }

    export interface VMwareClusterControlPlaneNodeAutoResizeConfig {
        /**
         * Whether to enable control plane node auto resizing.
         *
         * <a name="nestedVsphereConfig"></a>The `vsphereConfig` block contains:
         */
        enabled: boolean;
    }

    export interface VMwareClusterControlPlaneNodeVsphereConfig {
        /**
         * (Output)
         * The Vsphere datastore used by the Control Plane Node.
         */
        datastore: string;
        /**
         * (Output)
         * The Vsphere storage policy used by the control plane Node.
         *
         * - - -
         */
        storagePolicyName: string;
    }

    export interface VMwareClusterDataplaneV2 {
        /**
         * Enable advanced networking which requires dataplaneV2Enabled to be set true.
         */
        advancedNetworking?: boolean;
        /**
         * Enables Dataplane V2.
         */
        dataplaneV2Enabled?: boolean;
        /**
         * Enable Dataplane V2 for clusters with Windows nodes.
         */
        windowsDataplaneV2Enabled?: boolean;
    }

    export interface VMwareClusterFleet {
        /**
         * (Output)
         * The name of the managed Hub Membership resource associated to this cluster.
         * Membership names are formatted as
         * `projects/<project-number>/locations/<location>/memberships/<cluster-id>`.
         */
        membership: string;
    }

    export interface VMwareClusterLoadBalancer {
        /**
         * Configuration for F5 Big IP typed load balancers.
         * Structure is documented below.
         */
        f5Config?: outputs.gkeonprem.VMwareClusterLoadBalancerF5Config;
        /**
         * Manually configured load balancers.
         * Structure is documented below.
         */
        manualLbConfig?: outputs.gkeonprem.VMwareClusterLoadBalancerManualLbConfig;
        /**
         * Configuration for MetalLB typed load balancers.
         * Structure is documented below.
         */
        metalLbConfig?: outputs.gkeonprem.VMwareClusterLoadBalancerMetalLbConfig;
        /**
         * The VIPs used by the load balancer.
         * Structure is documented below.
         */
        vipConfig?: outputs.gkeonprem.VMwareClusterLoadBalancerVipConfig;
    }

    export interface VMwareClusterLoadBalancerF5Config {
        /**
         * The load balancer's IP address.
         */
        address?: string;
        /**
         * he preexisting partition to be used by the load balancer. T
         * his partition is usually created for the admin cluster for example:
         * 'my-f5-admin-partition'.
         */
        partition?: string;
        /**
         * The pool name. Only necessary, if using SNAT.
         */
        snatPool: string;
    }

    export interface VMwareClusterLoadBalancerManualLbConfig {
        /**
         * NodePort for control plane service. The Kubernetes API server in the admin
         * cluster is implemented as a Service of type NodePort (ex. 30968).
         */
        controlPlaneNodePort: number;
        /**
         * NodePort for ingress service's http. The ingress service in the admin
         * cluster is implemented as a Service of type NodePort (ex. 32527).
         */
        ingressHttpNodePort: number;
        /**
         * NodePort for ingress service's https. The ingress service in the admin
         * cluster is implemented as a Service of type NodePort (ex. 30139).
         */
        ingressHttpsNodePort: number;
        /**
         * NodePort for konnectivity server service running as a sidecar in each
         * kube-apiserver pod (ex. 30564).
         */
        konnectivityServerNodePort: number;
    }

    export interface VMwareClusterLoadBalancerMetalLbConfig {
        /**
         * AddressPools is a list of non-overlapping IP pools used by load balancer
         * typed services. All addresses must be routable to load balancer nodes.
         * IngressVIP must be included in the pools.
         * Structure is documented below.
         */
        addressPools: outputs.gkeonprem.VMwareClusterLoadBalancerMetalLbConfigAddressPool[];
    }

    export interface VMwareClusterLoadBalancerMetalLbConfigAddressPool {
        /**
         * The addresses that are part of this pool. Each address
         * must be either in the CIDR form (1.2.3.0/24) or range
         * form (1.2.3.1-1.2.3.5).
         */
        addresses: string[];
        /**
         * If true, avoid using IPs ending in .0 or .255.
         * This avoids buggy consumer devices mistakenly dropping IPv4 traffic for
         * those special IP addresses.
         */
        avoidBuggyIps: boolean;
        /**
         * If true, prevent IP addresses from being automatically assigned.
         *
         * <a name="nestedDataplaneV2"></a>The `dataplaneV2` block supports:
         */
        manualAssign: boolean;
        /**
         * The name of the address pool.
         */
        pool: string;
    }

    export interface VMwareClusterLoadBalancerVipConfig {
        /**
         * The VIP which you previously set aside for the Kubernetes API of this cluster.
         */
        controlPlaneVip?: string;
        /**
         * The VIP which you previously set aside for ingress traffic into this cluster.
         *
         * <a name="nestedF5Config"></a>The `f5Config` block supports:
         */
        ingressVip?: string;
    }

    export interface VMwareClusterNetworkConfig {
        /**
         * Configuration for control plane V2 mode.
         * Structure is documented below.
         */
        controlPlaneV2Config?: outputs.gkeonprem.VMwareClusterNetworkConfigControlPlaneV2Config;
        /**
         * Configuration settings for a DHCP IP configuration.
         * Structure is documented below.
         */
        dhcpIpConfig: outputs.gkeonprem.VMwareClusterNetworkConfigDhcpIpConfig;
        /**
         * Represents common network settings irrespective of the host's IP address.
         * Structure is documented below.
         */
        hostConfig: outputs.gkeonprem.VMwareClusterNetworkConfigHostConfig;
        /**
         * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges.
         * Only a single range is supported. This field cannot be changed after creation.
         */
        podAddressCidrBlocks: string[];
        /**
         * All services in the cluster are assigned an RFC1918 IPv4 address
         * from these ranges. Only a single range is supported.. This field
         * cannot be changed after creation.
         */
        serviceAddressCidrBlocks: string[];
        /**
         * Configuration settings for a static IP configuration.
         * Structure is documented below.
         */
        staticIpConfig?: outputs.gkeonprem.VMwareClusterNetworkConfigStaticIpConfig;
        /**
         * (Output)
         * vcenterNetwork specifies vCenter network name. Inherited from the admin cluster.
         */
        vcenterNetwork: string;
    }

    export interface VMwareClusterNetworkConfigControlPlaneV2Config {
        /**
         * Static IP addresses for the control plane nodes.
         * Structure is documented below.
         */
        controlPlaneIpBlock?: outputs.gkeonprem.VMwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlock;
    }

    export interface VMwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlock {
        /**
         * The network gateway used by the VMware User Cluster.
         */
        gateway?: string;
        /**
         * The node's network configurations used by the VMware User Cluster.
         * Structure is documented below.
         */
        ips?: outputs.gkeonprem.VMwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlockIp[];
        /**
         * The netmask used by the VMware User Cluster.
         */
        netmask?: string;
    }

    export interface VMwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlockIp {
        /**
         * Hostname of the machine. VM's name will be used if this field is empty.
         */
        hostname: string;
        /**
         * IP could be an IP address (like 1.2.3.4) or a CIDR (like 1.2.3.0/24).
         */
        ip?: string;
    }

    export interface VMwareClusterNetworkConfigDhcpIpConfig {
        /**
         * enabled is a flag to mark if DHCP IP allocation is
         * used for VMware user clusters.
         */
        enabled: boolean;
    }

    export interface VMwareClusterNetworkConfigHostConfig {
        /**
         * DNS search domains.
         *
         * <a name="nestedControlPlaneV2Config"></a>The `controlPlaneV2Config` block supports:
         */
        dnsSearchDomains?: string[];
        /**
         * DNS servers.
         */
        dnsServers?: string[];
        /**
         * NTP servers.
         */
        ntpServers?: string[];
    }

    export interface VMwareClusterNetworkConfigStaticIpConfig {
        /**
         * Represents the configuration values for static IP allocation to nodes.
         * Structure is documented below.
         */
        ipBlocks: outputs.gkeonprem.VMwareClusterNetworkConfigStaticIpConfigIpBlock[];
    }

    export interface VMwareClusterNetworkConfigStaticIpConfigIpBlock {
        /**
         * The network gateway used by the VMware User Cluster.
         */
        gateway: string;
        /**
         * The node's network configurations used by the VMware User Cluster.
         * Structure is documented below.
         */
        ips: outputs.gkeonprem.VMwareClusterNetworkConfigStaticIpConfigIpBlockIp[];
        /**
         * The netmask used by the VMware User Cluster.
         */
        netmask: string;
    }

    export interface VMwareClusterNetworkConfigStaticIpConfigIpBlockIp {
        /**
         * Hostname of the machine. VM's name will be used if this field is empty.
         */
        hostname: string;
        /**
         * IP could be an IP address (like 1.2.3.4) or a CIDR (like 1.2.3.0/24).
         */
        ip: string;
    }

    export interface VMwareClusterStatus {
        /**
         * (Output)
         * ResourceConditions provide a standard mechanism for higher-level status reporting from user cluster controller.
         * Structure is documented below.
         */
        conditions: outputs.gkeonprem.VMwareClusterStatusCondition[];
        /**
         * (Output)
         * Human-friendly representation of the error message from the user cluster
         * controller. The error message can be temporary as the user cluster
         * controller creates a cluster or node pool. If the error message persists
         * for a longer period of time, it can be used to surface error message to
         * indicate real problems requiring user intervention.
         */
        errorMessage: string;
    }

    export interface VMwareClusterStatusCondition {
        /**
         * (Output)
         * Last time the condition transit from one status to another.
         */
        lastTransitionTime: string;
        /**
         * (Output)
         * Human-readable message indicating details about last transition.
         */
        message: string;
        /**
         * (Output)
         * Machine-readable message indicating details about last transition.
         */
        reason: string;
        /**
         * (Output)
         * The lifecycle state of the condition.
         */
        state: string;
        /**
         * (Output)
         * Type of the condition.
         * (e.g., ClusterRunning, NodePoolRunning or ServerSidePreflightReady)
         */
        type: string;
    }

    export interface VMwareClusterStorage {
        /**
         * Whether or not to deploy vSphere CSI components in the VMware User Cluster.
         * Enabled by default.
         */
        vsphereCsiDisabled: boolean;
    }

    export interface VMwareClusterUpgradePolicy {
        /**
         * Controls whether the upgrade applies to the control plane only.
         */
        controlPlaneOnly?: boolean;
    }

    export interface VMwareClusterValidationCheck {
        /**
         * (Output)
         * Options used for the validation check.
         */
        options: string;
        /**
         * (Output)
         * The scenario when the preflight checks were run..
         */
        scenario: string;
        /**
         * (Output)
         * Specifies the detailed validation check status
         * Structure is documented below.
         */
        statuses: outputs.gkeonprem.VMwareClusterValidationCheckStatus[];
    }

    export interface VMwareClusterValidationCheckStatus {
        /**
         * (Output)
         * Individual checks which failed as part of the Preflight check execution.
         * Structure is documented below.
         */
        results: outputs.gkeonprem.VMwareClusterValidationCheckStatusResult[];
    }

    export interface VMwareClusterValidationCheckStatusResult {
        /**
         * (Output)
         * The category of the validation.
         */
        category: string;
        /**
         * A human readable description of this VMware User Cluster.
         */
        description: string;
        /**
         * (Output)
         * Detailed failure information, which might be unformatted.
         */
        details: string;
        /**
         * (Output)
         * Options used for the validation check.
         */
        options: string;
        /**
         * (Output)
         * Machine-readable message indicating details about last transition.
         */
        reason: string;
    }

    export interface VMwareClusterVcenter {
        /**
         * The load balancer's IP address.
         */
        address: string;
        /**
         * (Output)
         * Contains the vCenter CA certificate public key for SSL verification.
         */
        caCertData: string;
        /**
         * (Output)
         * The name of the vCenter cluster for the user cluster.
         */
        cluster: string;
        /**
         * (Output)
         * The name of the vCenter datacenter for the user cluster.
         */
        datacenter: string;
        /**
         * (Output)
         * The Vsphere datastore used by the Control Plane Node.
         */
        datastore: string;
        /**
         * (Output)
         * The name of the vCenter folder for the user cluster.
         */
        folder: string;
        /**
         * (Output)
         * The name of the vCenter resource pool for the user cluster.
         */
        resourcePool: string;
        /**
         * (Output)
         * The Vsphere storage policy used by the control plane Node.
         *
         * - - -
         */
        storagePolicyName: string;
    }

    export interface VMwareNodePoolConfig {
        /**
         * VMware disk size to be used during creation.
         */
        bootDiskSizeGb?: number;
        /**
         * The number of CPUs for each node in the node pool.
         */
        cpus?: number;
        /**
         * Allow node pool traffic to be load balanced. Only works for clusters with
         * MetalLB load balancers.
         */
        enableLoadBalancer?: boolean;
        /**
         * The OS image name in vCenter, only valid when using Windows.
         */
        image?: string;
        /**
         * The OS image to be used for each node in a node pool.
         * Currently `cos`, `ubuntu`, `ubuntuContainerd` and `windows` are supported.
         */
        imageType: string;
        /**
         * The map of Kubernetes labels (key/value pairs) to be applied to each node.
         * These will added in addition to any default label(s) that
         * Kubernetes may apply to the node.
         * In case of conflict in label keys, the applied set may differ depending on
         * the Kubernetes version -- it's best to assume the behavior is undefined
         * and conflicts should be avoided.
         */
        labels: {[key: string]: string};
        /**
         * The megabytes of memory for each node in the node pool.
         */
        memoryMb?: number;
        /**
         * The number of nodes in the node pool.
         */
        replicas?: number;
        /**
         * The initial taints assigned to nodes of this node pool.
         * Structure is documented below.
         */
        taints?: outputs.gkeonprem.VMwareNodePoolConfigTaint[];
        /**
         * (Output)
         * Specifies the vSphere config for node pool.
         * Structure is documented below.
         */
        vsphereConfigs: outputs.gkeonprem.VMwareNodePoolConfigVsphereConfig[];
    }

    export interface VMwareNodePoolConfigTaint {
        /**
         * Available taint effects.
         * Possible values are: `EFFECT_UNSPECIFIED`, `NO_SCHEDULE`, `PREFER_NO_SCHEDULE`, `NO_EXECUTE`.
         *
         * <a name="nestedVsphereConfig"></a>The `vsphereConfig` block contains:
         */
        effect?: string;
        /**
         * Key associated with the effect.
         */
        key: string;
        /**
         * Value associated with the effect.
         */
        value: string;
    }

    export interface VMwareNodePoolConfigVsphereConfig {
        /**
         * (Output)
         * The name of the vCenter datastore. Inherited from the user cluster.
         */
        datastore: string;
        /**
         * (Output)
         * Tags to apply to VMs.
         * Structure is documented below.
         *
         *
         * <a name="nestedTags"></a>The `tags` block contains:
         */
        tags: outputs.gkeonprem.VMwareNodePoolConfigVsphereConfigTag[];
    }

    export interface VMwareNodePoolConfigVsphereConfigTag {
        /**
         * (Output)
         * The Vsphere tag category.
         */
        category: string;
        /**
         * (Output)
         * The Vsphere tag name.
         *
         * - - -
         */
        tag: string;
    }

    export interface VMwareNodePoolNodePoolAutoscaling {
        /**
         * Maximum number of replicas in the NodePool.
         */
        maxReplicas: number;
        /**
         * Minimum number of replicas in the NodePool.
         */
        minReplicas: number;
    }

    export interface VMwareNodePoolStatus {
        /**
         * (Output)
         * ResourceConditions provide a standard mechanism for higher-level status reporting from user cluster controller.
         * Structure is documented below.
         */
        conditions: outputs.gkeonprem.VMwareNodePoolStatusCondition[];
        /**
         * (Output)
         * Human-friendly representation of the error message from the user cluster
         * controller. The error message can be temporary as the user cluster
         * controller creates a cluster or node pool. If the error message persists
         * for a longer period of time, it can be used to surface error message to
         * indicate real problems requiring user intervention.
         */
        errorMessage: string;
    }

    export interface VMwareNodePoolStatusCondition {
        /**
         * (Output)
         * Last time the condition transit from one status to another.
         */
        lastTransitionTime: string;
        /**
         * (Output)
         * Human-readable message indicating details about last transition.
         */
        message: string;
        /**
         * (Output)
         * Machine-readable message indicating details about last transition.
         */
        reason: string;
        /**
         * (Output)
         * The lifecycle state of the condition.
         */
        state: string;
        /**
         * (Output)
         * Type of the condition.
         * (e.g., ClusterRunning, NodePoolRunning or ServerSidePreflightReady)
         */
        type: string;
    }

}

export namespace healthcare {
    export interface ConsentStoreIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ConsentStoreIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DatasetIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DatasetIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DicomStoreIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DicomStoreIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DicomStoreNotificationConfig {
        /**
         * The Cloud Pub/Sub topic that notifications of changes are published on. Supplied by the client.
         * PubsubMessage.Data will contain the resource name. PubsubMessage.MessageId is the ID of this message.
         * It is guaranteed to be unique within the topic. PubsubMessage.PublishTime is the time at which the message
         * was published. Notifications are only sent if the topic is non-empty. Topic names must be scoped to a
         * project. service-PROJECT_NUMBER@gcp-sa-healthcare.iam.gserviceaccount.com must have publisher permissions on the given
         * Cloud Pub/Sub topic. Not having adequate permissions will cause the calls that send notifications to fail.
         */
        pubsubTopic: string;
    }

    export interface DicomStoreStreamConfig {
        /**
         * BigQueryDestination to include a fully qualified BigQuery table URI where DICOM instance metadata will be streamed.
         * Structure is documented below.
         */
        bigqueryDestination: outputs.healthcare.DicomStoreStreamConfigBigqueryDestination;
    }

    export interface DicomStoreStreamConfigBigqueryDestination {
        /**
         * a fully qualified BigQuery table URI where DICOM instance metadata will be streamed.
         */
        tableUri: string;
    }

    export interface FhirStoreIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FhirStoreIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface FhirStoreNotificationConfig {
        /**
         * The Cloud Pub/Sub topic that notifications of changes are published on. Supplied by the client.
         * PubsubMessage.Data will contain the resource name. PubsubMessage.MessageId is the ID of this message.
         * It is guaranteed to be unique within the topic. PubsubMessage.PublishTime is the time at which the message
         * was published. Notifications are only sent if the topic is non-empty. Topic names must be scoped to a
         * project. service-PROJECT_NUMBER@gcp-sa-healthcare.iam.gserviceaccount.com must have publisher permissions on the given
         * Cloud Pub/Sub topic. Not having adequate permissions will cause the calls that send notifications to fail.
         */
        pubsubTopic: string;
        /**
         * Whether to send full FHIR resource to this Pub/Sub topic for Create and Update operation.
         * Note that setting this to true does not guarantee that all resources will be sent in the format of
         * full FHIR resource. When a resource change is too large or during heavy traffic, only the resource name will be
         * sent. Clients should always check the "payloadType" label from a Pub/Sub message to determine whether
         * it needs to fetch the full resource as a separate operation.
         */
        sendFullResource?: boolean;
        /**
         * Whether to send full FHIR resource to this Pub/Sub topic for deleting FHIR resource. Note that setting this to
         * true does not guarantee that all previous resources will be sent in the format of full FHIR resource. When a
         * resource change is too large or during heavy traffic, only the resource name will be sent. Clients should always
         * check the "payloadType" label from a Pub/Sub message to determine whether it needs to fetch the full previous
         * resource as a separate operation.
         */
        sendPreviousResourceOnDelete?: boolean;
    }

    export interface FhirStoreStreamConfig {
        /**
         * The destination BigQuery structure that contains both the dataset location and corresponding schema config.
         * The output is organized in one table per resource type. The server reuses the existing tables (if any) that
         * are named after the resource types, e.g. "Patient", "Observation". When there is no existing table for a given
         * resource type, the server attempts to create one.
         * See the [streaming config reference](https://cloud.google.com/healthcare/docs/reference/rest/v1beta1/projects.locations.datasets.fhirStores#streamconfig) for more details.
         * Structure is documented below.
         */
        bigqueryDestination: outputs.healthcare.FhirStoreStreamConfigBigqueryDestination;
        /**
         * Supply a FHIR resource type (such as "Patient" or "Observation"). See
         * https://www.hl7.org/fhir/valueset-resource-types.html for a list of all FHIR resource types. The server treats
         * an empty list as an intent to stream all the supported resource types in this FHIR store.
         */
        resourceTypes?: string[];
    }

    export interface FhirStoreStreamConfigBigqueryDestination {
        /**
         * BigQuery URI to a dataset, up to 2000 characters long, in the format bq://projectId.bqDatasetId
         */
        datasetUri: string;
        /**
         * The configuration for the exported BigQuery schema.
         * Structure is documented below.
         */
        schemaConfig: outputs.healthcare.FhirStoreStreamConfigBigqueryDestinationSchemaConfig;
    }

    export interface FhirStoreStreamConfigBigqueryDestinationSchemaConfig {
        /**
         * The configuration for exported BigQuery tables to be partitioned by FHIR resource's last updated time column.
         * Structure is documented below.
         */
        lastUpdatedPartitionConfig?: outputs.healthcare.FhirStoreStreamConfigBigqueryDestinationSchemaConfigLastUpdatedPartitionConfig;
        /**
         * The depth for all recursive structures in the output analytics schema. For example, concept in the CodeSystem
         * resource is a recursive structure; when the depth is 2, the CodeSystem table will have a column called
         * concept.concept but not concept.concept.concept. If not specified or set to 0, the server will use the default
         * value 2. The maximum depth allowed is 5.
         */
        recursiveStructureDepth: number;
        /**
         * Specifies the output schema type.
         * * ANALYTICS: Analytics schema defined by the FHIR community.
         * See https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md.
         * * ANALYTICS_V2: Analytics V2, similar to schema defined by the FHIR community, with added support for extensions with one or more occurrences and contained resources in stringified JSON.
         * * LOSSLESS: A data-driven schema generated from the fields present in the FHIR data being exported, with no additional simplification.
         * Default value is `ANALYTICS`.
         * Possible values are: `ANALYTICS`, `ANALYTICS_V2`, `LOSSLESS`.
         */
        schemaType?: string;
    }

    export interface FhirStoreStreamConfigBigqueryDestinationSchemaConfigLastUpdatedPartitionConfig {
        /**
         * Number of milliseconds for which to keep the storage for a partition.
         */
        expirationMs?: string;
        /**
         * Type of partitioning.
         * Possible values are: `PARTITION_TYPE_UNSPECIFIED`, `HOUR`, `DAY`, `MONTH`, `YEAR`.
         */
        type: string;
    }

    export interface Hl7StoreIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface Hl7StoreIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface Hl7StoreNotificationConfig {
        /**
         * The Cloud Pub/Sub topic that notifications of changes are published on. Supplied by the client.
         * PubsubMessage.Data will contain the resource name. PubsubMessage.MessageId is the ID of this message.
         * It is guaranteed to be unique within the topic. PubsubMessage.PublishTime is the time at which the message
         * was published. Notifications are only sent if the topic is non-empty. Topic names must be scoped to a
         * project. service-PROJECT_NUMBER@gcp-sa-healthcare.iam.gserviceaccount.com must have publisher permissions on the given
         * Cloud Pub/Sub topic. Not having adequate permissions will cause the calls that send notifications to fail.
         */
        pubsubTopic: string;
    }

    export interface Hl7StoreNotificationConfigs {
        /**
         * Restricts notifications sent for messages matching a filter. If this is empty, all messages
         * are matched. Syntax: https://cloud.google.com/appengine/docs/standard/python/search/query_strings
         * Fields/functions available for filtering are:
         * * messageType, from the MSH-9.1 field. For example, NOT messageType = "ADT".
         * * sendDate or sendDate, the YYYY-MM-DD date the message was sent in the dataset's timeZone, from the MSH-7 segment. For example, sendDate < "2017-01-02".
         * * sendTime, the timestamp when the message was sent, using the RFC3339 time format for comparisons, from the MSH-7 segment. For example, sendTime < "2017-01-02T00:00:00-05:00".
         * * sendFacility, the care center that the message came from, from the MSH-4 segment. For example, sendFacility = "ABC".
         * * PatientId(value, type), which matches if the message lists a patient having an ID of the given value and type in the PID-2, PID-3, or PID-4 segments. For example, PatientId("123456", "MRN").
         * * labels.x, a string value of the label with key x as set using the Message.labels map. For example, labels."priority"="high". The operator :* can be used to assert the existence of a label. For example, labels."priority":*.
         */
        filter?: string;
        /**
         * The Cloud Pub/Sub topic that notifications of changes are published on. Supplied by the client.
         * PubsubMessage.Data will contain the resource name. PubsubMessage.MessageId is the ID of this message.
         * It is guaranteed to be unique within the topic. PubsubMessage.PublishTime is the time at which the message
         * was published. Notifications are only sent if the topic is non-empty. Topic names must be scoped to a
         * project. service-PROJECT_NUMBER@gcp-sa-healthcare.iam.gserviceaccount.com must have publisher permissions on the given
         * Cloud Pub/Sub topic. Not having adequate permissions will cause the calls that send notifications to fail.
         * If a notification cannot be published to Cloud Pub/Sub, errors will be logged to Stackdriver
         */
        pubsubTopic: string;
    }

    export interface Hl7StoreParserConfig {
        /**
         * Determines whether messages with no header are allowed.
         */
        allowNullHeader?: boolean;
        /**
         * JSON encoded string for schemas used to parse messages in this
         * store if schematized parsing is desired.
         */
        schema?: string;
        /**
         * Byte(s) to be used as the segment terminator. If this is unset, '\r' will be used as segment terminator.
         * A base64-encoded string.
         */
        segmentTerminator?: string;
        /**
         * The version of the unschematized parser to be used when a custom `schema` is not set.
         * Default value is `V1`.
         * Possible values are: `V1`, `V2`, `V3`.
         */
        version?: string;
    }

}

export namespace iam {
    export interface AccessBoundaryPolicyRule {
        /**
         * An access boundary rule in an IAM policy.
         * Structure is documented below.
         */
        accessBoundaryRule?: outputs.iam.AccessBoundaryPolicyRuleAccessBoundaryRule;
        /**
         * The description of the rule.
         */
        description?: string;
    }

    export interface AccessBoundaryPolicyRuleAccessBoundaryRule {
        /**
         * The availability condition further constrains the access allowed by the access boundary rule.
         * Structure is documented below.
         */
        availabilityCondition?: outputs.iam.AccessBoundaryPolicyRuleAccessBoundaryRuleAvailabilityCondition;
        /**
         * A list of permissions that may be allowed for use on the specified resource.
         */
        availablePermissions?: string[];
        /**
         * The full resource name of a Google Cloud resource entity.
         */
        availableResource?: string;
    }

    export interface AccessBoundaryPolicyRuleAccessBoundaryRuleAvailabilityCondition {
        /**
         * Description of the expression. This is a longer text which describes the expression,
         * e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting,
         * e.g. a file name and a position in the file.
         *
         * - - -
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose.
         * This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface DenyPolicyRule {
        /**
         * A deny rule in an IAM deny policy.
         * Structure is documented below.
         */
        denyRule?: outputs.iam.DenyPolicyRuleDenyRule;
        /**
         * The description of the rule.
         */
        description?: string;
    }

    export interface DenyPolicyRuleDenyRule {
        /**
         * User defined CEVAL expression. A CEVAL expression is used to specify match criteria such as origin.ip, source.region_code and contents in the request header.
         * Structure is documented below.
         */
        denialCondition?: outputs.iam.DenyPolicyRuleDenyRuleDenialCondition;
        /**
         * The permissions that are explicitly denied by this rule. Each permission uses the format `{service-fqdn}/{resource}.{verb}`,
         * where `{service-fqdn}` is the fully qualified domain name for the service. For example, `iam.googleapis.com/roles.list`.
         */
        deniedPermissions?: string[];
        /**
         * The identities that are prevented from using one or more permissions on Google Cloud resources.
         */
        deniedPrincipals?: string[];
        /**
         * Specifies the permissions that this rule excludes from the set of denied permissions given by deniedPermissions.
         * If a permission appears in deniedPermissions and in exceptionPermissions then it will not be denied.
         * The excluded permissions can be specified using the same syntax as deniedPermissions.
         */
        exceptionPermissions?: string[];
        /**
         * The identities that are excluded from the deny rule, even if they are listed in the deniedPrincipals.
         * For example, you could add a Google group to the deniedPrincipals, then exclude specific users who belong to that group.
         */
        exceptionPrincipals?: string[];
    }

    export interface DenyPolicyRuleDenyRuleDenialCondition {
        /**
         * Description of the expression. This is a longer text which describes the expression,
         * e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting,
         * e.g. a file name and a position in the file.
         *
         * - - -
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose.
         * This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface GetTestablePermissionsPermission {
        /**
         * Whether the corresponding API has been enabled for the resource.
         */
        apiDisabled: boolean;
        /**
         * The level of support for custom roles. Can be one of `"NOT_SUPPORTED"`, `"SUPPORTED"`, `"TESTING"`. Default is `"SUPPORTED"`
         */
        customSupportLevel: string;
        /**
         * Name of the permission.
         */
        name: string;
        /**
         * Release stage of the permission.
         */
        stage: string;
        /**
         * Human readable title of the permission.
         */
        title: string;
    }

    export interface GetWorkloadIdentityPoolProviderAw {
        accountId: string;
    }

    export interface GetWorkloadIdentityPoolProviderOidc {
        allowedAudiences: string[];
        issuerUri: string;
        jwksJson: string;
    }

    export interface WorkforcePoolProviderOidc {
        /**
         * The client ID. Must match the audience claim of the JWT issued by the identity provider.
         */
        clientId: string;
        /**
         * The optional client secret. Required to enable Authorization Code flow for web sign-in.
         * Structure is documented below.
         */
        clientSecret?: outputs.iam.WorkforcePoolProviderOidcClientSecret;
        /**
         * The OIDC issuer URI. Must be a valid URI using the 'https' scheme.
         */
        issuerUri: string;
        jwksJson?: string;
        /**
         * Configuration for web single sign-on for the OIDC provider. Here, web sign-in refers to console sign-in and gcloud sign-in through the browser.
         * Structure is documented below.
         */
        webSsoConfig: outputs.iam.WorkforcePoolProviderOidcWebSsoConfig;
    }

    export interface WorkforcePoolProviderOidcClientSecret {
        /**
         * The value of the client secret.
         * Structure is documented below.
         */
        value?: outputs.iam.WorkforcePoolProviderOidcClientSecretValue;
    }

    export interface WorkforcePoolProviderOidcClientSecretValue {
        /**
         * The plain text of the client secret value.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        plainText: string;
        /**
         * (Output)
         * A thumbprint to represent the current client secret value.
         */
        thumbprint: string;
    }

    export interface WorkforcePoolProviderOidcWebSsoConfig {
        /**
         * Additional scopes to request for in the OIDC authentication request on top of scopes requested by default. By default, the `openid`, `profile` and `email` scopes that are supported by the identity provider are requested.
         * Each additional scope may be at most 256 characters. A maximum of 10 additional scopes may be configured.
         */
        additionalScopes?: string[];
        /**
         * The behavior for how OIDC Claims are included in the `assertion` object used for attribute mapping and attribute condition.
         * * MERGE_USER_INFO_OVER_ID_TOKEN_CLAIMS: Merge the UserInfo Endpoint Claims with ID Token Claims, preferring UserInfo Claim Values for the same Claim Name. This option is available only for the Authorization Code Flow.
         * * ONLY_ID_TOKEN_CLAIMS: Only include ID Token Claims.
         * Possible values are: `MERGE_USER_INFO_OVER_ID_TOKEN_CLAIMS`, `ONLY_ID_TOKEN_CLAIMS`.
         */
        assertionClaimsBehavior: string;
        /**
         * The Response Type to request for in the OIDC Authorization Request for web sign-in.
         * The `CODE` Response Type is recommended to avoid the Implicit Flow, for security reasons.
         * * CODE: The `response_type=code` selection uses the Authorization Code Flow for web sign-in. Requires a configured client secret.
         * * ID_TOKEN: The `response_type=id_token` selection uses the Implicit Flow for web sign-in.
         * Possible values are: `CODE`, `ID_TOKEN`.
         */
        responseType: string;
    }

    export interface WorkforcePoolProviderSaml {
        /**
         * SAML Identity provider configuration metadata xml doc.
         * The xml document should comply with [SAML 2.0 specification](https://docs.oasis-open.org/security/saml/v2.0/saml-metadata-2.0-os.pdf).
         * The max size of the acceptable xml document will be bounded to 128k characters.
         * The metadata xml document should satisfy the following constraints:
         * 1) Must contain an Identity Provider Entity ID.
         * 2) Must contain at least one non-expired signing key certificate.
         * 3) For each signing key:
         * a) Valid from should be no more than 7 days from now.
         * b) Valid to should be no more than 10 years in the future.
         * 4) Up to 3 IdP signing keys are allowed in the metadata xml.
         * When updating the provider's metadata xml, at least one non-expired signing key
         * must overlap with the existing metadata. This requirement is skipped if there are
         * no non-expired signing keys present in the existing metadata.
         */
        idpMetadataXml: string;
    }

    export interface WorkloadIdentityPoolProviderAws {
        /**
         * The AWS account ID.
         */
        accountId: string;
    }

    export interface WorkloadIdentityPoolProviderOidc {
        /**
         * Acceptable values for the `aud` field (audience) in the OIDC token. Token exchange
         * requests are rejected if the token audience does not match one of the configured
         * values. Each audience may be at most 256 characters. A maximum of 10 audiences may
         * be configured.
         * If this list is empty, the OIDC token audience must be equal to the full canonical
         * resource name of the WorkloadIdentityPoolProvider, with or without the HTTPS prefix.
         * For example:
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        allowedAudiences?: string[];
        /**
         * The OIDC issuer URL.
         */
        issuerUri: string;
        /**
         * OIDC JWKs in JSON String format. For details on definition of a
         * JWK, see https:tools.ietf.org/html/rfc7517. If not set, then we
         * use the `jwksUri` from the discovery document fetched from the
         * .well-known path for the `issuerUri`. Currently, RSA and EC asymmetric
         * keys are supported. The JWK must use following format and include only
         * the following fields:
         * ```typescript
         * import * as pulumi from "@pulumi/pulumi";
         * ```
         */
        jwksJson?: string;
    }

}

export namespace iap {
    export interface AppEngineServiceIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface AppEngineServiceIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface AppEngineVersionIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface AppEngineVersionIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface TunnelIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface TunnelIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface TunnelInstanceIAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface TunnelInstanceIAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebBackendServiceIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebBackendServiceIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebRegionBackendServiceIamBindingCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebRegionBackendServiceIamMemberCondition {
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebTypeAppEngingIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebTypeAppEngingIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebTypeComputeIamBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface WebTypeComputeIamMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

}

export namespace identityplatform {
    export interface ConfigBlockingFunctions {
        /**
         * The user credentials to include in the JWT payload that is sent to the registered Blocking Functions.
         * Structure is documented below.
         */
        forwardInboundCredentials?: outputs.identityplatform.ConfigBlockingFunctionsForwardInboundCredentials;
        /**
         * Map of Trigger to event type. Key should be one of the supported event types: "beforeCreate", "beforeSignIn".
         * Structure is documented below.
         */
        triggers: outputs.identityplatform.ConfigBlockingFunctionsTrigger[];
    }

    export interface ConfigBlockingFunctionsForwardInboundCredentials {
        /**
         * Whether to pass the user's OAuth identity provider's access token.
         */
        accessToken?: boolean;
        /**
         * Whether to pass the user's OIDC identity provider's ID token.
         */
        idToken?: boolean;
        /**
         * Whether to pass the user's OAuth identity provider's refresh token.
         */
        refreshToken?: boolean;
    }

    export interface ConfigBlockingFunctionsTrigger {
        /**
         * The identifier for this object. Format specified above.
         */
        eventType: string;
        /**
         * HTTP URI trigger for the Cloud Function.
         */
        functionUri: string;
        /**
         * (Output)
         * When the trigger was changed.
         */
        updateTime: string;
    }

    export interface ConfigQuota {
        /**
         * Quota for the Signup endpoint, if overwritten. Signup quota is measured in sign ups per project per hour per IP.
         * Structure is documented below.
         */
        signUpQuotaConfig?: outputs.identityplatform.ConfigQuotaSignUpQuotaConfig;
    }

    export interface ConfigQuotaSignUpQuotaConfig {
        /**
         * A sign up APIs quota that customers can override temporarily.
         */
        quota?: number;
        /**
         * How long this quota will be active for. It is measurred in seconds, e.g., Example: "9.615s".
         */
        quotaDuration?: string;
        /**
         * When this quota will take affect.
         */
        startTime?: string;
    }

    export interface ConfigSignIn {
        /**
         * Whether to allow more than one account to have the same email.
         */
        allowDuplicateEmails?: boolean;
        /**
         * Configuration options related to authenticating an anonymous user.
         * Structure is documented below.
         */
        anonymous?: outputs.identityplatform.ConfigSignInAnonymous;
        /**
         * Configuration options related to authenticating a user by their email address.
         * Structure is documented below.
         */
        email?: outputs.identityplatform.ConfigSignInEmail;
        /**
         * (Output)
         * Output only. Hash config information.
         * Structure is documented below.
         */
        hashConfigs: outputs.identityplatform.ConfigSignInHashConfig[];
        /**
         * Configuration options related to authenticated a user by their phone number.
         * Structure is documented below.
         */
        phoneNumber?: outputs.identityplatform.ConfigSignInPhoneNumber;
    }

    export interface ConfigSignInAnonymous {
        /**
         * Whether anonymous user auth is enabled for the project or not.
         *
         * <a name="nestedHashConfig"></a>The `hashConfig` block contains:
         */
        enabled: boolean;
    }

    export interface ConfigSignInEmail {
        /**
         * Whether email auth is enabled for the project or not.
         */
        enabled: boolean;
        /**
         * Whether a password is required for email auth or not. If true, both an email and
         * password must be provided to sign in. If false, a user may sign in via either
         * email/password or email link.
         */
        passwordRequired?: boolean;
    }

    export interface ConfigSignInHashConfig {
        /**
         * (Output)
         * Different password hash algorithms used in Identity Toolkit.
         */
        algorithm: string;
        /**
         * (Output)
         * Memory cost for hash calculation. Used by scrypt and other similar password derivation algorithms. See https://tools.ietf.org/html/rfc7914 for explanation of field.
         */
        memoryCost: number;
        /**
         * (Output)
         * How many rounds for hash calculation. Used by scrypt and other similar password derivation algorithms.
         */
        rounds: number;
        /**
         * (Output)
         * Non-printable character to be inserted between the salt and plain text password in base64.
         */
        saltSeparator: string;
        /**
         * (Output)
         * Signer key in base64.
         */
        signerKey: string;
    }

    export interface ConfigSignInPhoneNumber {
        /**
         * Whether phone number auth is enabled for the project or not.
         */
        enabled: boolean;
        /**
         * A map of <test phone number, fake code> that can be used for phone auth testing.
         */
        testPhoneNumbers?: {[key: string]: string};
    }

    export interface InboundSamlConfigIdpConfig {
        /**
         * The IdP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
         * Structure is documented below.
         */
        idpCertificates: outputs.identityplatform.InboundSamlConfigIdpConfigIdpCertificate[];
        /**
         * Unique identifier for all SAML entities
         */
        idpEntityId: string;
        /**
         * Indicates if outbounding SAMLRequest should be signed.
         */
        signRequest?: boolean;
        /**
         * URL to send Authentication request to.
         */
        ssoUrl: string;
    }

    export interface InboundSamlConfigIdpConfigIdpCertificate {
        /**
         * The IdP's x509 certificate.
         */
        x509Certificate?: string;
    }

    export interface InboundSamlConfigSpConfig {
        /**
         * Callback URI where responses from IDP are handled. Must start with `https://`.
         */
        callbackUri?: string;
        /**
         * (Output)
         * The IDP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
         * Structure is documented below.
         *
         *
         * <a name="nestedSpCertificates"></a>The `spCertificates` block contains:
         */
        spCertificates: outputs.identityplatform.InboundSamlConfigSpConfigSpCertificate[];
        /**
         * Unique identifier for all SAML entities.
         */
        spEntityId?: string;
    }

    export interface InboundSamlConfigSpConfigSpCertificate {
        /**
         * The IdP's x509 certificate.
         *
         * (Output)
         * The x509 certificate
         *
         * - - -
         */
        x509Certificate: string;
    }

    export interface ProjectDefaultConfigSignIn {
        /**
         * Whether to allow more than one account to have the same email.
         */
        allowDuplicateEmails?: boolean;
        /**
         * Configuration options related to authenticating an anonymous user.
         * Structure is documented below.
         */
        anonymous?: outputs.identityplatform.ProjectDefaultConfigSignInAnonymous;
        /**
         * Configuration options related to authenticating a user by their email address.
         * Structure is documented below.
         */
        email?: outputs.identityplatform.ProjectDefaultConfigSignInEmail;
        /**
         * (Output)
         * Output only. Hash config information.
         * Structure is documented below.
         */
        hashConfigs: outputs.identityplatform.ProjectDefaultConfigSignInHashConfig[];
        /**
         * Configuration options related to authenticated a user by their phone number.
         * Structure is documented below.
         */
        phoneNumber?: outputs.identityplatform.ProjectDefaultConfigSignInPhoneNumber;
    }

    export interface ProjectDefaultConfigSignInAnonymous {
        /**
         * Whether anonymous user auth is enabled for the project or not.
         *
         * <a name="nestedHashConfig"></a>The `hashConfig` block contains:
         */
        enabled: boolean;
    }

    export interface ProjectDefaultConfigSignInEmail {
        /**
         * Whether email auth is enabled for the project or not.
         */
        enabled?: boolean;
        /**
         * Whether a password is required for email auth or not. If true, both an email and
         * password must be provided to sign in. If false, a user may sign in via either
         * email/password or email link.
         */
        passwordRequired?: boolean;
    }

    export interface ProjectDefaultConfigSignInHashConfig {
        /**
         * (Output)
         * Different password hash algorithms used in Identity Toolkit.
         */
        algorithm: string;
        /**
         * (Output)
         * Memory cost for hash calculation. Used by scrypt and other similar password derivation algorithms. See https://tools.ietf.org/html/rfc7914 for explanation of field.
         */
        memoryCost: number;
        /**
         * (Output)
         * How many rounds for hash calculation. Used by scrypt and other similar password derivation algorithms.
         */
        rounds: number;
        /**
         * (Output)
         * Non-printable character to be inserted between the salt and plain text password in base64.
         */
        saltSeparator: string;
        /**
         * (Output)
         * Signer key in base64.
         */
        signerKey: string;
    }

    export interface ProjectDefaultConfigSignInPhoneNumber {
        /**
         * Whether phone number auth is enabled for the project or not.
         */
        enabled?: boolean;
        /**
         * A map of <test phone number, fake code> that can be used for phone auth testing.
         */
        testPhoneNumbers?: {[key: string]: string};
    }

    export interface TenantInboundSamlConfigIdpConfig {
        /**
         * The IDP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
         * Structure is documented below.
         */
        idpCertificates: outputs.identityplatform.TenantInboundSamlConfigIdpConfigIdpCertificate[];
        /**
         * Unique identifier for all SAML entities
         */
        idpEntityId: string;
        /**
         * Indicates if outbounding SAMLRequest should be signed.
         */
        signRequest?: boolean;
        /**
         * URL to send Authentication request to.
         */
        ssoUrl: string;
    }

    export interface TenantInboundSamlConfigIdpConfigIdpCertificate {
        /**
         * The x509 certificate
         */
        x509Certificate?: string;
    }

    export interface TenantInboundSamlConfigSpConfig {
        /**
         * Callback URI where responses from IDP are handled. Must start with `https://`.
         */
        callbackUri: string;
        /**
         * (Output)
         * The IDP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
         * Structure is documented below.
         *
         *
         * <a name="nestedSpCertificates"></a>The `spCertificates` block contains:
         */
        spCertificates: outputs.identityplatform.TenantInboundSamlConfigSpConfigSpCertificate[];
        /**
         * Unique identifier for all SAML entities.
         */
        spEntityId: string;
    }

    export interface TenantInboundSamlConfigSpConfigSpCertificate {
        /**
         * The x509 certificate
         *
         * (Output)
         * The x509 certificate
         *
         * - - -
         */
        x509Certificate: string;
    }

}

export namespace iot {
    export interface DeviceConfig {
        /**
         * The device state data.
         */
        binaryData?: string;
        /**
         * (Output)
         * The time at which this configuration version was updated in Cloud IoT Core.
         */
        cloudUpdateTime: string;
        /**
         * (Output)
         * The time at which Cloud IoT Core received the acknowledgment from the device,
         * indicating that the device has received this configuration version.
         */
        deviceAckTime: string;
        /**
         * (Output)
         * The version of this update.
         */
        version: string;
    }

    export interface DeviceCredential {
        /**
         * The time at which this credential becomes invalid.
         */
        expirationTime: string;
        /**
         * A public key used to verify the signature of JSON Web Tokens (JWTs).
         * Structure is documented below.
         */
        publicKey: outputs.iot.DeviceCredentialPublicKey;
    }

    export interface DeviceCredentialPublicKey {
        /**
         * The format of the key.
         * Possible values are: `RSA_PEM`, `RSA_X509_PEM`, `ES256_PEM`, `ES256_X509_PEM`.
         */
        format: string;
        /**
         * The key data.
         */
        key: string;
    }

    export interface DeviceGatewayConfig {
        /**
         * Indicates whether the device is a gateway.
         * Possible values are: `ASSOCIATION_ONLY`, `DEVICE_AUTH_TOKEN_ONLY`, `ASSOCIATION_AND_DEVICE_AUTH_TOKEN`.
         */
        gatewayAuthMethod?: string;
        /**
         * Indicates whether the device is a gateway.
         * Default value is `NON_GATEWAY`.
         * Possible values are: `GATEWAY`, `NON_GATEWAY`.
         */
        gatewayType?: string;
        /**
         * (Output)
         * The ID of the gateway the device accessed most recently.
         */
        lastAccessedGatewayId: string;
        /**
         * (Output)
         * The most recent time at which the device accessed the gateway specified in last_accessed_gateway.
         */
        lastAccessedGatewayTime: string;
    }

    export interface DeviceLastErrorStatus {
        /**
         * A list of messages that carry the error details.
         */
        details?: {[key: string]: any}[];
        /**
         * A developer-facing error message, which should be in English.
         */
        message?: string;
        /**
         * The status code, which should be an enum value of google.rpc.Code.
         */
        number?: number;
    }

    export interface DeviceState {
        /**
         * The device state data.
         */
        binaryData?: string;
        /**
         * The time at which this state version was updated in Cloud IoT Core.
         */
        updateTime?: string;
    }

    export interface RegistryCredential {
        /**
         * A public key certificate format and data.
         */
        publicKeyCertificate: {[key: string]: any};
    }

    export interface RegistryEventNotificationConfigItem {
        /**
         * PubSub topic name to publish device events.
         */
        pubsubTopicName: string;
        /**
         * If the subfolder name matches this string exactly, this
         * configuration will be used. The string must not include the
         * leading '/' character. If empty, all strings are matched. Empty
         * value can only be used for the last `eventNotificationConfigs`
         * item.
         */
        subfolderMatches?: string;
    }

    export interface RegistryIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RegistryIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace kms {
    export interface CryptoKeyIAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface CryptoKeyIAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface CryptoKeyVersionAttestation {
        /**
         * The certificate chains needed to validate the attestation
         * Structure is documented below.
         */
        certChains?: outputs.kms.CryptoKeyVersionAttestationCertChains;
        /**
         * (Output)
         * The attestation data provided by the HSM when the key operation was performed.
         */
        content: string;
        /**
         * ExternalProtectionLevelOptions stores a group of additional fields for configuring a CryptoKeyVersion that are specific to the EXTERNAL protection level and EXTERNAL_VPC protection levels.
         * Structure is documented below.
         */
        externalProtectionLevelOptions?: outputs.kms.CryptoKeyVersionAttestationExternalProtectionLevelOptions;
        /**
         * (Output)
         * The format of the attestation data.
         */
        format: string;
    }

    export interface CryptoKeyVersionAttestationCertChains {
        /**
         * Cavium certificate chain corresponding to the attestation.
         */
        caviumCerts?: string;
        /**
         * Google card certificate chain corresponding to the attestation.
         */
        googleCardCerts?: string;
        /**
         * Google partition certificate chain corresponding to the attestation.
         */
        googlePartitionCerts?: string;
    }

    export interface CryptoKeyVersionAttestationExternalProtectionLevelOptions {
        /**
         * The path to the external key material on the EKM when using EkmConnection e.g., "v0/my/key". Set this field instead of externalKeyUri when using an EkmConnection.
         */
        ekmConnectionKeyPath?: string;
        /**
         * The URI for an external resource that this CryptoKeyVersion represents.
         */
        externalKeyUri?: string;
    }

    export interface CryptoKeyVersionTemplate {
        /**
         * The algorithm to use when creating a version based on this template.
         * See the [algorithm reference](https://cloud.google.com/kms/docs/reference/rest/v1/CryptoKeyVersionAlgorithm) for possible inputs.
         */
        algorithm: string;
        /**
         * The protection level to use when creating a version based on this template. Possible values include "SOFTWARE", "HSM", "EXTERNAL", "EXTERNAL_VPC". Defaults to "SOFTWARE".
         */
        protectionLevel?: string;
    }

    export interface GetKMSCryptoKeyVersionPublicKey {
        /**
         * The CryptoKeyVersionAlgorithm that this CryptoKeyVersion supports.
         */
        algorithm: string;
        /**
         * The public key, encoded in PEM format. For more information, see the RFC 7468 sections for General Considerations and Textual Encoding of Subject Public Key Info.
         */
        pem: string;
    }

    export interface GetKMSCryptoKeyVersionTemplate {
        algorithm: string;
        protectionLevel: string;
    }

    export interface KeyRingIAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface KeyRingIAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** The provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface KeyRingImportJobAttestation {
        /**
         * (Output)
         * The attestation data provided by the HSM when the key operation was performed.
         * A base64-encoded string.
         */
        content: string;
        /**
         * (Output)
         * The format of the attestation data.
         */
        format: string;
    }

    export interface KeyRingImportJobPublicKey {
        /**
         * (Output)
         * The public key, encoded in PEM format. For more information, see the RFC 7468 sections
         * for General Considerations and Textual Encoding of Subject Public Key Info.
         */
        pem: string;
    }

    export interface RegistryCredential {
        /**
         * A public key certificate format and data.
         */
        publicKeyCertificate: {[key: string]: any};
    }

    export interface RegistryEventNotificationConfigItem {
        /**
         * PubSub topic name to publish device events.
         */
        pubsubTopicName: string;
        /**
         * If the subfolder name matches this string exactly, this
         * configuration will be used. The string must not include the
         * leading '/' character. If empty, all strings are matched. Empty
         * value can only be used for the last `eventNotificationConfigs`
         * item.
         */
        subfolderMatches?: string;
    }

}

export namespace logging {
    export interface BillingAccountBucketConfigCmekSettings {
        kmsKeyName: string;
        kmsKeyVersionName: string;
        /**
         * The resource name of the bucket. For example: "projects/my-project-id/locations/my-location/buckets/my-bucket-id"
         */
        name: string;
        serviceAccountId: string;
    }

    export interface BillingAccountSinkBigqueryOptions {
        /**
         * Whether to use [BigQuery's partition tables](https://cloud.google.com/bigquery/docs/partitioned-tables).
         * By default, Logging creates dated tables based on the log entries' timestamps, e.g. syslog_20170523. With partitioned
         * tables, the date suffix is no longer present and [special query syntax](https://cloud.google.com/bigquery/docs/querying-partitioned-tables)
         * has to be used instead. In both cases, tables are sharded based on UTC timezone.
         */
        usePartitionedTables: boolean;
    }

    export interface BillingAccountSinkExclusion {
        /**
         * A description of this exclusion.
         */
        description?: string;
        /**
         * If set to True, then this exclusion is disabled and it does not exclude any log entries.
         */
        disabled?: boolean;
        /**
         * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries. See [Advanced Log Filters](https://cloud.google.com/logging/docs/view/advanced_filters) for information on how to
         * write a filter.
         */
        filter: string;
        /**
         * A client-assigned identifier, such as `load-balancer-exclusion`. Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
         */
        name: string;
    }

    export interface FolderBucketConfigCmekSettings {
        kmsKeyName: string;
        kmsKeyVersionName: string;
        /**
         * The resource name of the bucket. For example: "folders/my-folder-id/locations/my-location/buckets/my-bucket-id"
         */
        name: string;
        serviceAccountId: string;
    }

    export interface FolderSinkBigqueryOptions {
        /**
         * Whether to use [BigQuery's partition tables](https://cloud.google.com/bigquery/docs/partitioned-tables).
         * By default, Logging creates dated tables based on the log entries' timestamps, e.g. syslog_20170523. With partitioned
         * tables, the date suffix is no longer present and [special query syntax](https://cloud.google.com/bigquery/docs/querying-partitioned-tables)
         * has to be used instead. In both cases, tables are sharded based on UTC timezone.
         */
        usePartitionedTables: boolean;
    }

    export interface FolderSinkExclusion {
        /**
         * A description of this exclusion.
         */
        description?: string;
        /**
         * If set to True, then this exclusion is disabled and it does not exclude any log entries.
         */
        disabled?: boolean;
        /**
         * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries. See [Advanced Log Filters](https://cloud.google.com/logging/docs/view/advanced_filters) for information on how to
         * write a filter.
         */
        filter: string;
        /**
         * A client-assigned identifier, such as `load-balancer-exclusion`. Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
         */
        name: string;
    }

    export interface GetSinkBigqueryOption {
        /**
         * Whether [BigQuery's partition tables](https://cloud.google.com/bigquery/docs/partitioned-tables) are used.
         */
        usePartitionedTables: boolean;
    }

    export interface GetSinkExclusion {
        /**
         * A description of this exclusion.
         */
        description: string;
        /**
         * Whether this exclusion is disabled and it does not exclude any log entries.
         */
        disabled: boolean;
        /**
         * An advanced logs filter that matches the log entries to be excluded.
         */
        filter: string;
        /**
         * A client-assigned identifier, such as `load-balancer-exclusion`.
         */
        name: string;
    }

    export interface LinkedDatasetBigqueryDataset {
        /**
         * (Output)
         * Output only. The full resource name of the BigQuery dataset. The DATASET_ID will match the ID
         * of the link, so the link must match the naming restrictions of BigQuery datasets
         * (alphanumeric characters and underscores only). The dataset will have a resource path of
         * "bigquery.googleapis.com/projects/[PROJECT_ID]/datasets/[DATASET_ID]"
         */
        datasetId: string;
    }

    export interface MetricBucketOptions {
        /**
         * Specifies a set of buckets with arbitrary widths.
         * Structure is documented below.
         */
        explicitBuckets?: outputs.logging.MetricBucketOptionsExplicitBuckets;
        /**
         * Specifies an exponential sequence of buckets that have a width that is proportional to the value of
         * the lower bound. Each bucket represents a constant relative uncertainty on a specific value in the bucket.
         * Structure is documented below.
         */
        exponentialBuckets?: outputs.logging.MetricBucketOptionsExponentialBuckets;
        /**
         * Specifies a linear sequence of buckets that all have the same width (except overflow and underflow).
         * Each bucket represents a constant absolute uncertainty on the specific value in the bucket.
         * Structure is documented below.
         */
        linearBuckets?: outputs.logging.MetricBucketOptionsLinearBuckets;
    }

    export interface MetricBucketOptionsExplicitBuckets {
        /**
         * The values must be monotonically increasing.
         */
        bounds: number[];
    }

    export interface MetricBucketOptionsExponentialBuckets {
        /**
         * Must be greater than 1.
         */
        growthFactor?: number;
        /**
         * Must be greater than 0.
         */
        numFiniteBuckets?: number;
        /**
         * Must be greater than 0.
         */
        scale?: number;
    }

    export interface MetricBucketOptionsLinearBuckets {
        /**
         * Must be greater than 0.
         */
        numFiniteBuckets?: number;
        /**
         * Lower bound of the first bucket.
         */
        offset?: number;
        /**
         * Must be greater than 0.
         */
        width?: number;
    }

    export interface MetricMetricDescriptor {
        /**
         * A concise name for the metric, which can be displayed in user interfaces. Use sentence case
         * without an ending period, for example "Request count". This field is optional but it is
         * recommended to be set for any metrics associated with user-visible concepts, such as Quota.
         */
        displayName?: string;
        /**
         * The set of labels that can be used to describe a specific instance of this metric type. For
         * example, the appengine.googleapis.com/http/server/response_latencies metric type has a label
         * for the HTTP response code, response_code, so you can look at latencies for successful responses
         * or just for responses that failed.
         * Structure is documented below.
         */
        labels?: outputs.logging.MetricMetricDescriptorLabel[];
        /**
         * Whether the metric records instantaneous values, changes to a value, etc.
         * Some combinations of metricKind and valueType might not be supported.
         * For counter metrics, set this to DELTA.
         * Possible values are: `DELTA`, `GAUGE`, `CUMULATIVE`.
         */
        metricKind: string;
        /**
         * The unit in which the metric value is reported. It is only applicable if the valueType is
         * `INT64`, `DOUBLE`, or `DISTRIBUTION`. The supported units are a subset of
         * [The Unified Code for Units of Measure](http://unitsofmeasure.org/ucum.html) standard
         */
        unit?: string;
        /**
         * Whether the measurement is an integer, a floating-point number, etc.
         * Some combinations of metricKind and valueType might not be supported.
         * For counter metrics, set this to INT64.
         * Possible values are: `BOOL`, `INT64`, `DOUBLE`, `STRING`, `DISTRIBUTION`, `MONEY`.
         */
        valueType: string;
    }

    export interface MetricMetricDescriptorLabel {
        /**
         * A human-readable description for the label.
         */
        description?: string;
        /**
         * The label key.
         */
        key: string;
        /**
         * The type of data that can be assigned to the label.
         * Default value is `STRING`.
         * Possible values are: `BOOL`, `INT64`, `STRING`.
         */
        valueType?: string;
    }

    export interface OrganizationBucketConfigCmekSettings {
        kmsKeyName: string;
        kmsKeyVersionName: string;
        /**
         * The resource name of the bucket. For example: "organizations/my-organization-id/locations/my-location/buckets/my-bucket-id"
         */
        name: string;
        serviceAccountId: string;
    }

    export interface OrganizationSinkBigqueryOptions {
        /**
         * Whether to use [BigQuery's partition tables](https://cloud.google.com/bigquery/docs/partitioned-tables).
         * By default, Logging creates dated tables based on the log entries' timestamps, e.g. syslog_20170523. With partitioned
         * tables the date suffix is no longer present and [special query syntax](https://cloud.google.com/bigquery/docs/querying-partitioned-tables)
         * has to be used instead. In both cases, tables are sharded based on UTC timezone.
         */
        usePartitionedTables: boolean;
    }

    export interface OrganizationSinkExclusion {
        /**
         * A description of this exclusion.
         */
        description?: string;
        /**
         * If set to True, then this exclusion is disabled and it does not exclude any log entries.
         */
        disabled?: boolean;
        /**
         * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries. See [Advanced Log Filters](https://cloud.google.com/logging/docs/view/advanced_filters) for information on how to
         * write a filter.
         */
        filter: string;
        /**
         * A client-assigned identifier, such as `load-balancer-exclusion`. Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
         */
        name: string;
    }

    export interface ProjectBucketConfigCmekSettings {
        /**
         * The resource name for the configured Cloud KMS key.
         * KMS key name format:
         * `'projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]'`
         * To enable CMEK for the bucket, set this field to a valid kmsKeyName for which the associated service account has the required cloudkms.cryptoKeyEncrypterDecrypter roles assigned for the key.
         * The Cloud KMS key used by the bucket can be updated by changing the kmsKeyName to a new valid key name. Encryption operations that are in progress will be completed with the key that was in use when they started. Decryption operations will be completed using the key that was used at the time of encryption unless access to that key has been revoked.
         * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
         */
        kmsKeyName: string;
        /**
         * The CryptoKeyVersion resource name for the configured Cloud KMS key.
         * KMS key name format:
         * `'projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]/cryptoKeyVersions/[VERSION]'`
         * For example:
         * "projects/my-project/locations/us-central1/keyRings/my-ring/cryptoKeys/my-key/cryptoKeyVersions/1"
         * This is a read-only field used to convey the specific configured CryptoKeyVersion of kmsKey that has been configured. It will be populated in cases where the CMEK settings are bound to a single key version.
         */
        kmsKeyVersionName: string;
        /**
         * The resource name of the CMEK settings.
         */
        name: string;
        /**
         * The service account associated with a project for which CMEK will apply.
         * Before enabling CMEK for a logging bucket, you must first assign the cloudkms.cryptoKeyEncrypterDecrypter role to the service account associated with the project for which CMEK will apply. Use [v2.getCmekSettings](https://cloud.google.com/logging/docs/reference/v2/rest/v2/TopLevel/getCmekSettings#google.logging.v2.ConfigServiceV2.GetCmekSettings) to obtain the service account ID.
         * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
         */
        serviceAccountId: string;
    }

    export interface ProjectSinkBigqueryOptions {
        /**
         * Whether to use [BigQuery's partition tables](https://cloud.google.com/bigquery/docs/partitioned-tables).
         * By default, Logging creates dated tables based on the log entries' timestamps, e.g. `syslog20170523`. With partitioned
         * tables the date suffix is no longer present and [special query syntax](https://cloud.google.com/bigquery/docs/querying-partitioned-tables)
         * has to be used instead. In both cases, tables are sharded based on UTC timezone.
         */
        usePartitionedTables: boolean;
    }

    export interface ProjectSinkExclusion {
        /**
         * A description of this exclusion.
         */
        description?: string;
        /**
         * If set to True, then this exclusion is disabled and it does not exclude any log entries.
         */
        disabled?: boolean;
        /**
         * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries. See [Advanced Log Filters](https://cloud.google.com/logging/docs/view/advanced_filters) for information on how to
         * write a filter.
         */
        filter: string;
        /**
         * A client-assigned identifier, such as `load-balancer-exclusion`. Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
         */
        name: string;
    }

}

export namespace looker {
    export interface InstanceAdminSettings {
        allowedEmailDomains?: string[];
    }

    export interface InstanceDenyMaintenancePeriod {
        /**
         * Required. Start date of the deny maintenance period
         * Structure is documented below.
         */
        endDate: outputs.looker.InstanceDenyMaintenancePeriodEndDate;
        /**
         * Required. Start date of the deny maintenance period
         * Structure is documented below.
         */
        startDate: outputs.looker.InstanceDenyMaintenancePeriodStartDate;
        /**
         * Required. Start time of the window in UTC time.
         * Structure is documented below.
         */
        time: outputs.looker.InstanceDenyMaintenancePeriodTime;
    }

    export interface InstanceDenyMaintenancePeriodEndDate {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0
         * to specify a year by itself or a year and month where the day isn't significant.
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a
         * month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without
         * a year.
         */
        year?: number;
    }

    export interface InstanceDenyMaintenancePeriodStartDate {
        /**
         * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0
         * to specify a year by itself or a year and month where the day isn't significant.
         */
        day?: number;
        /**
         * Month of a year. Must be from 1 to 12, or 0 to specify a year without a
         * month and day.
         */
        month?: number;
        /**
         * Year of the date. Must be from 1 to 9999, or 0 to specify a date without
         * a year.
         */
        year?: number;
    }

    export interface InstanceDenyMaintenancePeriodTime {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59.
         */
        seconds?: number;
    }

    export interface InstanceEncryptionConfig {
        /**
         * Name of the customer managed encryption key (CMEK) in KMS.
         */
        kmsKeyName?: string;
        /**
         * (Output)
         * Full name and version of the CMEK key currently in use to encrypt Looker data.
         */
        kmsKeyNameVersion: string;
        /**
         * (Output)
         * Status of the customer managed encryption key (CMEK) in KMS.
         */
        kmsKeyState: string;
    }

    export interface InstanceMaintenanceWindow {
        /**
         * Required. Day of the week for this MaintenanceWindow (in UTC).
         * - MONDAY: Monday
         * - TUESDAY: Tuesday
         * - WEDNESDAY: Wednesday
         * - THURSDAY: Thursday
         * - FRIDAY: Friday
         * - SATURDAY: Saturday
         * - SUNDAY: Sunday
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeek: string;
        /**
         * Required. Start time of the window in UTC time.
         * Structure is documented below.
         */
        startTime: outputs.looker.InstanceMaintenanceWindowStartTime;
    }

    export interface InstanceMaintenanceWindowStartTime {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59.
         */
        seconds?: number;
    }

    export interface InstanceOauthConfig {
        /**
         * The client ID for the Oauth config.
         */
        clientId: string;
        /**
         * The client secret for the Oauth config.
         */
        clientSecret: string;
    }

    export interface InstanceUserMetadata {
        /**
         * Number of additional Developer Users to allocate to the Looker Instance.
         */
        additionalDeveloperUserCount?: number;
        /**
         * Number of additional Standard Users to allocate to the Looker Instance.
         */
        additionalStandardUserCount?: number;
        /**
         * Number of additional Viewer Users to allocate to the Looker Instance.
         */
        additionalViewerUserCount?: number;
    }

}

export namespace memcache {
    export interface InstanceMaintenancePolicy {
        /**
         * (Output)
         * Output only. The time when the policy was created.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits
         */
        createTime: string;
        /**
         * Optional. Description of what this policy is for.
         * Create/Update methods return INVALID_ARGUMENT if the
         * length is greater than 512.
         */
        description?: string;
        /**
         * (Output)
         * Output only. The time when the policy was updated.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        updateTime: string;
        /**
         * Required. Maintenance window that is applied to resources covered by this policy.
         * Minimum 1. For the current version, the maximum number of weeklyMaintenanceWindows
         * is expected to be one.
         * Structure is documented below.
         */
        weeklyMaintenanceWindows: outputs.memcache.InstanceMaintenancePolicyWeeklyMaintenanceWindow[];
    }

    export interface InstanceMaintenancePolicyWeeklyMaintenanceWindow {
        /**
         * Required. The day of week that maintenance updates occur.
         * - DAY_OF_WEEK_UNSPECIFIED: The day of the week is unspecified.
         * - MONDAY: Monday
         * - TUESDAY: Tuesday
         * - WEDNESDAY: Wednesday
         * - THURSDAY: Thursday
         * - FRIDAY: Friday
         * - SATURDAY: Saturday
         * - SUNDAY: Sunday
         * Possible values are: `DAY_OF_WEEK_UNSPECIFIED`, `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        day: string;
        /**
         * Required. The length of the maintenance window, ranging from 3 hours to 8 hours.
         * A duration in seconds with up to nine fractional digits,
         * terminated by 's'. Example: "3.5s".
         */
        duration: string;
        /**
         * Required. Start time of the window in UTC time.
         * Structure is documented below.
         */
        startTime: outputs.memcache.InstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime;
    }

    export interface InstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23.
         * An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59.
         * An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface InstanceMaintenanceSchedule {
        /**
         * (Output)
         * Output only. The end time of any upcoming scheduled maintenance for this instance.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        endTime: string;
        /**
         * (Output)
         * Output only. The deadline that the maintenance schedule start time
         * can not go beyond, including reschedule.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        scheduleDeadlineTime: string;
        /**
         * Required. Start time of the window in UTC time.
         * Structure is documented below.
         */
        startTime: string;
    }

    export interface InstanceMemcacheNode {
        /**
         * (Output)
         * Hostname or IP address of the Memcached node used by the clients to connect to the Memcached server on this node.
         */
        host: string;
        /**
         * (Output)
         * Identifier of the Memcached node. The node id does not include project or location like the Memcached instance name.
         */
        nodeId: string;
        /**
         * (Output)
         * The port number of the Memcached server on this node.
         */
        port: number;
        /**
         * (Output)
         * Current state of the Memcached node.
         */
        state: string;
        /**
         * (Output)
         * Location (GCP Zone) for the Memcached node.
         */
        zone: string;
    }

    export interface InstanceMemcacheParameters {
        /**
         * (Output)
         * This is a unique ID associated with this set of parameters.
         */
        id: string;
        /**
         * User-defined set of parameters to use in the memcache process.
         */
        params?: {[key: string]: string};
    }

    export interface InstanceNodeConfig {
        /**
         * Number of CPUs per node.
         */
        cpuCount: number;
        /**
         * Memory size in Mebibytes for each memcache node.
         *
         * - - -
         */
        memorySizeMb: number;
    }

}

export namespace ml {
    export interface EngineModelDefaultVersion {
        /**
         * The name specified for the version when it was created.
         */
        name: string;
    }

}

export namespace monitoring {
    export interface AlertPolicyAlertStrategy {
        /**
         * If an alert policy that was active has no data for this long, any open incidents will close.
         */
        autoClose?: string;
        /**
         * Control over how the notification channels in `notificationChannels`
         * are notified when this alert fires, on a per-channel basis.
         * Structure is documented below.
         */
        notificationChannelStrategies?: outputs.monitoring.AlertPolicyAlertStrategyNotificationChannelStrategy[];
        /**
         * Required for alert policies with a LogMatch condition.
         * This limit is not implemented for alert policies that are not log-based.
         * Structure is documented below.
         */
        notificationRateLimit?: outputs.monitoring.AlertPolicyAlertStrategyNotificationRateLimit;
    }

    export interface AlertPolicyAlertStrategyNotificationChannelStrategy {
        /**
         * The notification channels that these settings apply to. Each of these
         * correspond to the name field in one of the NotificationChannel objects
         * referenced in the notificationChannels field of this AlertPolicy. The format is
         * `projects/[PROJECT_ID_OR_NUMBER]/notificationChannels/[CHANNEL_ID]`
         */
        notificationChannelNames?: string[];
        /**
         * The frequency at which to send reminder notifications for open incidents.
         */
        renotifyInterval?: string;
    }

    export interface AlertPolicyAlertStrategyNotificationRateLimit {
        /**
         * Not more than one notification per period.
         */
        period?: string;
    }

    export interface AlertPolicyCondition {
        /**
         * A condition that checks that a time series
         * continues to receive new data points.
         * Structure is documented below.
         */
        conditionAbsent?: outputs.monitoring.AlertPolicyConditionConditionAbsent;
        /**
         * A condition that checks for log messages matching given constraints.
         * If set, no other conditions can be present.
         * Structure is documented below.
         */
        conditionMatchedLog?: outputs.monitoring.AlertPolicyConditionConditionMatchedLog;
        /**
         * A Monitoring Query Language query that outputs a boolean stream
         * Structure is documented below.
         */
        conditionMonitoringQueryLanguage?: outputs.monitoring.AlertPolicyConditionConditionMonitoringQueryLanguage;
        /**
         * A Monitoring Query Language query that outputs a boolean stream
         * A condition type that allows alert policies to be defined using
         * Prometheus Query Language (PromQL).
         * The PrometheusQueryLanguageCondition message contains information
         * from a Prometheus alerting rule and its associated rule group.
         * Structure is documented below.
         */
        conditionPrometheusQueryLanguage?: outputs.monitoring.AlertPolicyConditionConditionPrometheusQueryLanguage;
        /**
         * A condition that compares a time series against a
         * threshold.
         * Structure is documented below.
         */
        conditionThreshold?: outputs.monitoring.AlertPolicyConditionConditionThreshold;
        /**
         * A short name or phrase used to identify the
         * condition in dashboards, notifications, and
         * incidents. To avoid confusion, don't use the same
         * display name for multiple conditions in the same
         * policy.
         */
        displayName: string;
        /**
         * (Output)
         * The unique resource name for this condition.
         * Its syntax is:
         * projects/[PROJECT_ID]/alertPolicies/[POLICY_ID]/conditions/[CONDITION_ID]
         * [CONDITION_ID] is assigned by Stackdriver Monitoring when
         * the condition is created as part of a new or updated alerting
         * policy.
         */
        name: string;
    }

    export interface AlertPolicyConditionConditionAbsent {
        /**
         * Specifies the alignment of data points in
         * individual time series as well as how to
         * combine the retrieved time series together
         * (such as when aggregating multiple streams
         * on each resource to a single stream for each
         * resource or when aggregating streams across
         * all members of a group of resources).
         * Multiple aggregations are applied in the
         * order specified.
         * Structure is documented below.
         */
        aggregations?: outputs.monitoring.AlertPolicyConditionConditionAbsentAggregation[];
        /**
         * The amount of time that a time series must
         * fail to report new data to be considered
         * failing. Currently, only values that are a
         * multiple of a minute--e.g. 60s, 120s, or 300s
         * --are supported.
         */
        duration: string;
        /**
         * A filter that identifies which time series
         * should be compared with the threshold.The
         * filter is similar to the one that is
         * specified in the
         * MetricService.ListTimeSeries request (that
         * call is useful to verify the time series
         * that will be retrieved / processed) and must
         * specify the metric type and optionally may
         * contain restrictions on resource type,
         * resource labels, and metric labels. This
         * field may not exceed 2048 Unicode characters
         * in length.
         */
        filter?: string;
        /**
         * The number/percent of time series for which
         * the comparison must hold in order for the
         * condition to trigger. If unspecified, then
         * the condition will trigger if the comparison
         * is true for any of the time series that have
         * been identified by filter and aggregations.
         * Structure is documented below.
         */
        trigger?: outputs.monitoring.AlertPolicyConditionConditionAbsentTrigger;
    }

    export interface AlertPolicyConditionConditionAbsentAggregation {
        /**
         * The alignment period for per-time
         * series alignment. If present,
         * alignmentPeriod must be at least
         * 60 seconds. After per-time series
         * alignment, each time series will
         * contain data points only on the
         * period boundaries. If
         * perSeriesAligner is not specified
         * or equals ALIGN_NONE, then this
         * field is ignored. If
         * perSeriesAligner is specified and
         * does not equal ALIGN_NONE, then
         * this field must be defined;
         * otherwise an error is returned.
         */
        alignmentPeriod?: string;
        /**
         * The approach to be used to combine
         * time series. Not all reducer
         * functions may be applied to all
         * time series, depending on the
         * metric type and the value type of
         * the original time series.
         * Reduction may change the metric
         * type of value type of the time
         * series.Time series data must be
         * aligned in order to perform cross-
         * time series reduction. If
         * crossSeriesReducer is specified,
         * then perSeriesAligner must be
         * specified and not equal ALIGN_NONE
         * and alignmentPeriod must be
         * specified; otherwise, an error is
         * returned.
         * Possible values are: `REDUCE_NONE`, `REDUCE_MEAN`, `REDUCE_MIN`, `REDUCE_MAX`, `REDUCE_SUM`, `REDUCE_STDDEV`, `REDUCE_COUNT`, `REDUCE_COUNT_TRUE`, `REDUCE_COUNT_FALSE`, `REDUCE_FRACTION_TRUE`, `REDUCE_PERCENTILE_99`, `REDUCE_PERCENTILE_95`, `REDUCE_PERCENTILE_50`, `REDUCE_PERCENTILE_05`.
         */
        crossSeriesReducer?: string;
        /**
         * The set of fields to preserve when
         * crossSeriesReducer is specified.
         * The groupByFields determine how
         * the time series are partitioned
         * into subsets prior to applying the
         * aggregation function. Each subset
         * contains time series that have the
         * same value for each of the
         * grouping fields. Each individual
         * time series is a member of exactly
         * one subset. The crossSeriesReducer
         * is applied to each subset of time
         * series. It is not possible to
         * reduce across different resource
         * types, so this field implicitly
         * contains resource.type. Fields not
         * specified in groupByFields are
         * aggregated away. If groupByFields
         * is not specified and all the time
         * series have the same resource
         * type, then the time series are
         * aggregated into a single output
         * time series. If crossSeriesReducer
         * is not defined, this field is
         * ignored.
         */
        groupByFields?: string[];
        /**
         * The approach to be used to align
         * individual time series. Not all
         * alignment functions may be applied
         * to all time series, depending on
         * the metric type and value type of
         * the original time series.
         * Alignment may change the metric
         * type or the value type of the time
         * series.Time series data must be
         * aligned in order to perform cross-
         * time series reduction. If
         * crossSeriesReducer is specified,
         * then perSeriesAligner must be
         * specified and not equal ALIGN_NONE
         * and alignmentPeriod must be
         * specified; otherwise, an error is
         * returned.
         * Possible values are: `ALIGN_NONE`, `ALIGN_DELTA`, `ALIGN_RATE`, `ALIGN_INTERPOLATE`, `ALIGN_NEXT_OLDER`, `ALIGN_MIN`, `ALIGN_MAX`, `ALIGN_MEAN`, `ALIGN_COUNT`, `ALIGN_SUM`, `ALIGN_STDDEV`, `ALIGN_COUNT_TRUE`, `ALIGN_COUNT_FALSE`, `ALIGN_FRACTION_TRUE`, `ALIGN_PERCENTILE_99`, `ALIGN_PERCENTILE_95`, `ALIGN_PERCENTILE_50`, `ALIGN_PERCENTILE_05`, `ALIGN_PERCENT_CHANGE`.
         */
        perSeriesAligner?: string;
    }

    export interface AlertPolicyConditionConditionAbsentTrigger {
        /**
         * The absolute number of time series
         * that must fail the predicate for the
         * condition to be triggered.
         */
        count?: number;
        /**
         * The percentage of time series that
         * must fail the predicate for the
         * condition to be triggered.
         */
        percent?: number;
    }

    export interface AlertPolicyConditionConditionMatchedLog {
        /**
         * A logs-based filter.
         */
        filter: string;
        /**
         * A map from a label key to an extractor expression, which is used to
         * extract the value for this label key. Each entry in this map is
         * a specification for how data should be extracted from log entries that
         * match filter. Each combination of extracted values is treated as
         * a separate rule for the purposes of triggering notifications.
         * Label keys and corresponding values can be used in notifications
         * generated by this condition.
         */
        labelExtractors?: {[key: string]: string};
    }

    export interface AlertPolicyConditionConditionMonitoringQueryLanguage {
        /**
         * The amount of time that a time series must
         * violate the threshold to be considered
         * failing. Currently, only values that are a
         * multiple of a minute--e.g., 0, 60, 120, or
         * 300 seconds--are supported. If an invalid
         * value is given, an error will be returned.
         * When choosing a duration, it is useful to
         * keep in mind the frequency of the underlying
         * time series data (which may also be affected
         * by any alignments specified in the
         * aggregations field); a good duration is long
         * enough so that a single outlier does not
         * generate spurious alerts, but short enough
         * that unhealthy states are detected and
         * alerted on quickly.
         */
        duration: string;
        /**
         * A condition control that determines how
         * metric-threshold conditions are evaluated when
         * data stops arriving.
         * Possible values are: `EVALUATION_MISSING_DATA_INACTIVE`, `EVALUATION_MISSING_DATA_ACTIVE`, `EVALUATION_MISSING_DATA_NO_OP`.
         */
        evaluationMissingData?: string;
        /**
         * Monitoring Query Language query that outputs a boolean stream.
         */
        query: string;
        /**
         * The number/percent of time series for which
         * the comparison must hold in order for the
         * condition to trigger. If unspecified, then
         * the condition will trigger if the comparison
         * is true for any of the time series that have
         * been identified by filter and aggregations,
         * or by the ratio, if denominatorFilter and
         * denominatorAggregations are specified.
         * Structure is documented below.
         */
        trigger?: outputs.monitoring.AlertPolicyConditionConditionMonitoringQueryLanguageTrigger;
    }

    export interface AlertPolicyConditionConditionMonitoringQueryLanguageTrigger {
        /**
         * The absolute number of time series
         * that must fail the predicate for the
         * condition to be triggered.
         */
        count?: number;
        /**
         * The percentage of time series that
         * must fail the predicate for the
         * condition to be triggered.
         */
        percent?: number;
    }

    export interface AlertPolicyConditionConditionPrometheusQueryLanguage {
        /**
         * The alerting rule name of this alert in the corresponding Prometheus
         * configuration file.
         * Some external tools may require this field to be populated correctly
         * in order to refer to the original Prometheus configuration file.
         * The rule group name and the alert name are necessary to update the
         * relevant AlertPolicies in case the definition of the rule group changes
         * in the future.
         * This field is optional. If this field is not empty, then it must be a
         * valid Prometheus label name.
         *
         * - - -
         */
        alertRule?: string;
        /**
         * Alerts are considered firing once their PromQL expression evaluated
         * to be "true" for this long. Alerts whose PromQL expression was not
         * evaluated to be "true" for long enough are considered pending. The
         * default value is zero. Must be zero or positive.
         */
        duration?: string;
        /**
         * How often this rule should be evaluated. Must be a positive multiple
         * of 30 seconds or missing. The default value is 30 seconds. If this
         * PrometheusQueryLanguageCondition was generated from a Prometheus
         * alerting rule, then this value should be taken from the enclosing
         * rule group.
         */
        evaluationInterval?: string;
        /**
         * Labels to add to or overwrite in the PromQL query result. Label names
         * must be valid.
         * Label values can be templatized by using variables. The only available
         * variable names are the names of the labels in the PromQL result, including
         * "__name__" and "value". "labels" may be empty. This field is intended to be
         * used for organizing and identifying the AlertPolicy
         */
        labels?: {[key: string]: string};
        /**
         * The PromQL expression to evaluate. Every evaluation cycle this
         * expression is evaluated at the current time, and all resultant time
         * series become pending/firing alerts. This field must not be empty.
         */
        query: string;
        /**
         * The rule group name of this alert in the corresponding Prometheus
         * configuration file.
         * Some external tools may require this field to be populated correctly
         * in order to refer to the original Prometheus configuration file.
         * The rule group name and the alert name are necessary to update the
         * relevant AlertPolicies in case the definition of the rule group changes
         * in the future.
         * This field is optional. If this field is not empty, then it must be a
         * valid Prometheus label name.
         */
        ruleGroup?: string;
    }

    export interface AlertPolicyConditionConditionThreshold {
        /**
         * Specifies the alignment of data points in
         * individual time series as well as how to
         * combine the retrieved time series together
         * (such as when aggregating multiple streams
         * on each resource to a single stream for each
         * resource or when aggregating streams across
         * all members of a group of resources).
         * Multiple aggregations are applied in the
         * order specified.This field is similar to the
         * one in the MetricService.ListTimeSeries
         * request. It is advisable to use the
         * ListTimeSeries method when debugging this
         * field.
         * Structure is documented below.
         */
        aggregations?: outputs.monitoring.AlertPolicyConditionConditionThresholdAggregation[];
        /**
         * The comparison to apply between the time
         * series (indicated by filter and aggregation)
         * and the threshold (indicated by
         * threshold_value). The comparison is applied
         * on each time series, with the time series on
         * the left-hand side and the threshold on the
         * right-hand side. Only COMPARISON_LT and
         * COMPARISON_GT are supported currently.
         * Possible values are: `COMPARISON_GT`, `COMPARISON_GE`, `COMPARISON_LT`, `COMPARISON_LE`, `COMPARISON_EQ`, `COMPARISON_NE`.
         */
        comparison: string;
        /**
         * Specifies the alignment of data points in
         * individual time series selected by
         * denominatorFilter as well as how to combine
         * the retrieved time series together (such as
         * when aggregating multiple streams on each
         * resource to a single stream for each
         * resource or when aggregating streams across
         * all members of a group of resources).When
         * computing ratios, the aggregations and
         * denominatorAggregations fields must use the
         * same alignment period and produce time
         * series that have the same periodicity and
         * labels.This field is similar to the one in
         * the MetricService.ListTimeSeries request. It
         * is advisable to use the ListTimeSeries
         * method when debugging this field.
         * Structure is documented below.
         */
        denominatorAggregations?: outputs.monitoring.AlertPolicyConditionConditionThresholdDenominatorAggregation[];
        /**
         * A filter that identifies a time series that
         * should be used as the denominator of a ratio
         * that will be compared with the threshold. If
         * a denominatorFilter is specified, the time
         * series specified by the filter field will be
         * used as the numerator.The filter is similar
         * to the one that is specified in the
         * MetricService.ListTimeSeries request (that
         * call is useful to verify the time series
         * that will be retrieved / processed) and must
         * specify the metric type and optionally may
         * contain restrictions on resource type,
         * resource labels, and metric labels. This
         * field may not exceed 2048 Unicode characters
         * in length.
         */
        denominatorFilter?: string;
        /**
         * The amount of time that a time series must
         * violate the threshold to be considered
         * failing. Currently, only values that are a
         * multiple of a minute--e.g., 0, 60, 120, or
         * 300 seconds--are supported. If an invalid
         * value is given, an error will be returned.
         * When choosing a duration, it is useful to
         * keep in mind the frequency of the underlying
         * time series data (which may also be affected
         * by any alignments specified in the
         * aggregations field); a good duration is long
         * enough so that a single outlier does not
         * generate spurious alerts, but short enough
         * that unhealthy states are detected and
         * alerted on quickly.
         */
        duration: string;
        /**
         * A condition control that determines how
         * metric-threshold conditions are evaluated when
         * data stops arriving.
         * Possible values are: `EVALUATION_MISSING_DATA_INACTIVE`, `EVALUATION_MISSING_DATA_ACTIVE`, `EVALUATION_MISSING_DATA_NO_OP`.
         */
        evaluationMissingData?: string;
        /**
         * A filter that identifies which time series
         * should be compared with the threshold.The
         * filter is similar to the one that is
         * specified in the
         * MetricService.ListTimeSeries request (that
         * call is useful to verify the time series
         * that will be retrieved / processed) and must
         * specify the metric type and optionally may
         * contain restrictions on resource type,
         * resource labels, and metric labels. This
         * field may not exceed 2048 Unicode characters
         * in length.
         */
        filter?: string;
        /**
         * When this field is present, the `MetricThreshold`
         * condition forecasts whether the time series is
         * predicted to violate the threshold within the
         * `forecastHorizon`. When this field is not set, the
         * `MetricThreshold` tests the current value of the
         * timeseries against the threshold.
         * Structure is documented below.
         */
        forecastOptions?: outputs.monitoring.AlertPolicyConditionConditionThresholdForecastOptions;
        /**
         * A value against which to compare the time
         * series.
         */
        thresholdValue?: number;
        /**
         * The number/percent of time series for which
         * the comparison must hold in order for the
         * condition to trigger. If unspecified, then
         * the condition will trigger if the comparison
         * is true for any of the time series that have
         * been identified by filter and aggregations,
         * or by the ratio, if denominatorFilter and
         * denominatorAggregations are specified.
         * Structure is documented below.
         */
        trigger?: outputs.monitoring.AlertPolicyConditionConditionThresholdTrigger;
    }

    export interface AlertPolicyConditionConditionThresholdAggregation {
        /**
         * The alignment period for per-time
         * series alignment. If present,
         * alignmentPeriod must be at least
         * 60 seconds. After per-time series
         * alignment, each time series will
         * contain data points only on the
         * period boundaries. If
         * perSeriesAligner is not specified
         * or equals ALIGN_NONE, then this
         * field is ignored. If
         * perSeriesAligner is specified and
         * does not equal ALIGN_NONE, then
         * this field must be defined;
         * otherwise an error is returned.
         */
        alignmentPeriod?: string;
        /**
         * The approach to be used to combine
         * time series. Not all reducer
         * functions may be applied to all
         * time series, depending on the
         * metric type and the value type of
         * the original time series.
         * Reduction may change the metric
         * type of value type of the time
         * series.Time series data must be
         * aligned in order to perform cross-
         * time series reduction. If
         * crossSeriesReducer is specified,
         * then perSeriesAligner must be
         * specified and not equal ALIGN_NONE
         * and alignmentPeriod must be
         * specified; otherwise, an error is
         * returned.
         * Possible values are: `REDUCE_NONE`, `REDUCE_MEAN`, `REDUCE_MIN`, `REDUCE_MAX`, `REDUCE_SUM`, `REDUCE_STDDEV`, `REDUCE_COUNT`, `REDUCE_COUNT_TRUE`, `REDUCE_COUNT_FALSE`, `REDUCE_FRACTION_TRUE`, `REDUCE_PERCENTILE_99`, `REDUCE_PERCENTILE_95`, `REDUCE_PERCENTILE_50`, `REDUCE_PERCENTILE_05`.
         */
        crossSeriesReducer?: string;
        /**
         * The set of fields to preserve when
         * crossSeriesReducer is specified.
         * The groupByFields determine how
         * the time series are partitioned
         * into subsets prior to applying the
         * aggregation function. Each subset
         * contains time series that have the
         * same value for each of the
         * grouping fields. Each individual
         * time series is a member of exactly
         * one subset. The crossSeriesReducer
         * is applied to each subset of time
         * series. It is not possible to
         * reduce across different resource
         * types, so this field implicitly
         * contains resource.type. Fields not
         * specified in groupByFields are
         * aggregated away. If groupByFields
         * is not specified and all the time
         * series have the same resource
         * type, then the time series are
         * aggregated into a single output
         * time series. If crossSeriesReducer
         * is not defined, this field is
         * ignored.
         */
        groupByFields?: string[];
        /**
         * The approach to be used to align
         * individual time series. Not all
         * alignment functions may be applied
         * to all time series, depending on
         * the metric type and value type of
         * the original time series.
         * Alignment may change the metric
         * type or the value type of the time
         * series.Time series data must be
         * aligned in order to perform cross-
         * time series reduction. If
         * crossSeriesReducer is specified,
         * then perSeriesAligner must be
         * specified and not equal ALIGN_NONE
         * and alignmentPeriod must be
         * specified; otherwise, an error is
         * returned.
         * Possible values are: `ALIGN_NONE`, `ALIGN_DELTA`, `ALIGN_RATE`, `ALIGN_INTERPOLATE`, `ALIGN_NEXT_OLDER`, `ALIGN_MIN`, `ALIGN_MAX`, `ALIGN_MEAN`, `ALIGN_COUNT`, `ALIGN_SUM`, `ALIGN_STDDEV`, `ALIGN_COUNT_TRUE`, `ALIGN_COUNT_FALSE`, `ALIGN_FRACTION_TRUE`, `ALIGN_PERCENTILE_99`, `ALIGN_PERCENTILE_95`, `ALIGN_PERCENTILE_50`, `ALIGN_PERCENTILE_05`, `ALIGN_PERCENT_CHANGE`.
         */
        perSeriesAligner?: string;
    }

    export interface AlertPolicyConditionConditionThresholdDenominatorAggregation {
        /**
         * The alignment period for per-time
         * series alignment. If present,
         * alignmentPeriod must be at least
         * 60 seconds. After per-time series
         * alignment, each time series will
         * contain data points only on the
         * period boundaries. If
         * perSeriesAligner is not specified
         * or equals ALIGN_NONE, then this
         * field is ignored. If
         * perSeriesAligner is specified and
         * does not equal ALIGN_NONE, then
         * this field must be defined;
         * otherwise an error is returned.
         */
        alignmentPeriod?: string;
        /**
         * The approach to be used to combine
         * time series. Not all reducer
         * functions may be applied to all
         * time series, depending on the
         * metric type and the value type of
         * the original time series.
         * Reduction may change the metric
         * type of value type of the time
         * series.Time series data must be
         * aligned in order to perform cross-
         * time series reduction. If
         * crossSeriesReducer is specified,
         * then perSeriesAligner must be
         * specified and not equal ALIGN_NONE
         * and alignmentPeriod must be
         * specified; otherwise, an error is
         * returned.
         * Possible values are: `REDUCE_NONE`, `REDUCE_MEAN`, `REDUCE_MIN`, `REDUCE_MAX`, `REDUCE_SUM`, `REDUCE_STDDEV`, `REDUCE_COUNT`, `REDUCE_COUNT_TRUE`, `REDUCE_COUNT_FALSE`, `REDUCE_FRACTION_TRUE`, `REDUCE_PERCENTILE_99`, `REDUCE_PERCENTILE_95`, `REDUCE_PERCENTILE_50`, `REDUCE_PERCENTILE_05`.
         */
        crossSeriesReducer?: string;
        /**
         * The set of fields to preserve when
         * crossSeriesReducer is specified.
         * The groupByFields determine how
         * the time series are partitioned
         * into subsets prior to applying the
         * aggregation function. Each subset
         * contains time series that have the
         * same value for each of the
         * grouping fields. Each individual
         * time series is a member of exactly
         * one subset. The crossSeriesReducer
         * is applied to each subset of time
         * series. It is not possible to
         * reduce across different resource
         * types, so this field implicitly
         * contains resource.type. Fields not
         * specified in groupByFields are
         * aggregated away. If groupByFields
         * is not specified and all the time
         * series have the same resource
         * type, then the time series are
         * aggregated into a single output
         * time series. If crossSeriesReducer
         * is not defined, this field is
         * ignored.
         */
        groupByFields?: string[];
        /**
         * The approach to be used to align
         * individual time series. Not all
         * alignment functions may be applied
         * to all time series, depending on
         * the metric type and value type of
         * the original time series.
         * Alignment may change the metric
         * type or the value type of the time
         * series.Time series data must be
         * aligned in order to perform cross-
         * time series reduction. If
         * crossSeriesReducer is specified,
         * then perSeriesAligner must be
         * specified and not equal ALIGN_NONE
         * and alignmentPeriod must be
         * specified; otherwise, an error is
         * returned.
         * Possible values are: `ALIGN_NONE`, `ALIGN_DELTA`, `ALIGN_RATE`, `ALIGN_INTERPOLATE`, `ALIGN_NEXT_OLDER`, `ALIGN_MIN`, `ALIGN_MAX`, `ALIGN_MEAN`, `ALIGN_COUNT`, `ALIGN_SUM`, `ALIGN_STDDEV`, `ALIGN_COUNT_TRUE`, `ALIGN_COUNT_FALSE`, `ALIGN_FRACTION_TRUE`, `ALIGN_PERCENTILE_99`, `ALIGN_PERCENTILE_95`, `ALIGN_PERCENTILE_50`, `ALIGN_PERCENTILE_05`, `ALIGN_PERCENT_CHANGE`.
         */
        perSeriesAligner?: string;
    }

    export interface AlertPolicyConditionConditionThresholdForecastOptions {
        /**
         * The length of time into the future to forecast
         * whether a timeseries will violate the threshold.
         * If the predicted value is found to violate the
         * threshold, and the violation is observed in all
         * forecasts made for the Configured `duration`,
         * then the timeseries is considered to be failing.
         */
        forecastHorizon: string;
    }

    export interface AlertPolicyConditionConditionThresholdTrigger {
        /**
         * The absolute number of time series
         * that must fail the predicate for the
         * condition to be triggered.
         */
        count?: number;
        /**
         * The percentage of time series that
         * must fail the predicate for the
         * condition to be triggered.
         */
        percent?: number;
    }

    export interface AlertPolicyCreationRecord {
        /**
         * (Output)
         * When the change occurred.
         */
        mutateTime: string;
        /**
         * (Output)
         * The email address of the user making the change.
         */
        mutatedBy: string;
    }

    export interface AlertPolicyDocumentation {
        /**
         * The text of the documentation, interpreted according to mimeType.
         * The content may not exceed 8,192 Unicode characters and may not
         * exceed more than 10,240 bytes when encoded in UTF-8 format,
         * whichever is smaller.
         */
        content?: string;
        /**
         * The format of the content field. Presently, only the value
         * "text/markdown" is supported.
         */
        mimeType?: string;
    }

    export interface CustomServiceTelemetry {
        /**
         * The full name of the resource that defines this service.
         * Formatted as described in
         * https://cloud.google.com/apis/design/resource_names.
         */
        resourceName?: string;
    }

    export interface GenericServiceBasicService {
        /**
         * Labels that specify the resource that emits the monitoring data
         * which is used for SLO reporting of this `Service`.
         */
        serviceLabels?: {[key: string]: string};
        /**
         * The type of service that this basic service defines, e.g.
         * APP_ENGINE service type
         */
        serviceType?: string;
    }

    export interface GenericServiceTelemetry {
        /**
         * The full name of the resource that defines this service.
         * Formatted as described in
         * https://cloud.google.com/apis/design/resource_names.
         */
        resourceName?: string;
    }

    export interface GetAppEngineServiceTelemetry {
        /**
         * The full name of the resource that defines this service.
         * Formatted as described in
         * https://cloud.google.com/apis/design/resource_names.
         */
        resourceName: string;
    }

    export interface GetClusterIstioServiceTelemetry {
        /**
         * The full name of the resource that defines this service.
         * Formatted as described in
         * https://cloud.google.com/apis/design/resource_names.
         */
        resourceName: string;
    }

    export interface GetIstioCanonicalServiceTelemetry {
        /**
         * The full name of the resource that defines this service.
         * Formatted as described in
         * https://cloud.google.com/apis/design/resource_names.
         */
        resourceName: string;
    }

    export interface GetMeshIstioServiceTelemetry {
        /**
         * The full name of the resource that defines this service.
         * Formatted as described in
         * https://cloud.google.com/apis/design/resource_names.
         */
        resourceName: string;
    }

    export interface GetNotificationChannelSensitiveLabel {
        authToken: string;
        password: string;
        serviceKey: string;
    }

    export interface GetUptimeCheckIPsUptimeCheckIp {
        /**
         * The IP address from which the Uptime check originates. This is a fully specified IP address
         * (not an IP address range). Most IP addresses, as of this publication, are in IPv4 format; however, one should not
         * rely on the IP addresses being in IPv4 format indefinitely, and should support interpreting this field in either
         * IPv4 or IPv6 format.
         */
        ipAddress: string;
        /**
         * A more specific location within the region that typically encodes a particular city/town/metro
         * (and its containing state/province or country) within the broader umbrella region category.
         */
        location: string;
        /**
         * A broad region category in which the IP address is located.
         */
        region: string;
    }

    export interface MetricDescriptorLabel {
        /**
         * A human-readable description for the label.
         */
        description?: string;
        /**
         * The key for this label. The key must not exceed 100 characters. The first character of the key must be an upper- or lower-case letter, the remaining characters must be letters, digits or underscores, and the key must match the regular expression [a-zA-Z][a-zA-Z0-9_]*
         */
        key: string;
        /**
         * The type of data that can be assigned to the label.
         * Default value is `STRING`.
         * Possible values are: `STRING`, `BOOL`, `INT64`.
         */
        valueType?: string;
    }

    export interface MetricDescriptorMetadata {
        /**
         * The delay of data points caused by ingestion. Data points older than this age are guaranteed to be ingested and available to be read, excluding data loss due to errors. In `[duration format](https://developers.google.com/protocol-buffers/docs/reference/google.protobuf?&_ga=2.264881487.1507873253.1593446723-935052455.1591817775#google.protobuf.Duration)`.
         */
        ingestDelay?: string;
        /**
         * The sampling period of metric data points. For metrics which are written periodically, consecutive data points are stored at this time interval, excluding data loss due to errors. Metrics with a higher granularity have a smaller sampling period. In `[duration format](https://developers.google.com/protocol-buffers/docs/reference/google.protobuf?&_ga=2.264881487.1507873253.1593446723-935052455.1591817775#google.protobuf.Duration)`.
         */
        samplePeriod?: string;
    }

    export interface NotificationChannelSensitiveLabels {
        /**
         * An authorization token for a notification channel. Channel types that support this field include: slack
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        authToken?: string;
        /**
         * An password for a notification channel. Channel types that support this field include: webhookBasicauth
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password?: string;
        /**
         * An servicekey token for a notification channel. Channel types that support this field include: pagerduty
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        serviceKey?: string;
    }

    export interface SloBasicSli {
        /**
         * Availability based SLI, dervied from count of requests made to this service that return successfully.
         * Structure is documented below.
         */
        availability?: outputs.monitoring.SloBasicSliAvailability;
        /**
         * Parameters for a latency threshold SLI.
         * Structure is documented below.
         */
        latency?: outputs.monitoring.SloBasicSliLatency;
        /**
         * An optional set of locations to which this SLI is relevant.
         * Telemetry from other locations will not be used to calculate
         * performance for this SLI. If omitted, this SLI applies to all
         * locations in which the Service has activity. For service types
         * that don't support breaking down by location, setting this
         * field will result in an error.
         */
        locations?: string[];
        /**
         * An optional set of RPCs to which this SLI is relevant.
         * Telemetry from other methods will not be used to calculate
         * performance for this SLI. If omitted, this SLI applies to all
         * the Service's methods. For service types that don't support
         * breaking down by method, setting this field will result in an
         * error.
         */
        methods?: string[];
        /**
         * The set of API versions to which this SLI is relevant.
         * Telemetry from other API versions will not be used to
         * calculate performance for this SLI. If omitted,
         * this SLI applies to all API versions. For service types
         * that don't support breaking down by version, setting this
         * field will result in an error.
         */
        versions?: string[];
    }

    export interface SloBasicSliAvailability {
        /**
         * Whether an availability SLI is enabled or not. Must be set to `true. Defaults to `true`.
         */
        enabled?: boolean;
    }

    export interface SloBasicSliLatency {
        /**
         * A duration string, e.g. 10s.
         * Good service is defined to be the count of requests made to
         * this service that return in no more than threshold.
         */
        threshold: string;
    }

    export interface SloRequestBasedSli {
        /**
         * Used when goodService is defined by a count of values aggregated in a
         * Distribution that fall into a good range. The totalService is the
         * total count of all values aggregated in the Distribution.
         * Defines a distribution TimeSeries filter and thresholds used for
         * measuring good service and total service.
         * Exactly one of `distributionCut` or `goodTotalRatio` can be set.
         * Structure is documented below.
         */
        distributionCut?: outputs.monitoring.SloRequestBasedSliDistributionCut;
        /**
         * A means to compute a ratio of `goodService` to `totalService`.
         * Defines computing this ratio with two TimeSeries [monitoring filters](https://cloud.google.com/monitoring/api/v3/filters)
         * Must specify exactly two of good, bad, and total service filters.
         * The relationship goodService + badService = totalService
         * will be assumed.
         * Exactly one of `distributionCut` or `goodTotalRatio` can be set.
         * Structure is documented below.
         */
        goodTotalRatio?: outputs.monitoring.SloRequestBasedSliGoodTotalRatio;
    }

    export interface SloRequestBasedSliDistributionCut {
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * aggregating values to quantify the good service provided.
         * Must have ValueType = DISTRIBUTION and
         * MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        distributionFilter: string;
        /**
         * Range of numerical values. The computed goodService
         * will be the count of values x in the Distribution such
         * that range.min <= x <= range.max. inclusive of min and
         * max. Open ranges can be defined by setting
         * just one of min or max.
         * Structure is documented below.
         */
        range: outputs.monitoring.SloRequestBasedSliDistributionCutRange;
    }

    export interface SloRequestBasedSliDistributionCutRange {
        /**
         * max value for the range (inclusive). If not given,
         * will be set to "infinity", defining an open range
         * ">= range.min"
         */
        max?: number;
        /**
         * Min value for the range (inclusive). If not given,
         * will be set to "-infinity", defining an open range
         * "< range.max"
         */
        min?: number;
    }

    export interface SloRequestBasedSliGoodTotalRatio {
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * quantifying bad service provided, either demanded service that
         * was not provided or demanded service that was of inadequate
         * quality. Exactly two of
         * good, bad, or total service filter must be defined (where
         * good + bad = total is assumed)
         * Must have ValueType = DOUBLE or ValueType = INT64 and
         * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        badServiceFilter?: string;
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * quantifying good service provided. Exactly two of
         * good, bad, or total service filter must be defined (where
         * good + bad = total is assumed)
         * Must have ValueType = DOUBLE or ValueType = INT64 and
         * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        goodServiceFilter?: string;
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * quantifying total demanded service. Exactly two of
         * good, bad, or total service filter must be defined (where
         * good + bad = total is assumed)
         * Must have ValueType = DOUBLE or ValueType = INT64 and
         * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        totalServiceFilter?: string;
    }

    export interface SloWindowsBasedSli {
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * with ValueType = BOOL. The window is good if any true values
         * appear in the window. One of `goodBadMetricFilter`,
         * `goodTotalRatioThreshold`, `metricMeanInRange`,
         * `metricSumInRange` must be set for `windowsBasedSli`.
         */
        goodBadMetricFilter?: string;
        /**
         * Criterion that describes a window as good if its performance is
         * high enough. One of `goodBadMetricFilter`,
         * `goodTotalRatioThreshold`, `metricMeanInRange`,
         * `metricSumInRange` must be set for `windowsBasedSli`.
         * Structure is documented below.
         */
        goodTotalRatioThreshold?: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThreshold;
        /**
         * Criterion that describes a window as good if the metric's value
         * is in a good range, *averaged* across returned streams.
         * One of `goodBadMetricFilter`,
         * `goodTotalRatioThreshold`, `metricMeanInRange`,
         * `metricSumInRange` must be set for `windowsBasedSli`.
         * Average value X of `timeSeries` should satisfy
         * `range.min <= X <= range.max` for a good window.
         * Structure is documented below.
         */
        metricMeanInRange?: outputs.monitoring.SloWindowsBasedSliMetricMeanInRange;
        /**
         * Criterion that describes a window as good if the metric's value
         * is in a good range, *summed* across returned streams.
         * Summed value `X` of `timeSeries` should satisfy
         * `range.min <= X <= range.max` for a good window.
         * One of `goodBadMetricFilter`,
         * `goodTotalRatioThreshold`, `metricMeanInRange`,
         * `metricSumInRange` must be set for `windowsBasedSli`.
         * Structure is documented below.
         */
        metricSumInRange?: outputs.monitoring.SloWindowsBasedSliMetricSumInRange;
        /**
         * Duration over which window quality is evaluated, given as a
         * duration string "{X}s" representing X seconds. Must be an
         * integer fraction of a day and at least 60s.
         */
        windowPeriod?: string;
    }

    export interface SloWindowsBasedSliGoodTotalRatioThreshold {
        /**
         * Basic SLI to evaluate to judge window quality.
         * Structure is documented below.
         */
        basicSliPerformance?: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformance;
        /**
         * Request-based SLI to evaluate to judge window quality.
         * Structure is documented below.
         */
        performance?: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThresholdPerformance;
        /**
         * If window performance >= threshold, the window is counted
         * as good.
         */
        threshold?: number;
    }

    export interface SloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformance {
        /**
         * Availability based SLI, dervied from count of requests made to this service that return successfully.
         * Structure is documented below.
         */
        availability?: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceAvailability;
        /**
         * Parameters for a latency threshold SLI.
         * Structure is documented below.
         */
        latency?: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceLatency;
        /**
         * An optional set of locations to which this SLI is relevant.
         * Telemetry from other locations will not be used to calculate
         * performance for this SLI. If omitted, this SLI applies to all
         * locations in which the Service has activity. For service types
         * that don't support breaking down by location, setting this
         * field will result in an error.
         */
        locations?: string[];
        /**
         * An optional set of RPCs to which this SLI is relevant.
         * Telemetry from other methods will not be used to calculate
         * performance for this SLI. If omitted, this SLI applies to all
         * the Service's methods. For service types that don't support
         * breaking down by method, setting this field will result in an
         * error.
         */
        methods?: string[];
        /**
         * The set of API versions to which this SLI is relevant.
         * Telemetry from other API versions will not be used to
         * calculate performance for this SLI. If omitted,
         * this SLI applies to all API versions. For service types
         * that don't support breaking down by version, setting this
         * field will result in an error.
         */
        versions?: string[];
    }

    export interface SloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceAvailability {
        /**
         * Whether an availability SLI is enabled or not. Must be set to `true. Defaults to `true`.
         */
        enabled?: boolean;
    }

    export interface SloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceLatency {
        /**
         * A duration string, e.g. 10s.
         * Good service is defined to be the count of requests made to
         * this service that return in no more than threshold.
         */
        threshold: string;
    }

    export interface SloWindowsBasedSliGoodTotalRatioThresholdPerformance {
        /**
         * Used when goodService is defined by a count of values aggregated in a
         * Distribution that fall into a good range. The totalService is the
         * total count of all values aggregated in the Distribution.
         * Defines a distribution TimeSeries filter and thresholds used for
         * measuring good service and total service.
         * Structure is documented below.
         */
        distributionCut?: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCut;
        /**
         * A means to compute a ratio of `goodService` to `totalService`.
         * Defines computing this ratio with two TimeSeries [monitoring filters](https://cloud.google.com/monitoring/api/v3/filters)
         * Must specify exactly two of good, bad, and total service filters.
         * The relationship goodService + badService = totalService
         * will be assumed.
         * Structure is documented below.
         */
        goodTotalRatio?: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThresholdPerformanceGoodTotalRatio;
    }

    export interface SloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCut {
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * aggregating values to quantify the good service provided.
         * Must have ValueType = DISTRIBUTION and
         * MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        distributionFilter: string;
        /**
         * Range of numerical values. The computed goodService
         * will be the count of values x in the Distribution such
         * that range.min <= x <= range.max. inclusive of min and
         * max. Open ranges can be defined by setting
         * just one of min or max.
         * Structure is documented below.
         */
        range: outputs.monitoring.SloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCutRange;
    }

    export interface SloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCutRange {
        /**
         * max value for the range (inclusive). If not given,
         * will be set to "infinity", defining an open range
         * ">= range.min"
         */
        max?: number;
        /**
         * Min value for the range (inclusive). If not given,
         * will be set to "-infinity", defining an open range
         * "< range.max"
         */
        min?: number;
    }

    export interface SloWindowsBasedSliGoodTotalRatioThresholdPerformanceGoodTotalRatio {
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * quantifying bad service provided, either demanded service that
         * was not provided or demanded service that was of inadequate
         * quality. Exactly two of
         * good, bad, or total service filter must be defined (where
         * good + bad = total is assumed)
         * Must have ValueType = DOUBLE or ValueType = INT64 and
         * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        badServiceFilter?: string;
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * quantifying good service provided. Exactly two of
         * good, bad, or total service filter must be defined (where
         * good + bad = total is assumed)
         * Must have ValueType = DOUBLE or ValueType = INT64 and
         * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        goodServiceFilter?: string;
        /**
         * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * quantifying total demanded service. Exactly two of
         * good, bad, or total service filter must be defined (where
         * good + bad = total is assumed)
         * Must have ValueType = DOUBLE or ValueType = INT64 and
         * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
         */
        totalServiceFilter?: string;
    }

    export interface SloWindowsBasedSliMetricMeanInRange {
        /**
         * Range of numerical values. The computed goodService
         * will be the count of values x in the Distribution such
         * that range.min <= x <= range.max. inclusive of min and
         * max. Open ranges can be defined by setting
         * just one of min or max. Mean value `X` of `timeSeries`
         * values should satisfy `range.min <= X <= range.max` for a
         * good service.
         * Structure is documented below.
         */
        range: outputs.monitoring.SloWindowsBasedSliMetricMeanInRangeRange;
        /**
         * A [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * specifying the TimeSeries to use for evaluating window
         * The provided TimeSeries must have ValueType = INT64 or
         * ValueType = DOUBLE and MetricKind = GAUGE. Mean value `X`
         * should satisfy `range.min <= X <= range.max`
         * under good service.
         */
        timeSeries: string;
    }

    export interface SloWindowsBasedSliMetricMeanInRangeRange {
        /**
         * max value for the range (inclusive). If not given,
         * will be set to "infinity", defining an open range
         * ">= range.min"
         */
        max?: number;
        /**
         * Min value for the range (inclusive). If not given,
         * will be set to "-infinity", defining an open range
         * "< range.max"
         */
        min?: number;
    }

    export interface SloWindowsBasedSliMetricSumInRange {
        /**
         * Range of numerical values. The computed goodService
         * will be the count of values x in the Distribution such
         * that range.min <= x <= range.max. inclusive of min and
         * max. Open ranges can be defined by setting
         * just one of min or max. Summed value `X` should satisfy
         * `range.min <= X <= range.max` for a good window.
         * Structure is documented below.
         */
        range: outputs.monitoring.SloWindowsBasedSliMetricSumInRangeRange;
        /**
         * A [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
         * specifying the TimeSeries to use for evaluating window
         * quality. The provided TimeSeries must have
         * ValueType = INT64 or ValueType = DOUBLE and
         * MetricKind = GAUGE.
         * Summed value `X` should satisfy
         * `range.min <= X <= range.max` for a good window.
         */
        timeSeries: string;
    }

    export interface SloWindowsBasedSliMetricSumInRangeRange {
        /**
         * max value for the range (inclusive). If not given,
         * will be set to "infinity", defining an open range
         * ">= range.min"
         */
        max?: number;
        /**
         * Min value for the range (inclusive). If not given,
         * will be set to "-infinity", defining an open range
         * "< range.max"
         */
        min?: number;
    }

    export interface UptimeCheckConfigContentMatcher {
        /**
         * String or regex content to match (max 1024 bytes)
         */
        content: string;
        /**
         * Information needed to perform a JSONPath content match. Used for `ContentMatcherOption::MATCHES_JSON_PATH` and `ContentMatcherOption::NOT_MATCHES_JSON_PATH`.
         * Structure is documented below.
         */
        jsonPathMatcher?: outputs.monitoring.UptimeCheckConfigContentMatcherJsonPathMatcher;
        /**
         * The type of content matcher that will be applied to the server output, compared to the content string when the check is run.
         * Default value is `CONTAINS_STRING`.
         * Possible values are: `CONTAINS_STRING`, `NOT_CONTAINS_STRING`, `MATCHES_REGEX`, `NOT_MATCHES_REGEX`, `MATCHES_JSON_PATH`, `NOT_MATCHES_JSON_PATH`.
         */
        matcher?: string;
    }

    export interface UptimeCheckConfigContentMatcherJsonPathMatcher {
        /**
         * Options to perform JSONPath content matching.
         * Default value is `EXACT_MATCH`.
         * Possible values are: `EXACT_MATCH`, `REGEX_MATCH`.
         */
        jsonMatcher?: string;
        /**
         * JSONPath within the response output pointing to the expected `ContentMatcher::content` to match against.
         */
        jsonPath: string;
    }

    export interface UptimeCheckConfigHttpCheck {
        /**
         * If present, the check will only pass if the HTTP response status code is in this set of status codes. If empty, the HTTP status code will only pass if the HTTP status code is 200-299.
         * Structure is documented below.
         */
        acceptedResponseStatusCodes?: outputs.monitoring.UptimeCheckConfigHttpCheckAcceptedResponseStatusCode[];
        /**
         * The authentication information. Optional when creating an HTTP check; defaults to empty.
         * Structure is documented below.
         */
        authInfo?: outputs.monitoring.UptimeCheckConfigHttpCheckAuthInfo;
        /**
         * The request body associated with the HTTP POST request. If contentType is URL_ENCODED, the body passed in must be URL-encoded. Users can provide a Content-Length header via the headers field or the API will do so. If the requestMethod is GET and body is not empty, the API will return an error. The maximum byte size is 1 megabyte. Note - As with all bytes fields JSON representations are base64 encoded. e.g. "foo=bar" in URL-encoded form is "foo%3Dbar" and in base64 encoding is "Zm9vJTI1M0RiYXI=".
         */
        body?: string;
        /**
         * The content type to use for the check.
         * Possible values are: `TYPE_UNSPECIFIED`, `URL_ENCODED`.
         */
        contentType?: string;
        /**
         * The list of headers to send as part of the uptime check request. If two headers have the same key and different values, they should be entered as a single header, with the value being a comma-separated list of all the desired values as described at https://www.w3.org/Protocols/rfc2616/rfc2616.txt (page 31). Entering two separate headers with the same key in a Create call will cause the first to be overwritten by the second. The maximum number of headers allowed is 100.
         */
        headers: {[key: string]: string};
        /**
         * Boolean specifying whether to encrypt the header information. Encryption should be specified for any headers related to authentication that you do not wish to be seen when retrieving the configuration. The server will be responsible for encrypting the headers. On Get/List calls, if maskHeaders is set to True then the headers will be obscured with ******.
         */
        maskHeaders?: boolean;
        /**
         * The path to the page to run the check against. Will be combined with the host (specified within the MonitoredResource) and port to construct the full URL. If the provided path does not begin with "/", a "/" will be prepended automatically. Optional (defaults to "/").
         */
        path?: string;
        /**
         * The port to the page to run the check against. Will be combined with host (specified within the MonitoredResource) and path to construct the full URL. Optional (defaults to 80 without SSL, or 443 with SSL).
         */
        port: number;
        /**
         * The HTTP request method to use for the check. If set to METHOD_UNSPECIFIED then requestMethod defaults to GET.
         * Default value is `GET`.
         * Possible values are: `METHOD_UNSPECIFIED`, `GET`, `POST`.
         */
        requestMethod?: string;
        /**
         * If true, use HTTPS instead of HTTP to run the check.
         */
        useSsl?: boolean;
        /**
         * Boolean specifying whether to include SSL certificate validation as a part of the Uptime check. Only applies to checks where monitoredResource is set to uptime_url. If useSsl is false, setting validateSsl to true has no effect.
         */
        validateSsl?: boolean;
    }

    export interface UptimeCheckConfigHttpCheckAcceptedResponseStatusCode {
        /**
         * A class of status codes to accept.
         * Possible values are: `STATUS_CLASS_1XX`, `STATUS_CLASS_2XX`, `STATUS_CLASS_3XX`, `STATUS_CLASS_4XX`, `STATUS_CLASS_5XX`, `STATUS_CLASS_ANY`.
         */
        statusClass?: string;
        /**
         * A status code to accept.
         */
        statusValue?: number;
    }

    export interface UptimeCheckConfigHttpCheckAuthInfo {
        /**
         * The password to authenticate.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        password: string;
        /**
         * The username to authenticate.
         */
        username: string;
    }

    export interface UptimeCheckConfigMonitoredResource {
        /**
         * Values for all of the labels listed in the associated monitored resource descriptor. For example, Compute Engine VM instances use the labels "projectId", "instanceId", and "zone".
         */
        labels: {[key: string]: string};
        /**
         * The monitored resource type. This field must match the type field of a MonitoredResourceDescriptor (https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.monitoredResourceDescriptors#MonitoredResourceDescriptor) object. For example, the type of a Compute Engine VM instance is gce_instance. For a list of types, see Monitoring resource types (https://cloud.google.com/monitoring/api/resources) and Logging resource types (https://cloud.google.com/logging/docs/api/v2/resource-list).
         */
        type: string;
    }

    export interface UptimeCheckConfigResourceGroup {
        /**
         * The group of resources being monitored. Should be the `name` of a group
         */
        groupId?: string;
        /**
         * The resource type of the group members.
         * Possible values are: `RESOURCE_TYPE_UNSPECIFIED`, `INSTANCE`, `AWS_ELB_LOAD_BALANCER`.
         */
        resourceType?: string;
    }

    export interface UptimeCheckConfigSyntheticMonitor {
        /**
         * Target a Synthetic Monitor GCFv2 Instance
         * Structure is documented below.
         *
         *
         * <a name="nestedCloudFunctionV2"></a>The `cloudFunctionV2` block supports:
         */
        cloudFunctionV2: outputs.monitoring.UptimeCheckConfigSyntheticMonitorCloudFunctionV2;
    }

    export interface UptimeCheckConfigSyntheticMonitorCloudFunctionV2 {
        /**
         * The fully qualified name of the cloud function resource.
         */
        name: string;
    }

    export interface UptimeCheckConfigTcpCheck {
        /**
         * The port to the page to run the check against. Will be combined with host (specified within the MonitoredResource) to construct the full URL.
         */
        port: number;
    }

}

export namespace networkconnectivity {
    export interface HubRoutingVpc {
        uri: string;
    }

    export interface ServiceConnectionPolicyPscConfig {
        /**
         * Max number of PSC connections for this policy.
         */
        limit?: string;
        /**
         * IDs of the subnetworks or fully qualified identifiers for the subnetworks
         */
        subnetworks: string[];
    }

    export interface ServiceConnectionPolicyPscConnection {
        /**
         * The resource reference of the consumer address.
         */
        consumerAddress?: string;
        /**
         * The resource reference of the PSC Forwarding Rule within the consumer VPC.
         */
        consumerForwardingRule?: string;
        /**
         * The project where the PSC connection is created.
         */
        consumerTargetProject?: string;
        /**
         * The most recent error during operating this connection.
         * Structure is documented below.
         */
        error?: outputs.networkconnectivity.ServiceConnectionPolicyPscConnectionError;
        /**
         * The error info for the latest error during operating this connection.
         * Structure is documented below.
         */
        errorInfo?: outputs.networkconnectivity.ServiceConnectionPolicyPscConnectionErrorInfo;
        /**
         * The error type indicates whether the error is consumer facing, producer
         * facing or system internal.
         * Possible values are: `CONNECTION_ERROR_TYPE_UNSPECIFIED`, `ERROR_INTERNAL`, `ERROR_CONSUMER_SIDE`, `ERROR_PRODUCER_SIDE`.
         */
        errorType?: string;
        /**
         * The last Compute Engine operation to setup PSC connection.
         */
        gceOperation?: string;
        /**
         * The PSC connection id of the PSC forwarding rule.
         */
        pscConnectionId?: string;
        /**
         * The state of the PSC connection.
         * Possible values are: `STATE_UNSPECIFIED`, `ACTIVE`, `CREATING`, `DELETING`, `FAILED`.
         */
        state?: string;
    }

    export interface ServiceConnectionPolicyPscConnectionError {
        /**
         * The status code, which should be an enum value of [google.rpc.Code][].
         */
        code?: number;
        /**
         * (Output)
         * A list of messages that carry the error details.
         */
        details: {[key: string]: any}[];
        /**
         * A developer-facing error message.
         */
        message?: string;
    }

    export interface ServiceConnectionPolicyPscConnectionErrorInfo {
        /**
         * The logical grouping to which the "reason" belongs.
         */
        domain?: string;
        /**
         * Additional structured details about this error.
         */
        metadata?: {[key: string]: string};
        /**
         * The reason of the error.
         */
        reason?: string;
    }

    export interface SpokeLinkedInterconnectAttachments {
        /**
         * A value that controls whether site-to-site data transfer is enabled for these resources. Note that data transfer is available only in supported locations.
         */
        siteToSiteDataTransfer: boolean;
        /**
         * The URIs of linked interconnect attachment resources
         */
        uris: string[];
    }

    export interface SpokeLinkedRouterApplianceInstances {
        /**
         * The list of router appliance instances
         */
        instances: outputs.networkconnectivity.SpokeLinkedRouterApplianceInstancesInstance[];
        /**
         * A value that controls whether site-to-site data transfer is enabled for these resources. Note that data transfer is available only in supported locations.
         */
        siteToSiteDataTransfer: boolean;
    }

    export interface SpokeLinkedRouterApplianceInstancesInstance {
        /**
         * The IP address on the VM to use for peering.
         */
        ipAddress?: string;
        /**
         * The URI of the virtual machine resource
         *
         * - - -
         */
        virtualMachine?: string;
    }

    export interface SpokeLinkedVpcNetwork {
        /**
         * IP ranges encompassing the subnets to be excluded from peering.
         */
        excludeExportRanges?: string[];
        /**
         * The URI of the VPC network resource.
         */
        uri: string;
    }

    export interface SpokeLinkedVpnTunnels {
        /**
         * A value that controls whether site-to-site data transfer is enabled for these resources. Note that data transfer is available only in supported locations.
         */
        siteToSiteDataTransfer: boolean;
        /**
         * The URIs of linked VPN tunnel resources.
         */
        uris: string[];
    }

}

export namespace networkmanagement {
    export interface ConnectivityTestDestination {
        /**
         * A Compute Engine instance URI.
         */
        instance?: string;
        /**
         * The IP address of the endpoint, which can be an external or
         * internal IP. An IPv6 address is only allowed when the test's
         * destination is a global load balancer VIP.
         */
        ipAddress?: string;
        /**
         * A Compute Engine network URI.
         */
        network?: string;
        /**
         * The IP protocol port of the endpoint. Only applicable when
         * protocol is TCP or UDP.
         */
        port?: number;
        /**
         * Project ID where the endpoint is located. The Project ID can be
         * derived from the URI if you provide a VM instance or network URI.
         * The following are two cases where you must provide the project ID:
         * 1. Only the IP address is specified, and the IP address is within
         * a GCP project. 2. When you are using Shared VPC and the IP address
         * that you provide is from the service project. In this case, the
         * network that the IP address resides in is defined in the host
         * project.
         *
         * - - -
         */
        projectId?: string;
    }

    export interface ConnectivityTestSource {
        /**
         * A Compute Engine instance URI.
         */
        instance?: string;
        /**
         * The IP address of the endpoint, which can be an external or
         * internal IP. An IPv6 address is only allowed when the test's
         * destination is a global load balancer VIP.
         */
        ipAddress?: string;
        /**
         * A Compute Engine network URI.
         */
        network?: string;
        /**
         * Type of the network where the endpoint is located.
         * Possible values are: `GCP_NETWORK`, `NON_GCP_NETWORK`.
         */
        networkType?: string;
        /**
         * The IP protocol port of the endpoint. Only applicable when
         * protocol is TCP or UDP.
         */
        port?: number;
        /**
         * Project ID where the endpoint is located. The Project ID can be
         * derived from the URI if you provide a VM instance or network URI.
         * The following are two cases where you must provide the project ID:
         * 1. Only the IP address is specified, and the IP address is
         * within a GCP project.
         * 2. When you are using Shared VPC and the IP address
         * that you provide is from the service project. In this case,
         * the network that the IP address resides in is defined in the
         * host project.
         */
        projectId?: string;
    }

}

export namespace networksecurity {
    export interface AuthorizationPolicyRule {
        /**
         * List of attributes for the traffic destination. All of the destinations must match. A destination is a match if a request matches all the specified hosts, ports, methods and headers.
         * If not set, the action specified in the 'action' field will be applied without any rule checks for the destination.
         * Structure is documented below.
         */
        destinations?: outputs.networksecurity.AuthorizationPolicyRuleDestination[];
        /**
         * List of attributes for the traffic source. All of the sources must match. A source is a match if both principals and ipBlocks match.
         * If not set, the action specified in the 'action' field will be applied without any rule checks for the source.
         * Structure is documented below.
         */
        sources?: outputs.networksecurity.AuthorizationPolicyRuleSource[];
    }

    export interface AuthorizationPolicyRuleDestination {
        /**
         * List of host names to match. Matched against the ":authority" header in http requests. At least one host should match. Each host can be an exact match, or a prefix match (example "mydomain.*") or a suffix match (example "*.myorg.com") or a presence (any) match "*".
         */
        hosts: string[];
        /**
         * Match against key:value pair in http header. Provides a flexible match based on HTTP headers, for potentially advanced use cases. At least one header should match.
         * Avoid using header matches to make authorization decisions unless there is a strong guarantee that requests arrive through a trusted client or proxy.
         * Structure is documented below.
         */
        httpHeaderMatch?: outputs.networksecurity.AuthorizationPolicyRuleDestinationHttpHeaderMatch;
        /**
         * A list of HTTP methods to match. At least one method should match. Should not be set for gRPC services.
         */
        methods: string[];
        /**
         * List of destination ports to match. At least one port should match.
         */
        ports: number[];
    }

    export interface AuthorizationPolicyRuleDestinationHttpHeaderMatch {
        /**
         * The name of the HTTP header to match. For matching against the HTTP request's authority, use a headerMatch with the header name ":authority". For matching a request's method, use the headerName ":method".
         */
        headerName: string;
        /**
         * The value of the header must match the regular expression specified in regexMatch. For regular expression grammar, please see: en.cppreference.com/w/cpp/regex/ecmascript For matching against a port specified in the HTTP request, use a headerMatch with headerName set to Host and a regular expression that satisfies the RFC2616 Host header's port specifier.
         */
        regexMatch: string;
    }

    export interface AuthorizationPolicyRuleSource {
        /**
         * List of CIDR ranges to match based on source IP address. At least one IP block should match. Single IP (e.g., "1.2.3.4") and CIDR (e.g., "1.2.3.0/24") are supported. Authorization based on source IP alone should be avoided.
         * The IP addresses of any load balancers or proxies should be considered untrusted.
         */
        ipBlocks?: string[];
        /**
         * List of peer identities to match for authorization. At least one principal should match. Each peer can be an exact match, or a prefix match (example, "namespace/*") or a suffix match (example, "*&#47;service-account") or a presence match "*".
         * Authorization based on the principal name without certificate validation (configured by ServerTlsPolicy resource) is considered insecure.
         */
        principals?: string[];
    }

    export interface ClientTlsPolicyClientCertificate {
        /**
         * The certificate provider instance specification that will be passed to the data plane, which will be used to load necessary credential information.
         * Structure is documented below.
         */
        certificateProviderInstance?: outputs.networksecurity.ClientTlsPolicyClientCertificateCertificateProviderInstance;
        /**
         * gRPC specific configuration to access the gRPC server to obtain the cert and private key.
         * Structure is documented below.
         */
        grpcEndpoint?: outputs.networksecurity.ClientTlsPolicyClientCertificateGrpcEndpoint;
    }

    export interface ClientTlsPolicyClientCertificateCertificateProviderInstance {
        /**
         * Plugin instance name, used to locate and load CertificateProvider instance configuration. Set to "googleCloudPrivateSpiffe" to use Certificate Authority Service certificate provider instance.
         */
        pluginInstance: string;
    }

    export interface ClientTlsPolicyClientCertificateGrpcEndpoint {
        /**
         * The target URI of the gRPC endpoint. Only UDS path is supported, and should start with "unix:".
         */
        targetUri: string;
    }

    export interface ClientTlsPolicyServerValidationCa {
        /**
         * The certificate provider instance specification that will be passed to the data plane, which will be used to load necessary credential information.
         * Structure is documented below.
         */
        certificateProviderInstance?: outputs.networksecurity.ClientTlsPolicyServerValidationCaCertificateProviderInstance;
        /**
         * gRPC specific configuration to access the gRPC server to obtain the cert and private key.
         * Structure is documented below.
         */
        grpcEndpoint?: outputs.networksecurity.ClientTlsPolicyServerValidationCaGrpcEndpoint;
    }

    export interface ClientTlsPolicyServerValidationCaCertificateProviderInstance {
        /**
         * Plugin instance name, used to locate and load CertificateProvider instance configuration. Set to "googleCloudPrivateSpiffe" to use Certificate Authority Service certificate provider instance.
         */
        pluginInstance: string;
    }

    export interface ClientTlsPolicyServerValidationCaGrpcEndpoint {
        /**
         * The target URI of the gRPC endpoint. Only UDS path is supported, and should start with "unix:".
         */
        targetUri: string;
    }

    export interface ServerTlsPolicyMtlsPolicy {
        /**
         * Required if the policy is to be used with Traffic Director. For external HTTPS load balancers it must be empty.
         * Defines the mechanism to obtain the Certificate Authority certificate to validate the client certificate.
         * Structure is documented below.
         */
        clientValidationCas?: outputs.networksecurity.ServerTlsPolicyMtlsPolicyClientValidationCa[];
        /**
         * When the client presents an invalid certificate or no certificate to the load balancer, the clientValidationMode specifies how the client connection is handled.
         * Required if the policy is to be used with the external HTTPS load balancing. For Traffic Director it must be empty.
         * Possible values are: `CLIENT_VALIDATION_MODE_UNSPECIFIED`, `ALLOW_INVALID_OR_MISSING_CLIENT_CERT`, `REJECT_INVALID`.
         */
        clientValidationMode?: string;
        /**
         * Reference to the TrustConfig from certificatemanager.googleapis.com namespace.
         * If specified, the chain validation will be performed against certificates configured in the given TrustConfig.
         * Allowed only if the policy is to be used with external HTTPS load balancers.
         */
        clientValidationTrustConfig?: string;
    }

    export interface ServerTlsPolicyMtlsPolicyClientValidationCa {
        /**
         * Optional if policy is to be used with Traffic Director. For external HTTPS load balancer must be empty.
         * Defines a mechanism to provision server identity (public and private keys). Cannot be combined with allowOpen as a permissive mode that allows both plain text and TLS is not supported.
         * Structure is documented below.
         */
        certificateProviderInstance?: outputs.networksecurity.ServerTlsPolicyMtlsPolicyClientValidationCaCertificateProviderInstance;
        /**
         * gRPC specific configuration to access the gRPC server to obtain the cert and private key.
         * Structure is documented below.
         */
        grpcEndpoint?: outputs.networksecurity.ServerTlsPolicyMtlsPolicyClientValidationCaGrpcEndpoint;
    }

    export interface ServerTlsPolicyMtlsPolicyClientValidationCaCertificateProviderInstance {
        /**
         * Plugin instance name, used to locate and load CertificateProvider instance configuration. Set to "googleCloudPrivateSpiffe" to use Certificate Authority Service certificate provider instance.
         */
        pluginInstance: string;
    }

    export interface ServerTlsPolicyMtlsPolicyClientValidationCaGrpcEndpoint {
        /**
         * The target URI of the gRPC endpoint. Only UDS path is supported, and should start with "unix:".
         */
        targetUri: string;
    }

    export interface ServerTlsPolicyServerCertificate {
        /**
         * Optional if policy is to be used with Traffic Director. For external HTTPS load balancer must be empty.
         * Defines a mechanism to provision server identity (public and private keys). Cannot be combined with allowOpen as a permissive mode that allows both plain text and TLS is not supported.
         * Structure is documented below.
         */
        certificateProviderInstance?: outputs.networksecurity.ServerTlsPolicyServerCertificateCertificateProviderInstance;
        /**
         * gRPC specific configuration to access the gRPC server to obtain the cert and private key.
         * Structure is documented below.
         */
        grpcEndpoint?: outputs.networksecurity.ServerTlsPolicyServerCertificateGrpcEndpoint;
    }

    export interface ServerTlsPolicyServerCertificateCertificateProviderInstance {
        /**
         * Plugin instance name, used to locate and load CertificateProvider instance configuration. Set to "googleCloudPrivateSpiffe" to use Certificate Authority Service certificate provider instance.
         */
        pluginInstance: string;
    }

    export interface ServerTlsPolicyServerCertificateGrpcEndpoint {
        /**
         * The target URI of the gRPC endpoint. Only UDS path is supported, and should start with "unix:".
         */
        targetUri: string;
    }

}

export namespace networkservices {
    export interface EdgeCacheKeysetPublicKey {
        /**
         * The ID of the public key. The ID must be 1-63 characters long, and comply with RFC1035.
         * The name must be 1-64 characters long, and match the regular expression [a-zA-Z][a-zA-Z0-9_-]*
         * which means the first character must be a letter, and all following characters must be a dash, underscore, letter or digit.
         */
        id: string;
        /**
         * Set to true to have the CDN automatically manage this public key value.
         */
        managed?: boolean;
        /**
         * The base64-encoded value of the Ed25519 public key. The base64 encoding can be padded (44 bytes) or unpadded (43 bytes).
         * Representations or encodings of the public key other than this will be rejected with an error.
         * **Note**: This property is sensitive and will not be displayed in the plan.
         */
        value?: string;
    }

    export interface EdgeCacheKeysetValidationSharedKey {
        /**
         * The name of the secret version in Secret Manager.
         * The resource name of the secret version must be in the format `projects/*&#47;secrets/*&#47;versions/*` where the `*` values are replaced by the secrets themselves.
         * The secrets must be at least 16 bytes large.  The recommended secret size depends on the signature algorithm you are using.
         * * If you are using HMAC-SHA1, we suggest 20-byte secrets.
         * * If you are using HMAC-SHA256, we suggest 32-byte secrets.
         * See RFC 2104, Section 3 for more details on these recommendations.
         */
        secretVersion: string;
    }

    export interface EdgeCacheOriginAwsV4Authentication {
        /**
         * The access key ID your origin uses to identify the key.
         */
        accessKeyId: string;
        /**
         * The name of the AWS region that your origin is in.
         */
        originRegion: string;
        /**
         * The Secret Manager secret version of the secret access key used by your origin.
         * This is the resource name of the secret version in the format `projects/*&#47;secrets/*&#47;versions/*` where the `*` values are replaced by the project, secret, and version you require.
         */
        secretAccessKeyVersion: string;
    }

    export interface EdgeCacheOriginOriginOverrideAction {
        /**
         * The header actions, including adding and removing
         * headers, for request handled by this origin.
         * Structure is documented below.
         */
        headerAction?: outputs.networkservices.EdgeCacheOriginOriginOverrideActionHeaderAction;
        /**
         * The URL rewrite configuration for request that are
         * handled by this origin.
         * Structure is documented below.
         */
        urlRewrite?: outputs.networkservices.EdgeCacheOriginOriginOverrideActionUrlRewrite;
    }

    export interface EdgeCacheOriginOriginOverrideActionHeaderAction {
        /**
         * Describes a header to add.
         * You may add a maximum of 25 request headers.
         * Structure is documented below.
         */
        requestHeadersToAdds?: outputs.networkservices.EdgeCacheOriginOriginOverrideActionHeaderActionRequestHeadersToAdd[];
    }

    export interface EdgeCacheOriginOriginOverrideActionHeaderActionRequestHeadersToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * Whether to replace all existing headers with the same name.
         * By default, added header values are appended
         * to the response or request headers with the
         * same field names. The added values are
         * separated by commas.
         * To overwrite existing values, set `replace` to `true`.
         */
        replace?: boolean;
    }

    export interface EdgeCacheOriginOriginOverrideActionUrlRewrite {
        /**
         * Prior to forwarding the request to the selected
         * origin, the request's host header is replaced with
         * contents of the hostRewrite.
         * This value must be between 1 and 255 characters.
         */
        hostRewrite?: string;
    }

    export interface EdgeCacheOriginOriginRedirect {
        /**
         * The set of redirect response codes that the CDN
         * follows. Values of
         * [RedirectConditions](https://cloud.google.com/media-cdn/docs/reference/rest/v1/projects.locations.edgeCacheOrigins#redirectconditions)
         * are accepted.
         */
        redirectConditions?: string[];
    }

    export interface EdgeCacheOriginTimeout {
        /**
         * The maximum duration to wait for a single origin connection to be established, including DNS lookup, TLS handshake and TCP/QUIC connection establishment.
         * Defaults to 5 seconds. The timeout must be a value between 1s and 15s.
         * The connectTimeout capped by the deadline set by the request's maxAttemptsTimeout.  The last connection attempt may have a smaller connectTimeout in order to adhere to the overall maxAttemptsTimeout.
         */
        connectTimeout?: string;
        /**
         * The maximum time across all connection attempts to the origin, including failover origins, before returning an error to the client. A HTTP 504 will be returned if the timeout is reached before a response is returned.
         * Defaults to 15 seconds. The timeout must be a value between 1s and 30s.
         * If a failoverOrigin is specified, the maxAttemptsTimeout of the first configured origin sets the deadline for all connection attempts across all failoverOrigins.
         */
        maxAttemptsTimeout?: string;
        /**
         * The maximum duration to wait between reads of a single HTTP connection/stream.
         * Defaults to 15 seconds.  The timeout must be a value between 1s and 30s.
         * The readTimeout is capped by the responseTimeout.  All reads of the HTTP connection/stream must be completed by the deadline set by the responseTimeout.
         * If the response headers have already been written to the connection, the response will be truncated and logged.
         *
         * <a name="nestedAwsV4Authentication"></a>The `awsV4Authentication` block supports:
         */
        readTimeout?: string;
        /**
         * The maximum duration to wait for the last byte of a response to arrive when reading from the HTTP connection/stream.
         * Defaults to 30 seconds. The timeout must be a value between 1s and 120s.
         * The responseTimeout starts after the connection has been established.
         * This also applies to HTTP Chunked Transfer Encoding responses, and/or when an open-ended Range request is made to the origin. Origins that take longer to write additional bytes to the response than the configured responseTimeout will result in an error being returned to the client.
         * If the response headers have already been written to the connection, the response will be truncated and logged.
         */
        responseTimeout?: string;
    }

    export interface EdgeCacheServiceLogConfig {
        /**
         * Specifies whether to enable logging for traffic served by this service.
         */
        enable: boolean;
        /**
         * Configures the sampling rate of requests, where 1.0 means all logged requests are reported and 0.0 means no logged requests are reported. The default value is 1.0, and the value of the field must be in [0, 1].
         * This field can only be specified if logging is enabled for this service.
         */
        sampleRate?: number;
    }

    export interface EdgeCacheServiceRouting {
        /**
         * The list of hostRules to match against. These rules define which hostnames the EdgeCacheService will match against, and which route configurations apply.
         * Structure is documented below.
         */
        hostRules: outputs.networkservices.EdgeCacheServiceRoutingHostRule[];
        /**
         * The list of pathMatchers referenced via name by hostRules. PathMatcher is used to match the path portion of the URL when a HostRule matches the URL's host portion.
         * Structure is documented below.
         */
        pathMatchers: outputs.networkservices.EdgeCacheServiceRoutingPathMatcher[];
    }

    export interface EdgeCacheServiceRoutingHostRule {
        /**
         * A human-readable description of the hostRule.
         */
        description?: string;
        /**
         * The list of host patterns to match.
         * Host patterns must be valid hostnames. Ports are not allowed. Wildcard hosts are supported in the suffix or prefix form. * matches any string of ([a-z0-9-.]*). It does not match the empty string.
         * When multiple hosts are specified, hosts are matched in the following priority:
         * 1. Exact domain names: ``www.foo.com``.
         * 2. Suffix domain wildcards: ``*.foo.com`` or ``*-bar.foo.com``.
         * 3. Prefix domain wildcards: ``foo.*`` or ``foo-*``.
         * 4. Special wildcard ``*`` matching any domain.
         * Notes:
         * The wildcard will not match the empty string. e.g. ``*-bar.foo.com`` will match ``baz-bar.foo.com`` but not ``-bar.foo.com``. The longest wildcards match first. Only a single host in the entire service can match on ``*``. A domain must be unique across all configured hosts within a service.
         * Hosts are matched against the HTTP Host header, or for HTTP/2 and HTTP/3, the ":authority" header, from the incoming request.
         * You may specify up to 10 hosts.
         */
        hosts: string[];
        /**
         * The name of the pathMatcher associated with this hostRule.
         */
        pathMatcher: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcher {
        /**
         * A human-readable description of the resource.
         */
        description?: string;
        /**
         * The name to which this PathMatcher is referred by the HostRule.
         */
        name: string;
        /**
         * The routeRules to match against. routeRules support advanced routing behaviour, and can match on paths, headers and query parameters, as well as status codes and HTTP methods.
         * Structure is documented below.
         */
        routeRules: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRule[];
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRule {
        /**
         * A human-readable description of the routeRule.
         */
        description?: string;
        /**
         * The header actions, including adding & removing headers, for requests that match this route.
         * Structure is documented below.
         */
        headerAction?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderAction;
        /**
         * The list of criteria for matching attributes of a request to this routeRule. This list has OR semantics: the request matches this routeRule when any of the matchRules are satisfied. However predicates
         * within a given matchRule have AND semantics. All predicates within a matchRule must match for the request to match the rule.
         * Structure is documented below.
         */
        matchRules: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleMatchRule[];
        /**
         * The Origin resource that requests to this route should fetch from when a matching response is not in cache. Origins can be defined as short names ("my-origin") or fully-qualified resource URLs - e.g. "networkservices.googleapis.com/projects/my-project/global/edgecacheorigins/my-origin"
         * Only one of origin or urlRedirect can be set.
         */
        origin?: string;
        /**
         * The priority of this route rule, where 1 is the highest priority.
         * You cannot configure two or more routeRules with the same priority. Priority for each rule must be set to a number between 1 and 999 inclusive.
         * Priority numbers can have gaps, which enable you to add or remove rules in the future without affecting the rest of the rules. For example, 1, 2, 3, 4, 5, 9, 12, 16 is a valid series of priority numbers
         * to which you could add rules numbered from 6 to 8, 10 to 11, and 13 to 15 in the future without any impact on existing rules.
         */
        priority: string;
        /**
         * In response to a matching path, the routeAction performs advanced routing actions like URL rewrites, header transformations, etc. prior to forwarding the request to the selected origin.
         * Structure is documented below.
         */
        routeAction?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleRouteAction;
        /**
         * The URL redirect configuration for requests that match this route.
         * Structure is documented below.
         */
        urlRedirect?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleUrlRedirect;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderAction {
        /**
         * Describes a header to add.
         * Structure is documented below.
         */
        requestHeaderToAdds?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to forwarding the request to the origin.
         * Structure is documented below.
         */
        requestHeaderToRemoves?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToRemove[];
        /**
         * Headers to add to the response prior to sending it back to the client.
         * Response headers are only sent to the client, and do not have an effect on the cache serving the response.
         * Structure is documented below.
         */
        responseHeaderToAdds?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToAdd[];
        /**
         * A list of header names for headers that need to be removed from the request prior to forwarding the request to the origin.
         * Structure is documented below.
         */
        responseHeaderToRemoves?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToRemove[];
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * Whether to replace all existing headers with the same name.
         */
        replace: boolean;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToRemove {
        /**
         * The name of the header to remove.
         */
        headerName: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToAdd {
        /**
         * The name of the header to add.
         */
        headerName: string;
        /**
         * The value of the header to add.
         */
        headerValue: string;
        /**
         * Whether to replace all existing headers with the same name.
         */
        replace: boolean;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToRemove {
        /**
         * Headers to remove from the response prior to sending it back to the client.
         * Response headers are only sent to the client, and do not have an effect on the cache serving the response.
         */
        headerName: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleMatchRule {
        /**
         * For satisfying the matchRule condition, the path of the request must exactly match the value specified in fullPathMatch after removing any query parameters and anchor that may be part of the original URL.
         */
        fullPathMatch?: string;
        /**
         * Specifies a list of header match criteria, all of which must match corresponding headers in the request.
         * Structure is documented below.
         */
        headerMatches?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleHeaderMatch[];
        /**
         * Specifies that prefixMatch and fullPathMatch matches are case sensitive.
         */
        ignoreCase: boolean;
        /**
         * For satisfying the matchRule condition, the path of the request
         * must match the wildcard pattern specified in pathTemplateMatch
         * after removing any query parameters and anchor that may be part
         * of the original URL.
         * pathTemplateMatch must be between 1 and 255 characters
         * (inclusive).  The pattern specified by pathTemplateMatch may
         * have at most 5 wildcard operators and at most 5 variable
         * captures in total.
         */
        pathTemplateMatch?: string;
        /**
         * For satisfying the matchRule condition, the request's path must begin with the specified prefixMatch. prefixMatch must begin with a /.
         */
        prefixMatch?: string;
        /**
         * Specifies a list of query parameter match criteria, all of which must match corresponding query parameters in the request.
         * Structure is documented below.
         */
        queryParameterMatches?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleQueryParameterMatch[];
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleHeaderMatch {
        /**
         * The value of the header should exactly match contents of exactMatch.
         */
        exactMatch?: string;
        /**
         * The header name to match on.
         */
        headerName: string;
        /**
         * If set to false (default), the headerMatch is considered a match if the match criteria above are met.
         * If set to true, the headerMatch is considered a match if the match criteria above are NOT met.
         */
        invertMatch: boolean;
        /**
         * The value of the header must start with the contents of prefixMatch.
         */
        prefixMatch?: string;
        /**
         * A header with the contents of headerName must exist. The match takes place whether or not the request's header has a value.
         */
        presentMatch?: boolean;
        /**
         * The value of the header must end with the contents of suffixMatch.
         */
        suffixMatch?: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleQueryParameterMatch {
        /**
         * The queryParameterMatch matches if the value of the parameter exactly matches the contents of exactMatch.
         */
        exactMatch?: string;
        /**
         * The name of the query parameter to match. The query parameter must exist in the request, in the absence of which the request match fails.
         */
        name: string;
        /**
         * Specifies that the queryParameterMatch matches if the request contains the query parameter, irrespective of whether the parameter has a value or not.
         */
        presentMatch?: boolean;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleRouteAction {
        /**
         * The policy to use for defining caching and signed request behaviour for requests that match this route.
         * Structure is documented below.
         */
        cdnPolicy?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicy;
        /**
         * CORSPolicy defines Cross-Origin-Resource-Sharing configuration, including which CORS response headers will be set.
         * Structure is documented below.
         */
        corsPolicy?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCorsPolicy;
        /**
         * The URL rewrite configuration for requests that match this route.
         * Structure is documented below.
         */
        urlRewrite?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionUrlRewrite;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicy {
        /**
         * Enable signature generation or propagation on this route.
         * This field may only be specified when signedRequestMode is set to REQUIRE_TOKENS.
         * Structure is documented below.
         */
        addSignatures?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyAddSignatures;
        /**
         * Defines the request parameters that contribute to the cache key.
         * Structure is documented below.
         */
        cacheKeyPolicy?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyCacheKeyPolicy;
        /**
         * Cache modes allow users to control the behaviour of the cache, what content it should cache automatically, whether to respect origin headers, or whether to unconditionally cache all responses.
         * For all cache modes, Cache-Control headers will be passed to the client. Use clientTtl to override what is sent to the client.
         * Possible values are: `CACHE_ALL_STATIC`, `USE_ORIGIN_HEADERS`, `FORCE_CACHE_ALL`, `BYPASS_CACHE`.
         */
        cacheMode: string;
        /**
         * Specifies a separate client (e.g. browser client) TTL, separate from the TTL used by the edge caches. Leaving this empty will use the same cache TTL for both the CDN and the client-facing response.
         * - The TTL must be > 0 and <= 86400s (1 day)
         * - The clientTtl cannot be larger than the defaultTtl (if set)
         * - Fractions of a second are not allowed.
         * Omit this field to use the defaultTtl, or the max-age set by the origin, as the client-facing TTL.
         * When the cache mode is set to "USE_ORIGIN_HEADERS" or "BYPASS_CACHE", you must omit this field.
         * A duration in seconds terminated by 's'. Example: "3s".
         */
        clientTtl?: string;
        /**
         * Specifies the default TTL for cached content served by this origin for responses that do not have an existing valid TTL (max-age or s-max-age).
         * Defaults to 3600s (1 hour).
         * - The TTL must be >= 0 and <= 31,536,000 seconds (1 year)
         * - Setting a TTL of "0" means "always revalidate" (equivalent to must-revalidate)
         * - The value of defaultTTL cannot be set to a value greater than that of maxTTL.
         * - Fractions of a second are not allowed.
         * - When the cacheMode is set to FORCE_CACHE_ALL, the defaultTTL will overwrite the TTL set in all responses.
         * Note that infrequently accessed objects may be evicted from the cache before the defined TTL. Objects that expire will be revalidated with the origin.
         * When the cache mode is set to "USE_ORIGIN_HEADERS" or "BYPASS_CACHE", you must omit this field.
         * A duration in seconds terminated by 's'. Example: "3s".
         */
        defaultTtl: string;
        /**
         * Specifies the maximum allowed TTL for cached content served by this origin.
         * Defaults to 86400s (1 day).
         * Cache directives that attempt to set a max-age or s-maxage higher than this, or an Expires header more than maxTtl seconds in the future will be capped at the value of maxTTL, as if it were the value of an s-maxage Cache-Control directive.
         * - The TTL must be >= 0 and <= 31,536,000 seconds (1 year)
         * - Setting a TTL of "0" means "always revalidate"
         * - The value of maxTtl must be equal to or greater than defaultTtl.
         * - Fractions of a second are not allowed.
         * When the cache mode is set to "USE_ORIGIN_HEADERS", "FORCE_CACHE_ALL", or "BYPASS_CACHE", you must omit this field.
         * A duration in seconds terminated by 's'. Example: "3s".
         */
        maxTtl: string;
        /**
         * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects. This can reduce the load on your origin and improve end-user experience by reducing response latency.
         * By default, the CDNPolicy will apply the following default TTLs to these status codes:
         * - HTTP 300 (Multiple Choice), 301, 308 (Permanent Redirects): 10m
         * - HTTP 404 (Not Found), 410 (Gone), 451 (Unavailable For Legal Reasons): 120s
         * - HTTP 405 (Method Not Found), 414 (URI Too Long), 501 (Not Implemented): 60s
         * These defaults can be overridden in negativeCachingPolicy
         */
        negativeCaching?: boolean;
        /**
         * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
         * - Omitting the policy and leaving negativeCaching enabled will use the default TTLs for each status code, defined in negativeCaching.
         * - TTLs must be >= 0 (where 0 is "always revalidate") and <= 86400s (1 day)
         * Note that when specifying an explicit negativeCachingPolicy, you should take care to specify a cache TTL for all response codes that you wish to cache. The CDNPolicy will not apply any default negative caching when a policy exists.
         */
        negativeCachingPolicy?: {[key: string]: string};
        /**
         * The EdgeCacheKeyset containing the set of public keys used to validate signed requests at the edge.
         */
        signedRequestKeyset: string;
        /**
         * Limit how far into the future the expiration time of a signed request may be.
         * When set, a signed request is rejected if its expiration time is later than now + signedRequestMaximumExpirationTtl, where now is the time at which the signed request is first handled by the CDN.
         * - The TTL must be > 0.
         * - Fractions of a second are not allowed.
         * By default, signedRequestMaximumExpirationTtl is not set and the expiration time of a signed request may be arbitrarily far into future.
         */
        signedRequestMaximumExpirationTtl?: string;
        /**
         * Whether to enforce signed requests. The default value is DISABLED, which means all content is public, and does not authorize access.
         * You must also set a signedRequestKeyset to enable signed requests.
         * When set to REQUIRE_SIGNATURES, all matching requests will have their signature validated. Requests that were not signed with the corresponding private key, or that are otherwise invalid (expired, do not match the signature, IP address, or header) will be rejected with a HTTP 403 and (if enabled) logged.
         * Possible values are: `DISABLED`, `REQUIRE_SIGNATURES`, `REQUIRE_TOKENS`.
         */
        signedRequestMode: string;
        /**
         * Additional options for signed tokens.
         * signedTokenOptions may only be specified when signedRequestMode is REQUIRE_TOKENS.
         * Structure is documented below.
         */
        signedTokenOptions?: outputs.networkservices.EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicySignedTokenOptions;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyAddSignatures {
        /**
         * The actions to take to add signatures to responses.
         * Each value may be one of: `GENERATE_COOKIE`, `GENERATE_TOKEN_HLS_COOKIELESS`, `PROPAGATE_TOKEN_HLS_COOKIELESS`.
         */
        actions: string;
        /**
         * The parameters to copy from the verified token to the generated token.
         * Only the following parameters may be copied:
         */
        copiedParameters?: string[];
        /**
         * The keyset to use for signature generation.
         * The following are both valid paths to an EdgeCacheKeyset resource:
         * * `projects/project/locations/global/edgeCacheKeysets/yourKeyset`
         */
        keyset?: string;
        /**
         * The query parameter in which to put the generated token.
         * If not specified, defaults to `edge-cache-token`.
         * If specified, the name must be 1-64 characters long and match the regular expression `a-zA-Z*` which means the first character must be a letter, and all following characters must be a dash, underscore, letter or digit.
         * This field may only be set when the GENERATE_TOKEN_HLS_COOKIELESS or PROPAGATE_TOKEN_HLS_COOKIELESS actions are specified.
         */
        tokenQueryParameter?: string;
        /**
         * The duration the token is valid starting from the moment the token is first generated.
         * Defaults to `86400s` (1 day).
         * The TTL must be >= 0 and <= 604,800 seconds (1 week).
         * This field may only be specified when the GENERATE_COOKIE or GENERATE_TOKEN_HLS_COOKIELESS actions are specified.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        tokenTtl?: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyCacheKeyPolicy {
        /**
         * If true, requests to different hosts will be cached separately.
         * Note: this should only be enabled if hosts share the same origin and content. Removing the host from the cache key may inadvertently result in different objects being cached than intended, depending on which route the first user matched.
         */
        excludeHost: boolean;
        /**
         * If true, exclude query string parameters from the cache key
         * If false (the default), include the query string parameters in
         * the cache key according to includeQueryParameters and
         * excludeQueryParameters. If neither includeQueryParameters nor
         * excludeQueryParameters is set, the entire query string will be
         * included.
         */
        excludeQueryString?: boolean;
        /**
         * Names of query string parameters to exclude from cache keys. All other parameters will be included.
         * Either specify includedQueryParameters or excludedQueryParameters, not both. '&' and '=' will be percent encoded and not treated as delimiters.
         */
        excludedQueryParameters?: string[];
        /**
         * If true, http and https requests will be cached separately.
         */
        includeProtocol: boolean;
        /**
         * Names of Cookies to include in cache keys.  The cookie name and cookie value of each cookie named will be used as part of the cache key.
         * Cookie names:
         * - must be valid RFC 6265 "cookie-name" tokens
         * - are case sensitive
         * - cannot start with "Edge-Cache-" (case insensitive)
         * Note that specifying several cookies, and/or cookies that have a large range of values (e.g., per-user) will dramatically impact the cache hit rate, and may result in a higher eviction rate and reduced performance.
         * You may specify up to three cookie names.
         */
        includedCookieNames?: string[];
        /**
         * Names of HTTP request headers to include in cache keys. The value of the header field will be used as part of the cache key.
         * - Header names must be valid HTTP RFC 7230 header field values.
         * - Header field names are case insensitive
         * - To include the HTTP method, use ":method"
         * Note that specifying several headers, and/or headers that have a large range of values (e.g. per-user) will dramatically impact the cache hit rate, and may result in a higher eviction rate and reduced performance.
         */
        includedHeaderNames?: string[];
        /**
         * Names of query string parameters to include in cache keys. All other parameters will be excluded.
         * Either specify includedQueryParameters or excludedQueryParameters, not both. '&' and '=' will be percent encoded and not treated as delimiters.
         */
        includedQueryParameters?: string[];
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicySignedTokenOptions {
        /**
         * The allowed signature algorithms to use.
         * Defaults to using only ED25519.
         * You may specify up to 3 signature algorithms to use.
         * Each value may be one of: `ED25519`, `HMAC_SHA_256`, `HMAC_SHA1`.
         */
        allowedSignatureAlgorithms?: string[];
        /**
         * The query parameter in which to find the token.
         * The name must be 1-64 characters long and match the regular expression `a-zA-Z*` which means the first character must be a letter, and all following characters must be a dash, underscore, letter or digit.
         * Defaults to `edge-cache-token`.
         */
        tokenQueryParameter?: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
         * This translates to the Access-Control-Allow-Credentials response header.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers response header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for the Access-Control-Allow-Methods response header.
         */
        allowMethods?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         * This translates to the Access-Control-Allow-Origin response header.
         */
        allowOrigins?: string[];
        /**
         * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
         */
        disabled?: boolean;
        /**
         * Specifies the content for the Access-Control-Allow-Headers response header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long results of a preflight request can be cached by a client in seconds. Note that many browser clients enforce a maximum TTL of 600s (10 minutes).
         * - Setting the value to -1 forces a pre-flight check for all requests (not recommended)
         * - A maximum TTL of 86400s can be set, but note that (as above) some clients may force pre-flight checks at a more regular interval.
         * - This translates to the Access-Control-Max-Age header.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        maxAge: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionUrlRewrite {
        /**
         * Prior to forwarding the request to the selected origin, the request's host header is replaced with contents of hostRewrite.
         */
        hostRewrite?: string;
        /**
         * Prior to forwarding the request to the selected origin, the matching portion of the request's path is replaced by pathPrefixRewrite.
         */
        pathPrefixRewrite?: string;
        /**
         * Prior to forwarding the request to the selected origin, if the
         * request matched a pathTemplateMatch, the matching portion of the
         * request's path is replaced re-written using the pattern specified
         * by pathTemplateRewrite.
         * pathTemplateRewrite must be between 1 and 255 characters
         * (inclusive), must start with a '/', and must only use variables
         * captured by the route's pathTemplate matchers.
         * pathTemplateRewrite may only be used when all of a route's
         * MatchRules specify pathTemplate.
         * Only one of pathPrefixRewrite and pathTemplateRewrite may be
         * specified.
         */
        pathTemplateRewrite?: string;
    }

    export interface EdgeCacheServiceRoutingPathMatcherRouteRuleUrlRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was supplied in the request.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https. If set to false, the URL scheme of the redirected request will remain the same as that of the request.
         * This can only be set if there is at least one (1) edgeSslCertificate set on the service.
         */
        httpsRedirect: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was supplied in the request.
         * pathRedirect cannot be supplied together with prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the original request will be used for the redirect.
         * The path value must be between 1 and 1024 characters.
         */
        pathRedirect?: string;
        /**
         * The prefix that replaces the prefixMatch specified in the routeRule, retaining the remaining portion of the URL before redirecting the request.
         * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or neither. If neither is supplied, the path of the original request will be used for the redirect.
         */
        prefixRedirect?: string;
        /**
         * The HTTP Status code to use for this RedirectAction.
         * The supported values are:
         */
        redirectResponseCode: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed prior to redirecting the request. If set to false, the query portion of the original URL is retained.
         *
         * - - -
         */
        stripQuery: boolean;
    }

    export interface EndpointPolicyEndpointMatcher {
        /**
         * The matcher is based on node metadata presented by xDS clients.
         * Structure is documented below.
         */
        metadataLabelMatcher: outputs.networkservices.EndpointPolicyEndpointMatcherMetadataLabelMatcher;
    }

    export interface EndpointPolicyEndpointMatcherMetadataLabelMatcher {
        /**
         * Specifies how matching should be done.
         * Possible values are: `MATCH_ANY`, `MATCH_ALL`.
         */
        metadataLabelMatchCriteria: string;
        /**
         * The list of label value pairs that must match labels in the provided metadata based on filterMatchCriteria
         * Structure is documented below.
         */
        metadataLabels?: outputs.networkservices.EndpointPolicyEndpointMatcherMetadataLabelMatcherMetadataLabel[];
    }

    export interface EndpointPolicyEndpointMatcherMetadataLabelMatcherMetadataLabel {
        /**
         * Required. Label name presented as key in xDS Node Metadata.
         */
        labelName: string;
        /**
         * Required. Label value presented as value corresponding to the above key, in xDS Node Metadata.
         *
         * - - -
         */
        labelValue: string;
    }

    export interface EndpointPolicyTrafficPortSelector {
        /**
         * List of ports. Can be port numbers or port range (example, [80-90] specifies all ports from 80 to 90, including 80 and 90) or named ports or * to specify all ports. If the list is empty, all ports are selected.
         */
        ports: string[];
    }

    export interface GrpcRouteRule {
        /**
         * Required. A detailed rule defining how to route traffic.
         * Structure is documented below.
         */
        action?: outputs.networkservices.GrpcRouteRuleAction;
        /**
         * Matches define conditions used for matching the rule against incoming gRPC requests.
         * Structure is documented below.
         */
        matches?: outputs.networkservices.GrpcRouteRuleMatch[];
    }

    export interface GrpcRouteRuleAction {
        /**
         * The destination to which traffic should be forwarded.
         * Structure is documented below.
         */
        destinations?: outputs.networkservices.GrpcRouteRuleActionDestination[];
        /**
         * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.networkservices.GrpcRouteRuleActionFaultInjectionPolicy;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.networkservices.GrpcRouteRuleActionRetryPolicy;
        /**
         * Specifies the timeout for selected route.
         */
        timeout?: string;
    }

    export interface GrpcRouteRuleActionDestination {
        /**
         * The URL of a BackendService to route traffic to.
         */
        serviceName?: string;
        /**
         * Specifies the proportion of requests forwarded to the backend referenced by the serviceName field.
         */
        weight?: number;
    }

    export interface GrpcRouteRuleActionFaultInjectionPolicy {
        /**
         * Specification of how client requests are aborted as part of fault injection before being sent to a destination.
         * Structure is documented below.
         */
        abort?: outputs.networkservices.GrpcRouteRuleActionFaultInjectionPolicyAbort;
        /**
         * Specification of how client requests are delayed as part of fault injection before being sent to a destination.
         * Structure is documented below.
         */
        delay?: outputs.networkservices.GrpcRouteRuleActionFaultInjectionPolicyDelay;
    }

    export interface GrpcRouteRuleActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         */
        httpStatus?: number;
        /**
         * The percentage of traffic which will be aborted.
         */
        percentage?: number;
    }

    export interface GrpcRouteRuleActionFaultInjectionPolicyDelay {
        /**
         * Specify a fixed delay before forwarding the request.
         */
        fixedDelay?: string;
        /**
         * The percentage of traffic on which delay will be injected.
         */
        percentage?: number;
    }

    export interface GrpcRouteRuleActionRetryPolicy {
        /**
         * Specifies the allowed number of retries.
         *
         * - - -
         */
        numRetries?: number;
        /**
         * Specifies one or more conditions when this retry policy applies.
         * Each value may be one of: `connect-failure`, `refused-stream`, `cancelled`, `deadline-exceeded`, `resource-exhausted`, `unavailable`.
         */
        retryConditions?: string[];
    }

    export interface GrpcRouteRuleMatch {
        /**
         * Specifies a list of HTTP request headers to match against.
         * Structure is documented below.
         */
        headers?: outputs.networkservices.GrpcRouteRuleMatchHeader[];
        /**
         * A gRPC method to match against. If this field is empty or omitted, will match all methods.
         * Structure is documented below.
         */
        method?: outputs.networkservices.GrpcRouteRuleMatchMethod;
    }

    export interface GrpcRouteRuleMatchHeader {
        /**
         * Required. The key of the header.
         */
        key: string;
        /**
         * The type of match.
         * Default value is `EXACT`.
         * Possible values are: `TYPE_UNSPECIFIED`, `EXACT`, `REGULAR_EXPRESSION`.
         */
        type?: string;
        /**
         * Required. The value of the header.
         */
        value: string;
    }

    export interface GrpcRouteRuleMatchMethod {
        /**
         * Specifies that matches are case sensitive. The default value is true.
         */
        caseSensitive?: boolean;
        /**
         * Required. Name of the method to match against.
         */
        grpcMethod: string;
        /**
         * Required. Name of the service to match against.
         */
        grpcService: string;
    }

    export interface HttpRouteRule {
        /**
         * The detailed rule defining how to route matched traffic.
         * Structure is documented below.
         */
        action?: outputs.networkservices.HttpRouteRuleAction;
        /**
         * A list of matches define conditions used for matching the rule against incoming HTTP requests. Each match is independent, i.e. this rule will be matched if ANY one of the matches is satisfied.
         * If no matches field is specified, this rule will unconditionally match traffic.
         * If a default rule is desired to be configured, add a rule with no matches specified to the end of the rules list.
         * Structure is documented below.
         */
        matches?: outputs.networkservices.HttpRouteRuleMatch[];
    }

    export interface HttpRouteRuleAction {
        /**
         * The specification for allowing client side cross-origin requests.
         * Structure is documented below.
         */
        corsPolicy?: outputs.networkservices.HttpRouteRuleActionCorsPolicy;
        /**
         * The destination to which traffic should be forwarded.
         * Structure is documented below.
         */
        destinations?: outputs.networkservices.HttpRouteRuleActionDestination[];
        /**
         * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
         * Structure is documented below.
         */
        faultInjectionPolicy?: outputs.networkservices.HttpRouteRuleActionFaultInjectionPolicy;
        /**
         * If set, the request is directed as configured by this field.
         * Structure is documented below.
         */
        redirect?: outputs.networkservices.HttpRouteRuleActionRedirect;
        /**
         * The specification for modifying the headers of a matching request prior to delivery of the request to the destination.
         * Structure is documented below.
         */
        requestHeaderModifier?: outputs.networkservices.HttpRouteRuleActionRequestHeaderModifier;
        /**
         * Specifies the policy on how requests intended for the routes destination are shadowed to a separate mirrored destination.
         * Structure is documented below.
         */
        requestMirrorPolicy?: outputs.networkservices.HttpRouteRuleActionRequestMirrorPolicy;
        /**
         * The specification for modifying the headers of a response prior to sending the response back to the client.
         * Structure is documented below.
         */
        responseHeaderModifier?: outputs.networkservices.HttpRouteRuleActionResponseHeaderModifier;
        /**
         * Specifies the retry policy associated with this route.
         * Structure is documented below.
         */
        retryPolicy?: outputs.networkservices.HttpRouteRuleActionRetryPolicy;
        /**
         * Specifies the timeout for selected route.
         */
        timeout?: string;
        /**
         * The specification for rewrite URL before forwarding requests to the destination.
         * Structure is documented below.
         */
        urlRewrite?: outputs.networkservices.HttpRouteRuleActionUrlRewrite;
    }

    export interface HttpRouteRuleActionCorsPolicy {
        /**
         * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
         */
        allowCredentials?: boolean;
        /**
         * Specifies the content for Access-Control-Allow-Headers header.
         */
        allowHeaders?: string[];
        /**
         * Specifies the content for Access-Control-Allow-Methods header.
         */
        allowMethods?: string[];
        /**
         * Specifies the regular expression patterns that match allowed origins.
         */
        allowOriginRegexes?: string[];
        /**
         * Specifies the list of origins that will be allowed to do CORS requests.
         */
        allowOrigins?: string[];
        /**
         * If true, the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
         *
         * - - -
         */
        disabled?: boolean;
        /**
         * Specifies the content for Access-Control-Expose-Headers header.
         */
        exposeHeaders?: string[];
        /**
         * Specifies how long result of a preflight request can be cached in seconds.
         */
        maxAge?: string;
    }

    export interface HttpRouteRuleActionDestination {
        /**
         * The URL of a BackendService to route traffic to.
         */
        serviceName?: string;
        /**
         * Specifies the proportion of requests forwarded to the backend referenced by the serviceName field. This is computed as: weight/Sum(weights in this destination list). For non-zero values, there may be some epsilon from the exact proportion defined here depending on the precision an implementation supports.
         * If only one serviceName is specified and it has a weight greater than 0, 100% of the traffic is forwarded to that backend.
         * If weights are specified for any one service name, they need to be specified for all of them.
         * If weights are unspecified for all services, then, traffic is distributed in equal proportions to all of them.
         */
        weight?: number;
    }

    export interface HttpRouteRuleActionFaultInjectionPolicy {
        /**
         * Specification of how client requests are aborted as part of fault injection before being sent to a destination.
         * Structure is documented below.
         */
        abort?: outputs.networkservices.HttpRouteRuleActionFaultInjectionPolicyAbort;
        /**
         * Specification of how client requests are delayed as part of fault injection before being sent to a destination.
         * Structure is documented below.
         */
        delay?: outputs.networkservices.HttpRouteRuleActionFaultInjectionPolicyDelay;
    }

    export interface HttpRouteRuleActionFaultInjectionPolicyAbort {
        /**
         * The HTTP status code used to abort the request.
         */
        httpStatus?: number;
        /**
         * The percentage of traffic which will be aborted.
         */
        percentage?: number;
    }

    export interface HttpRouteRuleActionFaultInjectionPolicyDelay {
        /**
         * Specify a fixed delay before forwarding the request.
         */
        fixedDelay?: string;
        /**
         * The percentage of traffic on which delay will be injected.
         */
        percentage?: number;
    }

    export interface HttpRouteRuleActionRedirect {
        /**
         * The host that will be used in the redirect response instead of the one that was supplied in the request.
         */
        hostRedirect?: string;
        /**
         * If set to true, the URL scheme in the redirected request is set to https.
         */
        httpsRedirect?: boolean;
        /**
         * The path that will be used in the redirect response instead of the one that was supplied in the request. pathRedirect can not be supplied together with prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the original request will be used for the redirect.
         */
        pathRedirect?: string;
        /**
         * The port that will be used in the redirected request instead of the one that was supplied in the request.
         */
        portRedirect?: number;
        /**
         * Indicates that during redirection, the matched prefix (or path) should be swapped with this value.
         */
        prefixRewrite?: string;
        /**
         * The HTTP Status code to use for the redirect.
         */
        responseCode?: string;
        /**
         * If set to true, any accompanying query portion of the original URL is removed prior to redirecting the request.
         */
        stripQuery?: boolean;
    }

    export interface HttpRouteRuleActionRequestHeaderModifier {
        /**
         * Add the headers with given map where key is the name of the header, value is the value of the header.
         */
        add?: {[key: string]: string};
        /**
         * Remove headers (matching by header names) specified in the list.
         */
        removes?: string[];
        /**
         * Completely overwrite/replace the headers with given map where key is the name of the header, value is the value of the header.
         */
        set?: {[key: string]: string};
    }

    export interface HttpRouteRuleActionRequestMirrorPolicy {
        /**
         * The destination the requests will be mirrored to.
         * Structure is documented below.
         */
        destination?: outputs.networkservices.HttpRouteRuleActionRequestMirrorPolicyDestination;
    }

    export interface HttpRouteRuleActionRequestMirrorPolicyDestination {
        /**
         * The URL of a BackendService to route traffic to.
         */
        serviceName?: string;
        /**
         * Specifies the proportion of requests forwarded to the backend referenced by the serviceName field. This is computed as: weight/Sum(weights in this destination list). For non-zero values, there may be some epsilon from the exact proportion defined here depending on the precision an implementation supports.
         * If only one serviceName is specified and it has a weight greater than 0, 100% of the traffic is forwarded to that backend.
         * If weights are specified for any one service name, they need to be specified for all of them.
         * If weights are unspecified for all services, then, traffic is distributed in equal proportions to all of them.
         */
        weight?: number;
    }

    export interface HttpRouteRuleActionResponseHeaderModifier {
        /**
         * Add the headers with given map where key is the name of the header, value is the value of the header.
         */
        add?: {[key: string]: string};
        /**
         * Remove headers (matching by header names) specified in the list.
         */
        removes?: string[];
        /**
         * Completely overwrite/replace the headers with given map where key is the name of the header, value is the value of the header.
         */
        set?: {[key: string]: string};
    }

    export interface HttpRouteRuleActionRetryPolicy {
        /**
         * Specifies the allowed number of retries.
         */
        numRetries?: number;
        /**
         * Specifies a non-zero timeout per retry attempt. A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
         */
        perTryTimeout?: string;
        /**
         * Specifies one or more conditions when this retry policy applies.
         */
        retryConditions?: string[];
    }

    export interface HttpRouteRuleActionUrlRewrite {
        /**
         * Prior to forwarding the request to the selected destination, the requests host header is replaced by this value.
         */
        hostRewrite?: string;
        /**
         * Prior to forwarding the request to the selected destination, the matching portion of the requests path is replaced by this value.
         */
        pathPrefixRewrite?: string;
    }

    export interface HttpRouteRuleMatch {
        /**
         * The HTTP request path value should exactly match this value.
         */
        fullPathMatch?: string;
        /**
         * Specifies a list of HTTP request headers to match against.
         * Structure is documented below.
         */
        headers?: outputs.networkservices.HttpRouteRuleMatchHeader[];
        /**
         * Specifies if prefixMatch and fullPathMatch matches are case sensitive. The default value is false.
         */
        ignoreCase?: boolean;
        /**
         * The HTTP request path value must begin with specified prefixMatch. prefixMatch must begin with a /.
         */
        prefixMatch?: string;
        /**
         * Specifies a list of query parameters to match against.
         * Structure is documented below.
         */
        queryParameters?: outputs.networkservices.HttpRouteRuleMatchQueryParameter[];
        /**
         * The HTTP request path value must satisfy the regular expression specified by regexMatch after removing any query parameters and anchor supplied with the original URL. For regular expression grammar, please see https://github.com/google/re2/wiki/Syntax
         */
        regexMatch?: string;
    }

    export interface HttpRouteRuleMatchHeader {
        /**
         * The value of the header should match exactly the content of exactMatch.
         */
        exactMatch?: string;
        /**
         * The name of the HTTP header to match against.
         */
        header?: string;
        /**
         * If specified, the match result will be inverted before checking. Default value is set to false.
         */
        invertMatch?: boolean;
        /**
         * The value of the header must start with the contents of prefixMatch.
         */
        prefixMatch?: string;
        /**
         * A header with headerName must exist. The match takes place whether or not the header has a value.
         */
        presentMatch?: boolean;
        /**
         * If specified, the rule will match if the request header value is within the range.
         * Structure is documented below.
         */
        rangeMatch?: outputs.networkservices.HttpRouteRuleMatchHeaderRangeMatch;
        /**
         * The value of the header must match the regular expression specified in regexMatch.
         */
        regexMatch?: string;
        /**
         * The value of the header must end with the contents of suffixMatch.
         */
        suffixMatch?: string;
    }

    export interface HttpRouteRuleMatchHeaderRangeMatch {
        /**
         * End of the range (exclusive).
         */
        end: number;
        /**
         * Start of the range (inclusive).
         */
        start: number;
    }

    export interface HttpRouteRuleMatchQueryParameter {
        /**
         * The value of the query parameter must exactly match the contents of exactMatch.
         */
        exactMatch?: string;
        /**
         * Specifies that the QueryParameterMatcher matches if request contains query parameter, irrespective of whether the parameter has a value or not.
         */
        presentMatch?: boolean;
        /**
         * The name of the query parameter to match.
         */
        queryParameter?: string;
        /**
         * The value of the query parameter must match the regular expression specified by regexMatch.For regular expression grammar, please see https://github.com/google/re2/wiki/Syntax
         */
        regexMatch?: string;
    }

    export interface TcpRouteRule {
        /**
         * A detailed rule defining how to route traffic.
         * Structure is documented below.
         */
        action: outputs.networkservices.TcpRouteRuleAction;
        /**
         * RouteMatch defines the predicate used to match requests to a given action. Multiple match types are "OR"ed for evaluation.
         * If no routeMatch field is specified, this rule will unconditionally match traffic.
         * Structure is documented below.
         */
        matches?: outputs.networkservices.TcpRouteRuleMatch[];
    }

    export interface TcpRouteRuleAction {
        /**
         * The destination services to which traffic should be forwarded. At least one destination service is required.
         * Structure is documented below.
         */
        destinations?: outputs.networkservices.TcpRouteRuleActionDestination[];
        /**
         * If true, Router will use the destination IP and port of the original connection as the destination of the request.
         */
        originalDestination?: boolean;
    }

    export interface TcpRouteRuleActionDestination {
        /**
         * The URL of a BackendService to route traffic to.
         */
        serviceName?: string;
        /**
         * Specifies the proportion of requests forwarded to the backend referenced by the serviceName field. This is computed as: weight/Sum(weights in this destination list). For non-zero values, there may be some epsilon from the exact proportion defined here depending on the precision an implementation supports.
         * If only one serviceName is specified and it has a weight greater than 0, 100% of the traffic is forwarded to that backend.
         * If weights are specified for any one service name, they need to be specified for all of them.
         * If weights are unspecified for all services, then, traffic is distributed in equal proportions to all of them.
         *
         * - - -
         */
        weight?: number;
    }

    export interface TcpRouteRuleMatch {
        /**
         * Must be specified in the CIDR range format. A CIDR range consists of an IP Address and a prefix length to construct the subnet mask.
         * By default, the prefix length is 32 (i.e. matches a single IP address). Only IPV4 addresses are supported. Examples: "10.0.0.1" - matches against this exact IP address. "10.0.0.0/8" - matches against any IP address within the 10.0.0.0 subnet and 255.255.255.0 mask. "0.0.0.0/0" - matches against any IP address'.
         */
        address: string;
        /**
         * Specifies the destination port to match against.
         */
        port: string;
    }

    export interface TlsRouteRule {
        /**
         * Required. A detailed rule defining how to route traffic.
         * Structure is documented below.
         */
        action: outputs.networkservices.TlsRouteRuleAction;
        /**
         * Matches define the predicate used to match requests to a given action.
         * Structure is documented below.
         */
        matches: outputs.networkservices.TlsRouteRuleMatch[];
    }

    export interface TlsRouteRuleAction {
        /**
         * The destination to which traffic should be forwarded.
         * Structure is documented below.
         */
        destinations?: outputs.networkservices.TlsRouteRuleActionDestination[];
    }

    export interface TlsRouteRuleActionDestination {
        /**
         * The URL of a BackendService to route traffic to.
         */
        serviceName?: string;
        /**
         * Specifies the proportion of requests forwarded to the backend referenced by the serviceName field.
         *
         * - - -
         */
        weight?: number;
    }

    export interface TlsRouteRuleMatch {
        /**
         * ALPN (Application-Layer Protocol Negotiation) to match against. Examples: "http/1.1", "h2". At least one of sniHost and alpn is required. Up to 5 alpns across all matches can be set.
         */
        alpns?: string[];
        /**
         * SNI (server name indicator) to match against. SNI will be matched against all wildcard domains, i.e. www.example.com will be first matched against www.example.com, then *.example.com, then *.com.
         * Partial wildcards are not supported, and values like *w.example.com are invalid. At least one of sniHost and alpn is required. Up to 5 sni hosts across all matches can be set.
         */
        sniHosts?: string[];
    }

}

export namespace notebooks {
    export interface EnvironmentContainerImage {
        /**
         * The path to the container image repository.
         * For example: gcr.io/{project_id}/{imageName}
         */
        repository: string;
        /**
         * The tag of the container image. If not specified, this defaults to the latest tag.
         */
        tag?: string;
    }

    export interface EnvironmentVmImage {
        /**
         * Use this VM image family to find the image; the newest image in this family will be used.
         */
        imageFamily?: string;
        /**
         * Use VM image name to find the image.
         */
        imageName?: string;
        /**
         * The name of the Google Cloud project that this VM image belongs to.
         * Format: projects/{project_id}
         */
        project: string;
    }

    export interface InstanceAcceleratorConfig {
        /**
         * Count of cores of this accelerator.
         */
        coreCount: number;
        /**
         * Type of this accelerator.
         * Possible values are: `ACCELERATOR_TYPE_UNSPECIFIED`, `NVIDIA_TESLA_K80`, `NVIDIA_TESLA_P100`, `NVIDIA_TESLA_V100`, `NVIDIA_TESLA_P4`, `NVIDIA_TESLA_T4`, `NVIDIA_TESLA_T4_VWS`, `NVIDIA_TESLA_P100_VWS`, `NVIDIA_TESLA_P4_VWS`, `NVIDIA_TESLA_A100`, `TPU_V2`, `TPU_V3`.
         */
        type: string;
    }

    export interface InstanceContainerImage {
        /**
         * The path to the container image repository.
         * For example: gcr.io/{project_id}/{imageName}
         */
        repository: string;
        /**
         * The tag of the container image. If not specified, this defaults to the latest tag.
         */
        tag?: string;
    }

    export interface InstanceIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface InstanceIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface InstanceReservationAffinity {
        /**
         * The type of Compute Reservation.
         * Possible values are: `NO_RESERVATION`, `ANY_RESERVATION`, `SPECIFIC_RESERVATION`.
         */
        consumeReservationType: string;
        /**
         * Corresponds to the label key of reservation resource.
         */
        key?: string;
        /**
         * Corresponds to the label values of reservation resource.
         */
        values?: string[];
    }

    export interface InstanceShieldedInstanceConfig {
        /**
         * Defines whether the instance has integrity monitoring enabled. Enables monitoring and attestation of the
         * boot integrity of the instance. The attestation is performed against the integrity policy baseline.
         * This baseline is initially derived from the implicitly trusted boot image when the instance is created.
         * Enabled by default.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Defines whether the instance has Secure Boot enabled. Secure Boot helps ensure that the system only runs
         * authentic software by verifying the digital signature of all boot components, and halting the boot process
         * if signature verification fails.
         * Disabled by default.
         */
        enableSecureBoot?: boolean;
        /**
         * Defines whether the instance has the vTPM enabled.
         * Enabled by default.
         */
        enableVtpm?: boolean;
    }

    export interface InstanceVmImage {
        /**
         * Use this VM image family to find the image; the newest image in this family will be used.
         */
        imageFamily?: string;
        /**
         * Use VM image name to find the image.
         */
        imageName?: string;
        /**
         * The name of the Google Cloud project that this VM image belongs to.
         * Format: projects/{project_id}
         */
        project: string;
    }

    export interface RuntimeAccessConfig {
        /**
         * The type of access mode this instance. For valid values, see
         * `https://cloud.google.com/vertex-ai/docs/workbench/reference/
         * rest/v1/projects.locations.runtimes#RuntimeAccessType`.
         */
        accessType?: string;
        /**
         * (Output)
         * The proxy endpoint that is used to access the runtime.
         */
        proxyUri: string;
        /**
         * The owner of this runtime after creation. Format: `alias@example.com`.
         * Currently supports one owner only.
         */
        runtimeOwner?: string;
    }

    export interface RuntimeIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RuntimeIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RuntimeMetric {
        /**
         * (Output)
         * Contains runtime daemon metrics, such as OS and kernels and
         * sessions stats.
         */
        systemMetrics: {[key: string]: string};
    }

    export interface RuntimeSoftwareConfig {
        /**
         * Specify a custom Cloud Storage path where the GPU driver is stored.
         * If not specified, we'll automatically choose from official GPU drivers.
         */
        customGpuDriverPath?: string;
        /**
         * Verifies core internal services are running. Default: True.
         */
        enableHealthMonitoring?: boolean;
        /**
         * Runtime will automatically shutdown after idle_shutdown_time.
         * Default: True
         */
        idleShutdown?: boolean;
        /**
         * Time in minutes to wait before shuting down runtime.
         * Default: 180 minutes
         */
        idleShutdownTimeout?: number;
        /**
         * Install Nvidia Driver automatically.
         */
        installGpuDriver?: boolean;
        /**
         * Use a list of container images to use as Kernels in the notebook instance.
         * Structure is documented below.
         */
        kernels?: outputs.notebooks.RuntimeSoftwareConfigKernel[];
        /**
         * Cron expression in UTC timezone for schedule instance auto upgrade.
         * Please follow the [cron format](https://en.wikipedia.org/wiki/Cron).
         */
        notebookUpgradeSchedule?: string;
        /**
         * Path to a Bash script that automatically runs after a notebook instance
         * fully boots up. The path must be a URL or
         * Cloud Storage path (gs://path-to-file/file-name).
         */
        postStartupScript?: string;
        /**
         * Behavior for the post startup script.
         * Possible values are: `POST_STARTUP_SCRIPT_BEHAVIOR_UNSPECIFIED`, `RUN_EVERY_START`, `DOWNLOAD_AND_RUN_EVERY_START`.
         */
        postStartupScriptBehavior?: string;
        /**
         * (Output)
         * Bool indicating whether an newer image is available in an image family.
         */
        upgradeable: boolean;
    }

    export interface RuntimeSoftwareConfigKernel {
        /**
         * The path to the container image repository.
         * For example: gcr.io/{project_id}/{imageName}
         */
        repository: string;
        /**
         * The tag of the container image. If not specified, this defaults to the latest tag.
         */
        tag?: string;
    }

    export interface RuntimeVirtualMachine {
        /**
         * (Output)
         * The unique identifier of the Managed Compute Engine instance.
         */
        instanceId: string;
        /**
         * (Output)
         * The user-friendly name of the Managed Compute Engine instance.
         */
        instanceName: string;
        /**
         * Virtual Machine configuration settings.
         * Structure is documented below.
         */
        virtualMachineConfig?: outputs.notebooks.RuntimeVirtualMachineVirtualMachineConfig;
    }

    export interface RuntimeVirtualMachineVirtualMachineConfig {
        /**
         * The Compute Engine accelerator configuration for this runtime.
         * Structure is documented below.
         */
        acceleratorConfig?: outputs.notebooks.RuntimeVirtualMachineVirtualMachineConfigAcceleratorConfig;
        /**
         * Use a list of container images to start the notebook instance.
         * Structure is documented below.
         */
        containerImages: outputs.notebooks.RuntimeVirtualMachineVirtualMachineConfigContainerImage[];
        /**
         * Data disk option configuration settings.
         * Structure is documented below.
         */
        dataDisk: outputs.notebooks.RuntimeVirtualMachineVirtualMachineConfigDataDisk;
        /**
         * Encryption settings for virtual machine data disk.
         * Structure is documented below.
         */
        encryptionConfig?: outputs.notebooks.RuntimeVirtualMachineVirtualMachineConfigEncryptionConfig;
        /**
         * (Output)
         * The Compute Engine guest attributes. (see [Project and instance
         * guest attributes](https://cloud.google.com/compute/docs/
         * storing-retrieving-metadata#guest_attributes)).
         */
        guestAttributes: {[key: string]: string};
        /**
         * If true, runtime will only have internal IP addresses. By default,
         * runtimes are not restricted to internal IP addresses, and will
         * have ephemeral external IP addresses assigned to each vm. This
         * `internalIpOnly` restriction can only be enabled for subnetwork
         * enabled networks, and all dependencies must be configured to be
         * accessible without external IP addresses.
         */
        internalIpOnly?: boolean;
        /**
         * The labels to associate with this runtime. Label **keys** must
         * contain 1 to 63 characters, and must conform to [RFC 1035]
         * (https://www.ietf.org/rfc/rfc1035.txt). Label **values** may be
         * empty, but, if present, must contain 1 to 63 characters, and must
         * conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). No
         * more than 32 labels can be associated with a cluster.
         */
        labels: {[key: string]: string};
        /**
         * The Compute Engine machine type used for runtimes.
         */
        machineType: string;
        /**
         * The Compute Engine metadata entries to add to virtual machine.
         * (see [Project and instance metadata](https://cloud.google.com
         * /compute/docs/storing-retrieving-metadata#project_and_instance
         * _metadata)).
         */
        metadata: {[key: string]: string};
        /**
         * The Compute Engine network to be used for machine communications.
         * Cannot be specified with subnetwork. If neither `network` nor
         * `subnet` is specified, the "default" network of the project is
         * used, if it exists. A full URL or partial URI. Examples:
         * * `https://www.googleapis.com/compute/v1/projects/[projectId]/
         * regions/global/default`
         * * `projects/[projectId]/regions/global/default`
         * Runtimes are managed resources inside Google Infrastructure.
         * Runtimes support the following network configurations:
         * * Google Managed Network (Network & subnet are empty)
         * * Consumer Project VPC (network & subnet are required). Requires
         * configuring Private Service Access.
         * * Shared VPC (network & subnet are required). Requires
         * configuring Private Service Access.
         */
        network?: string;
        /**
         * The type of vNIC to be used on this interface. This may be gVNIC
         * or VirtioNet.
         * Possible values are: `UNSPECIFIED_NIC_TYPE`, `VIRTIO_NET`, `GVNIC`.
         */
        nicType?: string;
        /**
         * Reserved IP Range name is used for VPC Peering. The
         * subnetwork allocation will use the range *name* if it's assigned.
         */
        reservedIpRange?: string;
        /**
         * Shielded VM Instance configuration settings.
         * Structure is documented below.
         */
        shieldedInstanceConfig?: outputs.notebooks.RuntimeVirtualMachineVirtualMachineConfigShieldedInstanceConfig;
        /**
         * The Compute Engine subnetwork to be used for machine
         * communications. Cannot be specified with network. A full URL or
         * partial URI are valid. Examples:
         * * `https://www.googleapis.com/compute/v1/projects/[projectId]/
         * regions/us-east1/subnetworks/sub0`
         * * `projects/[projectId]/regions/us-east1/subnetworks/sub0`
         */
        subnet?: string;
        /**
         * The Compute Engine tags to add to runtime (see [Tagging instances]
         * (https://cloud.google.com/compute/docs/
         * label-or-tag-resources#tags)).
         */
        tags: string[];
        /**
         * (Output)
         * The zone where the virtual machine is located.
         */
        zone: string;
    }

    export interface RuntimeVirtualMachineVirtualMachineConfigAcceleratorConfig {
        /**
         * Count of cores of this accelerator.
         */
        coreCount?: number;
        /**
         * Accelerator model. For valid values, see
         * `https://cloud.google.com/vertex-ai/docs/workbench/reference/
         * rest/v1/projects.locations.runtimes#AcceleratorType`
         */
        type?: string;
    }

    export interface RuntimeVirtualMachineVirtualMachineConfigContainerImage {
        /**
         * The path to the container image repository.
         * For example: gcr.io/{project_id}/{imageName}
         */
        repository: string;
        /**
         * The tag of the container image. If not specified, this defaults to the latest tag.
         */
        tag?: string;
    }

    export interface RuntimeVirtualMachineVirtualMachineConfigDataDisk {
        /**
         * (Output)
         * Optional. Specifies whether the disk will be auto-deleted
         * when the instance is deleted (but not when the disk is
         * detached from the instance).
         */
        autoDelete: boolean;
        /**
         * (Output)
         * Optional. Indicates that this is a boot disk. The virtual
         * machine will use the first partition of the disk for its
         * root filesystem.
         */
        boot: boolean;
        /**
         * (Output)
         * Optional. Specifies a unique device name of your choice
         * that is reflected into the /dev/disk/by-id/google-* tree
         * of a Linux operating system running within the instance.
         * This name can be used to reference the device for mounting,
         * resizing, and so on, from within the instance.
         * If not specified, the server chooses a default device name
         * to apply to this disk, in the form persistent-disk-x, where
         * x is a number assigned by Google Compute Engine. This field
         * is only applicable for persistent disks.
         */
        deviceName: string;
        /**
         * (Output)
         * Indicates a list of features to enable on the guest operating
         * system. Applicable only for bootable images. To see a list of
         * available features, read `https://cloud.google.com/compute/docs/
         * images/create-delete-deprecate-private-images#guest-os-features`
         * options. ``
         */
        guestOsFeatures: string[];
        /**
         * (Output)
         * Output only. A zero-based index to this disk, where 0 is
         * reserved for the boot disk. If you have many disks attached
         * to an instance, each disk would have a unique index number.
         */
        index: number;
        /**
         * Input only. Specifies the parameters for a new disk that will
         * be created alongside the new instance. Use initialization
         * parameters to create boot disks or local SSDs attached to the
         * new instance. This property is mutually exclusive with the
         * source property; you can only define one or the other, but not
         * both.
         * Structure is documented below.
         */
        initializeParams?: outputs.notebooks.RuntimeVirtualMachineVirtualMachineConfigDataDiskInitializeParams;
        /**
         * "Specifies the disk interface to use for attaching this disk,
         * which is either SCSI or NVME. The default is SCSI. Persistent
         * disks must always use SCSI and the request will fail if you attempt
         * to attach a persistent disk in any other format than SCSI. Local SSDs
         * can use either NVME or SCSI. For performance characteristics of SCSI
         * over NVMe, see Local SSD performance. Valid values: * NVME * SCSI".
         */
        interface?: string;
        /**
         * (Output)
         * Type of the resource. Always compute#attachedDisk for attached
         * disks.
         */
        kind: string;
        /**
         * (Output)
         * Output only. Any valid publicly visible licenses.
         */
        licenses: string[];
        /**
         * The mode in which to attach this disk, either READ_WRITE
         * or READ_ONLY. If not specified, the default is to attach
         * the disk in READ_WRITE mode.
         */
        mode?: string;
        /**
         * Specifies a valid partial or full URL to an existing
         * Persistent Disk resource.
         */
        source?: string;
        /**
         * Specifies the type of the disk, either SCRATCH or PERSISTENT.
         * If not specified, the default is PERSISTENT.
         */
        type?: string;
    }

    export interface RuntimeVirtualMachineVirtualMachineConfigDataDiskInitializeParams {
        /**
         * Provide this property when creating the disk.
         */
        description?: string;
        /**
         * Specifies the disk name. If not specified, the default is
         * to use the name of the instance. If the disk with the
         * instance name exists already in the given zone/region, a
         * new name will be automatically generated.
         */
        diskName?: string;
        /**
         * Specifies the size of the disk in base-2 GB. If not
         * specified, the disk will be the same size as the image
         * (usually 10GB). If specified, the size must be equal to
         * or larger than 10GB. Default 100 GB.
         */
        diskSizeGb?: number;
        /**
         * The type of the boot disk attached to this runtime,
         * defaults to standard persistent disk. For valid values,
         * see `https://cloud.google.com/vertex-ai/docs/workbench/
         * reference/rest/v1/projects.locations.runtimes#disktype`
         */
        diskType?: string;
        /**
         * Labels to apply to this disk. These can be later modified
         * by the disks.setLabels method. This field is only
         * applicable for persistent disks.
         */
        labels: {[key: string]: string};
    }

    export interface RuntimeVirtualMachineVirtualMachineConfigEncryptionConfig {
        /**
         * The Cloud KMS resource identifier of the customer-managed
         * encryption key used to protect a resource, such as a disks.
         * It has the following format:
         * `projects/{PROJECT_ID}/locations/{REGION}/keyRings/
         * {KEY_RING_NAME}/cryptoKeys/{KEY_NAME}`
         */
        kmsKey?: string;
    }

    export interface RuntimeVirtualMachineVirtualMachineConfigShieldedInstanceConfig {
        /**
         * Defines whether the instance has integrity monitoring enabled.
         * Enables monitoring and attestation of the boot integrity of
         * the instance. The attestation is performed against the
         * integrity policy baseline. This baseline is initially derived
         * from the implicitly trusted boot image when the instance is
         * created. Enabled by default.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Defines whether the instance has Secure Boot enabled.Secure
         * Boot helps ensure that the system only runs authentic software
         * by verifying the digital signature of all boot components, and
         * halting the boot process if signature verification fails.
         * Disabled by default.
         */
        enableSecureBoot?: boolean;
        /**
         * Defines whether the instance has the vTPM enabled. Enabled by
         * default.
         */
        enableVtpm?: boolean;
    }

}

export namespace organizations {
    export interface AccessApprovalSettingsEnrolledService {
        /**
         * The product for which Access Approval will be enrolled. Allowed values are listed (case-sensitive):
         * all
         * appengine.googleapis.com
         * bigquery.googleapis.com
         * bigtable.googleapis.com
         * cloudkms.googleapis.com
         * compute.googleapis.com
         * dataflow.googleapis.com
         * iam.googleapis.com
         * pubsub.googleapis.com
         * storage.googleapis.com
         */
        cloudProduct: string;
        /**
         * The enrollment level of the service.
         * Default value is `BLOCK_ALL`.
         * Possible values are: `BLOCK_ALL`.
         *
         * - - -
         */
        enrollmentLevel?: string;
    }

    export interface GetFoldersFolder {
        /**
         * The timestamp of when the folder was created
         */
        createTime: string;
        /**
         * The timestamp of when the folder was requested to be deleted (if applicable)
         */
        deleteTime: string;
        /**
         * The display name of the folder
         */
        displayName: string;
        /**
         * Entity tag identifier of the folder
         */
        etag: string;
        /**
         * The id of the folder
         */
        name: string;
        /**
         * The parent id of the folder
         */
        parent: string;
        /**
         * The lifecycle state of the folder
         */
        state: string;
        /**
         * The timestamp of when the folder was last modified
         */
        updateTime: string;
    }

    export interface GetIAMPolicyAuditConfig {
        /**
         * A nested block that defines the operations you'd like to log.
         */
        auditLogConfigs: outputs.organizations.GetIAMPolicyAuditConfigAuditLogConfig[];
        /**
         * Defines a service that will be enabled for audit logging. For example, `storage.googleapis.com`, `cloudsql.googleapis.com`. `allServices` is a special value that covers all services.
         */
        service: string;
    }

    export interface GetIAMPolicyAuditConfigAuditLogConfig {
        /**
         * Specifies the identities that are exempt from these types of logging operations. Follows the same format of the `members` array for `binding`.
         */
        exemptedMembers?: string[];
        /**
         * Defines the logging level. `DATA_READ`, `DATA_WRITE` and `ADMIN_READ` capture different types of events. See [the audit configuration documentation](https://cloud.google.com/resource-manager/reference/rest/Shared.Types/AuditConfig) for more details.
         */
        logType: string;
    }

    export interface GetIAMPolicyBinding {
        /**
         * An [IAM Condition](https://cloud.google.com/iam/docs/conditions-overview) for a given binding. Structure is documented below.
         */
        condition?: outputs.organizations.GetIAMPolicyBindingCondition;
        /**
         * An array of identities that will be granted the privilege in the `role`. For more details on format and restrictions see https://cloud.google.com/billing/reference/rest/v1/Policy#Binding
         * Each entry can have one of the following values:
         * * **allUsers**: A special identifier that represents anyone who is on the internet; with or without a Google account. Some resources **don't** support this identity.
         * * **allAuthenticatedUsers**: A special identifier that represents anyone who is authenticated with a Google account or a service account. Some resources **don't** support this identity.
         * * **user:{emailid}**: An email address that represents a specific Google account. For example, alice@gmail.com.
         * * **serviceAccount:{emailid}**: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com.
         * * **group:{emailid}**: An email address that represents a Google group. For example, admins@example.com.
         * * **domain:{domain}**: A G Suite domain (primary, instead of alias) name that represents all the users of that domain. For example, google.com or example.com.
         */
        members: string[];
        /**
         * The role/permission that will be granted to the members.
         * See the [IAM Roles](https://cloud.google.com/compute/docs/access/iam) documentation for a complete list of roles.
         * Note that custom roles must be of the format `[projects|organizations]/{parent-name}/roles/{role-name}`.
         */
        role: string;
    }

    export interface GetIAMPolicyBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface IAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface IAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface IamAuditConfigAuditLogConfig {
        /**
         * Identities that do not cause logging for this type of permission.
         * Each entry can have one of the following values:
         * * **user:{emailid}**: An email address that represents a specific Google account. For example, alice@gmail.com or joe@example.com.
         * * **serviceAccount:{emailid}**: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com.
         * * **group:{emailid}**: An email address that represents a Google group. For example, admins@example.com.
         * * **domain:{domain}**: A G Suite domain (primary, instead of alias) name that represents all the users of that domain. For example, google.com or example.com.
         */
        exemptedMembers?: string[];
        /**
         * Permission type for which logging is to be configured.  Must be one of `DATA_READ`, `DATA_WRITE`, or `ADMIN_READ`.
         */
        logType: string;
    }

    export interface PolicyBooleanPolicy {
        /**
         * If true, then the Policy is enforced. If false, then any configuration is acceptable.
         */
        enforced: boolean;
    }

    export interface PolicyListPolicy {
        /**
         * or `deny` - (Optional) One or the other must be set.
         */
        allow?: outputs.organizations.PolicyListPolicyAllow;
        deny?: outputs.organizations.PolicyListPolicyDeny;
        /**
         * If set to true, the values from the effective Policy of the parent resource
         * are inherited, meaning the values set in this Policy are added to the values inherited up the hierarchy.
         *
         * The `allow` or `deny` blocks support:
         */
        inheritFromParent?: boolean;
        /**
         * The Google Cloud Console will try to default to a configuration that matches the value specified in this field.
         */
        suggestedValue: string;
    }

    export interface PolicyListPolicyAllow {
        /**
         * The policy allows or denies all values.
         */
        all?: boolean;
        /**
         * The policy can define specific values that are allowed or denied.
         */
        values?: string[];
    }

    export interface PolicyListPolicyDeny {
        /**
         * The policy allows or denies all values.
         */
        all?: boolean;
        /**
         * The policy can define specific values that are allowed or denied.
         */
        values?: string[];
    }

    export interface PolicyRestorePolicy {
        /**
         * May only be set to true. If set, then the default Policy is restored.
         */
        default: boolean;
    }

}

export namespace orgpolicy {
    export interface PolicySpec {
        /**
         * An opaque tag indicating the current version of the `Policy`, used for concurrency control. This field is ignored if used in a `CreatePolicy` request. When the `Policy` is returned from either a `GetPolicy` or a `ListPolicies` request, this `etag` indicates the version of the current `Policy` to use when executing a read-modify-write loop. When the `Policy` is returned from a `GetEffectivePolicy` request, the `etag` will be unset.
         */
        etag: string;
        /**
         * Determines the inheritance behavior for this `Policy`. If `inheritFromParent` is true, PolicyRules set higher up in the hierarchy (up to the closest root) are inherited and present in the effective policy. If it is false, then no rules are inherited, and this Policy becomes the new root for evaluation. This field can be set only for Policies which configure list constraints.
         */
        inheritFromParent?: boolean;
        /**
         * Ignores policies set above this resource and restores the `constraintDefault` enforcement behavior of the specific `Constraint` at this resource. This field can be set in policies for either list or boolean constraints. If set, `rules` must be empty and `inheritFromParent` must be set to false.
         */
        reset?: boolean;
        /**
         * Up to 10 PolicyRules are allowed. In Policies for boolean constraints, the following requirements apply: - There must be one and only one PolicyRule where condition is unset. - BooleanPolicyRules with conditions must set `enforced` to the opposite of the PolicyRule without a condition. - During policy evaluation, PolicyRules with conditions that are true for a target resource take precedence.
         */
        rules?: outputs.orgpolicy.PolicySpecRule[];
        /**
         * Output only. The time stamp this was previously updated. This represents the last time a call to `CreatePolicy` or `UpdatePolicy` was made for that `Policy`.
         */
        updateTime: string;
    }

    export interface PolicySpecRule {
        /**
         * Setting this to true means that all values are allowed. This field can be set only in Policies for list constraints.
         */
        allowAll?: string;
        /**
         * A condition which determines whether this rule is used in the evaluation of the policy. When set, the `expression` field in the `Expr' must include from 1 to 10 subexpressions, joined by the "||" or "&&" operators. Each subexpression must be of the form "resource.matchTag('/tag_key_short_name, 'tag_value_short_name')". or "resource.matchTagId('tagKeys/key_id', 'tagValues/value_id')". where keyName and valueName are the resource names for Label Keys and Values. These names are available from the Tag Manager Service. An example expression is: "resource.matchTag('123456789/environment, 'prod')". or "resource.matchTagId('tagKeys/123', 'tagValues/456')".
         */
        condition?: outputs.orgpolicy.PolicySpecRuleCondition;
        /**
         * Setting this to true means that all values are denied. This field can be set only in Policies for list constraints.
         */
        denyAll?: string;
        /**
         * If `true`, then the `Policy` is enforced. If `false`, then any configuration is acceptable. This field can be set only in Policies for boolean constraints.
         */
        enforce?: string;
        /**
         * List of values to be used for this PolicyRule. This field can be set only in Policies for list constraints.
         */
        values?: outputs.orgpolicy.PolicySpecRuleValues;
    }

    export interface PolicySpecRuleCondition {
        /**
         * Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression?: string;
        /**
         * Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
         */
        location?: string;
        /**
         * Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface PolicySpecRuleValues {
        /**
         * List of values allowed at this resource.
         */
        allowedValues?: string[];
        /**
         * List of values denied at this resource.
         */
        deniedValues?: string[];
    }

}

export namespace osconfig {
    export interface GuestPoliciesAssignment {
        /**
         * Targets instances matching at least one of these label sets. This allows an assignment to target disparate groups,
         * for example "env=prod or env=staging".
         * Structure is documented below.
         */
        groupLabels?: outputs.osconfig.GuestPoliciesAssignmentGroupLabel[];
        /**
         * Targets VM instances whose name starts with one of these prefixes.
         * Like labels, this is another way to group VM instances when targeting configs,
         * for example prefix="prod-".
         * Only supported for project-level policies.
         */
        instanceNamePrefixes?: string[];
        /**
         * Targets any of the instances specified. Instances are specified by their URI in the form
         * zones/[ZONE]/instances/[INSTANCE_NAME].
         * Instance targeting is uncommon and is supported to facilitate the management of changes
         * by the instance or to target specific VM instances for development and testing.
         * Only supported for project-level policies and must reference instances within this project.
         */
        instances?: string[];
        /**
         * Targets VM instances matching at least one of the following OS types.
         * VM instances must match all supplied criteria for a given OsType to be included.
         * Structure is documented below.
         */
        osTypes?: outputs.osconfig.GuestPoliciesAssignmentOsType[];
        /**
         * Targets instances in any of these zones. Leave empty to target instances in any zone.
         * Zonal targeting is uncommon and is supported to facilitate the management of changes by zone.
         */
        zones?: string[];
    }

    export interface GuestPoliciesAssignmentGroupLabel {
        /**
         * Google Compute Engine instance labels that must be present for an instance to be included in this assignment group.
         */
        labels: {[key: string]: string};
    }

    export interface GuestPoliciesAssignmentOsType {
        /**
         * Targets VM instances with OS Inventory enabled and having the following OS architecture.
         *
         * - - -
         */
        osArchitecture?: string;
        /**
         * Targets VM instances with OS Inventory enabled and having the following OS short name, for example "debian" or "windows".
         */
        osShortName?: string;
        /**
         * Targets VM instances with OS Inventory enabled and having the following following OS version.
         */
        osVersion?: string;
    }

    export interface GuestPoliciesPackage {
        /**
         * The desiredState the agent should maintain for this package. The default is to ensure the package is installed.
         * Possible values are: `INSTALLED`, `UPDATED`, `REMOVED`.
         */
        desiredState?: string;
        /**
         * Type of package manager that can be used to install this package. If a system does not have the package manager,
         * the package is not installed or removed no error message is returned. By default, or if you specify ANY,
         * the agent attempts to install and remove this package using the default package manager.
         * This is useful when creating a policy that applies to different types of systems.
         * The default behavior is ANY.
         * Default value is `ANY`.
         * Possible values are: `ANY`, `APT`, `YUM`, `ZYPPER`, `GOO`.
         */
        manager?: string;
        /**
         * The name of the package. A package is uniquely identified for conflict validation
         * by checking the package name and the manager(s) that the package targets.
         */
        name: string;
    }

    export interface GuestPoliciesPackageRepository {
        /**
         * An Apt Repository.
         * Structure is documented below.
         */
        apt?: outputs.osconfig.GuestPoliciesPackageRepositoryApt;
        /**
         * A Goo Repository.
         * Structure is documented below.
         */
        goo?: outputs.osconfig.GuestPoliciesPackageRepositoryGoo;
        /**
         * A Yum Repository.
         * Structure is documented below.
         */
        yum?: outputs.osconfig.GuestPoliciesPackageRepositoryYum;
        /**
         * A Zypper Repository.
         * Structure is documented below.
         */
        zypper?: outputs.osconfig.GuestPoliciesPackageRepositoryZypper;
    }

    export interface GuestPoliciesPackageRepositoryApt {
        /**
         * Type of archive files in this repository. The default behavior is DEB.
         * Default value is `DEB`.
         * Possible values are: `DEB`, `DEB_SRC`.
         */
        archiveType?: string;
        /**
         * List of components for this repository. Must contain at least one item.
         */
        components: string[];
        /**
         * Distribution of this repository.
         */
        distribution: string;
        /**
         * URI of the key file for this repository. The agent maintains a keyring at
         * /etc/apt/trusted.gpg.d/osconfig_agent_managed.gpg containing all the keys in any applied guest policy.
         */
        gpgKey?: string;
        /**
         * URI for this repository.
         */
        uri: string;
    }

    export interface GuestPoliciesPackageRepositoryGoo {
        /**
         * The name of the repository.
         */
        name: string;
        /**
         * The url of the repository.
         */
        url: string;
    }

    export interface GuestPoliciesPackageRepositoryYum {
        /**
         * The location of the repository directory.
         */
        baseUrl: string;
        /**
         * The display name of the repository.
         */
        displayName?: string;
        /**
         * URIs of GPG keys.
         */
        gpgKeys?: string[];
        /**
         * A one word, unique name for this repository. This is the repo id in the Yum config file and also the displayName
         * if displayName is omitted. This id is also used as the unique identifier when checking for guest policy conflicts.
         */
        id: string;
    }

    export interface GuestPoliciesPackageRepositoryZypper {
        /**
         * The location of the repository directory.
         */
        baseUrl: string;
        /**
         * The display name of the repository.
         */
        displayName?: string;
        /**
         * URIs of GPG keys.
         */
        gpgKeys?: string[];
        /**
         * A one word, unique name for this repository. This is the repo id in the zypper config file and also the displayName
         * if displayName is omitted. This id is also used as the unique identifier when checking for guest policy conflicts.
         */
        id: string;
    }

    export interface GuestPoliciesRecipe {
        /**
         * Resources available to be used in the steps in the recipe.
         * Structure is documented below.
         */
        artifacts?: outputs.osconfig.GuestPoliciesRecipeArtifact[];
        /**
         * Default is INSTALLED. The desired state the agent should maintain for this recipe.
         * INSTALLED: The software recipe is installed on the instance but won't be updated to new versions.
         * INSTALLED_KEEP_UPDATED: The software recipe is installed on the instance. The recipe is updated to a higher version,
         * if a higher version of the recipe is assigned to this instance.
         * REMOVE: Remove is unsupported for software recipes and attempts to create or update a recipe to the REMOVE state is rejected.
         * Default value is `INSTALLED`.
         * Possible values are: `INSTALLED`, `UPDATED`, `REMOVED`.
         */
        desiredState?: string;
        /**
         * Actions to be taken for installing this recipe. On failure it stops executing steps and does not attempt another installation.
         * Any steps taken (including partially completed steps) are not rolled back.
         * Structure is documented below.
         */
        installSteps?: outputs.osconfig.GuestPoliciesRecipeInstallStep[];
        /**
         * Unique identifier for the recipe. Only one recipe with a given name is installed on an instance.
         * Names are also used to identify resources which helps to determine whether guest policies have conflicts.
         * This means that requests to create multiple recipes with the same name and version are rejected since they
         * could potentially have conflicting assignments.
         */
        name: string;
        /**
         * Actions to be taken for updating this recipe. On failure it stops executing steps and does not attempt another update for this recipe.
         * Any steps taken (including partially completed steps) are not rolled back.
         * Structure is documented below.
         */
        updateSteps?: outputs.osconfig.GuestPoliciesRecipeUpdateStep[];
        /**
         * The version of this software recipe. Version can be up to 4 period separated numbers (e.g. 12.34.56.78).
         */
        version?: string;
    }

    export interface GuestPoliciesRecipeArtifact {
        /**
         * Defaults to false. When false, recipes are subject to validations based on the artifact type:
         * Remote: A checksum must be specified, and only protocols with transport-layer security are permitted.
         * GCS: An object generation number must be specified.
         */
        allowInsecure?: boolean;
        /**
         * A Google Cloud Storage artifact.
         * Structure is documented below.
         */
        gcs?: outputs.osconfig.GuestPoliciesRecipeArtifactGcs;
        /**
         * Id of the artifact, which the installation and update steps of this recipe can reference.
         * Artifacts in a recipe cannot have the same id.
         */
        id: string;
        /**
         * A generic remote artifact.
         * Structure is documented below.
         */
        remote?: outputs.osconfig.GuestPoliciesRecipeArtifactRemote;
    }

    export interface GuestPoliciesRecipeArtifactGcs {
        /**
         * Bucket of the Google Cloud Storage object. Given an example URL: https://storage.googleapis.com/my-bucket/foo/bar#1234567
         * this value would be my-bucket.
         */
        bucket?: string;
        /**
         * Must be provided if allowInsecure is false. Generation number of the Google Cloud Storage object.
         * https://storage.googleapis.com/my-bucket/foo/bar#1234567 this value would be 1234567.
         */
        generation?: number;
        /**
         * Name of the Google Cloud Storage object. Given an example URL: https://storage.googleapis.com/my-bucket/foo/bar#1234567
         * this value would be foo/bar.
         */
        object?: string;
    }

    export interface GuestPoliciesRecipeArtifactRemote {
        /**
         * Must be provided if allowInsecure is false. SHA256 checksum in hex format, to compare to the checksum of the artifact.
         * If the checksum is not empty and it doesn't match the artifact then the recipe installation fails before running any
         * of the steps.
         */
        checkSum?: string;
        /**
         * URI from which to fetch the object. It should contain both the protocol and path following the format {protocol}://{location}.
         */
        uri?: string;
    }

    export interface GuestPoliciesRecipeInstallStep {
        /**
         * Extracts an archive into the specified directory.
         * Structure is documented below.
         */
        archiveExtraction?: outputs.osconfig.GuestPoliciesRecipeInstallStepArchiveExtraction;
        /**
         * Installs a deb file via dpkg.
         * Structure is documented below.
         */
        dpkgInstallation?: outputs.osconfig.GuestPoliciesRecipeInstallStepDpkgInstallation;
        /**
         * Copies a file onto the instance.
         * Structure is documented below.
         */
        fileCopy?: outputs.osconfig.GuestPoliciesRecipeInstallStepFileCopy;
        /**
         * Executes an artifact or local file.
         * Structure is documented below.
         */
        fileExec?: outputs.osconfig.GuestPoliciesRecipeInstallStepFileExec;
        /**
         * Installs an MSI file.
         * Structure is documented below.
         */
        msiInstallation?: outputs.osconfig.GuestPoliciesRecipeInstallStepMsiInstallation;
        /**
         * Installs an rpm file via the rpm utility.
         * Structure is documented below.
         */
        rpmInstallation?: outputs.osconfig.GuestPoliciesRecipeInstallStepRpmInstallation;
        /**
         * Runs commands in a shell.
         * Structure is documented below.
         */
        scriptRun?: outputs.osconfig.GuestPoliciesRecipeInstallStepScriptRun;
    }

    export interface GuestPoliciesRecipeInstallStepArchiveExtraction {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
        /**
         * Directory to extract archive to. Defaults to / on Linux or C:\ on Windows.
         */
        destination: string;
        /**
         * The type of the archive to extract.
         * Possible values are: `TAR`, `TAR_GZIP`, `TAR_BZIP`, `TAR_LZMA`, `TAR_XZ`, `ZIP`.
         */
        type: string;
    }

    export interface GuestPoliciesRecipeInstallStepDpkgInstallation {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
    }

    export interface GuestPoliciesRecipeInstallStepFileCopy {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
        /**
         * The absolute path on the instance to put the file.
         */
        destination: string;
        /**
         * Whether to allow this step to overwrite existing files.If this is false and the file already exists the file
         * is not overwritten and the step is considered a success. Defaults to false.
         */
        overwrite?: boolean;
        /**
         * Consists of three octal digits which represent, in order, the permissions of the owner, group, and other users
         * for the file (similarly to the numeric mode used in the linux chmod utility). Each digit represents a three bit
         * number with the 4 bit corresponding to the read permissions, the 2 bit corresponds to the write bit, and the one
         * bit corresponds to the execute permission. Default behavior is 755.
         * Below are some examples of permissions and their associated values:
         * read, write, and execute: 7 read and execute: 5 read and write: 6 read only: 4
         */
        permissions?: string;
    }

    export interface GuestPoliciesRecipeInstallStepFileExec {
        /**
         * A list of possible return values that the program can return to indicate a success. Defaults to [0].
         */
        allowedExitCodes: string;
        /**
         * Arguments to be passed to the provided executable.
         */
        args?: string[];
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId?: string;
        /**
         * The absolute path of the file on the local filesystem.
         */
        localPath?: string;
    }

    export interface GuestPoliciesRecipeInstallStepMsiInstallation {
        /**
         * Return codes that indicate that the software installed or updated successfully. Behaviour defaults to [0]
         */
        allowedExitCodes: number[];
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
        /**
         * The flags to use when installing the MSI. Defaults to the install flag.
         */
        flags: string[];
    }

    export interface GuestPoliciesRecipeInstallStepRpmInstallation {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
    }

    export interface GuestPoliciesRecipeInstallStepScriptRun {
        /**
         * Return codes that indicate that the software installed or updated successfully. Behaviour defaults to [0]
         */
        allowedExitCodes: number[];
        /**
         * The script interpreter to use to run the script. If no interpreter is specified the script is executed directly,
         * which likely only succeed for scripts with shebang lines.
         * Possible values are: `SHELL`, `POWERSHELL`.
         */
        interpreter?: string;
        /**
         * The shell script to be executed.
         */
        script: string;
    }

    export interface GuestPoliciesRecipeUpdateStep {
        /**
         * Extracts an archive into the specified directory.
         * Structure is documented below.
         */
        archiveExtraction?: outputs.osconfig.GuestPoliciesRecipeUpdateStepArchiveExtraction;
        /**
         * Installs a deb file via dpkg.
         * Structure is documented below.
         */
        dpkgInstallation?: outputs.osconfig.GuestPoliciesRecipeUpdateStepDpkgInstallation;
        /**
         * Copies a file onto the instance.
         * Structure is documented below.
         */
        fileCopy?: outputs.osconfig.GuestPoliciesRecipeUpdateStepFileCopy;
        /**
         * Executes an artifact or local file.
         * Structure is documented below.
         */
        fileExec?: outputs.osconfig.GuestPoliciesRecipeUpdateStepFileExec;
        /**
         * Installs an MSI file.
         * Structure is documented below.
         */
        msiInstallation?: outputs.osconfig.GuestPoliciesRecipeUpdateStepMsiInstallation;
        /**
         * Installs an rpm file via the rpm utility.
         * Structure is documented below.
         */
        rpmInstallation?: outputs.osconfig.GuestPoliciesRecipeUpdateStepRpmInstallation;
        /**
         * Runs commands in a shell.
         * Structure is documented below.
         */
        scriptRun?: outputs.osconfig.GuestPoliciesRecipeUpdateStepScriptRun;
    }

    export interface GuestPoliciesRecipeUpdateStepArchiveExtraction {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
        /**
         * Directory to extract archive to. Defaults to / on Linux or C:\ on Windows.
         */
        destination: string;
        /**
         * The type of the archive to extract.
         * Possible values are: `TAR`, `TAR_GZIP`, `TAR_BZIP`, `TAR_LZMA`, `TAR_XZ`, `ZIP`.
         */
        type: string;
    }

    export interface GuestPoliciesRecipeUpdateStepDpkgInstallation {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
    }

    export interface GuestPoliciesRecipeUpdateStepFileCopy {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
        /**
         * The absolute path on the instance to put the file.
         */
        destination: string;
        /**
         * Whether to allow this step to overwrite existing files.If this is false and the file already exists the file
         * is not overwritten and the step is considered a success. Defaults to false.
         */
        overwrite?: boolean;
        /**
         * Consists of three octal digits which represent, in order, the permissions of the owner, group, and other users
         * for the file (similarly to the numeric mode used in the linux chmod utility). Each digit represents a three bit
         * number with the 4 bit corresponding to the read permissions, the 2 bit corresponds to the write bit, and the one
         * bit corresponds to the execute permission. Default behavior is 755.
         * Below are some examples of permissions and their associated values:
         * read, write, and execute: 7 read and execute: 5 read and write: 6 read only: 4
         */
        permissions?: string;
    }

    export interface GuestPoliciesRecipeUpdateStepFileExec {
        /**
         * A list of possible return values that the program can return to indicate a success. Defaults to [0].
         */
        allowedExitCodes: number[];
        /**
         * Arguments to be passed to the provided executable.
         */
        args?: string[];
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId?: string;
        /**
         * The absolute path of the file on the local filesystem.
         */
        localPath?: string;
    }

    export interface GuestPoliciesRecipeUpdateStepMsiInstallation {
        /**
         * Return codes that indicate that the software installed or updated successfully. Behaviour defaults to [0]
         */
        allowedExitCodes: number[];
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
        /**
         * The flags to use when installing the MSI. Defaults to the install flag.
         */
        flags: string[];
    }

    export interface GuestPoliciesRecipeUpdateStepRpmInstallation {
        /**
         * The id of the relevant artifact in the recipe.
         */
        artifactId: string;
    }

    export interface GuestPoliciesRecipeUpdateStepScriptRun {
        /**
         * Return codes that indicate that the software installed or updated successfully. Behaviour defaults to [0]
         */
        allowedExitCodes: number[];
        /**
         * The script interpreter to use to run the script. If no interpreter is specified the script is executed directly,
         * which likely only succeed for scripts with shebang lines.
         * Possible values are: `SHELL`, `POWERSHELL`.
         */
        interpreter?: string;
        /**
         * The shell script to be executed.
         */
        script: string;
    }

    export interface OsPolicyAssignmentInstanceFilter {
        /**
         * Target all VMs in the project. If true, no other criteria
         * is permitted.
         */
        all?: boolean;
        /**
         * List of label sets used for VM exclusion. If
         * the list has more than one label set, the VM is excluded if any of the label
         * sets are applicable for the VM. Structure is
         * documented below.
         */
        exclusionLabels?: outputs.osconfig.OsPolicyAssignmentInstanceFilterExclusionLabel[];
        /**
         * List of label sets used for VM inclusion. If
         * the list has more than one `LabelSet`, the VM is included if any of the
         * label sets are applicable for the VM. Structure is
         * documented below.
         */
        inclusionLabels?: outputs.osconfig.OsPolicyAssignmentInstanceFilterInclusionLabel[];
        /**
         * List of inventories to select VMs. A VM is
         * selected if its inventory data matches at least one of the following
         * inventories. Structure is documented below.
         */
        inventories?: outputs.osconfig.OsPolicyAssignmentInstanceFilterInventory[];
    }

    export interface OsPolicyAssignmentInstanceFilterExclusionLabel {
        /**
         * Labels are identified by key/value pairs in this map.
         * A VM should contain all the key/value pairs specified in this map to be
         * selected.
         */
        labels?: {[key: string]: string};
    }

    export interface OsPolicyAssignmentInstanceFilterInclusionLabel {
        /**
         * Labels are identified by key/value pairs in this map.
         * A VM should contain all the key/value pairs specified in this map to be
         * selected.
         */
        labels?: {[key: string]: string};
    }

    export interface OsPolicyAssignmentInstanceFilterInventory {
        /**
         * The OS short name
         */
        osShortName: string;
        /**
         * The OS version Prefix matches are supported if
         * asterisk(*) is provided as the last character. For example, to match all
         * versions with a major version of `7`, specify the following value for this
         * field `7.*` An empty string matches all OS versions.
         */
        osVersion?: string;
    }

    export interface OsPolicyAssignmentOsPolicy {
        /**
         * This flag determines the OS
         * policy compliance status when none of the resource groups within the policy
         * are applicable for a VM. Set this value to `true` if the policy needs to be
         * reported as compliant even if the policy has nothing to validate or enforce.
         */
        allowNoResourceGroupMatch?: boolean;
        /**
         * Policy description. Length of the description is
         * limited to 1024 characters.
         */
        description?: string;
        /**
         * The id of the OS policy with the following restrictions:
         *
         * *   Must contain only lowercase letters, numbers, and hyphens.
         * *   Must start with a letter.
         * *   Must be between 1-63 characters.
         * *   Must end with a number or a letter.
         * *   Must be unique within the assignment.
         */
        id: string;
        /**
         * Policy mode Possible values are: `MODE_UNSPECIFIED`,
         * `VALIDATION`, `ENFORCEMENT`.
         */
        mode: string;
        /**
         * List of resource groups for the policy. For a
         * particular VM, resource groups are evaluated in the order specified and the
         * first resource group that is applicable is selected and the rest are
         * ignored. If none of the resource groups are applicable for a VM, the VM is
         * considered to be non-compliant w.r.t this policy. This behavior can be
         * toggled by the flag `allowNoResourceGroupMatch` Structure is
         * documented below.
         */
        resourceGroups: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroup[];
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroup {
        /**
         * List of inventory filters for the resource
         * group. The resources in this resource group are applied to the target VM if
         * it satisfies at least one of the following inventory filters. For example,
         * to apply this resource group to VMs running either `RHEL` or `CentOS`
         * operating systems, specify 2 items for the list with following values:
         * inventory_filters[0].os_short_name='rhel' and
         * inventory_filters[1].os_short_name='centos' If the list is empty, this
         * resource group will be applied to the target VM unconditionally. Structure
         * is documented below.
         */
        inventoryFilters?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupInventoryFilter[];
        /**
         * List of resources configured for this resource
         * group. The resources are executed in the exact order specified here.
         * Structure is documented below.
         */
        resources: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResource[];
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupInventoryFilter {
        /**
         * The OS short name
         */
        osShortName: string;
        /**
         * The OS version Prefix matches are supported if
         * asterisk(*) is provided as the last character. For example, to match all
         * versions with a major version of `7`, specify the following value for this
         * field `7.*` An empty string matches all OS versions.
         */
        osVersion?: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResource {
        /**
         * Exec resource Structure is
         * documented below.
         */
        exec?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExec;
        /**
         * File resource Structure is
         * documented below.
         */
        file?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceFile;
        /**
         * The id of the resource with the following restrictions:
         *
         * *   Must contain only lowercase letters, numbers, and hyphens.
         * *   Must start with a letter.
         * *   Must be between 1-63 characters.
         * *   Must end with a number or a letter.
         * *   Must be unique within the OS policy.
         */
        id: string;
        /**
         * Package resource Structure is
         * documented below.
         */
        pkg?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkg;
        /**
         * Package repository resource Structure is
         * documented below.
         */
        repository?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceRepository;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExec {
        /**
         * What to run to bring this resource into the desired
         * state. An exit code of 100 indicates "success", any other exit code
         * indicates a failure running enforce. Structure is
         * documented below.
         */
        enforce?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforce;
        /**
         * What to run to validate this resource is in the
         * desired state. An exit code of 100 indicates "in desired state", and exit
         * code of 101 indicates "not in desired state". Any other exit code indicates
         * a failure running validate. Structure is
         * documented below.
         */
        validate: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidate;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforce {
        /**
         * Optional arguments to pass to the source during
         * execution.
         */
        args?: string[];
        /**
         * A remote or local file. Structure is
         * documented below.
         */
        file?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFile;
        /**
         * The script interpreter to use. Possible values
         * are: `INTERPRETER_UNSPECIFIED`, `NONE`, `SHELL`, `POWERSHELL`.
         */
        interpreter: string;
        /**
         * Only recorded for enforce Exec. Path to an
         * output file (that is created by this Exec) whose content will be recorded in
         * OSPolicyResourceCompliance after a successful run. Absence or failure to
         * read this file will result in this ExecResource being non-compliant. Output
         * file size is limited to 100K bytes.
         */
        outputFilePath?: string;
        /**
         * An inline script. The size of the script is limited to
         * 1024 characters.
         */
        script?: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFile {
        /**
         * Defaults to false. When false, files are
         * subject to validations based on the file type: Remote: A checksum must be
         * specified. Cloud Storage: An object generation number must be specified.
         */
        allowInsecure?: boolean;
        /**
         * A Cloud Storage object. Structure is
         * documented below.
         */
        gcs?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileGcs;
        /**
         * A local path within the VM to use.
         */
        localPath?: string;
        /**
         * A generic remote file. Structure is
         * documented below.
         */
        remote?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileRemote;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileGcs {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object.
         */
        generation?: number;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileRemote {
        /**
         * SHA256 checksum of the remote file.
         */
        sha256Checksum?: string;
        /**
         * URI from which to fetch the object. It should contain
         * both the protocol and path following the format `{protocol}://{location}`.
         */
        uri: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidate {
        /**
         * Optional arguments to pass to the source during
         * execution.
         */
        args?: string[];
        /**
         * A remote or local file. Structure is
         * documented below.
         */
        file?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFile;
        /**
         * The script interpreter to use. Possible values
         * are: `INTERPRETER_UNSPECIFIED`, `NONE`, `SHELL`, `POWERSHELL`.
         */
        interpreter: string;
        /**
         * Only recorded for enforce Exec. Path to an
         * output file (that is created by this Exec) whose content will be recorded in
         * OSPolicyResourceCompliance after a successful run. Absence or failure to
         * read this file will result in this ExecResource being non-compliant. Output
         * file size is limited to 100K bytes.
         */
        outputFilePath?: string;
        /**
         * An inline script. The size of the script is limited to
         * 1024 characters.
         */
        script?: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFile {
        /**
         * Defaults to false. When false, files are
         * subject to validations based on the file type: Remote: A checksum must be
         * specified. Cloud Storage: An object generation number must be specified.
         */
        allowInsecure?: boolean;
        /**
         * A Cloud Storage object. Structure is
         * documented below.
         */
        gcs?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileGcs;
        /**
         * A local path within the VM to use.
         */
        localPath?: string;
        /**
         * A generic remote file. Structure is
         * documented below.
         */
        remote?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileRemote;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileGcs {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object.
         */
        generation?: number;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileRemote {
        /**
         * SHA256 checksum of the remote file.
         */
        sha256Checksum?: string;
        /**
         * URI from which to fetch the object. It should contain
         * both the protocol and path following the format `{protocol}://{location}`.
         */
        uri: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceFile {
        /**
         * A a file with this content. The size of the content
         * is limited to 1024 characters.
         */
        content?: string;
        /**
         * A remote or local source. Structure is
         * documented below.
         */
        file?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceFileFile;
        /**
         * The absolute path of the file within the VM.
         */
        path: string;
        /**
         * Consists of three octal digits which represent, in
         * order, the permissions of the owner, group, and other users for the file
         * (similarly to the numeric mode used in the linux chmod utility). Each digit
         * represents a three bit number with the 4 bit corresponding to the read
         * permissions, the 2 bit corresponds to the write bit, and the one bit
         * corresponds to the execute permission. Default behavior is 755. Below are
         * some examples of permissions and their associated values: read, write, and
         * execute: 7 read and execute: 5 read and write: 6 read only: 4
         */
        permissions: string;
        /**
         * Desired state of the file. Possible values are:
         * `DESIRED_STATE_UNSPECIFIED`, `PRESENT`, `ABSENT`, `CONTENTS_MATCH`.
         */
        state: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceFileFile {
        /**
         * Defaults to false. When false, files are
         * subject to validations based on the file type: Remote: A checksum must be
         * specified. Cloud Storage: An object generation number must be specified.
         */
        allowInsecure?: boolean;
        /**
         * A Cloud Storage object. Structure is
         * documented below.
         */
        gcs?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceFileFileGcs;
        /**
         * A local path within the VM to use.
         */
        localPath?: string;
        /**
         * A generic remote file. Structure is
         * documented below.
         */
        remote?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceFileFileRemote;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceFileFileGcs {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object.
         */
        generation?: number;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceFileFileRemote {
        /**
         * SHA256 checksum of the remote file.
         */
        sha256Checksum?: string;
        /**
         * URI from which to fetch the object. It should contain
         * both the protocol and path following the format `{protocol}://{location}`.
         */
        uri: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkg {
        /**
         * A package managed by Apt. Structure is
         * documented below.
         */
        apt?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgApt;
        /**
         * A deb package file. Structure is
         * documented below.
         */
        deb?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDeb;
        /**
         * The desired state the agent should maintain for
         * this package. Possible values are: `DESIRED_STATE_UNSPECIFIED`, `INSTALLED`,
         * `REMOVED`.
         */
        desiredState: string;
        /**
         * A package managed by GooGet. Structure is
         * documented below.
         */
        googet?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgGooget;
        /**
         * An MSI package. Structure is
         * documented below.
         */
        msi?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsi;
        /**
         * An rpm package file. Structure is
         * documented below.
         */
        rpm?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpm;
        /**
         * A package managed by YUM. Structure is
         * documented below.
         */
        yum?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgYum;
        /**
         * A package managed by Zypper. Structure is
         * documented below.
         */
        zypper?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgZypper;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgApt {
        /**
         * Package name.
         */
        name: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDeb {
        /**
         * Whether dependencies should also be installed. -
         * install when false: `dpkg -i package` - install when true: `apt-get update
         * && apt-get -y install package.deb`
         */
        pullDeps?: boolean;
        /**
         * A deb package. Structure is
         * documented below.
         */
        source: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSource;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSource {
        /**
         * Defaults to false. When false, files are
         * subject to validations based on the file type: Remote: A checksum must be
         * specified. Cloud Storage: An object generation number must be specified.
         */
        allowInsecure?: boolean;
        /**
         * A Cloud Storage object. Structure is
         * documented below.
         */
        gcs?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceGcs;
        /**
         * A local path within the VM to use.
         */
        localPath?: string;
        /**
         * A generic remote file. Structure is
         * documented below.
         */
        remote?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceRemote;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceGcs {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object.
         */
        generation?: number;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceRemote {
        /**
         * SHA256 checksum of the remote file.
         */
        sha256Checksum?: string;
        /**
         * URI from which to fetch the object. It should contain
         * both the protocol and path following the format `{protocol}://{location}`.
         */
        uri: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgGooget {
        /**
         * Package name.
         */
        name: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsi {
        /**
         * Additional properties to use during installation.
         * This should be in the format of Property=Setting. Appended to the defaults
         * of `ACTION=INSTALL REBOOT=ReallySuppress`.
         */
        properties?: string[];
        /**
         * The MSI package. Structure is
         * documented below.
         */
        source: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSource;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSource {
        /**
         * Defaults to false. When false, files are
         * subject to validations based on the file type: Remote: A checksum must be
         * specified. Cloud Storage: An object generation number must be specified.
         */
        allowInsecure?: boolean;
        /**
         * A Cloud Storage object. Structure is
         * documented below.
         */
        gcs?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceGcs;
        /**
         * A local path within the VM to use.
         */
        localPath?: string;
        /**
         * A generic remote file. Structure is
         * documented below.
         */
        remote?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceRemote;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceGcs {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object.
         */
        generation?: number;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceRemote {
        /**
         * SHA256 checksum of the remote file.
         */
        sha256Checksum?: string;
        /**
         * URI from which to fetch the object. It should contain
         * both the protocol and path following the format `{protocol}://{location}`.
         */
        uri: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpm {
        /**
         * Whether dependencies should also be installed. -
         * install when false: `rpm --upgrade --replacepkgs package.rpm` - install when
         * true: `yum -y install package.rpm` or `zypper -y install package.rpm`
         */
        pullDeps?: boolean;
        /**
         * An rpm package. Structure is
         * documented below.
         */
        source: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSource;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSource {
        /**
         * Defaults to false. When false, files are
         * subject to validations based on the file type: Remote: A checksum must be
         * specified. Cloud Storage: An object generation number must be specified.
         */
        allowInsecure?: boolean;
        /**
         * A Cloud Storage object. Structure is
         * documented below.
         */
        gcs?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceGcs;
        /**
         * A local path within the VM to use.
         */
        localPath?: string;
        /**
         * A generic remote file. Structure is
         * documented below.
         */
        remote?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceRemote;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceGcs {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object.
         */
        generation?: number;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceRemote {
        /**
         * SHA256 checksum of the remote file.
         */
        sha256Checksum?: string;
        /**
         * URI from which to fetch the object. It should contain
         * both the protocol and path following the format `{protocol}://{location}`.
         */
        uri: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgYum {
        /**
         * Package name.
         */
        name: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourcePkgZypper {
        /**
         * Package name.
         */
        name: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceRepository {
        /**
         * An Apt Repository. Structure is
         * documented below.
         */
        apt?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryApt;
        /**
         * A Goo Repository. Structure is
         * documented below.
         */
        goo?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryGoo;
        /**
         * A Yum Repository. Structure is
         * documented below.
         */
        yum?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryYum;
        /**
         * A Zypper Repository. Structure is
         * documented below.
         */
        zypper?: outputs.osconfig.OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryZypper;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryApt {
        /**
         * Type of archive files in this repository.
         * Possible values are: `ARCHIVE_TYPE_UNSPECIFIED`, `DEB`, `DEB_SRC`.
         */
        archiveType: string;
        /**
         * List of components for this repository. Must
         * contain at least one item.
         */
        components: string[];
        /**
         * Distribution of this repository.
         */
        distribution: string;
        /**
         * URI of the key file for this repository. The agent
         * maintains a keyring at `/etc/apt/trusted.gpg.d/osconfig_agent_managed.gpg`.
         */
        gpgKey?: string;
        /**
         * URI for this repository.
         */
        uri: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryGoo {
        /**
         * The name of the repository.
         */
        name: string;
        /**
         * The url of the repository.
         */
        url: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryYum {
        /**
         * The location of the repository directory.
         */
        baseUrl: string;
        /**
         * The display name of the repository.
         */
        displayName?: string;
        /**
         * URIs of GPG keys.
         */
        gpgKeys?: string[];
        /**
         * A one word, unique name for this repository. This is the
         * `repo id` in the yum config file and also the `displayName` if
         * `displayName` is omitted. This id is also used as the unique identifier
         * when checking for resource conflicts.
         */
        id: string;
    }

    export interface OsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryZypper {
        /**
         * The location of the repository directory.
         */
        baseUrl: string;
        /**
         * The display name of the repository.
         */
        displayName?: string;
        /**
         * URIs of GPG keys.
         */
        gpgKeys?: string[];
        /**
         * A one word, unique name for this repository. This is the
         * `repo id` in the zypper config file and also the `displayName` if
         * `displayName` is omitted. This id is also used as the unique identifier
         * when checking for GuestPolicy conflicts.
         */
        id: string;
    }

    export interface OsPolicyAssignmentRollout {
        /**
         * The maximum number (or percentage) of VMs
         * per zone to disrupt at any given moment. Structure is
         * documented below.
         */
        disruptionBudget: outputs.osconfig.OsPolicyAssignmentRolloutDisruptionBudget;
        /**
         * This determines the minimum duration of
         * time to wait after the configuration changes are applied through the current
         * rollout. A VM continues to count towards the `disruptionBudget` at least
         * until this duration of time has passed after configuration changes are
         * applied.
         */
        minWaitDuration: string;
    }

    export interface OsPolicyAssignmentRolloutDisruptionBudget {
        /**
         * Specifies a fixed value.
         */
        fixed?: number;
        /**
         * Specifies the relative value defined as a percentage,
         * which will be multiplied by a reference value.
         *
         * --------------------------------------------------------------------------------
         */
        percent?: number;
    }

    export interface PatchDeploymentInstanceFilter {
        /**
         * Target all VM instances in the project. If true, no other criteria is permitted.
         */
        all?: boolean;
        /**
         * Targets VM instances matching ANY of these GroupLabels. This allows targeting of disparate groups of VM instances.
         * Structure is documented below.
         */
        groupLabels?: outputs.osconfig.PatchDeploymentInstanceFilterGroupLabel[];
        /**
         * Targets VMs whose name starts with one of these prefixes. Similar to labels, this is another way to group
         * VMs when targeting configs, for example prefix="prod-".
         */
        instanceNamePrefixes?: string[];
        /**
         * Targets any of the VM instances specified. Instances are specified by their URI in the `form zones/{{zone}}/instances/{{instance_name}}`,
         * `projects/{{project_id}}/zones/{{zone}}/instances/{{instance_name}}`, or
         * `https://www.googleapis.com/compute/v1/projects/{{project_id}}/zones/{{zone}}/instances/{{instance_name}}`
         */
        instances?: string[];
        /**
         * Targets VM instances in ANY of these zones. Leave empty to target VM instances in any zone.
         */
        zones?: string[];
    }

    export interface PatchDeploymentInstanceFilterGroupLabel {
        /**
         * Compute Engine instance labels that must be present for a VM instance to be targeted by this filter
         *
         * - - -
         */
        labels: {[key: string]: string};
    }

    export interface PatchDeploymentOneTimeSchedule {
        /**
         * The desired patch job execution time. A timestamp in RFC3339 UTC "Zulu" format,
         * accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
         */
        executeTime: string;
    }

    export interface PatchDeploymentPatchConfig {
        /**
         * Apt update settings. Use this setting to override the default apt patch rules.
         * Structure is documented below.
         */
        apt?: outputs.osconfig.PatchDeploymentPatchConfigApt;
        /**
         * goo update settings. Use this setting to override the default goo patch rules.
         * Structure is documented below.
         */
        goo?: outputs.osconfig.PatchDeploymentPatchConfigGoo;
        /**
         * Allows the patch job to run on Managed instance groups (MIGs).
         */
        migInstancesAllowed?: boolean;
        /**
         * The ExecStep to run after the patch update.
         * Structure is documented below.
         */
        postStep?: outputs.osconfig.PatchDeploymentPatchConfigPostStep;
        /**
         * The ExecStep to run before the patch update.
         * Structure is documented below.
         */
        preStep?: outputs.osconfig.PatchDeploymentPatchConfigPreStep;
        /**
         * Post-patch reboot settings.
         * Possible values are: `DEFAULT`, `ALWAYS`, `NEVER`.
         */
        rebootConfig?: string;
        /**
         * Windows update settings. Use this setting to override the default Windows patch rules.
         * Structure is documented below.
         */
        windowsUpdate?: outputs.osconfig.PatchDeploymentPatchConfigWindowsUpdate;
        /**
         * Yum update settings. Use this setting to override the default yum patch rules.
         * Structure is documented below.
         */
        yum?: outputs.osconfig.PatchDeploymentPatchConfigYum;
        /**
         * zypper update settings. Use this setting to override the default zypper patch rules.
         * Structure is documented below.
         */
        zypper?: outputs.osconfig.PatchDeploymentPatchConfigZypper;
    }

    export interface PatchDeploymentPatchConfigApt {
        /**
         * List of packages to exclude from update. These packages will be excluded.
         */
        excludes?: string[];
        /**
         * An exclusive list of packages to be updated. These are the only packages that will be updated.
         * If these packages are not installed, they will be ignored. This field cannot be specified with
         * any other patch configuration fields.
         */
        exclusivePackages?: string[];
        /**
         * By changing the type to DIST, the patching is performed using apt-get dist-upgrade instead.
         * Possible values are: `DIST`, `UPGRADE`.
         */
        type?: string;
    }

    export interface PatchDeploymentPatchConfigGoo {
        /**
         * goo update settings. Use this setting to override the default goo patch rules.
         */
        enabled: boolean;
    }

    export interface PatchDeploymentPatchConfigPostStep {
        /**
         * The ExecStepConfig for all Linux VMs targeted by the PatchJob.
         * Structure is documented below.
         */
        linuxExecStepConfig?: outputs.osconfig.PatchDeploymentPatchConfigPostStepLinuxExecStepConfig;
        /**
         * The ExecStepConfig for all Windows VMs targeted by the PatchJob.
         * Structure is documented below.
         */
        windowsExecStepConfig?: outputs.osconfig.PatchDeploymentPatchConfigPostStepWindowsExecStepConfig;
    }

    export interface PatchDeploymentPatchConfigPostStepLinuxExecStepConfig {
        /**
         * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
         */
        allowedSuccessCodes?: number[];
        /**
         * A Cloud Storage object containing the executable.
         * Structure is documented below.
         */
        gcsObject?: outputs.osconfig.PatchDeploymentPatchConfigPostStepLinuxExecStepConfigGcsObject;
        /**
         * The script interpreter to use to run the script. If no interpreter is specified the script will
         * be executed directly, which will likely only succeed for scripts with shebang lines.
         * Possible values are: `SHELL`, `POWERSHELL`.
         */
        interpreter?: string;
        /**
         * An absolute path to the executable on the VM.
         */
        localPath?: string;
    }

    export interface PatchDeploymentPatchConfigPostStepLinuxExecStepConfigGcsObject {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
         */
        generationNumber: string;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface PatchDeploymentPatchConfigPostStepWindowsExecStepConfig {
        /**
         * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
         */
        allowedSuccessCodes?: number[];
        /**
         * A Cloud Storage object containing the executable.
         * Structure is documented below.
         */
        gcsObject?: outputs.osconfig.PatchDeploymentPatchConfigPostStepWindowsExecStepConfigGcsObject;
        /**
         * The script interpreter to use to run the script. If no interpreter is specified the script will
         * be executed directly, which will likely only succeed for scripts with shebang lines.
         * Possible values are: `SHELL`, `POWERSHELL`.
         */
        interpreter?: string;
        /**
         * An absolute path to the executable on the VM.
         */
        localPath?: string;
    }

    export interface PatchDeploymentPatchConfigPostStepWindowsExecStepConfigGcsObject {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
         */
        generationNumber: string;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface PatchDeploymentPatchConfigPreStep {
        /**
         * The ExecStepConfig for all Linux VMs targeted by the PatchJob.
         * Structure is documented below.
         */
        linuxExecStepConfig?: outputs.osconfig.PatchDeploymentPatchConfigPreStepLinuxExecStepConfig;
        /**
         * The ExecStepConfig for all Windows VMs targeted by the PatchJob.
         * Structure is documented below.
         */
        windowsExecStepConfig?: outputs.osconfig.PatchDeploymentPatchConfigPreStepWindowsExecStepConfig;
    }

    export interface PatchDeploymentPatchConfigPreStepLinuxExecStepConfig {
        /**
         * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
         */
        allowedSuccessCodes?: number[];
        /**
         * A Cloud Storage object containing the executable.
         * Structure is documented below.
         */
        gcsObject?: outputs.osconfig.PatchDeploymentPatchConfigPreStepLinuxExecStepConfigGcsObject;
        /**
         * The script interpreter to use to run the script. If no interpreter is specified the script will
         * be executed directly, which will likely only succeed for scripts with shebang lines.
         * Possible values are: `SHELL`, `POWERSHELL`.
         */
        interpreter?: string;
        /**
         * An absolute path to the executable on the VM.
         */
        localPath?: string;
    }

    export interface PatchDeploymentPatchConfigPreStepLinuxExecStepConfigGcsObject {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
         */
        generationNumber: string;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface PatchDeploymentPatchConfigPreStepWindowsExecStepConfig {
        /**
         * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
         */
        allowedSuccessCodes?: number[];
        /**
         * A Cloud Storage object containing the executable.
         * Structure is documented below.
         */
        gcsObject?: outputs.osconfig.PatchDeploymentPatchConfigPreStepWindowsExecStepConfigGcsObject;
        /**
         * The script interpreter to use to run the script. If no interpreter is specified the script will
         * be executed directly, which will likely only succeed for scripts with shebang lines.
         * Possible values are: `SHELL`, `POWERSHELL`.
         */
        interpreter?: string;
        /**
         * An absolute path to the executable on the VM.
         */
        localPath?: string;
    }

    export interface PatchDeploymentPatchConfigPreStepWindowsExecStepConfigGcsObject {
        /**
         * Bucket of the Cloud Storage object.
         */
        bucket: string;
        /**
         * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
         */
        generationNumber: string;
        /**
         * Name of the Cloud Storage object.
         */
        object: string;
    }

    export interface PatchDeploymentPatchConfigWindowsUpdate {
        /**
         * Only apply updates of these windows update classifications. If empty, all updates are applied.
         * Each value may be one of: `CRITICAL`, `SECURITY`, `DEFINITION`, `DRIVER`, `FEATURE_PACK`, `SERVICE_PACK`, `TOOL`, `UPDATE_ROLLUP`, `UPDATE`.
         */
        classifications?: string[];
        /**
         * List of KBs to exclude from update.
         */
        excludes?: string[];
        /**
         * An exclusive list of kbs to be updated. These are the only patches that will be updated.
         * This field must not be used with other patch configurations.
         */
        exclusivePatches?: string[];
    }

    export interface PatchDeploymentPatchConfigYum {
        /**
         * List of packages to exclude from update. These packages will be excluded.
         */
        excludes?: string[];
        /**
         * An exclusive list of packages to be updated. These are the only packages that will be updated.
         * If these packages are not installed, they will be ignored. This field cannot be specified with
         * any other patch configuration fields.
         */
        exclusivePackages?: string[];
        /**
         * Will cause patch to run yum update-minimal instead.
         */
        minimal?: boolean;
        /**
         * Adds the --security flag to yum update. Not supported on all platforms.
         */
        security?: boolean;
    }

    export interface PatchDeploymentPatchConfigZypper {
        /**
         * Install only patches with these categories. Common categories include security, recommended, and feature.
         */
        categories?: string[];
        /**
         * List of packages to exclude from update.
         */
        excludes?: string[];
        /**
         * An exclusive list of patches to be updated. These are the only patches that will be installed using 'zypper patch patch:' command.
         * This field must not be used with any other patch configuration fields.
         */
        exclusivePatches?: string[];
        /**
         * Install only patches with these severities. Common severities include critical, important, moderate, and low.
         */
        severities?: string[];
        /**
         * Adds the --with-optional flag to zypper patch.
         */
        withOptional?: boolean;
        /**
         * Adds the --with-update flag, to zypper patch.
         */
        withUpdate?: boolean;
    }

    export interface PatchDeploymentRecurringSchedule {
        /**
         * The end time at which a recurring patch deployment schedule is no longer active.
         * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
         */
        endTime?: string;
        /**
         * (Output)
         * The time the last patch job ran successfully.
         * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
         */
        lastExecuteTime: string;
        /**
         * Schedule with monthly executions.
         * Structure is documented below.
         */
        monthly?: outputs.osconfig.PatchDeploymentRecurringScheduleMonthly;
        /**
         * (Output)
         * The time the next patch job is scheduled to run.
         * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
         */
        nextExecuteTime: string;
        /**
         * The time that the recurring schedule becomes effective. Defaults to createTime of the patch deployment.
         * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
         */
        startTime?: string;
        /**
         * Time of the day to run a recurring deployment.
         * Structure is documented below.
         */
        timeOfDay: outputs.osconfig.PatchDeploymentRecurringScheduleTimeOfDay;
        /**
         * Defines the time zone that timeOfDay is relative to. The rules for daylight saving time are
         * determined by the chosen time zone.
         * Structure is documented below.
         */
        timeZone: outputs.osconfig.PatchDeploymentRecurringScheduleTimeZone;
        /**
         * Schedule with weekly executions.
         * Structure is documented below.
         */
        weekly?: outputs.osconfig.PatchDeploymentRecurringScheduleWeekly;
    }

    export interface PatchDeploymentRecurringScheduleMonthly {
        /**
         * One day of the month. 1-31 indicates the 1st to the 31st day. -1 indicates the last day of the month.
         * Months without the target day will be skipped. For example, a schedule to run "every month on the 31st"
         * will not run in February, April, June, etc.
         */
        monthDay?: number;
        /**
         * Week day in a month.
         * Structure is documented below.
         */
        weekDayOfMonth?: outputs.osconfig.PatchDeploymentRecurringScheduleMonthlyWeekDayOfMonth;
    }

    export interface PatchDeploymentRecurringScheduleMonthlyWeekDayOfMonth {
        /**
         * A day of the week.
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeek: string;
        /**
         * Week number in a month. 1-4 indicates the 1st to 4th week of the month. -1 indicates the last week of the month.
         */
        weekOrdinal: number;
    }

    export interface PatchDeploymentRecurringScheduleTimeOfDay {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23.
         * An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface PatchDeploymentRecurringScheduleTimeZone {
        /**
         * IANA Time Zone Database time zone, e.g. "America/New_York".
         */
        id: string;
        /**
         * IANA Time Zone Database version number, e.g. "2019a".
         */
        version?: string;
    }

    export interface PatchDeploymentRecurringScheduleWeekly {
        /**
         * IANA Time Zone Database time zone, e.g. "America/New_York".
         * Possible values are: `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        dayOfWeek: string;
    }

    export interface PatchDeploymentRollout {
        /**
         * The maximum number (or percentage) of VMs per zone to disrupt at any given moment. The number of VMs calculated from multiplying the percentage by the total number of VMs in a zone is rounded up.
         * During patching, a VM is considered disrupted from the time the agent is notified to begin until patching has completed. This disruption time includes the time to complete reboot and any post-patch steps.
         * A VM contributes to the disruption budget if its patching operation fails either when applying the patches, running pre or post patch steps, or if it fails to respond with a success notification before timing out. VMs that are not running or do not have an active agent do not count toward this disruption budget.
         * For zone-by-zone rollouts, if the disruption budget in a zone is exceeded, the patch job stops, because continuing to the next zone requires completion of the patch process in the previous zone.
         * For example, if the disruption budget has a fixed value of 10, and 8 VMs fail to patch in the current zone, the patch job continues to patch 2 VMs at a time until the zone is completed. When that zone is completed successfully, patching begins with 10 VMs at a time in the next zone. If 10 VMs in the next zone fail to patch, the patch job stops.
         * Structure is documented below.
         */
        disruptionBudget: outputs.osconfig.PatchDeploymentRolloutDisruptionBudget;
        /**
         * Mode of the patch rollout.
         * Possible values are: `ZONE_BY_ZONE`, `CONCURRENT_ZONES`.
         */
        mode: string;
    }

    export interface PatchDeploymentRolloutDisruptionBudget {
        /**
         * Specifies a fixed value.
         */
        fixed?: number;
        /**
         * Specifies the relative value defined as a percentage, which will be multiplied by a reference value.
         */
        percentage?: number;
    }

}

export namespace projects {
    export interface AccessApprovalSettingsEnrolledService {
        /**
         * The product for which Access Approval will be enrolled. Allowed values are listed (case-sensitive):
         * all
         * appengine.googleapis.com
         * bigquery.googleapis.com
         * bigtable.googleapis.com
         * cloudkms.googleapis.com
         * compute.googleapis.com
         * dataflow.googleapis.com
         * iam.googleapis.com
         * pubsub.googleapis.com
         * storage.googleapis.com
         */
        cloudProduct: string;
        /**
         * The enrollment level of the service.
         * Default value is `BLOCK_ALL`.
         * Possible values are: `BLOCK_ALL`.
         *
         * - - -
         */
        enrollmentLevel?: string;
    }

    export interface ApiKeyRestrictions {
        /**
         * The Android apps that are allowed to use the key.
         */
        androidKeyRestrictions?: outputs.projects.ApiKeyRestrictionsAndroidKeyRestrictions;
        /**
         * A restriction for a specific service and optionally one or more specific methods. Requests are allowed if they match any of these restrictions. If no restrictions are specified, all targets are allowed.
         */
        apiTargets?: outputs.projects.ApiKeyRestrictionsApiTarget[];
        /**
         * The HTTP referrers (websites) that are allowed to use the key.
         */
        browserKeyRestrictions?: outputs.projects.ApiKeyRestrictionsBrowserKeyRestrictions;
        /**
         * The iOS apps that are allowed to use the key.
         */
        iosKeyRestrictions?: outputs.projects.ApiKeyRestrictionsIosKeyRestrictions;
        /**
         * The IP addresses of callers that are allowed to use the key.
         */
        serverKeyRestrictions?: outputs.projects.ApiKeyRestrictionsServerKeyRestrictions;
    }

    export interface ApiKeyRestrictionsAndroidKeyRestrictions {
        /**
         * A list of Android applications that are allowed to make API calls with this key.
         */
        allowedApplications: outputs.projects.ApiKeyRestrictionsAndroidKeyRestrictionsAllowedApplication[];
    }

    export interface ApiKeyRestrictionsAndroidKeyRestrictionsAllowedApplication {
        /**
         * The package name of the application.
         */
        packageName: string;
        /**
         * The SHA1 fingerprint of the application. For example, both sha1 formats are acceptable : DA:39:A3:EE:5E:6B:4B:0D:32:55:BF:EF:95:60:18:90:AF:D8:07:09 or DA39A3EE5E6B4B0D3255BFEF95601890AFD80709. Output format is the latter.
         *
         * - - -
         */
        sha1Fingerprint: string;
    }

    export interface ApiKeyRestrictionsApiTarget {
        /**
         * Optional. List of one or more methods that can be called. If empty, all methods for the service are allowed. A wildcard (*) can be used as the last symbol. Valid examples: `google.cloud.translate.v2.TranslateService.GetSupportedLanguage` `TranslateText` `Get*` `translate.googleapis.com.Get*`
         */
        methods?: string[];
        /**
         * The service for this restriction. It should be the canonical service name, for example: `translate.googleapis.com`. You can use `gcloud services list` to get a list of services that are enabled in the project.
         */
        service: string;
    }

    export interface ApiKeyRestrictionsBrowserKeyRestrictions {
        /**
         * A list of regular expressions for the referrer URLs that are allowed to make API calls with this key.
         */
        allowedReferrers: string[];
    }

    export interface ApiKeyRestrictionsIosKeyRestrictions {
        /**
         * A list of bundle IDs that are allowed when making API calls with this key.
         */
        allowedBundleIds: string[];
    }

    export interface ApiKeyRestrictionsServerKeyRestrictions {
        /**
         * A list of the caller IP addresses that are allowed to make API calls with this key.
         */
        allowedIps: string[];
    }

    export interface GetOrganizationPolicyBooleanPolicy {
        enforced: boolean;
    }

    export interface GetOrganizationPolicyListPolicy {
        allows: outputs.projects.GetOrganizationPolicyListPolicyAllow[];
        denies: outputs.projects.GetOrganizationPolicyListPolicyDeny[];
        inheritFromParent: boolean;
        suggestedValue: string;
    }

    export interface GetOrganizationPolicyListPolicyAllow {
        all: boolean;
        values: string[];
    }

    export interface GetOrganizationPolicyListPolicyDeny {
        all: boolean;
        values: string[];
    }

    export interface GetOrganizationPolicyRestorePolicy {
        default: boolean;
    }

    export interface GetProjectProject {
        /**
         * Creation time in RFC3339 UTC "Zulu" format.
         */
        createTime: string;
        /**
         * A set of key/value label pairs assigned on a project.
         */
        labels: {[key: string]: string};
        /**
         * The Project lifecycle state.
         */
        lifecycleState: string;
        /**
         * The optional user-assigned display name of the project.
         */
        name: string;
        /**
         * The numeric identifier of the project.
         */
        number: string;
        /**
         * An optional reference to a parent resource.
         */
        parent: {[key: string]: string};
        /**
         * The project id of the project.
         */
        projectId: string;
    }

    export interface IAMAuditConfigAuditLogConfig {
        /**
         * Identities that do not cause logging for this type of permission.  The format is the same as that for `members`.
         */
        exemptedMembers?: string[];
        /**
         * Permission type for which logging is to be configured.  Must be one of `DATA_READ`, `DATA_WRITE`, or `ADMIN_READ`.
         */
        logType: string;
    }

    export interface IAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface IAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface OrganizationPolicyBooleanPolicy {
        /**
         * If true, then the Policy is enforced. If false, then any configuration is acceptable.
         */
        enforced: boolean;
    }

    export interface OrganizationPolicyListPolicy {
        /**
         * or `deny` - (Optional) One or the other must be set.
         */
        allow?: outputs.projects.OrganizationPolicyListPolicyAllow;
        deny?: outputs.projects.OrganizationPolicyListPolicyDeny;
        /**
         * If set to true, the values from the effective Policy of the parent resource
         * are inherited, meaning the values set in this Policy are added to the values inherited up the hierarchy.
         *
         * The `allow` or `deny` blocks support:
         */
        inheritFromParent?: boolean;
        /**
         * The Google Cloud Console will try to default to a configuration that matches the value specified in this field.
         */
        suggestedValue: string;
    }

    export interface OrganizationPolicyListPolicyAllow {
        /**
         * The policy allows or denies all values.
         */
        all?: boolean;
        /**
         * The policy can define specific values that are allowed or denied.
         */
        values?: string[];
    }

    export interface OrganizationPolicyListPolicyDeny {
        /**
         * The policy allows or denies all values.
         */
        all?: boolean;
        /**
         * The policy can define specific values that are allowed or denied.
         */
        values?: string[];
    }

    export interface OrganizationPolicyRestorePolicy {
        /**
         * May only be set to true. If set, then the default Policy is restored.
         */
        default: boolean;
    }

}

export namespace pubsub {
    export interface GetSubscriptionBigqueryConfig {
        dropUnknownFields: boolean;
        table: string;
        useTopicSchema: boolean;
        writeMetadata: boolean;
    }

    export interface GetSubscriptionCloudStorageConfig {
        avroConfigs: outputs.pubsub.GetSubscriptionCloudStorageConfigAvroConfig[];
        bucket: string;
        filenamePrefix: string;
        filenameSuffix: string;
        maxBytes: number;
        maxDuration: string;
        state: string;
    }

    export interface GetSubscriptionCloudStorageConfigAvroConfig {
        writeMetadata: boolean;
    }

    export interface GetSubscriptionDeadLetterPolicy {
        deadLetterTopic: string;
        maxDeliveryAttempts: number;
    }

    export interface GetSubscriptionExpirationPolicy {
        ttl: string;
    }

    export interface GetSubscriptionPushConfig {
        attributes: {[key: string]: string};
        noWrappers: outputs.pubsub.GetSubscriptionPushConfigNoWrapper[];
        oidcTokens: outputs.pubsub.GetSubscriptionPushConfigOidcToken[];
        pushEndpoint: string;
    }

    export interface GetSubscriptionPushConfigNoWrapper {
        writeMetadata: boolean;
    }

    export interface GetSubscriptionPushConfigOidcToken {
        audience: string;
        serviceAccountEmail: string;
    }

    export interface GetSubscriptionRetryPolicy {
        maximumBackoff: string;
        minimumBackoff: string;
    }

    export interface GetTopicMessageStoragePolicy {
        allowedPersistenceRegions: string[];
    }

    export interface GetTopicSchemaSetting {
        encoding: string;
        schema: string;
    }

    export interface LiteSubscriptionDeliveryConfig {
        /**
         * When this subscription should send messages to subscribers relative to messages persistence in storage.
         * Possible values are: `DELIVER_IMMEDIATELY`, `DELIVER_AFTER_STORED`, `DELIVERY_REQUIREMENT_UNSPECIFIED`.
         */
        deliveryRequirement: string;
    }

    export interface LiteTopicPartitionConfig {
        /**
         * The capacity configuration.
         * Structure is documented below.
         */
        capacity?: outputs.pubsub.LiteTopicPartitionConfigCapacity;
        /**
         * The number of partitions in the topic. Must be at least 1.
         */
        count: number;
    }

    export interface LiteTopicPartitionConfigCapacity {
        /**
         * Subscribe throughput capacity per partition in MiB/s. Must be >= 4 and <= 16.
         */
        publishMibPerSec: number;
        /**
         * Publish throughput capacity per partition in MiB/s. Must be >= 4 and <= 16.
         */
        subscribeMibPerSec: number;
    }

    export interface LiteTopicReservationConfig {
        /**
         * The Reservation to use for this topic's throughput capacity.
         */
        throughputReservation?: string;
    }

    export interface LiteTopicRetentionConfig {
        /**
         * The provisioned storage, in bytes, per partition. If the number of bytes stored
         * in any of the topic's partitions grows beyond this value, older messages will be
         * dropped to make room for newer ones, regardless of the value of period.
         */
        perPartitionBytes: string;
        /**
         * How long a published message is retained. If unset, messages will be retained as
         * long as the bytes retained for each partition is below perPartitionBytes. A
         * duration in seconds with up to nine fractional digits, terminated by 's'.
         * Example: "3.5s".
         */
        period?: string;
    }

    export interface SubscriptionBigqueryConfig {
        /**
         * When true and useTopicSchema is true, any fields that are a part of the topic schema that are not part of the BigQuery table schema are dropped when writing to BigQuery.
         * Otherwise, the schemas must be kept in sync and any messages with extra fields are not written and remain in the subscription's backlog.
         */
        dropUnknownFields?: boolean;
        /**
         * The name of the table to which to write data, of the form {projectId}:{datasetId}.{tableId}
         */
        table: string;
        /**
         * When true, use the topic's schema as the columns to write to in BigQuery, if it exists.
         */
        useTopicSchema?: boolean;
        /**
         * When true, write the subscription name, messageId, publishTime, attributes, and orderingKey to additional columns in the table.
         * The subscription name, messageId, and publishTime fields are put in their own columns while all other message properties (other than data) are written to a JSON object in the attributes column.
         */
        writeMetadata?: boolean;
    }

    export interface SubscriptionCloudStorageConfig {
        /**
         * If set, message data will be written to Cloud Storage in Avro format.
         * Structure is documented below.
         */
        avroConfig?: outputs.pubsub.SubscriptionCloudStorageConfigAvroConfig;
        /**
         * User-provided name for the Cloud Storage bucket. The bucket must be created by the user. The bucket name must be without any prefix like "gs://".
         */
        bucket: string;
        /**
         * User-provided prefix for Cloud Storage filename.
         */
        filenamePrefix?: string;
        /**
         * User-provided suffix for Cloud Storage filename. Must not end in "/".
         */
        filenameSuffix?: string;
        /**
         * The maximum bytes that can be written to a Cloud Storage file before a new file is created. Min 1 KB, max 10 GiB.
         * The maxBytes limit may be exceeded in cases where messages are larger than the limit.
         */
        maxBytes?: number;
        /**
         * The maximum duration that can elapse before a new Cloud Storage file is created. Min 1 minute, max 10 minutes, default 5 minutes.
         * May not exceed the subscription's acknowledgement deadline.
         * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
         */
        maxDuration?: string;
        /**
         * (Output)
         * An output-only field that indicates whether or not the subscription can receive messages.
         */
        state: string;
    }

    export interface SubscriptionCloudStorageConfigAvroConfig {
        /**
         * When true, write the subscription name, messageId, publishTime, attributes, and orderingKey as additional fields in the output.
         */
        writeMetadata?: boolean;
    }

    export interface SubscriptionDeadLetterPolicy {
        /**
         * The name of the topic to which dead letter messages should be published.
         * Format is `projects/{project}/topics/{topic}`.
         * The Cloud Pub/Sub service account associated with the enclosing subscription's
         * parent project (i.e.,
         * service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com) must have
         * permission to Publish() to this topic.
         * The operation will fail if the topic does not exist.
         * Users should ensure that there is a subscription attached to this topic
         * since messages published to a topic with no subscriptions are lost.
         */
        deadLetterTopic?: string;
        /**
         * The maximum number of delivery attempts for any message. The value must be
         * between 5 and 100.
         * The number of delivery attempts is defined as 1 + (the sum of number of
         * NACKs and number of times the acknowledgement deadline has been exceeded for the message).
         * A NACK is any call to ModifyAckDeadline with a 0 deadline. Note that
         * client libraries may automatically extend ack_deadlines.
         * This field will be honored on a best effort basis.
         * If this parameter is 0, a default value of 5 is used.
         */
        maxDeliveryAttempts?: number;
    }

    export interface SubscriptionExpirationPolicy {
        /**
         * Specifies the "time-to-live" duration for an associated resource. The
         * resource expires if it is not active for a period of ttl.
         * If ttl is set to "", the associated resource never expires.
         * A duration in seconds with up to nine fractional digits, terminated by 's'.
         * Example - "3.5s".
         */
        ttl: string;
    }

    export interface SubscriptionIAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface SubscriptionIAMMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface SubscriptionPushConfig {
        /**
         * Endpoint configuration attributes.
         * Every endpoint has a set of API supported attributes that can
         * be used to control different aspects of the message delivery.
         * The currently supported attribute is x-goog-version, which you
         * can use to change the format of the pushed message. This
         * attribute indicates the version of the data expected by
         * the endpoint. This controls the shape of the pushed message
         * (i.e., its fields and metadata). The endpoint version is
         * based on the version of the Pub/Sub API.
         * If not present during the subscriptions.create call,
         * it will default to the version of the API used to make
         * such call. If not present during a subscriptions.modifyPushConfig
         * call, its value will not be changed. subscriptions.get
         * calls will always return a valid version, even if the
         * subscription was created without this attribute.
         * The possible values for this attribute are:
         * - v1beta1: uses the push format defined in the v1beta1 Pub/Sub API.
         * - v1 or v1beta2: uses the push format defined in the v1 Pub/Sub API.
         */
        attributes?: {[key: string]: string};
        /**
         * When set, the payload to the push endpoint is not wrapped.Sets the
         * `data` field as the HTTP body for delivery.
         * Structure is documented below.
         */
        noWrapper?: outputs.pubsub.SubscriptionPushConfigNoWrapper;
        /**
         * If specified, Pub/Sub will generate and attach an OIDC JWT token as
         * an Authorization header in the HTTP request for every pushed message.
         * Structure is documented below.
         */
        oidcToken?: outputs.pubsub.SubscriptionPushConfigOidcToken;
        /**
         * A URL locating the endpoint to which messages should be pushed.
         * For example, a Webhook endpoint might use
         * "https://example.com/push".
         */
        pushEndpoint: string;
    }

    export interface SubscriptionPushConfigNoWrapper {
        /**
         * When true, writes the Pub/Sub message metadata to
         * `x-goog-pubsub-<KEY>:<VAL>` headers of the HTTP request. Writes the
         * Pub/Sub message attributes to `<KEY>:<VAL>` headers of the HTTP request.
         */
        writeMetadata: boolean;
    }

    export interface SubscriptionPushConfigOidcToken {
        /**
         * Audience to be used when generating OIDC token. The audience claim
         * identifies the recipients that the JWT is intended for. The audience
         * value is a single case-sensitive string. Having multiple values (array)
         * for the audience field is not supported. More info about the OIDC JWT
         * token audience here: https://tools.ietf.org/html/rfc7519#section-4.1.3
         * Note: if not specified, the Push endpoint URL will be used.
         */
        audience?: string;
        /**
         * Service account email to be used for generating the OIDC token.
         * The caller (for subscriptions.create, subscriptions.patch, and
         * subscriptions.modifyPushConfig RPCs) must have the
         * iam.serviceAccounts.actAs permission for the service account.
         */
        serviceAccountEmail: string;
    }

    export interface SubscriptionRetryPolicy {
        /**
         * The maximum delay between consecutive deliveries of a given message. Value should be between 0 and 600 seconds. Defaults to 600 seconds.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        maximumBackoff: string;
        /**
         * The minimum delay between consecutive deliveries of a given message. Value should be between 0 and 600 seconds. Defaults to 10 seconds.
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        minimumBackoff: string;
    }

    export interface TopicIAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TopicIAMMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TopicMessageStoragePolicy {
        /**
         * A list of IDs of GCP regions where messages that are published to
         * the topic may be persisted in storage. Messages published by
         * publishers running in non-allowed GCP regions (or running outside
         * of GCP altogether) will be routed for storage in one of the
         * allowed regions. An empty list means that no regions are allowed,
         * and is not a valid configuration.
         */
        allowedPersistenceRegions: string[];
    }

    export interface TopicSchemaSettings {
        /**
         * The encoding of messages validated against schema.
         * Default value is `ENCODING_UNSPECIFIED`.
         * Possible values are: `ENCODING_UNSPECIFIED`, `JSON`, `BINARY`.
         */
        encoding?: string;
        /**
         * The name of the schema that messages published should be
         * validated against. Format is projects/{project}/schemas/{schema}.
         * The value of this field will be _deleted-schema_
         * if the schema has been deleted.
         */
        schema: string;
    }

}

export namespace recaptcha {
    export interface EnterpriseKeyAndroidSettings {
        /**
         * If set to true, it means allowedPackageNames will not be enforced.
         */
        allowAllPackageNames?: boolean;
        /**
         * Android package names of apps allowed to use the key. Example: 'com.companyname.appname'
         */
        allowedPackageNames?: string[];
    }

    export interface EnterpriseKeyIosSettings {
        /**
         * If set to true, it means allowedBundleIds will not be enforced.
         */
        allowAllBundleIds?: boolean;
        /**
         * iOS bundle ids of apps allowed to use the key. Example: 'com.companyname.productname.appname'
         */
        allowedBundleIds?: string[];
    }

    export interface EnterpriseKeyTestingOptions {
        /**
         * For challenge-based keys only (CHECKBOX, INVISIBLE), all challenge requests for this site will return nocaptcha if NOCAPTCHA, or an unsolvable challenge if UNSOLVABLE_CHALLENGE. Possible values: TESTING_CHALLENGE_UNSPECIFIED, NOCAPTCHA, UNSOLVABLE_CHALLENGE
         */
        testingChallenge: string;
        /**
         * All assessments for this Key will return this score. Must be between 0 (likely not legitimate) and 1 (likely legitimate) inclusive.
         */
        testingScore?: number;
    }

    export interface EnterpriseKeyWebSettings {
        /**
         * If set to true, it means allowedDomains will not be enforced.
         */
        allowAllDomains?: boolean;
        /**
         * If set to true, the key can be used on AMP (Accelerated Mobile Pages) websites. This is supported only for the SCORE integration type.
         */
        allowAmpTraffic?: boolean;
        /**
         * Domains or subdomains of websites allowed to use the key. All subdomains of an allowed domain are automatically allowed. A valid domain requires a host and must not include any path, port, query or fragment. Examples: 'example.com' or 'subdomain.example.com'
         */
        allowedDomains?: string[];
        /**
         * Settings for the frequency and difficulty at which this key triggers captcha challenges. This should only be specified for IntegrationTypes CHECKBOX and INVISIBLE. Possible values: CHALLENGE_SECURITY_PREFERENCE_UNSPECIFIED, USABILITY, BALANCE, SECURITY
         */
        challengeSecurityPreference: string;
        /**
         * Required. Describes how this key is integrated with the website. Possible values: SCORE, CHECKBOX, INVISIBLE
         */
        integrationType: string;
    }

}

export namespace redis {
    export interface ClusterDiscoveryEndpoint {
        /**
         * Output only. The IP allocated on the consumer network for the PSC forwarding rule.
         */
        address?: string;
        /**
         * Output only. The port number of the exposed Redis endpoint.
         */
        port?: number;
        /**
         * Output only. Customer configuration for where the endpoint
         * is created and accessed from.
         * Structure is documented below.
         */
        pscConfig?: outputs.redis.ClusterDiscoveryEndpointPscConfig;
    }

    export interface ClusterDiscoveryEndpointPscConfig {
        /**
         * Required. The consumer network where the network address of
         * the discovery endpoint will be reserved, in the form of
         * projects/{network_project_id_or_number}/global/networks/{network_id}.
         *
         * - - -
         */
        network?: string;
    }

    export interface ClusterPscConfig {
        /**
         * Required. The consumer network where the network address of
         * the discovery endpoint will be reserved, in the form of
         * projects/{network_project_id_or_number}/global/networks/{network_id}.
         *
         * - - -
         */
        network: string;
    }

    export interface ClusterPscConnection {
        /**
         * Output only. The IP allocated on the consumer network for the PSC forwarding rule.
         */
        address?: string;
        /**
         * Output only. The URI of the consumer side forwarding rule. Example: projects/{projectNumOrId}/regions/us-east1/forwardingRules/{resourceId}.
         */
        forwardingRule?: string;
        /**
         * Required. The consumer network where the network address of
         * the discovery endpoint will be reserved, in the form of
         * projects/{network_project_id_or_number}/global/networks/{network_id}.
         *
         * - - -
         */
        network?: string;
        /**
         * Output only. The consumer projectId where the forwarding rule is created from.
         */
        projectId?: string;
        /**
         * Output only. The PSC connection id of the forwarding rule connected to the service attachment.
         */
        pscConnectionId?: string;
    }

    export interface ClusterStateInfo {
        /**
         * A nested object resource
         * Structure is documented below.
         */
        updateInfo?: outputs.redis.ClusterStateInfoUpdateInfo;
    }

    export interface ClusterStateInfoUpdateInfo {
        /**
         * Target number of replica nodes per shard.
         */
        targetReplicaCount?: number;
        /**
         * Target number of shards for redis cluster.
         */
        targetShardCount?: number;
    }

    export interface GetInstanceMaintenancePolicy {
        createTime: string;
        description: string;
        updateTime: string;
        weeklyMaintenanceWindows: outputs.redis.GetInstanceMaintenancePolicyWeeklyMaintenanceWindow[];
    }

    export interface GetInstanceMaintenancePolicyWeeklyMaintenanceWindow {
        day: string;
        duration: string;
        startTimes: outputs.redis.GetInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime[];
    }

    export interface GetInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime {
        hours: number;
        minutes: number;
        nanos: number;
        seconds: number;
    }

    export interface GetInstanceMaintenanceSchedule {
        endTime: string;
        scheduleDeadlineTime: string;
        startTime: string;
    }

    export interface GetInstanceNode {
        id: string;
        zone: string;
    }

    export interface GetInstancePersistenceConfig {
        persistenceMode: string;
        rdbNextSnapshotTime: string;
        rdbSnapshotPeriod: string;
        rdbSnapshotStartTime: string;
    }

    export interface GetInstanceServerCaCert {
        cert: string;
        createTime: string;
        expireTime: string;
        serialNumber: string;
        sha1Fingerprint: string;
    }

    export interface InstanceMaintenancePolicy {
        /**
         * (Output)
         * Output only. The time when the policy was created.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        createTime: string;
        /**
         * Optional. Description of what this policy is for.
         * Create/Update methods return INVALID_ARGUMENT if the
         * length is greater than 512.
         */
        description?: string;
        /**
         * (Output)
         * Output only. The time when the policy was last updated.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        updateTime: string;
        /**
         * Optional. Maintenance window that is applied to resources covered by this policy.
         * Minimum 1. For the current version, the maximum number
         * of weeklyWindow is expected to be one.
         * Structure is documented below.
         */
        weeklyMaintenanceWindows?: outputs.redis.InstanceMaintenancePolicyWeeklyMaintenanceWindow[];
    }

    export interface InstanceMaintenancePolicyWeeklyMaintenanceWindow {
        /**
         * Required. The day of week that maintenance updates occur.
         * - DAY_OF_WEEK_UNSPECIFIED: The day of the week is unspecified.
         * - MONDAY: Monday
         * - TUESDAY: Tuesday
         * - WEDNESDAY: Wednesday
         * - THURSDAY: Thursday
         * - FRIDAY: Friday
         * - SATURDAY: Saturday
         * - SUNDAY: Sunday
         * Possible values are: `DAY_OF_WEEK_UNSPECIFIED`, `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`, `SATURDAY`, `SUNDAY`.
         */
        day: string;
        /**
         * (Output)
         * Output only. Duration of the maintenance window.
         * The current window is fixed at 1 hour.
         * A duration in seconds with up to nine fractional digits,
         * terminated by 's'. Example: "3.5s".
         */
        duration: string;
        /**
         * Required. Start time of the window in UTC time.
         * Structure is documented below.
         */
        startTime: outputs.redis.InstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime;
    }

    export interface InstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23.
         * An API may choose to allow the value "24:00:00" for scenarios like business closing time.
         */
        hours?: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes?: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos?: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59.
         * An API may allow the value 60 if it allows leap-seconds.
         */
        seconds?: number;
    }

    export interface InstanceMaintenanceSchedule {
        /**
         * (Output)
         * Output only. The end time of any upcoming scheduled maintenance for this instance.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        endTime: string;
        /**
         * (Output)
         * Output only. The deadline that the maintenance schedule start time
         * can not go beyond, including reschedule.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        scheduleDeadlineTime: string;
        /**
         * Required. Start time of the window in UTC time.
         * Structure is documented below.
         */
        startTime: string;
    }

    export interface InstanceNode {
        /**
         * (Output)
         * Node identifying string. e.g. 'node-0', 'node-1'
         */
        id: string;
        /**
         * (Output)
         * Location of the node.
         */
        zone: string;
    }

    export interface InstancePersistenceConfig {
        /**
         * Optional. Controls whether Persistence features are enabled. If not provided, the existing value will be used.
         * - DISABLED: 	Persistence is disabled for the instance, and any existing snapshots are deleted.
         * - RDB: RDB based Persistence is enabled.
         * Possible values are: `DISABLED`, `RDB`.
         */
        persistenceMode: string;
        /**
         * (Output)
         * Output only. The next time that a snapshot attempt is scheduled to occur.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up
         * to nine fractional digits.
         * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        rdbNextSnapshotTime: string;
        /**
         * Optional. Available snapshot periods for scheduling.
         * - ONE_HOUR:	Snapshot every 1 hour.
         * - SIX_HOURS:	Snapshot every 6 hours.
         * - TWELVE_HOURS:	Snapshot every 12 hours.
         * - TWENTY_FOUR_HOURS:	Snapshot every 24 hours.
         * Possible values are: `ONE_HOUR`, `SIX_HOURS`, `TWELVE_HOURS`, `TWENTY_FOUR_HOURS`.
         */
        rdbSnapshotPeriod?: string;
        /**
         * Optional. Date and time that the first snapshot was/will be attempted,
         * and to which future snapshots will be aligned. If not provided,
         * the current time will be used.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution
         * and up to nine fractional digits.
         * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        rdbSnapshotStartTime: string;
    }

    export interface InstanceServerCaCert {
        /**
         * (Output)
         * The certificate data in PEM format.
         */
        cert: string;
        /**
         * (Output)
         * Output only. The time when the policy was created.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
         * resolution and up to nine fractional digits.
         */
        createTime: string;
        /**
         * (Output)
         * The time when the certificate expires.
         */
        expireTime: string;
        /**
         * (Output)
         * Serial number, as extracted from the certificate.
         */
        serialNumber: string;
        /**
         * (Output)
         * Sha1 Fingerprint of the certificate.
         */
        sha1Fingerprint: string;
    }

}

export namespace runtimeconfig {
    export interface ConfigIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ConfigIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace secretmanager {
    export interface GetSecretReplication {
        automatic: boolean;
        autos: outputs.secretmanager.GetSecretReplicationAuto[];
        userManageds: outputs.secretmanager.GetSecretReplicationUserManaged[];
    }

    export interface GetSecretReplicationAuto {
        customerManagedEncryptions: outputs.secretmanager.GetSecretReplicationAutoCustomerManagedEncryption[];
    }

    export interface GetSecretReplicationAutoCustomerManagedEncryption {
        kmsKeyName: string;
    }

    export interface GetSecretReplicationUserManaged {
        replicas: outputs.secretmanager.GetSecretReplicationUserManagedReplica[];
    }

    export interface GetSecretReplicationUserManagedReplica {
        customerManagedEncryptions: outputs.secretmanager.GetSecretReplicationUserManagedReplicaCustomerManagedEncryption[];
        location: string;
    }

    export interface GetSecretReplicationUserManagedReplicaCustomerManagedEncryption {
        kmsKeyName: string;
    }

    export interface GetSecretRotation {
        nextRotationTime: string;
        rotationPeriod: string;
    }

    export interface GetSecretTopic {
        name: string;
    }

    export interface SecretIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface SecretIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface SecretReplication {
        /**
         * The Secret will automatically be replicated without any restrictions.
         * Structure is documented below.
         */
        auto?: outputs.secretmanager.SecretReplicationAuto;
        /**
         * (Optional, Deprecated)
         * The Secret will automatically be replicated without any restrictions.
         *
         * > **Warning:** `automatic` is deprecated and will be removed in a future major release. Use `auto` instead.
         *
         * @deprecated `automatic` is deprecated and will be removed in a future major release. Use `auto` instead.
         */
        automatic?: boolean;
        /**
         * The Secret will be replicated to the regions specified by the user.
         * Structure is documented below.
         */
        userManaged?: outputs.secretmanager.SecretReplicationUserManaged;
    }

    export interface SecretReplicationAuto {
        /**
         * The customer-managed encryption configuration of the Secret.
         * If no configuration is provided, Google-managed default
         * encryption is used.
         * Structure is documented below.
         */
        customerManagedEncryption?: outputs.secretmanager.SecretReplicationAutoCustomerManagedEncryption;
    }

    export interface SecretReplicationAutoCustomerManagedEncryption {
        /**
         * Describes the Cloud KMS encryption key that will be used to protect destination secret.
         *
         * - - -
         */
        kmsKeyName: string;
    }

    export interface SecretReplicationUserManaged {
        /**
         * The list of Replicas for this Secret. Cannot be empty.
         * Structure is documented below.
         */
        replicas: outputs.secretmanager.SecretReplicationUserManagedReplica[];
    }

    export interface SecretReplicationUserManagedReplica {
        /**
         * Customer Managed Encryption for the secret.
         * Structure is documented below.
         */
        customerManagedEncryption?: outputs.secretmanager.SecretReplicationUserManagedReplicaCustomerManagedEncryption;
        /**
         * The canonical IDs of the location to replicate data. For example: "us-east1".
         */
        location: string;
    }

    export interface SecretReplicationUserManagedReplicaCustomerManagedEncryption {
        /**
         * Describes the Cloud KMS encryption key that will be used to protect destination secret.
         *
         * - - -
         */
        kmsKeyName: string;
    }

    export interface SecretRotation {
        /**
         * Timestamp in UTC at which the Secret is scheduled to rotate.
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        nextRotationTime?: string;
        /**
         * The Duration between rotation notifications. Must be in seconds and at least 3600s (1h) and at most 3153600000s (100 years).
         * If rotationPeriod is set, `nextRotationTime` must be set. `nextRotationTime` will be advanced by this period when the service automatically sends rotation notifications.
         */
        rotationPeriod?: string;
    }

    export interface SecretTopic {
        /**
         * The resource name of the Pub/Sub topic that will be published to, in the following format: projects/*&#47;topics/*.
         * For publication to succeed, the Secret Manager Service Agent service account must have pubsub.publisher permissions on the topic.
         */
        name: string;
    }

}

export namespace securitycenter {
    export interface InstanceIamBindingCondition {
        /**
         * An optional description of the instance.
         */
        description?: string;
        expression: string;
        title: string;
    }

    export interface InstanceIamMemberCondition {
        /**
         * An optional description of the instance.
         */
        description?: string;
        expression: string;
        title: string;
    }

    export interface NotificationConfigStreamingConfig {
        /**
         * Expression that defines the filter to apply across create/update
         * events of assets or findings as specified by the event type. The
         * expression is a list of zero or more restrictions combined via
         * logical operators AND and OR. Parentheses are supported, and OR
         * has higher precedence than AND.
         * Restrictions have the form <field> <operator> <value> and may have
         * a - character in front of them to indicate negation. The fields
         * map to those defined in the corresponding resource.
         * The supported operators are:
         * * = for all value types.
         * * >, <, >=, <= for integer values.
         * * :, meaning substring matching, for strings.
         * The supported value types are:
         * * string literals in quotes.
         * * integer literals without quotes.
         * * boolean literals true and false without quotes.
         * See
         * [Filtering notifications](https://cloud.google.com/security-command-center/docs/how-to-api-filter-notifications)
         * for information on how to write a filter.
         *
         * - - -
         */
        filter: string;
    }

    export interface ProjectCustomModuleCustomConfig {
        /**
         * Custom output properties.
         * Structure is documented below.
         */
        customOutput?: outputs.securitycenter.ProjectCustomModuleCustomConfigCustomOutput;
        /**
         * Text that describes the vulnerability or misconfiguration that the custom
         * module detects. This explanation is returned with each finding instance to
         * help investigators understand the detected issue. The text must be enclosed in quotation marks.
         */
        description?: string;
        /**
         * The CEL expression to evaluate to produce findings. When the expression evaluates
         * to true against a resource, a finding is generated.
         * Structure is documented below.
         */
        predicate: outputs.securitycenter.ProjectCustomModuleCustomConfigPredicate;
        /**
         * An explanation of the recommended steps that security teams can take to resolve
         * the detected issue. This explanation is returned with each finding generated by
         * this module in the nextSteps property of the finding JSON.
         */
        recommendation: string;
        /**
         * The resource types that the custom module operates on. Each custom module
         * can specify up to 5 resource types.
         * Structure is documented below.
         */
        resourceSelector: outputs.securitycenter.ProjectCustomModuleCustomConfigResourceSelector;
        /**
         * The severity to assign to findings generated by the module.
         * Possible values are: `CRITICAL`, `HIGH`, `MEDIUM`, `LOW`.
         */
        severity: string;
    }

    export interface ProjectCustomModuleCustomConfigCustomOutput {
        /**
         * A list of custom output properties to add to the finding.
         * Structure is documented below.
         */
        properties?: outputs.securitycenter.ProjectCustomModuleCustomConfigCustomOutputProperty[];
    }

    export interface ProjectCustomModuleCustomConfigCustomOutputProperty {
        /**
         * Name of the property for the custom output.
         */
        name?: string;
        /**
         * The CEL expression for the custom output. A resource property can be specified
         * to return the value of the property or a text string enclosed in quotation marks.
         * Structure is documented below.
         */
        valueExpression?: outputs.securitycenter.ProjectCustomModuleCustomConfigCustomOutputPropertyValueExpression;
    }

    export interface ProjectCustomModuleCustomConfigCustomOutputPropertyValueExpression {
        /**
         * Description of the expression. This is a longer text which describes the
         * expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a
         * file name and a position in the file.
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose. This can
         * be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface ProjectCustomModuleCustomConfigPredicate {
        /**
         * Description of the expression. This is a longer text which describes the
         * expression, e.g. when hovered over it in a UI.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * String indicating the location of the expression for error reporting, e.g. a
         * file name and a position in the file.
         */
        location?: string;
        /**
         * Title for the expression, i.e. a short string describing its purpose. This can
         * be used e.g. in UIs which allow to enter the expression.
         */
        title?: string;
    }

    export interface ProjectCustomModuleCustomConfigResourceSelector {
        /**
         * The resource types to run the detector on.
         *
         * - - -
         */
        resourceTypes: string[];
    }

    export interface SourceIamBindingCondition {
        /**
         * The description of the source (max of 1024 characters).
         */
        description?: string;
        expression: string;
        title: string;
    }

    export interface SourceIamMemberCondition {
        /**
         * The description of the source (max of 1024 characters).
         */
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace serviceAccount {
    export interface IAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface IAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

}

export namespace servicedirectory {
    export interface NamespaceIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface NamespaceIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServiceIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface ServiceIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace sourcerepo {
    export interface GetRepositoryPubsubConfig {
        messageFormat: string;
        serviceAccountEmail: string;
        topic: string;
    }

    export interface RepositoryIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RepositoryIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface RepositoryPubsubConfig {
        /**
         * The format of the Cloud Pub/Sub messages.
         * - PROTOBUF: The message payload is a serialized protocol buffer of SourceRepoEvent.
         * - JSON: The message payload is a JSON string of SourceRepoEvent.
         * Possible values are: `PROTOBUF`, `JSON`.
         */
        messageFormat: string;
        /**
         * Email address of the service account used for publishing Cloud Pub/Sub messages.
         * This service account needs to be in the same project as the PubsubConfig. When added,
         * the caller needs to have iam.serviceAccounts.actAs permission on this service account.
         * If unspecified, it defaults to the compute engine default service account.
         */
        serviceAccountEmail: string;
        /**
         * The identifier for this object. Format specified above.
         */
        topic: string;
    }

}

export namespace spanner {
    export interface DatabaseEncryptionConfig {
        /**
         * Fully qualified name of the KMS key to use to encrypt this database. This key must exist
         * in the same location as the Spanner Database.
         */
        kmsKeyName: string;
    }

    export interface DatabaseIAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface DatabaseIAMMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface InstanceIAMBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface InstanceIAMMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace sql {
    export interface DatabaseInstanceClone {
        /**
         * The name of the allocated ip range for the private ip CloudSQL instance. For example: "google-managed-services-default". If set, the cloned instance ip will be created in the allocated range. The range name must comply with [RFC 1035](https://tools.ietf.org/html/rfc1035). Specifically, the name must be 1-63 characters long and match the regular expression a-z?.
         */
        allocatedIpRange?: string;
        /**
         * (SQL Server only, use with `pointInTime`) Clone only the specified databases from the source instance. Clone all databases if empty.
         */
        databaseNames?: string[];
        /**
         * The timestamp of the point in time that should be restored.
         *
         * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        pointInTime?: string;
        /**
         * Name of the source instance which will be cloned.
         */
        sourceInstanceName: string;
    }

    export interface DatabaseInstanceIpAddress {
        ipAddress: string;
        timeToRetire: string;
        type: string;
    }

    export interface DatabaseInstanceReplicaConfiguration {
        /**
         * PEM representation of the trusted CA's x509
         * certificate.
         */
        caCertificate?: string;
        /**
         * PEM representation of the replica's x509
         * certificate.
         */
        clientCertificate?: string;
        /**
         * PEM representation of the replica's private key. The
         * corresponding public key in encoded in the `clientCertificate`.
         */
        clientKey?: string;
        /**
         * The number of seconds
         * between connect retries. MySQL's default is 60 seconds.
         */
        connectRetryInterval?: number;
        /**
         * Path to a SQL file in GCS from which replica
         * instances are created. Format is `gs://bucket/filename`.
         */
        dumpFilePath?: string;
        /**
         * Specifies if the replica is the failover target.
         * If the field is set to true the replica will be designated as a failover replica.
         * If the master instance fails, the replica instance will be promoted as
         * the new master instance.
         * > **NOTE:** Not supported for Postgres database.
         */
        failoverTarget?: boolean;
        /**
         * Time in ms between replication
         * heartbeats.
         */
        masterHeartbeatPeriod?: number;
        /**
         * Password for the replication connection.
         */
        password?: string;
        sslCipher?: string;
        /**
         * Username for replication connection.
         */
        username?: string;
        /**
         * True if the master's common name
         * value is checked during the SSL handshake.
         */
        verifyServerCertificate?: boolean;
    }

    export interface DatabaseInstanceRestoreBackupContext {
        /**
         * The ID of the backup run to restore from.
         */
        backupRunId: number;
        /**
         * The ID of the instance that the backup was taken from. If left empty,
         * this instance's ID will be used.
         */
        instanceId?: string;
        /**
         * The full project ID of the source instance.`
         */
        project?: string;
    }

    export interface DatabaseInstanceServerCaCert {
        cert: string;
        commonName: string;
        createTime: string;
        /**
         * The [RFC 3339](https://tools.ietf.org/html/rfc3339)
         * formatted date time string indicating when this whitelist expires.
         */
        expirationTime: string;
        sha1Fingerprint: string;
    }

    export interface DatabaseInstanceSettings {
        /**
         * This specifies when the instance should be
         * active. Can be either `ALWAYS`, `NEVER` or `ON_DEMAND`.
         */
        activationPolicy?: string;
        activeDirectoryConfig?: outputs.sql.DatabaseInstanceSettingsActiveDirectoryConfig;
        advancedMachineFeatures?: outputs.sql.DatabaseInstanceSettingsAdvancedMachineFeatures;
        /**
         * The availability type of the Cloud SQL
         * instance, high availability (`REGIONAL`) or single zone (`ZONAL`).' For all instances, ensure that
         * `settings.backup_configuration.enabled` is set to `true`.
         * For MySQL instances, ensure that `settings.backup_configuration.binary_log_enabled` is set to `true`.
         * For Postgres and SQL Server instances, ensure that `settings.backup_configuration.point_in_time_recovery_enabled`
         * is set to `true`. Defaults to `ZONAL`.
         */
        availabilityType?: string;
        backupConfiguration: outputs.sql.DatabaseInstanceSettingsBackupConfiguration;
        /**
         * The name of server instance collation.
         */
        collation?: string;
        /**
         * Specifies if connections must use Cloud SQL connectors.
         */
        connectorEnforcement: string;
        dataCacheConfig?: outputs.sql.DatabaseInstanceSettingsDataCacheConfig;
        databaseFlags?: outputs.sql.DatabaseInstanceSettingsDatabaseFlag[];
        deletionProtectionEnabled?: boolean;
        denyMaintenancePeriod?: outputs.sql.DatabaseInstanceSettingsDenyMaintenancePeriod;
        /**
         * Enables auto-resizing of the storage size. Defaults to `true`.
         */
        diskAutoresize?: boolean;
        /**
         * The maximum size to which storage capacity can be automatically increased. The default value is 0, which specifies that there is no limit.
         */
        diskAutoresizeLimit?: number;
        /**
         * The size of data disk, in GB. Size of a running instance cannot be reduced but can be increased. The minimum value is 10GB.
         */
        diskSize: number;
        /**
         * The type of data disk: PD_SSD or PD_HDD. Defaults to `PD_SSD`.
         */
        diskType?: string;
        /**
         * The edition of the instance, can be `ENTERPRISE` or `ENTERPRISE_PLUS`.
         */
        edition?: string;
        insightsConfig?: outputs.sql.DatabaseInstanceSettingsInsightsConfig;
        ipConfiguration: outputs.sql.DatabaseInstanceSettingsIpConfiguration;
        locationPreference: outputs.sql.DatabaseInstanceSettingsLocationPreference;
        maintenanceWindow?: outputs.sql.DatabaseInstanceSettingsMaintenanceWindow;
        passwordValidationPolicy?: outputs.sql.DatabaseInstanceSettingsPasswordValidationPolicy;
        /**
         * Pricing plan for this instance, can only be `PER_USE`.
         */
        pricingPlan?: string;
        sqlServerAuditConfig?: outputs.sql.DatabaseInstanceSettingsSqlServerAuditConfig;
        /**
         * The machine type to use. See [tiers](https://cloud.google.com/sql/docs/admin-api/v1beta4/tiers)
         * for more details and supported versions. Postgres supports only shared-core machine types,
         * and custom machine types such as `db-custom-2-13312`. See the [Custom Machine Type Documentation](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#create) to learn about specifying custom machine types.
         */
        tier: string;
        /**
         * The timeZone to be used by the database engine (supported only for SQL Server), in SQL Server timezone format.
         */
        timeZone?: string;
        /**
         * A set of key/value user label pairs to assign to the instance.
         */
        userLabels: {[key: string]: string};
        version: number;
    }

    export interface DatabaseInstanceSettingsActiveDirectoryConfig {
        /**
         * The domain name for the active directory (e.g., mydomain.com).
         * Can only be used with SQL Server.
         */
        domain: string;
    }

    export interface DatabaseInstanceSettingsAdvancedMachineFeatures {
        /**
         * The number of threads per core. The value of this flag can be 1 or 2. To disable SMT, set this flag to 1. Only available in Cloud SQL for SQL Server instances. See [smt](https://cloud.google.com/sql/docs/sqlserver/create-instance#smt-create-instance) for more details.
         */
        threadsPerCore?: number;
    }

    export interface DatabaseInstanceSettingsBackupConfiguration {
        /**
         * Backup retention settings. The configuration is detailed below.
         */
        backupRetentionSettings: outputs.sql.DatabaseInstanceSettingsBackupConfigurationBackupRetentionSettings;
        /**
         * True if binary logging is enabled.
         * Can only be used with MySQL.
         */
        binaryLogEnabled?: boolean;
        /**
         * True if backup configuration is enabled.
         */
        enabled?: boolean;
        /**
         * The region where the backup will be stored
         */
        location?: string;
        /**
         * True if Point-in-time recovery is enabled. Will restart database if enabled after instance creation. Valid only for PostgreSQL and SQL Server instances.
         */
        pointInTimeRecoveryEnabled?: boolean;
        /**
         * `HH:MM` format time indicating when backup
         * configuration starts.
         */
        startTime: string;
        /**
         * The number of days of transaction logs we retain for point in time restore, from 1-7. For PostgreSQL Enterprise Plus instances, the number of days of retained transaction logs can be set from 1 to 35.
         */
        transactionLogRetentionDays: number;
    }

    export interface DatabaseInstanceSettingsBackupConfigurationBackupRetentionSettings {
        /**
         * Depending on the value of retention_unit, this is used to determine if a backup needs to be deleted. If retentionUnit
         * is 'COUNT', we will retain this many backups.
         */
        retainedBackups: number;
        /**
         * The unit that 'retained_backups' represents. Defaults to `COUNT`.
         */
        retentionUnit?: string;
    }

    export interface DatabaseInstanceSettingsDataCacheConfig {
        /**
         * Whether data cache is enabled for the instance. Defaults to `false`
         * Can only be used with MYSQL.
         */
        dataCacheEnabled?: boolean;
    }

    export interface DatabaseInstanceSettingsDatabaseFlag {
        /**
         * Name of the flag.
         */
        name: string;
        /**
         * Value of the flag.
         */
        value: string;
    }

    export interface DatabaseInstanceSettingsDenyMaintenancePeriod {
        /**
         * "deny maintenance period" end date. If the year of the end date is empty, the year of the start date also must be empty. In this case, it means the no maintenance interval recurs every year. The date is in format yyyy-mm-dd i.e., 2020-11-01, or mm-dd, i.e., 11-01
         */
        endDate: string;
        /**
         * "deny maintenance period" start date. If the year of the start date is empty, the year of the end date also must be empty. In this case, it means the deny maintenance period recurs every year. The date is in format yyyy-mm-dd i.e., 2020-11-01, or mm-dd, i.e., 11-01
         */
        startDate: string;
        /**
         * Time in UTC when the "deny maintenance period" starts on startDate and ends on endDate. The time is in format: HH:mm:SS, i.e., 00:00:00
         */
        time: string;
    }

    export interface DatabaseInstanceSettingsInsightsConfig {
        /**
         * True if Query Insights feature is enabled.
         */
        queryInsightsEnabled?: boolean;
        /**
         * Number of query execution plans captured by Insights per minute for all queries combined. Between 0 and 20. Default to 5.
         *
         * The optional `settings.password_validation_policy` subblock for instances declares [Password Validation Policy](https://cloud.google.com/sql/docs/postgres/built-in-authentication) configuration. It contains:
         */
        queryPlansPerMinute: number;
        /**
         * Maximum query length stored in bytes. Between 256 and 4500. Default to 1024. Higher query lengths are more useful for analytical queries, but they also require more memory. Changing the query length requires you to restart the instance. You can still add tags to queries that exceed the length limit.
         */
        queryStringLength?: number;
        /**
         * True if Query Insights will record application tags from query when enabled.
         */
        recordApplicationTags?: boolean;
        /**
         * True if Query Insights will record client address when enabled.
         */
        recordClientAddress?: boolean;
    }

    export interface DatabaseInstanceSettingsIpConfiguration {
        /**
         * The name of the allocated ip range for the private ip CloudSQL instance. For example: "google-managed-services-default". If set, the instance ip will be created in the allocated range. The range name must comply with [RFC 1035](https://datatracker.ietf.org/doc/html/rfc1035). Specifically, the name must be 1-63 characters long and match the regular expression a-z?.
         */
        allocatedIpRange?: string;
        authorizedNetworks?: outputs.sql.DatabaseInstanceSettingsIpConfigurationAuthorizedNetwork[];
        /**
         * Whether Google Cloud services such as BigQuery are allowed to access data in this Cloud SQL instance over a private IP connection. SQLSERVER database type is not supported.
         */
        enablePrivatePathForGoogleCloudServices?: boolean;
        /**
         * Whether this Cloud SQL instance should be assigned
         * a public IPV4 address. At least `ipv4Enabled` must be enabled or a
         * `privateNetwork` must be configured.
         */
        ipv4Enabled?: boolean;
        /**
         * The VPC network from which the Cloud SQL
         * instance is accessible for private IP. For example,projects/myProject/global/networks/default.
         * Specifying a network enables private IP.
         * At least `ipv4Enabled` must be enabled or a `privateNetwork` must be configured.
         * This setting can be updated, but it cannot be removed after it is set.
         */
        privateNetwork?: string;
        pscConfigs?: outputs.sql.DatabaseInstanceSettingsIpConfigurationPscConfig[];
        /**
         * Whether SSL connections over IP are enforced or not.
         */
        requireSsl?: boolean;
    }

    export interface DatabaseInstanceSettingsIpConfigurationAuthorizedNetwork {
        /**
         * The [RFC 3339](https://tools.ietf.org/html/rfc3339)
         * formatted date time string indicating when this whitelist expires.
         */
        expirationTime?: string;
        /**
         * A name for this whitelist entry.
         */
        name?: string;
        /**
         * A CIDR notation IPv4 or IPv6 address that is allowed to
         * access this instance. Must be set even if other two attributes are not for
         * the whitelist to become active.
         */
        value: string;
    }

    export interface DatabaseInstanceSettingsIpConfigurationPscConfig {
        /**
         * List of consumer projects that are allow-listed for PSC connections to this instance. This instance can be connected to with PSC from any network in these projects. Each consumer project in this list may be represented by a project number (numeric) or by a project id (alphanumeric).
         */
        allowedConsumerProjects?: string[];
        /**
         * Whether PSC connectivity is enabled for this instance.
         */
        pscEnabled?: boolean;
    }

    export interface DatabaseInstanceSettingsLocationPreference {
        /**
         * A GAE application whose zone to remain
         * in. Must be in the same region as this instance.
         */
        followGaeApplication?: string;
        /**
         * The preferred Compute Engine zone for the secondary/failover.
         *
         * The optional `settings.maintenance_window` subblock for instances declares a one-hour
         * [maintenance window](https://cloud.google.com/sql/docs/instance-settings?hl=en#maintenance-window-2ndgen)
         * when an Instance can automatically restart to apply updates. The maintenance window is specified in UTC time. It supports:
         */
        secondaryZone?: string;
        /**
         * The preferred compute engine
         * [zone](https://cloud.google.com/compute/docs/zones?hl=en).
         */
        zone?: string;
    }

    export interface DatabaseInstanceSettingsMaintenanceWindow {
        /**
         * Day of week (`1-7`), starting on Monday
         */
        day?: number;
        /**
         * Hour of day (`0-23`), ignored if `day` not set
         */
        hour?: number;
        /**
         * Receive updates earlier (`canary`) or later
         * (`stable`)
         *
         * The optional `settings.insights_config` subblock for instances declares Query Insights([MySQL](https://cloud.google.com/sql/docs/mysql/using-query-insights), [PostgreSQL](https://cloud.google.com/sql/docs/postgres/using-query-insights)) configuration. It contains:
         */
        updateTrack?: string;
    }

    export interface DatabaseInstanceSettingsPasswordValidationPolicy {
        /**
         * Checks if the password is a combination of lowercase, uppercase, numeric, and non-alphanumeric characters.
         */
        complexity?: string;
        /**
         * Prevents the use of the username in the password.
         */
        disallowUsernameSubstring?: boolean;
        /**
         * Enables or disable the password validation policy.
         *
         * The optional `replicaConfiguration` block must have `masterInstanceName` set
         * to work, cannot be updated, and supports:
         */
        enablePasswordPolicy: boolean;
        /**
         * Specifies the minimum number of characters that the password must have.
         */
        minLength?: number;
        /**
         * Specifies the minimum duration after which you can change the password.
         */
        passwordChangeInterval?: string;
        /**
         * Specifies the number of previous passwords that you can't reuse.
         */
        reuseInterval?: number;
    }

    export interface DatabaseInstanceSettingsSqlServerAuditConfig {
        /**
         * The name of the destination bucket (e.g., gs://mybucket).
         */
        bucket?: string;
        /**
         * How long to keep generated audit files. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        retentionInterval?: string;
        /**
         * How often to upload generated audit files. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        uploadInterval?: string;
    }

    export interface GetCaCertsCert {
        /**
         * The CA certificate used to connect to the SQL instance via SSL.
         */
        cert: string;
        /**
         * The CN valid for the CA cert.
         */
        commonName: string;
        /**
         * Creation time of the CA cert.
         */
        createTime: string;
        /**
         * Expiration time of the CA cert.
         */
        expirationTime: string;
        /**
         * SHA1 fingerprint of the CA cert.
         */
        sha1Fingerprint: string;
    }

    export interface GetDatabaseInstanceClone {
        allocatedIpRange: string;
        databaseNames: string[];
        pointInTime: string;
        sourceInstanceName: string;
    }

    export interface GetDatabaseInstanceIpAddress {
        ipAddress: string;
        timeToRetire: string;
        type: string;
    }

    export interface GetDatabaseInstanceReplicaConfiguration {
        caCertificate: string;
        clientCertificate: string;
        clientKey: string;
        connectRetryInterval: number;
        dumpFilePath: string;
        failoverTarget: boolean;
        masterHeartbeatPeriod: number;
        password: string;
        sslCipher: string;
        username: string;
        verifyServerCertificate: boolean;
    }

    export interface GetDatabaseInstanceRestoreBackupContext {
        backupRunId: number;
        instanceId: string;
        /**
         * The ID of the project in which the resource belongs.
         */
        project: string;
    }

    export interface GetDatabaseInstanceServerCaCert {
        cert: string;
        commonName: string;
        createTime: string;
        expirationTime: string;
        sha1Fingerprint: string;
    }

    export interface GetDatabaseInstanceSetting {
        activationPolicy: string;
        activeDirectoryConfigs: outputs.sql.GetDatabaseInstanceSettingActiveDirectoryConfig[];
        advancedMachineFeatures: outputs.sql.GetDatabaseInstanceSettingAdvancedMachineFeature[];
        availabilityType: string;
        backupConfigurations: outputs.sql.GetDatabaseInstanceSettingBackupConfiguration[];
        collation: string;
        connectorEnforcement: string;
        dataCacheConfigs: outputs.sql.GetDatabaseInstanceSettingDataCacheConfig[];
        databaseFlags: outputs.sql.GetDatabaseInstanceSettingDatabaseFlag[];
        deletionProtectionEnabled: boolean;
        denyMaintenancePeriods: outputs.sql.GetDatabaseInstanceSettingDenyMaintenancePeriod[];
        diskAutoresize: boolean;
        diskAutoresizeLimit: number;
        diskSize: number;
        diskType: string;
        edition: string;
        insightsConfigs: outputs.sql.GetDatabaseInstanceSettingInsightsConfig[];
        ipConfigurations: outputs.sql.GetDatabaseInstanceSettingIpConfiguration[];
        locationPreferences: outputs.sql.GetDatabaseInstanceSettingLocationPreference[];
        maintenanceWindows: outputs.sql.GetDatabaseInstanceSettingMaintenanceWindow[];
        passwordValidationPolicies: outputs.sql.GetDatabaseInstanceSettingPasswordValidationPolicy[];
        pricingPlan: string;
        sqlServerAuditConfigs: outputs.sql.GetDatabaseInstanceSettingSqlServerAuditConfig[];
        tier: string;
        timeZone: string;
        userLabels: {[key: string]: string};
        version: number;
    }

    export interface GetDatabaseInstanceSettingActiveDirectoryConfig {
        domain: string;
    }

    export interface GetDatabaseInstanceSettingAdvancedMachineFeature {
        threadsPerCore: number;
    }

    export interface GetDatabaseInstanceSettingBackupConfiguration {
        backupRetentionSettings: outputs.sql.GetDatabaseInstanceSettingBackupConfigurationBackupRetentionSetting[];
        binaryLogEnabled: boolean;
        enabled: boolean;
        location: string;
        pointInTimeRecoveryEnabled: boolean;
        startTime: string;
        transactionLogRetentionDays: number;
    }

    export interface GetDatabaseInstanceSettingBackupConfigurationBackupRetentionSetting {
        retainedBackups: number;
        retentionUnit: string;
    }

    export interface GetDatabaseInstanceSettingDataCacheConfig {
        dataCacheEnabled: boolean;
    }

    export interface GetDatabaseInstanceSettingDatabaseFlag {
        /**
         * The name of the instance.
         */
        name: string;
        value: string;
    }

    export interface GetDatabaseInstanceSettingDenyMaintenancePeriod {
        endDate: string;
        startDate: string;
        time: string;
    }

    export interface GetDatabaseInstanceSettingInsightsConfig {
        queryInsightsEnabled: boolean;
        queryPlansPerMinute: number;
        queryStringLength: number;
        recordApplicationTags: boolean;
        recordClientAddress: boolean;
    }

    export interface GetDatabaseInstanceSettingIpConfiguration {
        allocatedIpRange: string;
        authorizedNetworks: outputs.sql.GetDatabaseInstanceSettingIpConfigurationAuthorizedNetwork[];
        enablePrivatePathForGoogleCloudServices: boolean;
        ipv4Enabled: boolean;
        privateNetwork: string;
        pscConfigs: outputs.sql.GetDatabaseInstanceSettingIpConfigurationPscConfig[];
        requireSsl: boolean;
    }

    export interface GetDatabaseInstanceSettingIpConfigurationAuthorizedNetwork {
        expirationTime: string;
        /**
         * The name of the instance.
         */
        name: string;
        value: string;
    }

    export interface GetDatabaseInstanceSettingIpConfigurationPscConfig {
        allowedConsumerProjects: string[];
        pscEnabled: boolean;
    }

    export interface GetDatabaseInstanceSettingLocationPreference {
        followGaeApplication: string;
        secondaryZone: string;
        zone: string;
    }

    export interface GetDatabaseInstanceSettingMaintenanceWindow {
        day: number;
        hour: number;
        updateTrack: string;
    }

    export interface GetDatabaseInstanceSettingPasswordValidationPolicy {
        complexity: string;
        disallowUsernameSubstring: boolean;
        enablePasswordPolicy: boolean;
        minLength: number;
        passwordChangeInterval: string;
        reuseInterval: number;
    }

    export interface GetDatabaseInstanceSettingSqlServerAuditConfig {
        bucket: string;
        retentionInterval: string;
        uploadInterval: string;
    }

    export interface GetDatabaseInstancesInstance {
        availableMaintenanceVersions: string[];
        clones: outputs.sql.GetDatabaseInstancesInstanceClone[];
        connectionName: string;
        /**
         * To filter out the Cloud SQL instances which are of the specified database version.
         */
        databaseVersion: string;
        deletionProtection: boolean;
        dnsName: string;
        encryptionKeyName: string;
        firstIpAddress: string;
        instanceType: string;
        ipAddresses: outputs.sql.GetDatabaseInstancesInstanceIpAddress[];
        maintenanceVersion: string;
        masterInstanceName: string;
        name: string;
        privateIpAddress: string;
        /**
         * The ID of the project in which the resources belong. If it is not provided, the provider project is used.
         */
        project: string;
        pscServiceAttachmentLink: string;
        publicIpAddress: string;
        /**
         * To filter out the Cloud SQL instances which are located in the specified region.
         */
        region: string;
        replicaConfigurations: outputs.sql.GetDatabaseInstancesInstanceReplicaConfiguration[];
        restoreBackupContexts: outputs.sql.GetDatabaseInstancesInstanceRestoreBackupContext[];
        rootPassword: string;
        selfLink: string;
        serverCaCerts: outputs.sql.GetDatabaseInstancesInstanceServerCaCert[];
        serviceAccountEmailAddress: string;
        settings: outputs.sql.GetDatabaseInstancesInstanceSetting[];
    }

    export interface GetDatabaseInstancesInstanceClone {
        allocatedIpRange: string;
        databaseNames: string[];
        pointInTime: string;
        sourceInstanceName: string;
    }

    export interface GetDatabaseInstancesInstanceIpAddress {
        ipAddress: string;
        timeToRetire: string;
        type: string;
    }

    export interface GetDatabaseInstancesInstanceReplicaConfiguration {
        caCertificate: string;
        clientCertificate: string;
        clientKey: string;
        connectRetryInterval: number;
        dumpFilePath: string;
        failoverTarget: boolean;
        masterHeartbeatPeriod: number;
        password: string;
        sslCipher: string;
        username: string;
        verifyServerCertificate: boolean;
    }

    export interface GetDatabaseInstancesInstanceRestoreBackupContext {
        backupRunId: number;
        instanceId: string;
        /**
         * The ID of the project in which the resources belong. If it is not provided, the provider project is used.
         */
        project: string;
    }

    export interface GetDatabaseInstancesInstanceServerCaCert {
        cert: string;
        commonName: string;
        createTime: string;
        expirationTime: string;
        sha1Fingerprint: string;
    }

    export interface GetDatabaseInstancesInstanceSetting {
        activationPolicy: string;
        activeDirectoryConfigs: outputs.sql.GetDatabaseInstancesInstanceSettingActiveDirectoryConfig[];
        advancedMachineFeatures: outputs.sql.GetDatabaseInstancesInstanceSettingAdvancedMachineFeature[];
        availabilityType: string;
        backupConfigurations: outputs.sql.GetDatabaseInstancesInstanceSettingBackupConfiguration[];
        collation: string;
        connectorEnforcement: string;
        dataCacheConfigs: outputs.sql.GetDatabaseInstancesInstanceSettingDataCacheConfig[];
        databaseFlags: outputs.sql.GetDatabaseInstancesInstanceSettingDatabaseFlag[];
        deletionProtectionEnabled: boolean;
        denyMaintenancePeriods: outputs.sql.GetDatabaseInstancesInstanceSettingDenyMaintenancePeriod[];
        diskAutoresize: boolean;
        diskAutoresizeLimit: number;
        diskSize: number;
        diskType: string;
        edition: string;
        insightsConfigs: outputs.sql.GetDatabaseInstancesInstanceSettingInsightsConfig[];
        ipConfigurations: outputs.sql.GetDatabaseInstancesInstanceSettingIpConfiguration[];
        locationPreferences: outputs.sql.GetDatabaseInstancesInstanceSettingLocationPreference[];
        maintenanceWindows: outputs.sql.GetDatabaseInstancesInstanceSettingMaintenanceWindow[];
        passwordValidationPolicies: outputs.sql.GetDatabaseInstancesInstanceSettingPasswordValidationPolicy[];
        pricingPlan: string;
        sqlServerAuditConfigs: outputs.sql.GetDatabaseInstancesInstanceSettingSqlServerAuditConfig[];
        /**
         * To filter out the Cloud SQL instances based on the tier(or machine type) of the database instances.
         */
        tier: string;
        timeZone: string;
        userLabels: {[key: string]: string};
        version: number;
    }

    export interface GetDatabaseInstancesInstanceSettingActiveDirectoryConfig {
        domain: string;
    }

    export interface GetDatabaseInstancesInstanceSettingAdvancedMachineFeature {
        threadsPerCore: number;
    }

    export interface GetDatabaseInstancesInstanceSettingBackupConfiguration {
        backupRetentionSettings: outputs.sql.GetDatabaseInstancesInstanceSettingBackupConfigurationBackupRetentionSetting[];
        binaryLogEnabled: boolean;
        enabled: boolean;
        location: string;
        pointInTimeRecoveryEnabled: boolean;
        startTime: string;
        transactionLogRetentionDays: number;
    }

    export interface GetDatabaseInstancesInstanceSettingBackupConfigurationBackupRetentionSetting {
        retainedBackups: number;
        retentionUnit: string;
    }

    export interface GetDatabaseInstancesInstanceSettingDataCacheConfig {
        dataCacheEnabled: boolean;
    }

    export interface GetDatabaseInstancesInstanceSettingDatabaseFlag {
        name: string;
        value: string;
    }

    export interface GetDatabaseInstancesInstanceSettingDenyMaintenancePeriod {
        endDate: string;
        startDate: string;
        time: string;
    }

    export interface GetDatabaseInstancesInstanceSettingInsightsConfig {
        queryInsightsEnabled: boolean;
        queryPlansPerMinute: number;
        queryStringLength: number;
        recordApplicationTags: boolean;
        recordClientAddress: boolean;
    }

    export interface GetDatabaseInstancesInstanceSettingIpConfiguration {
        allocatedIpRange: string;
        authorizedNetworks: outputs.sql.GetDatabaseInstancesInstanceSettingIpConfigurationAuthorizedNetwork[];
        enablePrivatePathForGoogleCloudServices: boolean;
        ipv4Enabled: boolean;
        privateNetwork: string;
        pscConfigs: outputs.sql.GetDatabaseInstancesInstanceSettingIpConfigurationPscConfig[];
        requireSsl: boolean;
    }

    export interface GetDatabaseInstancesInstanceSettingIpConfigurationAuthorizedNetwork {
        expirationTime: string;
        name: string;
        value: string;
    }

    export interface GetDatabaseInstancesInstanceSettingIpConfigurationPscConfig {
        allowedConsumerProjects: string[];
        pscEnabled: boolean;
    }

    export interface GetDatabaseInstancesInstanceSettingLocationPreference {
        followGaeApplication: string;
        secondaryZone: string;
        /**
         * To filter out the Cloud SQL instances which are located in the specified zone. This zone refers to the Compute Engine zone that the instance is currently serving from.
         */
        zone: string;
    }

    export interface GetDatabaseInstancesInstanceSettingMaintenanceWindow {
        day: number;
        hour: number;
        updateTrack: string;
    }

    export interface GetDatabaseInstancesInstanceSettingPasswordValidationPolicy {
        complexity: string;
        disallowUsernameSubstring: boolean;
        enablePasswordPolicy: boolean;
        minLength: number;
        passwordChangeInterval: string;
        reuseInterval: number;
    }

    export interface GetDatabaseInstancesInstanceSettingSqlServerAuditConfig {
        bucket: string;
        retentionInterval: string;
        uploadInterval: string;
    }

    export interface GetDatabasesDatabase {
        charset: string;
        collation: string;
        deletionPolicy: string;
        /**
         * The name of the Cloud SQL database instance in which the database belongs.
         */
        instance: string;
        name: string;
        /**
         * The ID of the project in which the instance belongs.
         *
         * > **Note** This datasource performs client-side sorting to provide consistent ordering of the databases.
         */
        project: string;
        selfLink: string;
    }

    export interface GetTiersTier {
        /**
         * The maximum disk size of this tier in bytes.
         */
        diskQuota: number;
        /**
         * The maximum ram usage of this tier in bytes.
         */
        ram: number;
        /**
         * The applicable regions for this tier.
         */
        regions: string[];
        /**
         * An identifier for the machine type, for example, db-custom-1-3840.
         */
        tier: string;
    }

    export interface UserPasswordPolicy {
        /**
         * Number of failed attempts allowed before the user get locked.
         */
        allowedFailedAttempts?: number;
        /**
         * If true, the check that will lock user after too many failed login attempts will be enabled.
         */
        enableFailedAttemptsCheck?: boolean;
        /**
         * If true, the user must specify the current password before changing the password. This flag is supported only for MySQL.
         */
        enablePasswordVerification?: boolean;
        /**
         * Password expiration duration with one week grace period.
         */
        passwordExpirationDuration?: string;
        statuses: outputs.sql.UserPasswordPolicyStatus[];
    }

    export interface UserPasswordPolicyStatus {
        /**
         * If true, user does not have login privileges.
         */
        locked: boolean;
        /**
         * Password expiration duration with one week grace period.
         */
        passwordExpirationTime: string;
    }

    export interface UserSqlServerUserDetail {
        disabled: boolean;
        serverRoles: string[];
    }

}

export namespace storage {
    export interface BucketAutoclass {
        /**
         * While set to `true`, autoclass automatically transitions objects in your bucket to appropriate storage classes based on each object's access pattern.
         */
        enabled: boolean;
    }

    export interface BucketCor {
        /**
         * The value, in seconds, to return in the [Access-Control-Max-Age header](https://www.w3.org/TR/cors/#access-control-max-age-response-header) used in preflight responses.
         */
        maxAgeSeconds?: number;
        /**
         * The list of HTTP methods on which to include CORS response headers, (GET, OPTIONS, POST, etc) Note: "*" is permitted in the list of methods, and means "any method".
         */
        methods?: string[];
        /**
         * The list of [Origins](https://tools.ietf.org/html/rfc6454) eligible to receive CORS response headers. Note: "*" is permitted in the list of origins, and means "any Origin".
         */
        origins?: string[];
        /**
         * The list of HTTP headers other than the [simple response headers](https://www.w3.org/TR/cors/#simple-response-header) to give permission for the user-agent to share across domains.
         */
        responseHeaders?: string[];
    }

    export interface BucketCustomPlacementConfig {
        /**
         * The list of individual regions that comprise a dual-region bucket. See [Cloud Storage bucket locations](https://cloud.google.com/storage/docs/dual-regions#availability) for a list of acceptable regions. **Note**: If any of the dataLocations changes, it will [recreate the bucket](https://cloud.google.com/storage/docs/locations#key-concepts).
         */
        dataLocations: string[];
    }

    export interface BucketEncryption {
        /**
         * The `id` of a Cloud KMS key that will be used to encrypt objects inserted into this bucket, if no encryption method is specified.
         * You must pay attention to whether the crypto key is available in the location that this bucket is created in.
         * See [the docs](https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys) for more details.
         *
         * > As per [the docs](https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys) for customer-managed encryption keys, the IAM policy for the
         * specified key must permit the [automatic Google Cloud Storage service account](https://cloud.google.com/storage/docs/projects#service-accounts) for the bucket's
         * project to use the specified key for encryption and decryption operations.
         * Although the service account email address follows a well-known format, the service account is created on-demand and may not necessarily exist for your project
         * until a relevant action has occurred which triggers its creation.
         * You should use the [`gcp.storage.getProjectServiceAccount`](https://www.terraform.io/docs/providers/google/d/storage_project_service_account.html) data source to obtain the email
         * address for the service account when configuring IAM policy on the Cloud KMS key.
         * This data source calls an API which creates the account if required, ensuring your provider applies cleanly and repeatedly irrespective of the
         * state of the project.
         * You should take care for race conditions when the same provider manages IAM policy on the Cloud KMS crypto key. See the data source page for more details.
         */
        defaultKmsKeyName: string;
    }

    export interface BucketIAMBindingCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface BucketIAMMemberCondition {
        /**
         * An optional description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
         *
         * > **Warning:** This provider considers the `role` and condition contents (`title`+`description`+`expression`) as the
         * identifier for the binding. This means that if any part of the condition is changed out-of-band, the provider will
         * consider it to be an entirely different resource and will treat it as such.
         */
        description?: string;
        /**
         * Textual representation of an expression in Common Expression Language syntax.
         */
        expression: string;
        /**
         * A title for the expression, i.e. a short string describing its purpose.
         */
        title: string;
    }

    export interface BucketLifecycleRule {
        /**
         * The Lifecycle Rule's action configuration. A single block of this type is supported. Structure is documented below.
         */
        action: outputs.storage.BucketLifecycleRuleAction;
        /**
         * The Lifecycle Rule's condition configuration. A single block of this type is supported. Structure is documented below.
         */
        condition: outputs.storage.BucketLifecycleRuleCondition;
    }

    export interface BucketLifecycleRuleAction {
        /**
         * The target [Storage Class](https://cloud.google.com/storage/docs/storage-classes) of objects affected by this Lifecycle Rule. Supported values include: `STANDARD`, `MULTI_REGIONAL`, `REGIONAL`, `NEARLINE`, `COLDLINE`, `ARCHIVE`.
         */
        storageClass?: string;
        /**
         * The type of the action of this Lifecycle Rule. Supported values include: `Delete`, `SetStorageClass` and `AbortIncompleteMultipartUpload`.
         */
        type: string;
    }

    export interface BucketLifecycleRuleCondition {
        /**
         * Minimum age of an object in days to satisfy this condition.
         */
        age?: number;
        /**
         * A date in the RFC 3339 format YYYY-MM-DD. This condition is satisfied when an object is created before midnight of the specified date in UTC.
         */
        createdBefore?: string;
        /**
         * A date in the RFC 3339 format YYYY-MM-DD. This condition is satisfied when the customTime metadata for the object is set to an earlier date than the date used in this lifecycle condition.
         */
        customTimeBefore?: string;
        /**
         * Days since the date set in the `customTime` metadata for the object. This condition is satisfied when the current date and time is at least the specified number of days after the `customTime`.
         */
        daysSinceCustomTime?: number;
        /**
         * Relevant only for versioned objects. Number of days elapsed since the noncurrent timestamp of an object.
         */
        daysSinceNoncurrentTime?: number;
        /**
         * One or more matching name prefixes to satisfy this condition.
         */
        matchesPrefixes?: string[];
        /**
         * [Storage Class](https://cloud.google.com/storage/docs/storage-classes) of objects to satisfy this condition. Supported values include: `STANDARD`, `MULTI_REGIONAL`, `REGIONAL`, `NEARLINE`, `COLDLINE`, `ARCHIVE`, `DURABLE_REDUCED_AVAILABILITY`.
         */
        matchesStorageClasses?: string[];
        /**
         * One or more matching name suffixes to satisfy this condition.
         */
        matchesSuffixes?: string[];
        /**
         * Relevant only for versioned objects. The date in RFC 3339 (e.g. `2017-06-13`) when the object became nonconcurrent.
         */
        noncurrentTimeBefore?: string;
        /**
         * Relevant only for versioned objects. The number of newer versions of an object to satisfy this condition.
         */
        numNewerVersions?: number;
        /**
         * Match to live and/or archived objects. Unversioned buckets have only live objects. Supported values include: `"LIVE"`, `"ARCHIVED"`, `"ANY"`.
         */
        withState: string;
    }

    export interface BucketLogging {
        /**
         * The bucket that will receive log objects.
         */
        logBucket: string;
        /**
         * The object prefix for log objects. If it's not provided,
         * by default GCS sets this to this bucket's name.
         */
        logObjectPrefix: string;
    }

    export interface BucketObjectCustomerEncryption {
        /**
         * Encryption algorithm. Default: AES256
         */
        encryptionAlgorithm?: string;
        /**
         * Base64 encoded Customer-Supplied Encryption Key.
         */
        encryptionKey: string;
    }

    export interface BucketRetentionPolicy {
        /**
         * If set to `true`, the bucket will be [locked](https://cloud.google.com/storage/docs/using-bucket-lock#lock-bucket) and permanently restrict edits to the bucket's retention policy.  Caution: Locking a bucket is an irreversible action.
         */
        isLocked?: boolean;
        /**
         * The period of time, in seconds, that objects in the bucket must be retained and cannot be deleted, overwritten, or archived. The value must be less than 2,147,483,647 seconds.
         */
        retentionPeriod: number;
    }

    export interface BucketVersioning {
        /**
         * While set to `true`, versioning is fully enabled for this bucket.
         */
        enabled: boolean;
    }

    export interface BucketWebsite {
        /**
         * Behaves as the bucket's directory index where
         * missing objects are treated as potential directories.
         */
        mainPageSuffix?: string;
        /**
         * The custom object to return when a requested
         * resource is not found.
         */
        notFoundPage?: string;
    }

    export interface DefaultObjectAccessControlProjectTeam {
        /**
         * The project team associated with the entity
         */
        projectNumber?: string;
        /**
         * The team.
         * Possible values are: `editors`, `owners`, `viewers`.
         */
        team?: string;
    }

    export interface GetBucketAutoclass {
        enabled: boolean;
    }

    export interface GetBucketCor {
        maxAgeSeconds: number;
        methods: string[];
        origins: string[];
        responseHeaders: string[];
    }

    export interface GetBucketCustomPlacementConfig {
        dataLocations: string[];
    }

    export interface GetBucketEncryption {
        defaultKmsKeyName: string;
    }

    export interface GetBucketLifecycleRule {
        actions: outputs.storage.GetBucketLifecycleRuleAction[];
        conditions: outputs.storage.GetBucketLifecycleRuleCondition[];
    }

    export interface GetBucketLifecycleRuleAction {
        storageClass: string;
        type: string;
    }

    export interface GetBucketLifecycleRuleCondition {
        age: number;
        createdBefore: string;
        customTimeBefore: string;
        daysSinceCustomTime: number;
        daysSinceNoncurrentTime: number;
        matchesPrefixes: string[];
        matchesStorageClasses: string[];
        matchesSuffixes: string[];
        noncurrentTimeBefore: string;
        numNewerVersions: number;
        withState: string;
    }

    export interface GetBucketLogging {
        logBucket: string;
        logObjectPrefix: string;
    }

    export interface GetBucketObjectContentCustomerEncryption {
        encryptionAlgorithm: string;
        encryptionKey: string;
    }

    export interface GetBucketObjectCustomerEncryption {
        encryptionAlgorithm: string;
        encryptionKey: string;
    }

    export interface GetBucketRetentionPolicy {
        isLocked: boolean;
        retentionPeriod: number;
    }

    export interface GetBucketVersioning {
        enabled: boolean;
    }

    export interface GetBucketWebsite {
        mainPageSuffix: string;
        notFoundPage: string;
    }

    export interface InsightsReportConfigCsvOptions {
        /**
         * The delimiter used to separate the fields in the inventory report CSV file.
         */
        delimiter?: string;
        /**
         * The boolean that indicates whether or not headers are included in the inventory report CSV file.
         *
         * - - -
         */
        headerRequired?: boolean;
        /**
         * The character used to separate the records in the inventory report CSV file.
         */
        recordSeparator?: string;
    }

    export interface InsightsReportConfigFrequencyOptions {
        /**
         * The date to stop generating inventory reports. For example, {"day": 15, "month": 9, "year": 2022}.
         * Structure is documented below.
         */
        endDate: outputs.storage.InsightsReportConfigFrequencyOptionsEndDate;
        /**
         * The frequency in which inventory reports are generated. Values are DAILY or WEEKLY.
         * Possible values are: `DAILY`, `WEEKLY`.
         */
        frequency: string;
        /**
         * The date to start generating inventory reports. For example, {"day": 15, "month": 8, "year": 2022}.
         * Structure is documented below.
         */
        startDate: outputs.storage.InsightsReportConfigFrequencyOptionsStartDate;
    }

    export interface InsightsReportConfigFrequencyOptionsEndDate {
        /**
         * The day of the month to stop generating inventory reports.
         */
        day: number;
        /**
         * The month to stop generating inventory reports.
         */
        month: number;
        /**
         * The year to stop generating inventory reports
         */
        year: number;
    }

    export interface InsightsReportConfigFrequencyOptionsStartDate {
        /**
         * The day of the month to start generating inventory reports.
         */
        day: number;
        /**
         * The month to start generating inventory reports.
         */
        month: number;
        /**
         * The year to start generating inventory reports
         */
        year: number;
    }

    export interface InsightsReportConfigObjectMetadataReportOptions {
        /**
         * The metadata fields included in an inventory report.
         */
        metadataFields: string[];
        /**
         * Options for where the inventory reports are stored.
         * Structure is documented below.
         */
        storageDestinationOptions: outputs.storage.InsightsReportConfigObjectMetadataReportOptionsStorageDestinationOptions;
        /**
         * A nested object resource
         * Structure is documented below.
         */
        storageFilters?: outputs.storage.InsightsReportConfigObjectMetadataReportOptionsStorageFilters;
    }

    export interface InsightsReportConfigObjectMetadataReportOptionsStorageDestinationOptions {
        /**
         * The destination bucket that stores the generated inventory reports.
         */
        bucket: string;
        /**
         * The path within the destination bucket to store generated inventory reports.
         */
        destinationPath?: string;
    }

    export interface InsightsReportConfigObjectMetadataReportOptionsStorageFilters {
        /**
         * The filter to use when specifying which bucket to generate inventory reports for.
         */
        bucket?: string;
    }

    export interface ObjectAccessControlProjectTeam {
        /**
         * The project team associated with the entity
         */
        projectNumber?: string;
        /**
         * The team.
         * Possible values are: `editors`, `owners`, `viewers`.
         */
        team?: string;
    }

    export interface TransferAgentPoolBandwidthLimit {
        /**
         * Bandwidth rate in megabytes per second, distributed across all the agents in the pool.
         */
        limitMbps: string;
    }

    export interface TransferJobNotificationConfig {
        /**
         * Event types for which a notification is desired. If empty, send notifications for all event types. The valid types are "TRANSFER_OPERATION_SUCCESS", "TRANSFER_OPERATION_FAILED", "TRANSFER_OPERATION_ABORTED".
         */
        eventTypes?: string[];
        /**
         * The desired format of the notification message payloads. One of "NONE" or "JSON".
         */
        payloadFormat: string;
        /**
         * The Topic.name of the Pub/Sub topic to which to publish notifications. Must be of the format: projects/{project}/topics/{topic}. Not matching this format results in an INVALID_ARGUMENT error.
         */
        pubsubTopic: string;
    }

    export interface TransferJobSchedule {
        /**
         * Interval between the start of each scheduled transfer. If unspecified, the default value is 24 hours. This value may not be less than 1 hour. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        repeatInterval?: string;
        /**
         * The last day the recurring transfer will be run. If `scheduleEndDate` is the same as `scheduleStartDate`, the transfer will be executed only once. Structure documented below.
         */
        scheduleEndDate?: outputs.storage.TransferJobScheduleScheduleEndDate;
        /**
         * The first day the recurring transfer is scheduled to run. If `scheduleStartDate` is in the past, the transfer will run for the first time on the following day. Structure documented below.
         */
        scheduleStartDate: outputs.storage.TransferJobScheduleScheduleStartDate;
        /**
         * The time in UTC at which the transfer will be scheduled to start in a day. Transfers may start later than this time. If not specified, recurring and one-time transfers that are scheduled to run today will run immediately; recurring transfers that are scheduled to run on a future date will start at approximately midnight UTC on that date. Note that when configuring a transfer with the Cloud Platform Console, the transfer's start time in a day is specified in your local timezone. Structure documented below.
         */
        startTimeOfDay?: outputs.storage.TransferJobScheduleStartTimeOfDay;
    }

    export interface TransferJobScheduleScheduleEndDate {
        /**
         * Day of month. Must be from 1 to 31 and valid for the year and month.
         *
         * <a name="nestedStartTimeOfDay"></a>The `startTimeOfDay` blocks support:
         */
        day: number;
        /**
         * Month of year. Must be from 1 to 12.
         */
        month: number;
        /**
         * Year of date. Must be from 1 to 9999.
         */
        year: number;
    }

    export interface TransferJobScheduleScheduleStartDate {
        /**
         * Day of month. Must be from 1 to 31 and valid for the year and month.
         *
         * <a name="nestedStartTimeOfDay"></a>The `startTimeOfDay` blocks support:
         */
        day: number;
        /**
         * Month of year. Must be from 1 to 12.
         */
        month: number;
        /**
         * Year of date. Must be from 1 to 9999.
         */
        year: number;
    }

    export interface TransferJobScheduleStartTimeOfDay {
        /**
         * Hours of day in 24 hour format. Should be from 0 to 23
         */
        hours: number;
        /**
         * Minutes of hour of day. Must be from 0 to 59.
         */
        minutes: number;
        /**
         * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
         */
        nanos: number;
        /**
         * Seconds of minutes of the time. Must normally be from 0 to 59.
         */
        seconds: number;
    }

    export interface TransferJobTransferSpec {
        /**
         * An AWS S3 data source. Structure documented below.
         */
        awsS3DataSource?: outputs.storage.TransferJobTransferSpecAwsS3DataSource;
        /**
         * An Azure Blob Storage data source. Structure documented below.
         */
        azureBlobStorageDataSource?: outputs.storage.TransferJobTransferSpecAzureBlobStorageDataSource;
        /**
         * A Google Cloud Storage data sink. Structure documented below.
         */
        gcsDataSink?: outputs.storage.TransferJobTransferSpecGcsDataSink;
        /**
         * A Google Cloud Storage data source. Structure documented below.
         */
        gcsDataSource?: outputs.storage.TransferJobTransferSpecGcsDataSource;
        /**
         * A HTTP URL data source. Structure documented below.
         */
        httpDataSource?: outputs.storage.TransferJobTransferSpecHttpDataSource;
        /**
         * Only objects that satisfy these object conditions are included in the set of data source and data sink objects. Object conditions based on objects' `lastModificationTime` do not exclude objects in a data sink. Structure documented below.
         */
        objectConditions?: outputs.storage.TransferJobTransferSpecObjectConditions;
        /**
         * A POSIX data sink. Structure documented below.
         */
        posixDataSink?: outputs.storage.TransferJobTransferSpecPosixDataSink;
        /**
         * A POSIX filesystem data source. Structure documented below.
         */
        posixDataSource?: outputs.storage.TransferJobTransferSpecPosixDataSource;
        /**
         * Specifies the agent pool name associated with the posix data sink. When unspecified, the default name is used.
         */
        sinkAgentPoolName: string;
        /**
         * Specifies the agent pool name associated with the posix data source. When unspecified, the default name is used.
         */
        sourceAgentPoolName: string;
        /**
         * Characteristics of how to treat files from datasource and sink during job. If the option `deleteObjectsUniqueInSink` is true, object conditions based on objects' `lastModificationTime` are ignored and do not exclude objects in a data source or a data sink. Structure documented below.
         */
        transferOptions?: outputs.storage.TransferJobTransferSpecTransferOptions;
    }

    export interface TransferJobTransferSpecAwsS3DataSource {
        /**
         * AWS credentials block.
         */
        awsAccessKey?: outputs.storage.TransferJobTransferSpecAwsS3DataSourceAwsAccessKey;
        /**
         * Google Cloud Storage bucket name.
         */
        bucketName: string;
        /**
         * Root path to transfer objects. Must be an empty string or full path name that ends with a '/'. This field is treated as an object prefix. As such, it should generally not begin with a '/'.
         */
        path?: string;
        /**
         * The Amazon Resource Name (ARN) of the role to support temporary credentials via 'AssumeRoleWithWebIdentity'. For more information about ARNs, see [IAM ARNs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns). When a role ARN is provided, Transfer Service fetches temporary credentials for the session using a 'AssumeRoleWithWebIdentity' call for the provided role using the [GoogleServiceAccount][] for this project.
         */
        roleArn?: string;
    }

    export interface TransferJobTransferSpecAwsS3DataSourceAwsAccessKey {
        /**
         * AWS Key ID.
         */
        accessKeyId: string;
        /**
         * AWS Secret Access Key.
         */
        secretAccessKey: string;
    }

    export interface TransferJobTransferSpecAzureBlobStorageDataSource {
        /**
         * Credentials used to authenticate API requests to Azure block.
         */
        azureCredentials: outputs.storage.TransferJobTransferSpecAzureBlobStorageDataSourceAzureCredentials;
        /**
         * The container to transfer from the Azure Storage account.`
         */
        container: string;
        /**
         * Root path to transfer objects. Must be an empty string or full path name that ends with a '/'. This field is treated as an object prefix. As such, it should generally not begin with a '/'.
         */
        path: string;
        /**
         * The name of the Azure Storage account.
         */
        storageAccount: string;
    }

    export interface TransferJobTransferSpecAzureBlobStorageDataSourceAzureCredentials {
        /**
         * Azure shared access signature. See [Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview).
         *
         * <a name="nestedScheduleStartEndDate"></a>The `scheduleStartDate` and `scheduleEndDate` blocks support:
         */
        sasToken: string;
    }

    export interface TransferJobTransferSpecGcsDataSink {
        /**
         * Google Cloud Storage bucket name.
         */
        bucketName: string;
        /**
         * Root path to transfer objects. Must be an empty string or full path name that ends with a '/'. This field is treated as an object prefix. As such, it should generally not begin with a '/'.
         */
        path: string;
    }

    export interface TransferJobTransferSpecGcsDataSource {
        /**
         * Google Cloud Storage bucket name.
         */
        bucketName: string;
        /**
         * Root path to transfer objects. Must be an empty string or full path name that ends with a '/'. This field is treated as an object prefix. As such, it should generally not begin with a '/'.
         */
        path: string;
    }

    export interface TransferJobTransferSpecHttpDataSource {
        /**
         * The URL that points to the file that stores the object list entries. This file must allow public access. Currently, only URLs with HTTP and HTTPS schemes are supported.
         */
        listUrl: string;
    }

    export interface TransferJobTransferSpecObjectConditions {
        /**
         * `excludePrefixes` must follow the requirements described for `includePrefixes`. See [Requirements](https://cloud.google.com/storage-transfer/docs/reference/rest/v1/TransferSpec#ObjectConditions).
         */
        excludePrefixes?: string[];
        /**
         * If `includePrefixes` is specified, objects that satisfy the object conditions must have names that start with one of the `includePrefixes` and that do not start with any of the `excludePrefixes`. If `includePrefixes` is not specified, all objects except those that have names starting with one of the `excludePrefixes` must satisfy the object conditions. See [Requirements](https://cloud.google.com/storage-transfer/docs/reference/rest/v1/TransferSpec#ObjectConditions).
         */
        includePrefixes?: string[];
        /**
         * If specified, only objects with a "last modification time" before this timestamp and objects that don't have a "last modification time" are transferred. A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        lastModifiedBefore?: string;
        /**
         * If specified, only objects with a "last modification time" on or after this timestamp and objects that don't have a "last modification time" are transferred. A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
         */
        lastModifiedSince?: string;
        /**
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        maxTimeElapsedSinceLastModification?: string;
        /**
         * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
         */
        minTimeElapsedSinceLastModification?: string;
    }

    export interface TransferJobTransferSpecPosixDataSink {
        /**
         * Root directory path to the filesystem.
         */
        rootDirectory: string;
    }

    export interface TransferJobTransferSpecPosixDataSource {
        /**
         * Root directory path to the filesystem.
         *
         * <a name="nestedAwsS3DataSource"></a>The `awsS3DataSource` block supports:
         */
        rootDirectory: string;
    }

    export interface TransferJobTransferSpecTransferOptions {
        /**
         * Whether objects should be deleted from the source after they are transferred to the sink. Note that this option and `deleteObjectsUniqueInSink` are mutually exclusive.
         */
        deleteObjectsFromSourceAfterTransfer?: boolean;
        /**
         * Whether objects that exist only in the sink should be deleted. Note that this option and
         * `deleteObjectsFromSourceAfterTransfer` are mutually exclusive.
         */
        deleteObjectsUniqueInSink?: boolean;
        /**
         * Whether overwriting objects that already exist in the sink is allowed.
         */
        overwriteObjectsAlreadyExistingInSink?: boolean;
        /**
         * When to overwrite objects that already exist in the sink. If not set, overwrite behavior is determined by `overwriteObjectsAlreadyExistingInSink`. Possible values: ALWAYS, DIFFERENT, NEVER.
         */
        overwriteWhen?: string;
    }

}

export namespace tags {
    export interface TagKeyIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TagKeyIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TagValueIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface TagValueIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}

export namespace tpu {
    export interface NodeNetworkEndpoint {
        /**
         * (Output)
         * The IP address of this network endpoint.
         */
        ipAddress: string;
        /**
         * (Output)
         * The port of this network endpoint.
         */
        port: number;
    }

    export interface NodeSchedulingConfig {
        /**
         * Defines whether the TPU instance is preemptible.
         */
        preemptible: boolean;
    }

}

export namespace vertex {
    export interface AiDatasetEncryptionSpec {
        /**
         * Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
         * Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
         */
        kmsKeyName?: string;
    }

    export interface AiEndpointDeployedModel {
        /**
         * (Output)
         * A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
         * Structure is documented below.
         */
        automaticResources: outputs.vertex.AiEndpointDeployedModelAutomaticResource[];
        /**
         * (Output)
         * Output only. Timestamp when the DeployedModel was created.
         */
        createTime: string;
        /**
         * (Output)
         * A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
         * Structure is documented below.
         */
        dedicatedResources: outputs.vertex.AiEndpointDeployedModelDedicatedResource[];
        /**
         * Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
         */
        displayName: string;
        /**
         * (Output)
         * These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
         */
        enableAccessLogging: boolean;
        /**
         * (Output)
         * If true, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
         */
        enableContainerLogging: boolean;
        /**
         * (Output)
         * The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
         */
        id: string;
        /**
         * (Output)
         * The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
         */
        model: string;
        /**
         * (Output)
         * Output only. The version ID of the model that is deployed.
         */
        modelVersionId: string;
        /**
         * (Output)
         * Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
         * Structure is documented below.
         */
        privateEndpoints: outputs.vertex.AiEndpointDeployedModelPrivateEndpoint[];
        /**
         * (Output)
         * The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
         */
        serviceAccount: string;
        /**
         * (Output)
         * The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
         */
        sharedResources: string;
    }

    export interface AiEndpointDeployedModelAutomaticResource {
        /**
         * (Output)
         * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
         */
        maxReplicaCount: number;
        /**
         * (Output)
         * The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
         */
        minReplicaCount: number;
    }

    export interface AiEndpointDeployedModelDedicatedResource {
        /**
         * (Output)
         * The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
         * Structure is documented below.
         */
        autoscalingMetricSpecs: outputs.vertex.AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec[];
        /**
         * (Output)
         * The specification of a single machine used by the prediction.
         * Structure is documented below.
         */
        machineSpecs: outputs.vertex.AiEndpointDeployedModelDedicatedResourceMachineSpec[];
        /**
         * (Output)
         * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
         */
        maxReplicaCount: number;
        /**
         * (Output)
         * The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
         */
        minReplicaCount: number;
    }

    export interface AiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec {
        /**
         * (Output)
         * The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
         */
        metricName: string;
        /**
         * (Output)
         * The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
         */
        target: number;
    }

    export interface AiEndpointDeployedModelDedicatedResourceMachineSpec {
        /**
         * (Output)
         * The number of accelerators to attach to the machine.
         */
        acceleratorCount: number;
        /**
         * (Output)
         * The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
         */
        acceleratorType: string;
        /**
         * (Output)
         * The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO(rsurowka): Try to better unify the required vs optional.
         */
        machineType: string;
    }

    export interface AiEndpointDeployedModelPrivateEndpoint {
        /**
         * (Output)
         * Output only. Http(s) path to send explain requests.
         */
        explainHttpUri: string;
        /**
         * (Output)
         * Output only. Http(s) path to send health check requests.
         */
        healthHttpUri: string;
        /**
         * (Output)
         * Output only. Http(s) path to send prediction requests.
         */
        predictHttpUri: string;
        /**
         * (Output)
         * Output only. The name of the service attachment resource. Populated if private service connect is enabled.
         */
        serviceAttachment: string;
    }

    export interface AiEndpointEncryptionSpec {
        /**
         * Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
         */
        kmsKeyName: string;
    }

    export interface AiFeatureStoreEncryptionSpec {
        /**
         * The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
         */
        kmsKeyName: string;
    }

    export interface AiFeatureStoreEntityTypeIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AiFeatureStoreEntityTypeIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AiFeatureStoreEntityTypeMonitoringConfig {
        /**
         * Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
         * Structure is documented below.
         */
        categoricalThresholdConfig?: outputs.vertex.AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfig;
        /**
         * The config for ImportFeatures Analysis Based Feature Monitoring.
         * Structure is documented below.
         */
        importFeaturesAnalysis?: outputs.vertex.AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis;
        /**
         * Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
         * Structure is documented below.
         */
        numericalThresholdConfig?: outputs.vertex.AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfig;
        /**
         * The config for Snapshot Analysis Based Feature Monitoring.
         * Structure is documented below.
         */
        snapshotAnalysis?: outputs.vertex.AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis;
    }

    export interface AiFeatureStoreEntityTypeMonitoringConfigCategoricalThresholdConfig {
        /**
         * Specify a threshold value that can trigger the alert. For categorical feature, the distribution distance is calculated by L-inifinity norm. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
         */
        value: number;
    }

    export interface AiFeatureStoreEntityTypeMonitoringConfigImportFeaturesAnalysis {
        /**
         * Defines the baseline to do anomaly detection for feature values imported by each [entityTypes.importFeatureValues][] operation. The value must be one of the values below:
         * * LATEST_STATS: Choose the later one statistics generated by either most recent snapshot analysis or previous import features analysis. If non of them exists, skip anomaly detection and only generate a statistics.
         * * MOST_RECENT_SNAPSHOT_STATS: Use the statistics generated by the most recent snapshot analysis if exists.
         * * PREVIOUS_IMPORT_FEATURES_STATS: Use the statistics generated by the previous import features analysis if exists.
         */
        anomalyDetectionBaseline?: string;
        /**
         * Whether to enable / disable / inherite default hebavior for import features analysis. The value must be one of the values below:
         * * DEFAULT: The default behavior of whether to enable the monitoring. EntityType-level config: disabled.
         * * ENABLED: Explicitly enables import features analysis. EntityType-level config: by default enables import features analysis for all Features under it.
         * * DISABLED: Explicitly disables import features analysis. EntityType-level config: by default disables import features analysis for all Features under it.
         */
        state?: string;
    }

    export interface AiFeatureStoreEntityTypeMonitoringConfigNumericalThresholdConfig {
        /**
         * Specify a threshold value that can trigger the alert. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
         */
        value: number;
    }

    export interface AiFeatureStoreEntityTypeMonitoringConfigSnapshotAnalysis {
        /**
         * The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoringInterval for Features under it.
         */
        disabled?: boolean;
        /**
         * @deprecated `monitoring_interval` is deprecated and will be removed in a future release.
         */
        monitoringInterval: string;
        /**
         * Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days. The default value is 1.
         * If both FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days and [FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval][] are set when creating/updating EntityTypes/Features, FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days will be used.
         */
        monitoringIntervalDays?: number;
        /**
         * Customized export features time window for snapshot analysis. Unit is one day. The default value is 21 days. Minimum value is 1 day. Maximum value is 4000 days.
         */
        stalenessDays?: number;
    }

    export interface AiFeatureStoreIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AiFeatureStoreIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface AiFeatureStoreOnlineServingConfig {
        /**
         * The number of nodes for each cluster. The number of nodes will not scale automatically but can be scaled manually by providing different values when updating.
         */
        fixedNodeCount?: number;
        /**
         * Online serving scaling configuration. Only one of fixedNodeCount and scaling can be set. Setting one will reset the other.
         * Structure is documented below.
         */
        scaling?: outputs.vertex.AiFeatureStoreOnlineServingConfigScaling;
    }

    export interface AiFeatureStoreOnlineServingConfigScaling {
        /**
         * The maximum number of nodes to scale up to. Must be greater than minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
         */
        maxNodeCount: number;
        /**
         * The minimum number of nodes to scale down to. Must be greater than or equal to 1.
         */
        minNodeCount: number;
    }

    export interface AiIndexDeployedIndex {
        /**
         * (Output)
         * The ID of the DeployedIndex in the above IndexEndpoint.
         */
        deployedIndexId: string;
        /**
         * (Output)
         * A resource name of the IndexEndpoint.
         */
        indexEndpoint: string;
    }

    export interface AiIndexIndexStat {
        /**
         * (Output)
         * The number of shards in the Index.
         */
        shardsCount: number;
        /**
         * (Output)
         * The number of vectors in the Index.
         */
        vectorsCount: string;
    }

    export interface AiIndexMetadata {
        /**
         * The configuration of the Matching Engine Index.
         * Structure is documented below.
         */
        config?: outputs.vertex.AiIndexMetadataConfig;
        /**
         * Allows inserting, updating  or deleting the contents of the Matching Engine Index.
         * The string must be a valid Cloud Storage directory path. If this
         * field is set when calling IndexService.UpdateIndex, then no other
         * Index field can be also updated as part of the same call.
         * The expected structure and format of the files this URI points to is
         * described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
         */
        contentsDeltaUri?: string;
        /**
         * If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
         * then existing content of the Index will be replaced by the data from the contentsDeltaUri.
         */
        isCompleteOverwrite?: boolean;
    }

    export interface AiIndexMetadataConfig {
        /**
         * The configuration with regard to the algorithms used for efficient search.
         * Structure is documented below.
         */
        algorithmConfig?: outputs.vertex.AiIndexMetadataConfigAlgorithmConfig;
        /**
         * The default number of neighbors to find via approximate search before exact reordering is
         * performed. Exact reordering is a procedure where results returned by an
         * approximate search algorithm are reordered via a more expensive distance computation.
         * Required if tree-AH algorithm is used.
         */
        approximateNeighborsCount?: number;
        /**
         * The number of dimensions of the input vectors.
         */
        dimensions: number;
        /**
         * The distance measure used in nearest neighbor search. The value must be one of the followings:
         * * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
         * * L1_DISTANCE: Manhattan (L_1) Distance
         * * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
         * * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
         */
        distanceMeasureType?: string;
        /**
         * Type of normalization to be carried out on each vector. The value must be one of the followings:
         * * UNIT_L2_NORM: Unit L2 normalization type
         * * NONE: No normalization type is specified.
         */
        featureNormType?: string;
        /**
         * Index data is split into equal parts to be processed. These are called "shards".
         * The shard size must be specified when creating an index. The value must be one of the followings:
         * * SHARD_SIZE_SMALL: Small (2GB)
         * * SHARD_SIZE_MEDIUM: Medium (20GB)
         * * SHARD_SIZE_LARGE: Large (50GB)
         */
        shardSize: string;
    }

    export interface AiIndexMetadataConfigAlgorithmConfig {
        /**
         * Configuration options for using brute force search, which simply implements the
         * standard linear search in the database for each query.
         */
        bruteForceConfig?: outputs.vertex.AiIndexMetadataConfigAlgorithmConfigBruteForceConfig;
        /**
         * Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
         * Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
         * Structure is documented below.
         */
        treeAhConfig?: outputs.vertex.AiIndexMetadataConfigAlgorithmConfigTreeAhConfig;
    }

    export interface AiIndexMetadataConfigAlgorithmConfigBruteForceConfig {
    }

    export interface AiIndexMetadataConfigAlgorithmConfigTreeAhConfig {
        /**
         * Number of embeddings on each leaf node. The default value is 1000 if not set.
         */
        leafNodeEmbeddingCount?: number;
        /**
         * The default percentage of leaf nodes that any query may be searched. Must be in
         * range 1-100, inclusive. The default value is 10 (means 10%) if not set.
         */
        leafNodesToSearchPercent?: number;
    }

    export interface AiMetadataStoreEncryptionSpec {
        /**
         * Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
         * Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
         */
        kmsKeyName?: string;
    }

    export interface AiMetadataStoreState {
        /**
         * (Output)
         * The disk utilization of the MetadataStore in bytes.
         */
        diskUtilizationBytes: string;
    }

    export interface AiTensorboardEncryptionSpec {
        /**
         * The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
         * Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
         */
        kmsKeyName: string;
    }

    export interface GetAiIndexDeployedIndex {
        deployedIndexId: string;
        indexEndpoint: string;
    }

    export interface GetAiIndexIndexStat {
        shardsCount: number;
        vectorsCount: string;
    }

    export interface GetAiIndexMetadata {
        configs: outputs.vertex.GetAiIndexMetadataConfig[];
        contentsDeltaUri: string;
        isCompleteOverwrite: boolean;
    }

    export interface GetAiIndexMetadataConfig {
        algorithmConfigs: outputs.vertex.GetAiIndexMetadataConfigAlgorithmConfig[];
        approximateNeighborsCount: number;
        dimensions: number;
        distanceMeasureType: string;
        featureNormType: string;
        shardSize: string;
    }

    export interface GetAiIndexMetadataConfigAlgorithmConfig {
        bruteForceConfigs: outputs.vertex.GetAiIndexMetadataConfigAlgorithmConfigBruteForceConfig[];
        treeAhConfigs: outputs.vertex.GetAiIndexMetadataConfigAlgorithmConfigTreeAhConfig[];
    }

    export interface GetAiIndexMetadataConfigAlgorithmConfigBruteForceConfig {
    }

    export interface GetAiIndexMetadataConfigAlgorithmConfigTreeAhConfig {
        leafNodeEmbeddingCount: number;
        leafNodesToSearchPercent: number;
    }

}

export namespace vmwareengine {
    export interface ClusterNodeTypeConfig {
        /**
         * Customized number of cores available to each node of the type.
         * This number must always be one of `nodeType.availableCustomCoreCounts`.
         * If zero is provided max value from `nodeType.availableCustomCoreCounts` will be used.
         * Once the customer is created then corecount cannot be changed.
         */
        customCoreCount?: number;
        /**
         * The number of nodes of this type in the cluster.
         */
        nodeCount: number;
        /**
         * The identifier for this object. Format specified above.
         */
        nodeTypeId: string;
    }

    export interface GetClusterNodeTypeConfig {
        customCoreCount: number;
        nodeCount: number;
        nodeTypeId: string;
    }

    export interface GetNetworkVpcNetwork {
        network: string;
        type: string;
    }

    export interface GetPrivateCloudHcx {
        fqdn: string;
        internalIp: string;
        state: string;
        version: string;
    }

    export interface GetPrivateCloudManagementCluster {
        clusterId: string;
        nodeTypeConfigs: outputs.vmwareengine.GetPrivateCloudManagementClusterNodeTypeConfig[];
    }

    export interface GetPrivateCloudManagementClusterNodeTypeConfig {
        customCoreCount: number;
        nodeCount: number;
        nodeTypeId: string;
    }

    export interface GetPrivateCloudNetworkConfig {
        managementCidr: string;
        managementIpAddressLayoutVersion: number;
        vmwareEngineNetwork: string;
        vmwareEngineNetworkCanonical: string;
    }

    export interface GetPrivateCloudNsx {
        fqdn: string;
        internalIp: string;
        state: string;
        version: string;
    }

    export interface GetPrivateCloudVcenter {
        fqdn: string;
        internalIp: string;
        state: string;
        version: string;
    }

    export interface NetworkVpcNetwork {
        /**
         * (Output)
         * The relative resource name of the service VPC network this VMware Engine network is attached to.
         * For example: projects/123123/global/networks/my-network
         */
        network: string;
        /**
         * VMware Engine network type.
         * Possible values are: `LEGACY`.
         */
        type: string;
    }

    export interface PrivateCloudHcx {
        /**
         * Fully qualified domain name of the appliance.
         */
        fqdn?: string;
        /**
         * Internal IP address of the appliance.
         */
        internalIp?: string;
        /**
         * State of the appliance.
         * Possible values are: `ACTIVE`, `CREATING`.
         */
        state?: string;
        /**
         * Version of the appliance.
         */
        version?: string;
    }

    export interface PrivateCloudManagementCluster {
        /**
         * The user-provided identifier of the new Cluster. The identifier must meet the following requirements:
         * * Only contains 1-63 alphanumeric characters and hyphens
         * * Begins with an alphabetical character
         * * Ends with a non-hyphen character
         * * Not formatted as a UUID
         * * Complies with RFC 1034 (https://datatracker.ietf.org/doc/html/rfc1034) (section 3.5)
         */
        clusterId: string;
        /**
         * The map of cluster node types in this cluster,
         * where the key is canonical identifier of the node type (corresponds to the NodeType).
         * Structure is documented below.
         */
        nodeTypeConfigs?: outputs.vmwareengine.PrivateCloudManagementClusterNodeTypeConfig[];
    }

    export interface PrivateCloudManagementClusterNodeTypeConfig {
        /**
         * Customized number of cores available to each node of the type.
         * This number must always be one of `nodeType.availableCustomCoreCounts`.
         * If zero is provided max value from `nodeType.availableCustomCoreCounts` will be used.
         * This cannot be changed once the PrivateCloud is created.
         *
         * - - -
         */
        customCoreCount?: number;
        /**
         * The number of nodes of this type in the cluster.
         */
        nodeCount: number;
        /**
         * The identifier for this object. Format specified above.
         */
        nodeTypeId: string;
    }

    export interface PrivateCloudNetworkConfig {
        /**
         * Management CIDR used by VMware management appliances.
         */
        managementCidr: string;
        /**
         * (Output)
         * The IP address layout version of the management IP address range.
         * Possible versions include:
         * * managementIpAddressLayoutVersion=1: Indicates the legacy IP address layout used by some existing private clouds. This is no longer supported for new private clouds
         * as it does not support all features.
         * * managementIpAddressLayoutVersion=2: Indicates the latest IP address layout
         * used by all newly created private clouds. This version supports all current features.
         */
        managementIpAddressLayoutVersion: number;
        /**
         * The relative resource name of the VMware Engine network attached to the private cloud.
         * Specify the name in the following form: projects/{project}/locations/{location}/vmwareEngineNetworks/{vmwareEngineNetworkId}
         * where {project} can either be a project number or a project ID.
         */
        vmwareEngineNetwork?: string;
        /**
         * (Output)
         * The canonical name of the VMware Engine network in
         * the form: projects/{project_number}/locations/{location}/vmwareEngineNetworks/{vmwareEngineNetworkId}
         */
        vmwareEngineNetworkCanonical: string;
    }

    export interface PrivateCloudNsx {
        /**
         * Fully qualified domain name of the appliance.
         */
        fqdn?: string;
        /**
         * Internal IP address of the appliance.
         */
        internalIp?: string;
        /**
         * State of the appliance.
         * Possible values are: `ACTIVE`, `CREATING`.
         */
        state?: string;
        /**
         * Version of the appliance.
         */
        version?: string;
    }

    export interface PrivateCloudVcenter {
        /**
         * Fully qualified domain name of the appliance.
         */
        fqdn?: string;
        /**
         * Internal IP address of the appliance.
         */
        internalIp?: string;
        /**
         * State of the appliance.
         * Possible values are: `ACTIVE`, `CREATING`.
         */
        state?: string;
        /**
         * Version of the appliance.
         */
        version?: string;
    }

}

export namespace vpcaccess {
    export interface ConnectorSubnet {
        /**
         * Subnet name (relative, not fully qualified). E.g. if the full subnet selfLink is
         * https://compute.googleapis.com/compute/v1/projects/{project}/regions/{region}/subnetworks/{subnetName} the correct input for this field would be {subnetName}"
         */
        name?: string;
        /**
         * Project in which the subnet exists. If not set, this project is assumed to be the project for which the connector create request was issued.
         */
        projectId: string;
    }

    export interface GetConnectorSubnet {
        /**
         * Name of the resource.
         *
         * - - -
         */
        name: string;
        projectId: string;
    }

}

export namespace workstations {
    export interface WorkstationClusterCondition {
        /**
         * (Output)
         * The status code, which should be an enum value of google.rpc.Code.
         */
        code: number;
        /**
         * (Output)
         * A list of messages that carry the error details.
         */
        details: {[key: string]: any}[];
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
    }

    export interface WorkstationClusterPrivateClusterConfig {
        /**
         * Additional project IDs that are allowed to attach to the workstation cluster's service attachment.
         * By default, the workstation cluster's project and the VPC host project (if different) are allowed.
         */
        allowedProjects: string[];
        /**
         * (Output)
         * Hostname for the workstation cluster.
         * This field will be populated only when private endpoint is enabled.
         * To access workstations in the cluster, create a new DNS zone mapping this domain name to an internal IP address and a forwarding rule mapping that address to the service attachment.
         */
        clusterHostname: string;
        /**
         * Whether Workstations endpoint is private.
         */
        enablePrivateEndpoint: boolean;
        /**
         * (Output)
         * Service attachment URI for the workstation cluster.
         * The service attachment is created when private endpoint is enabled.
         * To access workstations in the cluster, configure access to the managed service using (Private Service Connect)[https://cloud.google.com/vpc/docs/configure-private-service-connect-services].
         */
        serviceAttachmentUri: string;
    }

    export interface WorkstationConfigCondition {
        /**
         * (Output)
         * The status code, which should be an enum value of google.rpc.Code.
         */
        code: number;
        /**
         * (Output)
         * A list of messages that carry the error details.
         */
        details: {[key: string]: any}[];
        /**
         * (Output)
         * Human readable message indicating details about the current status.
         */
        message: string;
    }

    export interface WorkstationConfigContainer {
        /**
         * Arguments passed to the entrypoint.
         */
        args?: string[];
        /**
         * If set, overrides the default ENTRYPOINT specified by the image.
         */
        commands?: string[];
        /**
         * Environment variables passed to the container.
         * The elements are of the form "KEY=VALUE" for the environment variable "KEY" being given the value "VALUE".
         */
        env?: {[key: string]: string};
        /**
         * Docker image defining the container. This image must be accessible by the config's service account.
         */
        image: string;
        /**
         * If set, overrides the USER specified in the image with the given uid.
         */
        runAsUser?: number;
        /**
         * If set, overrides the default DIR specified by the image.
         */
        workingDir?: string;
    }

    export interface WorkstationConfigEncryptionKey {
        /**
         * The name of the Google Cloud KMS encryption key.
         */
        kmsKey: string;
        /**
         * The service account to use with the specified KMS key.
         */
        kmsKeyServiceAccount: string;
    }

    export interface WorkstationConfigHost {
        /**
         * A runtime using a Compute Engine instance.
         * Structure is documented below.
         */
        gceInstance: outputs.workstations.WorkstationConfigHostGceInstance;
    }

    export interface WorkstationConfigHostGceInstance {
        /**
         * An accelerator card attached to the instance.
         * Structure is documented below.
         */
        accelerators?: outputs.workstations.WorkstationConfigHostGceInstanceAccelerator[];
        /**
         * Size of the boot disk in GB.
         */
        bootDiskSizeGb: number;
        /**
         * A set of Compute Engine Confidential VM instance options.
         * Structure is documented below.
         */
        confidentialInstanceConfig: outputs.workstations.WorkstationConfigHostGceInstanceConfidentialInstanceConfig;
        /**
         * Whether instances have no public IP address.
         */
        disablePublicIpAddresses?: boolean;
        /**
         * Whether to enable nested virtualization on the Compute Engine VMs backing the Workstations.
         * See https://cloud.google.com/workstations/docs/reference/rest/v1beta/projects.locations.workstationClusters.workstationConfigs#GceInstance.FIELDS.enable_nested_virtualization
         */
        enableNestedVirtualization?: boolean;
        /**
         * The name of a Compute Engine machine type.
         */
        machineType: string;
        /**
         * Number of instances to pool for faster workstation startup.
         */
        poolSize: number;
        /**
         * Email address of the service account that will be used on VM instances used to support this config. This service account must have permission to pull the specified container image. If not set, VMs will run without a service account, in which case the image must be publicly accessible.
         */
        serviceAccount: string;
        /**
         * A set of Compute Engine Shielded instance options.
         * Structure is documented below.
         */
        shieldedInstanceConfig: outputs.workstations.WorkstationConfigHostGceInstanceShieldedInstanceConfig;
        /**
         * Network tags to add to the Compute Engine machines backing the Workstations.
         */
        tags?: string[];
    }

    export interface WorkstationConfigHostGceInstanceAccelerator {
        /**
         * Number of accelerator cards exposed to the instance.
         */
        count: number;
        /**
         * Type of accelerator resource to attach to the instance, for example, "nvidia-tesla-p100".
         */
        type: string;
    }

    export interface WorkstationConfigHostGceInstanceConfidentialInstanceConfig {
        /**
         * Whether the instance has confidential compute enabled.
         */
        enableConfidentialCompute?: boolean;
    }

    export interface WorkstationConfigHostGceInstanceShieldedInstanceConfig {
        /**
         * Whether the instance has integrity monitoring enabled.
         */
        enableIntegrityMonitoring?: boolean;
        /**
         * Whether the instance has Secure Boot enabled.
         */
        enableSecureBoot?: boolean;
        /**
         * Whether the instance has the vTPM enabled.
         */
        enableVtpm?: boolean;
    }

    export interface WorkstationConfigIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface WorkstationConfigIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface WorkstationConfigPersistentDirectory {
        /**
         * A directory to persist across workstation sessions, backed by a Compute Engine regional persistent disk. Can only be updated if not empty during creation.
         * Structure is documented below.
         */
        gcePd: outputs.workstations.WorkstationConfigPersistentDirectoryGcePd;
        /**
         * Location of this directory in the running workstation.
         */
        mountPath: string;
    }

    export interface WorkstationConfigPersistentDirectoryGcePd {
        /**
         * The type of the persistent disk for the home directory. Defaults to `pd-standard`.
         */
        diskType: string;
        /**
         * Type of file system that the disk should be formatted with. The workstation image must support this file system type. Must be empty if `sourceSnapshot` is set. Defaults to `ext4`.
         */
        fsType: string;
        /**
         * Whether the persistent disk should be deleted when the workstation is deleted. Valid values are `DELETE` and `RETAIN`. Defaults to `DELETE`.
         * Possible values are: `DELETE`, `RETAIN`.
         */
        reclaimPolicy?: string;
        /**
         * The GB capacity of a persistent home directory for each workstation created with this configuration. Must be empty if `sourceSnapshot` is set.
         * Valid values are `10`, `50`, `100`, `200`, `500`, or `1000`. Defaults to `200`. If less than `200` GB, the `diskType` must be `pd-balanced` or `pd-ssd`.
         */
        sizeGb: number;
        /**
         * Name of the snapshot to use as the source for the disk. This can be the snapshot's `selfLink`, `id`, or a string in the format of `projects/{project}/global/snapshots/{snapshot}`. If set, `sizeGb` and `fsType` must be empty. Can only be updated if it has an existing value.
         */
        sourceSnapshot?: string;
    }

    export interface WorkstationIamBindingCondition {
        description?: string;
        expression: string;
        title: string;
    }

    export interface WorkstationIamMemberCondition {
        description?: string;
        expression: string;
        title: string;
    }

}
